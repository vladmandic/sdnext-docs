
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="SD.Next Documentation">
      
      
      
        <link rel="canonical" href="https://vladmandic.github.io/sdnext-docs/CHANGELOG/">
      
      
        <link rel="prev" href="../Debug/">
      
      
      
      <link rel="icon" href="../favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Changelog - SD.Next Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../sdnext.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Changelog - SD.Next Documentation" >
      
        <meta  property="og:description"  content="SD.Next Documentation" >
      
        <meta  property="og:image"  content="https://vladmandic.github.io/sdnext-docs/assets/images/social/CHANGELOG.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://vladmandic.github.io/sdnext-docs/CHANGELOG/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Changelog - SD.Next Documentation" >
      
        <meta  name="twitter:description"  content="SD.Next Documentation" >
      
        <meta  name="twitter:image"  content="https://vladmandic.github.io/sdnext-docs/assets/images/social/CHANGELOG.png" >
      
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#change-log-for-sdnext" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="SD.Next Documentation" class="md-header__button md-logo" aria-label="SD.Next Documentation" data-md-component="logo">
      
  <img src="../favicon.ico" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SD.Next Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Changelog
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M2 2h14v14H2zm20 6v14H8v-4h2v2h10V10h-2V8z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="teal"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/vladmandic/automatic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    SD.Next
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../Installation/" class="md-tabs__link">
          
  
  
    
  
  Install

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../Model-Support/" class="md-tabs__link">
          
  
  
    
  
  Models

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../WSL/" class="md-tabs__link">
          
  
  
    
  
  Platforms

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../Getting-Started/" class="md-tabs__link">
          
  
  
    
  
  Guides

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../Themes/" class="md-tabs__link">
          
  
  
    
  
  User Interface

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../SD-Pipeline-How-it-Works/" class="md-tabs__link">
          
  
  
    
  
  Behind-the-Scenes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../Performance-Tuning/" class="md-tabs__link">
          
  
  
    
  
  Advanced

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../API/" class="md-tabs__link">
          
  
  
    
  
  API

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../FAQ/" class="md-tabs__link">
          
  
  
    
  
  Troubleshoot

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Changelog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="SD.Next Documentation" class="md-nav__button md-logo" aria-label="SD.Next Documentation" data-md-component="logo">
      
  <img src="../favicon.ico" alt="logo">

    </a>
    SD.Next Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/vladmandic/automatic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    SD.Next
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Install
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Install
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Install
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Update
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Advanced-Install/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Install
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CLI-Arguments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CLI Arguments
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Model-Support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supported Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Specs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Gated/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gated Access
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SD-XL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableDiffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SD3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableDiffusion 3.x
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Stable-Cascade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableCascade
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FLUX/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FLUX.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Using-LCM/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LCM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../HiDream/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HiDream
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FramePack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FramePack
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../LTX/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Platforms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Platforms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../WSL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WSL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Intel-ARC/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intel-ARC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DirectML/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DirectML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../OpenVINO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OpenVINO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ONNX-Runtime/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ONNX & Olive
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZLUDA/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ZLUDA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AMD-ROCm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AMD-ROCm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MacOS-Python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MacOS-Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nVidia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    nVidia-CUDA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Getting-Started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Features/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Features Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Models-Tab/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Working with Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Prompting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompting Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parameters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Parameters Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Prompt-Enhance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Enhacement
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Networks Interface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Networks-Search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Networks Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Detailer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Detailer Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../LoRA/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Control-HowTo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Control Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Control-Settings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Control Settings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Control-Technical/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Control Technicals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Outpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Process/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Caption/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Captioning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Styles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Using Styles
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Wildcards/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Using Wildcards
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CLiP-Skip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Using CLiP skip
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Reprocess/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reprocess Latents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Stability-Matrix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stability-Matrix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../IPAdapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP Adapters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../XYZ-Grid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    XYZ Grid
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../NudeNet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NudeNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Scripts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Built-in Scripts
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    User Interface
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            User Interface
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Themes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Themes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Locale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Localization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hotkeys/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shortcuts
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Behind-the-Scenes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Behind-the-Scenes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SD-Pipeline-How-it-Works/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    How Does It Work?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SD-Training-Methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Hints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hints and Tooltips
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wiki and Docs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Extensions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom Extensions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Theme-User/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    User Themes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Backend/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backends
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Profiling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Profiling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Advanced
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Advanced
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Performance-Tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Offload/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Offloading
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Malloc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Memory Allocation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SDNQ-Quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDNQ Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Loader/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Loader
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Variable Auto-Encoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmark Results
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Nunchaku/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Nunchaku Engine
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../API/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    API Usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CLI-Tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CLI and API Examples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Troubleshoot
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Troubleshoot
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FAQ/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Frequently Asked Questions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Troubleshooting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshooting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Debug/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Debugging
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Changelog
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Changelog
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-11-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-11-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-31
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-10-31">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-10-31
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-10-31
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-17
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-10-17">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-10-17
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-10-17
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-09-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-09-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-09-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-09-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-08-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-08-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-08-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-08-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-08-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-08-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-07-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-07-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-07-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-07-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-30
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-06-30">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-06-30
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-06-30
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-16" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-02
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-06-02">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-06-02
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-06-02
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-12
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-05-12">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-05-12
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-05-12
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#highlights-for-2025-04-28" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#details-for-2025-04-28" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-04-12
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-04-12">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-12
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-12
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-04-03
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-04-03">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-03
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-03
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-28" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-18
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-02-18">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlight-for-2025-02-18" class="md-nav__link">
    <span class="md-ellipsis">
      Highlight for 2025-02-18
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-02-20" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-02-20
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-04
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-02-04">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-02-04
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-02-04
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-01-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-01-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-01-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-01-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-01-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-01-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-01-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-01-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-12-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-12-31
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-12-24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-12-24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-12-24
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sdnext-xmass-edition-whats-new" class="md-nav__link">
    <span class="md-ellipsis">
      SD.Next Xmass edition: What's new?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#details-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-12-24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Details for 2024-12-24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-models-and-integrations" class="md-nav__link">
    <span class="md-ellipsis">
      New models and integrations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-models" class="md-nav__link">
    <span class="md-ellipsis">
      Video models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ui-and-workflow-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      UI and workflow improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#updates" class="md-nav__link">
    <span class="md-ellipsis">
      Updates
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-11-21
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-11-21">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-11-21
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-11-21">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-integrations" class="md-nav__link">
    <span class="md-ellipsis">
      New integrations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#workflow-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow Improvements
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-11-21
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-11-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-11-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-10-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-10-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-10-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-10-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-10-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-10-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-10-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-10-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#workflow-highlights-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow highlights for 2024-10-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-models-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      New models for 2024-10-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-else-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      What else for 2024-10-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-10-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-09-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-09-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-09-13
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-09-13
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-08-31
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-08-31">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-08-31
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-08-31
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-07-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-07-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-06-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-improvements-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Model Improvements for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-improvements-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      General Improvements for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes for 2024-06-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-else-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      What else 2024-06-13?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Full Changelog for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Full Changelog for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-models-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      New Models for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-functionality-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      New Functionality for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvements-fixes-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Improvements Fixes 2024-06-13
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes 2024-06-13
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-05-28
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-05-28">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-05-28
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-05-28">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-built-in-features" class="md-nav__link">
    <span class="md-ellipsis">
      New built-in features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-models" class="md-nav__link">
    <span class="md-ellipsis">
      New models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog for 2024-05-28
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-03-19
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-03-19">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-03-19
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Full Changelog 2024-03-19
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-02-22
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-02-22">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-02-22
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog for 2024-02-22
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-02-07
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-02-07">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-02-07
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog 2024-02-07
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-12-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2023-12-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2023-12-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog 2023-12-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-12-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-12-04
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-11-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-11-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-11-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-11-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-10-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-09-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-09-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-09-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-11" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-11
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-10" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-10
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-12
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-15
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-11" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-11
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-04
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-27" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-27
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-25" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-25
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-24" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-24
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-22" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-22
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-19" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-19
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-15
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-12
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-11-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-11-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-31
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-10-31">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-10-31
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-10-31" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-10-31
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-10-17
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-10-17">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-10-17
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-10-17
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-09-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-09-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-09-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-09-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-09-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-08-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-08-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-08-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-08-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-08-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-08-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-08-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-07-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-07-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-07-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-07-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-07-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-30
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-06-30">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-06-30
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-06-30
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-16" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-06-02
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-06-02">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-06-02
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-06-02
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-12
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-05-12">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-05-12
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-05-12" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-05-12
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-05-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-05-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#highlights-for-2025-04-28" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#details-for-2025-04-28" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-04-12
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-04-12">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-12
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-12
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-04-03
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-04-03">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-04-03
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-04-03" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-04-03
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-28" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-28
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-18
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-02-18">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlight-for-2025-02-18" class="md-nav__link">
    <span class="md-ellipsis">
      Highlight for 2025-02-18
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-02-20" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-02-20
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-02-04
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-02-04">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-02-04
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-02-04" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-02-04
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-01-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-01-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-01-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-01-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-01-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2025-01-15
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2025-01-15">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2025-01-15
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2025-01-15" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2025-01-15
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-12-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-12-31
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-12-24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-12-24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-12-24
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sdnext-xmass-edition-whats-new" class="md-nav__link">
    <span class="md-ellipsis">
      SD.Next Xmass edition: What's new?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#details-for-2024-12-24" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-12-24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Details for 2024-12-24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-models-and-integrations" class="md-nav__link">
    <span class="md-ellipsis">
      New models and integrations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-models" class="md-nav__link">
    <span class="md-ellipsis">
      Video models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ui-and-workflow-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      UI and workflow improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#updates" class="md-nav__link">
    <span class="md-ellipsis">
      Updates
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-11-21
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-11-21">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-11-21
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-11-21">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-integrations" class="md-nav__link">
    <span class="md-ellipsis">
      New integrations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#workflow-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow Improvements
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-11-21" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-11-21
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-11-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-11-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-10-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-10-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-10-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-10-29" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-10-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-10-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-10-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-10-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-10-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#workflow-highlights-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow highlights for 2024-10-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-models-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      New models for 2024-10-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-else-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      What else for 2024-10-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-10-23" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-10-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-09-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-09-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-09-13
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-09-13
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-08-31
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-08-31">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-08-31
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-2024-08-31" class="md-nav__link">
    <span class="md-ellipsis">
      Details for 2024-08-31
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-07-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-07-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-23
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-06-23">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-improvements-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Model Improvements for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-improvements-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      General Improvements for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes for 2024-06-23
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-else-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      What else 2024-06-13?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Full Changelog for 2024-06-13
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Full Changelog for 2024-06-13">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-models-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      New Models for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-functionality-for-2024-06-23" class="md-nav__link">
    <span class="md-ellipsis">
      New Functionality for 2024-06-23
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvements-fixes-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Improvements Fixes 2024-06-13
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixes-2024-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Fixes 2024-06-13
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-06-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-05-28
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-05-28">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights for 2024-05-28
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Highlights for 2024-05-28">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#new-built-in-features" class="md-nav__link">
    <span class="md-ellipsis">
      New built-in features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-models" class="md-nav__link">
    <span class="md-ellipsis">
      New models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-05-28" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog for 2024-05-28
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-03-19
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-03-19">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-03-19
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2024-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      Full Changelog 2024-03-19
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-02-22
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-02-22">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-02-22
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-for-2024-02-22" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog for 2024-02-22
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2024-02-07
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2024-02-07">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2024-02-07
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2024-02-07" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog 2024-02-07
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-12-29
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Update for 2023-12-29">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#highlights-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Highlights 2023-12-29
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-changelog-2023-12-29" class="md-nav__link">
    <span class="md-ellipsis">
      Full ChangeLog 2023-12-29
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-12-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-12-04
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-11-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-11-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-11-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-11-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-10-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-10-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-09-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-09-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-09-06" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-09-06
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-11" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-11
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-08-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-08-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-10" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-10
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-07-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-07-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-12
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-05" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-05
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-06-02" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-06-02
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-30" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-30
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-26" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-26
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-15
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-13" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-13
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-11" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-11
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-08" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-08
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-04" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-04
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-05-01" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-05-01
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-27" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-27
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-25" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-25
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-24" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-24
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-23" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-23
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-22" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-22
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-20" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-20
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-19" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-19
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-18" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-18
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-17" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-17
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-15" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-15
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-14
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-for-2023-04-12" class="md-nav__link">
    <span class="md-ellipsis">
      Update for 2023-04-12
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="change-log-for-sdnext">Change Log for SD.Next</h1>
<h2 id="update-for-2025-11-02">Update for 2025-11-02</h2>
<ul>
<li><strong>Features</strong></li>
<li><strong>detailer</strong>
    optional include detection image to output results<br />
    optional sort detection objects left-to-right for improved prompt consistency<br />
    enable multi-subject and multi-model prompts  </li>
<li>add inline wildcards using curly braces syntax  </li>
<li>add setting to control <code>cudnn</code> enable/disable  </li>
<li><strong>Fixes</strong></li>
<li>fix: better handling of detailer settings, thanks @awsr</li>
<li>fix: cleanup <code>--optional</code> installer  </li>
<li>fix: guard against multi-controlnet in hires  </li>
<li>fix: update diffusers  </li>
<li>fix: inpaint handling</li>
<li>fix: model type detection</li>
<li>fix: version detection when cloned with <code>.git</code> suffix, thanks @awsr</li>
<li>fix: init <code>sdnq</code> on video model load</li>
<li>fix: add vae scale override for chrono</li>
<li>fix: add tracing to model detection</li>
<li>ui: fix full-screen image viewer buttons with non-standard ui theme</li>
<li>ui: control tab show override section</li>
</ul>
<h2 id="update-for-2025-10-31">Update for 2025-10-31</h2>
<h3 id="highlights-for-2025-10-31">Highlights for 2025-10-31</h3>
<p>Less than 2 weeks since last release, here's a service-pack style update with a lot of fixes and improvements:
- Reorganization of <strong>Reference Models</strong> into <em>Base, Quantized, Distilled and Community</em> sections for easier navigation<br />
  and introduction of optimized <strong>pre-quantized</strong> variants for many popular models - use this as your quick start!<br />
- New models:<br />
<strong>HunyuanImage 2.1</strong> capable of 2K images natively, <strong>HunyuanImage 3.0</strong> large unified multimodal autoregressive model,<br />
<strong>ChronoEdit</strong> that re-purposes temporal consistency of generation for image editing<br />
<strong>Pony 7</strong> based on AuraFlow architecture, <strong>Kandinsky 5</strong> 10s video models<br />
- New <strong>offline mode</strong> to use previously downloaded models without internet connection<br />
- Optimizations to <strong>WAN-2.2</strong> given its popularity<br />
  plus addition of native <strong>VAE Upscaler</strong> and optimized <strong>pre-quantized</strong> variants<br />
- New SOTA model loader using <strong>Run:ai streamer</strong><br />
- Updates to <code>rocm</code> and <code>xpu</code> backends<br />
- Fixes, fixes, fixes... too many to list here!  </p>
<p><img alt="Screenshot" src="https://github.com/user-attachments/assets/d6119a63-6ee5-4597-95f6-29ed0701d3b5" /></p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a> | <a href="https://github.com/sponsors/vladmandic">Sponsor</a>  </p>
<h3 id="details-for-2025-10-31">Details for 2025-10-31</h3>
<ul>
<li><strong>Reference</strong> networks section is now split into actual <em>Base</em> models plus:  </li>
<li><strong>Quantized</strong>: pre-quantized variants of the base models using SDNQ-SVD quantization for optimal quality and smallest possible resource usage<br />
    examples: <em>FLUX.1-Dev/Krea/Kontext/Schnell, Qwen-Image/Edit/2509, Chroma1-HD, WAN-2.2-A44B, etc.</em><br />
<em>note</em>: pre-quantized <em>WAN-2.2-14B</em> is also available in video models and runs with only 12GB VRAM!  </li>
<li><strong>Distilled</strong>: distilled variants of base models<br />
    examples: <em>Turbo, Lightning, Lite, SRPO, Distill, Pruning, etc.</em>  </li>
<li><strong>Community</strong>: community highlights<br />
    examples: <em>Tempest, Juggernaut, Illustrious, Pony, NoobAI, etc.</em><br />
    and all reference models have new preview images, thanks @liutyi  </li>
<li><strong>Models Reference</strong>  </li>
<li><a href="https://huggingface.co/tencent/HunyuanImage-2.1">Tencent HunyuanImage 2.1</a> in <em>full</em>, <em>distilled</em> and <em>refiner</em> variants<br />
<em>HunyuanImage-2.1</em> is a large (51GB) T2I model capable of natively generating 2K images and uses Qwen2.5 + T5 text-encoders and 32x VAE  </li>
<li><a href="https://huggingface.co/tencent/HunyuanImage-3.0">Tencent HunyuanImage 3.0</a> in <a href="https://huggingface.co/Disty0/HunyuanImage3-SDNQ-uint4-svd-r32">pre-quant</a> only variant due to massive size<br />
<em>HunyuanImage 3.0</em> is very large at 47GB pre-quantized (oherwise its 157GB) that unifies multimodal understanding and generation within an autoregressive framework  </li>
<li><a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers">nVidia ChronoEdit</a><br />
<em>ChronoEdit</em> is a 14B image editing model based on <em>WAN</em><br />
    this model reframes image editing as a video generation task, using input and edited images as start/end frames to leverage pretrained video models with temporal consistency<br />
    to extend temporal consistency for image editing, set <em>settings -&gt; model options -&gt; chrono temporal steps</em> to desired number of temporaly reasoning steps  </li>
<li><a href="https://huggingface.co/ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers'">Kandinsky 5 Lite 10s</a> in <em>SFT, CFG-distilled and Steps-distilled</em> variants<br />
    second series of models in <em>Kandinsky5</em> series is T2V model optimized for 10sec videos and uses Qwen2.5 text encoder  </li>
<li><a href="https://huggingface.co/purplesmartai/pony-v7-base">Pony 7</a><br />
    Pony 7 steps in a different direction from previous Pony models and is based on AuraFlow architecture and UMT5 encoder  </li>
<li><strong>Models Auxiliary</strong>  </li>
<li><a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct">Qwen 3-VL</a> VLM for interrogate and prompt enhance, thanks @CalamitousFelicitousness<br />
    this includes <em>2B, 4B and 8B</em> variants  </li>
<li><a href="https://huggingface.co/spacepxl/Wan2.1-VAE-upscale2x">WAN Asymettric Upscale</a><br />
    available as general purpose upscaler that can be used during standard workflow or process tab<br />
    available as VAE for compatible video models: <em>WAN-2.x-14B, SkyReels-v2</em> models  </li>
<li><a href="https://huggingface.co/apple/DepthPro">Apple DepthPro</a> controlnet processor, thanks @nolbert82  </li>
<li><a href="https://huggingface.co/neuralvfx/LibreFlux-ControlNet">LibreFlux controlnet</a> segmentation controlnet for FLUX.1  </li>
<li><strong>Features</strong></li>
<li><strong>offline mode</strong>: enable in <em>settings -&gt; hugginface</em><br />
    enables fully offline mode where previously downloaded models can be used as-is<br />
<em>note</em>: must be enabled only after all packages have been installed and model has been run online at least once  </li>
<li><strong>model load</strong>: SOTA method using nVidia's <a href="https://github.com/run-ai/runai-model-streamer">Run:ai streamer</a><br />
    enable in <em>settings -&gt; model options -&gt; runai streamer</em><br />
    applies to <em>diffusers, transformers and sdnq</em> loaders, note this is linux-only feature<br />
<em>experimental</em> but shows significant model load speedups, 20-40% depending on model and hardware  </li>
<li><strong>Backend</strong></li>
<li>switch to <code>torch==2.9</code> for <em>ipex, rocm and openvino</em>  </li>
<li>switch to <code>rocm==7.0</code> for nightlies  </li>
<li>log <code>triton</code> availability on startup  </li>
<li>add <code>xpu</code> stats in gpu monitor  </li>
<li><strong>Other</strong></li>
<li>improved <strong>SDNQ SVD</strong> and low-bit matmul performance  </li>
<li>reduce RAM usage on model load using <strong>SDNQ SVD</strong></li>
<li>change default <strong>schedulers</strong> for sdxl  </li>
<li>warn on <code>python==3.9</code> end-of-life and <code>python==3.10</code> not actively supported  </li>
<li><strong>scheduler</strong> add base and max shift parameters for flow-matching samplers  </li>
<li>enhance <code>--optional</code> flag to pre-install optional packages  </li>
<li>add <code>[lora]</code> to recognized filename patterns  </li>
<li>when using <strong>shared-t5</strong> <em>(default)</em>, it will load standard or pre-quant depending on model  </li>
<li>enhanced LoRA support for <strong>Wan-2.2-14B</strong>  </li>
<li>log available attention mechanisms on startup  </li>
<li>support for switching back-and-forth <strong>t2i</strong> and <strong>t2v</strong> for <em>wan-2.x</em> models  </li>
<li>control <code>api</code> cache controlnets  </li>
<li>additional model modules <strong>deduplication</strong> for both normal and pre-quant models: <em>umt5, qwen25-vl</em>  </li>
<li><strong>Fixes</strong></li>
<li>startup error with <code>--profile</code> enabled if using <code>--skip</code>  </li>
<li>restore orig init image for each batch sequence  </li>
<li>fix modernui hints layout  </li>
<li>fix <code>wan-2.2-a14b</code> stage selection  </li>
<li>fix <code>wan-2.2-5b</code> vae decode  </li>
<li>disabling live preview should not disable progress updates  </li>
<li>video tab create <code>params.txt</code> with metadata  </li>
<li>fix full-screen image-viewer toolbar actions with control tab  </li>
<li>improve filename sanitization  </li>
<li>lora auto-detect low/high stage if not specified  </li>
<li>lora disable fuse on partially applied network  </li>
<li>fix networks display with extended characters, thanks @awsr  </li>
<li>installer handle different <code>opencv</code> package variants  </li>
<li>fix using pre-quantized shared-t5  </li>
<li>fix <code>wan-2.2-14b-vace</code> single-stage exectution  </li>
<li>fix <code>wan-2.2-5b</code> tiled vae decode  </li>
<li>fix <code>controlnet</code> loading with quantization  </li>
<li>video use pre-quantized text-encoder if selected model is pre-quantized  </li>
<li>handle sparse <code>controlnet</code> models  </li>
<li>catch <code>xet</code> warnings  </li>
<li>avoid unnecessary pipe variant switching  </li>
<li>validate pipelines on import  </li>
<li>fix <code>nudenet</code> process tab operations  </li>
<li><code>controlnet</code> input validation  </li>
<li>log metadata keys that cannot be applied  </li>
<li>fix <code>framepack</code> with image input  </li>
</ul>
<h2 id="update-for-2025-10-18">Update for 2025-10-18</h2>
<ul>
<li><strong>Models</strong>
  <a href="https://huggingface.co/ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers'">Kandinsky 5 Lite</a> in <em>SFT, CFG-distilled and Steps-distilled</em> variants<br />
  first model in Kandinsky5 series is T2V model optimized for 5sec videos and uses Qwen2.5 text encoder  </li>
<li><strong>Fixes</strong></li>
<li>ROCm-on-Windows additional checks  </li>
<li>SDNQ-SVD fallback on incompatible layers  </li>
<li>Huggingface model download  </li>
<li>Video implement dynamic and manual sampler shift  </li>
<li>Fix interrupt batch processing  </li>
<li>Delay import of control processors until used  </li>
<li>Fix tiny VAE with batched results  </li>
<li>Fix CFG scale not added to metadata and set valid range to &gt;=1.0  </li>
<li><strong>Other</strong></li>
<li>Optimized Video tab layout  </li>
<li>Video enable VAE slicing and framewise decoding when possible  </li>
<li>Detect and log <code>flash-attn</code> and <code>sageattention</code> if installed  </li>
<li>Remove unused UI settings  </li>
</ul>
<h2 id="update-for-2025-10-17">Update for 2025-10-17</h2>
<h3 id="highlights-for-2025-10-17">Highlights for 2025-10-17</h3>
<p>It's been a month since the last release and number of changes is yet again massive with over 300 commits!<br />
Highlight are:<br />
- <strong>Torch</strong>: ROCm on Windows for AMD GPUs<br />
  if you have a compatible GPU, performance gains are significant!<br />
- <strong>Models</strong>:<br />
  a lot of new stuff with <strong>Qwen-Image-Edit</strong> including multi-image edits and distilled variants,<br />
  new <strong>Flux</strong>, <strong>WAN</strong>, <strong>LTX</strong>, <strong>HiDream</strong> variants, expanded <strong>Nunchaku</strong> support and new SOTA upscaler with <strong>SeedVR2</strong><br />
  plus improved video support in general, including new methods of video encoding<br />
- <strong>Quantization</strong>:<br />
  new <strong>SVD</strong>-style quantization using SDNQ offers almost zero-loss even with <strong>4bit</strong> quantization<br />
  and now you can also test your favorite quantization on-the-fly and then save/load model for future use<br />
- Other: support for <strong>Huggingface</strong> mirrors, changes to installer to prevent unwanted <code>torch-cpu</code> operations, improved VAE previews, etc.  </p>
<p><img alt="Screenshot" src="https://github.com/user-attachments/assets/d6119a63-6ee5-4597-95f6-29ed0701d3b5" /></p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a> | <a href="https://github.com/sponsors/vladmandic">Sponsor</a>  </p>
<h3 id="details-for-2025-10-17">Details for 2025-10-17</h3>
<ul>
<li><strong>Models</strong></li>
<li><a href="https://huggingface.co/alibaba-pai/Wan2.2-VACE-Fun-A14B">WAN 2.2 14B VACE</a><br />
    available for <em>text-to-image</em> and <em>text-to-video</em> and <em>image-to-video</em> workflows  </li>
<li><a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Qwen Image Edit 2509</a> and <a href="https://huggingface.co/nunchaku-tech/nunchaku-qwen-image-edit-2509">Nunchaku Qwen Image Edit 2509</a><br />
    updated version of Qwen Image Edit with improved image consistency  </li>
<li><a href="https://huggingface.co/OPPOer/Qwen-Image-Pruning">Qwen Image Pruning</a> and <a href="https://huggingface.co/OPPOer/Qwen-Image-Edit-Pruning">Qwen Image Edit Pruning</a><br />
    pruned versions of Qwen with 13B params instead of 20B, with some quality tradeoff  </li>
<li><a href="https://huggingface.co/tencent/SRPO">Tencent FLUX.1 Dev SRPO</a><br />
    SRPO is trained by Tencent with specific technique: directly aligning the full diffusion trajectory with fine-grained human preference  </li>
<li><a href="https://huggingface.co/nunchaku-tech/nunchaku-sdxl">Nunchaku SDXL</a> and <a href="https://huggingface.co/nunchaku-tech/nunchaku-sdxl-turbo">Nunchaku SDXL Turbo</a><br />
    impact of nunchaku engine on unet-based model such as sdxl is much less than on a dit-based models, but its still significantly faster than baseline<br />
    note that nunchaku optimized and pre-quantized unet is replacement for base unet, so its only applicable to base models, not any of fine-tunes<br />
<em>how to use</em>: enable nunchaku in settings -&gt; quantization and then load either sdxl-base or sdxl-base-turbo reference models  </li>
<li><a href="https://huggingface.co/HiDream-ai/HiDream-E1-1">HiDream E1.1</a><br />
    updated version of HiDream-E1 image editing model  </li>
<li><a href="https://huggingface.co/Lightricks/LTX-Video-0.9.8-13B-distilled">LTXVideo 0.9.8</a><br />
    updated version of LTXVideo t2v/i2iv model  </li>
<li><a href="https://iceclear.github.io/projects/seedvr/">SeedVR2</a><br />
    originally designed for video restoration, seedvr works great for image detailing and upscaling!<br />
    available in 3B, 7B and 7B-sharp variants, use as any other upscaler!<br />
    note: seedvr is a very large model (6.4GB and 16GB respectively) and not designed for lower-end hardware, quantization is highly recommended<br />
    note: seedvr is highly sensitive to its cfg scale, set in <em>settings -&gt; postprocessing</em><br />
    lower values will result in smoother output while higher values add details  </li>
<li><a href="https://x-omni-team.github.io/">X-Omni SFT</a><br />
<em>experimental</em>: X-omni is a transformer-only discrete auto-regressive image generative model trained with reinforcement learning  </li>
<li><strong>Features</strong></li>
<li><strong>Model save</strong>: ability to save currently loaded model as a new standalone model<br />
    why? SD.Next always prefers to start with full model and quantize on-demand during load<br />
    however, when you find your exact preferred quantization settings that work well for you,<br />
    saving such model as a new model allows for faster loads and reduced disk space usage<br />
    so its best of both worlds: you can experiment and test different quantization methods and once you find the one that works for you, save it as a new model<br />
    saved models appear in network tab as normal models and can be loaded as such<br />
    available in <em>models</em> tab  </li>
<li><a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Qwen Image-Edit</a> multi-image editing
    requires qwen-image-edit-2509 or its variant as multi-image edits are not available in original qwen-image<br />
    in ui control tab: inputs -&gt; separate init image<br />
    add image for <em>input media</em> and <em>control media</em><br />
    can be </li>
<li><a href="https://github.com/vipshop/cache-dit">Cache-DiT</a><br />
    cache-dit is a unified, flexible and training-free cache acceleration framework<br />
    compatible with many dit-based models such as FLUX.1, Qwen, HunyuanImage, Wan2.2, Chroma, etc.<br />
    enable in <em>settings -&gt; pipeline modifiers -&gt; cache-dit</em>  </li>
<li><a href="https://nunchaku.tech/docs/nunchaku/python_api/nunchaku.pipeline.pipeline_flux_pulid.html">Nunchaku Flux.1 PulID</a><br />
    automatically enabled if loaded model is FLUX.1 with Nunchaku engine enabled and when PulID script is enabled  </li>
<li><strong>Huggingface mirror</strong> in <em>settings -&gt; huggingface</em><br />
    if you're working from location with limited access to huggingface, you can now specify a mirror site<br />
    for example enter, <code>https://hf-mirror.com</code>  </li>
<li><strong>Compute</strong></li>
<li><strong>ROCm</strong> for Windows<br />
    support for both official torch preview release of <code>torch-rocm</code> for windows and <strong>TheRock</strong> unofficial <code>torch-rocm</code> builds for windows<br />
    note that rocm for windows is still in preview and has limited gpu support, please check rocm docs for details  </li>
<li><strong>DirectML</strong> warn as <em>end-of-life</em><br />
<code>torch-directml</code> received no updates in over 1 year and its currently superseded by <code>rocm</code> or <code>zluda</code>  </li>
<li>command line params <code>--use-zluda</code> and <code>--use-rocm</code> will attempt desired operation or fail if not possible<br />
    previously sdnext was performing a fallback to <code>torch-cpu</code> which is not desired  </li>
<li><strong>installer</strong> if <code>--use-cuda</code> or <code>--use-rocm</code> are specified and <code>torch-cpu</code> is installed, installer will attempt to reinstall correct torch package  </li>
<li><strong>installer</strong> warn if <em>cuda</em> or <em>rocm</em> are available and <code>torch-cpu</code> is installed  </li>
<li>support for <code>torch==2.10-nightly</code> with <code>cuda==13.0</code>  </li>
<li><strong>Extensions</strong>  </li>
<li><a href="https://github.com/SipherAGI/sd-webui-agent-scheduler">Agent-Scheduler</a><br />
    was a high-value built-in extension, but it has not been maintained for 1.5 years<br />
    it also does not work with control and video tabs which are the core of sdnext nowadays<br />
    so it has been removed from built-in extensions: manual installation is still possible  </li>
<li><a href="https://github.com/castorini/daam">DAAM: Diffusion Attentive Attribution Maps</a><br />
    create heatmap visualizations of which parts of the prompt influenced which parts of the image<br />
    available in scripts for sdxl text-to-image workflows  </li>
<li><strong>Offloading</strong></li>
<li>improve offloading for pipelines with multiple stages such as <em>wan-2.2-14b</em>  </li>
<li>add timers to measure onload/offload times during generate  </li>
<li>experimental offloading using <code>torch.streams</code><br />
    enable in settings -&gt; model offloading  </li>
<li>new feature to specify which models types not to offload<br />
    in <em>settings -&gt; model offloading -&gt; model types not to offload</em>  </li>
<li><strong>UI</strong></li>
<li><strong>connection monitor</strong><br />
    main logo in top-left corner now indicates server connection status and hovering over it shows connection details  </li>
<li>separate guidance and detail sections  </li>
<li>networks ability to filter lora by base model version  </li>
<li>add interrogate button to input images  </li>
<li>disable spellchecks on all text inputs  </li>
<li><strong>SDNQ</strong></li>
<li>add <code>SVDQuant</code> quantization method support  </li>
<li>make sdnq scales compatible with balanced offload  </li>
<li>add int8 <code>matmul</code> support for RDNA2 GPUs via triton  </li>
<li>improve int8 <code>matmul</code> performance on Intel GPUs  </li>
<li><strong>Other</strong></li>
<li>server will note when restart is recommended due to package updates  </li>
<li><strong>interrupt</strong> will now show last known preview image<br />
<em>keep incomplete</em> setting is now <em>save interrupted</em>  </li>
<li><strong>logging</strong> enable <code>debug</code>, <code>docs</code> and <code>api-docs</code> by default  </li>
<li><strong>logging</strong> add detailed ram/vram utilization info to log<br />
    logging frequency can be specified using <code>--monitor x</code> command line param, where x is number of seconds  </li>
<li><strong>ipex</strong> simplify internal implementation  </li>
<li>refactor to use new libraries  </li>
<li>styles and wildcards now use same seed as main generate for reproducible results  </li>
<li><strong>api</strong> new endpoint POST <code>/sdapi/v1/civitai</code> to trigger civitai models metadata update<br />
    accepts optional <code>page</code> parameter to search specific networks page  </li>
<li><strong>reference models</strong> additional example images, thanks @liutyi  </li>
<li><strong>reference models</strong> add model size and release date, thanks @alerikaisattera  </li>
<li><strong>video</strong> support for configurable multi-stage models such as WAN-2.2-14B  </li>
<li><strong>video</strong> new LTX model selection  </li>
<li>replace <code>pynvml</code> with <code>nvidia-ml-py</code> for gpu monitoring  </li>
<li>update <strong>loopback</strong> script with radon seed option, thanks @rabanti  </li>
<li><strong>vae</strong> slicing enable for <em>lowvram/medvram</em>, tiling for <em>lowvram</em>, both disabled otherwise  </li>
<li><strong>attention</strong> remove split-attention and add explicitly attention slicing enable/disable option<br />
    enable in <em>settings -&gt; compute settings</em><br />
    can be combined with sdp, enabling may improve stability when used on iGPU or shared memory systems  </li>
<li><strong>nunchaku</strong> update to <code>1.0.1</code> and enhance installer  </li>
<li><strong>xyz-grid</strong> add guidance section  </li>
<li><strong>preview</strong> implement configurable layers for WAN, Qwen, HV  </li>
<li>update swagger <code>/docs</code> endpoint style  </li>
<li>add <code>[epoch]</code> to filename template  </li>
<li>starting <code>[seq]</code> for filename template is now higher of largest previous sequence or number of files in folder  </li>
<li><strong>Video</strong></li>
<li>use shared <strong>T5</strong> text encoder for video models when possible  </li>
<li>use shared <strong>LLama</strong> text encoder for video models when possible  </li>
<li>unified video save code across all video models<br />
    also avoids creation of temporary files for each frame unless user wants to save them  </li>
<li>unified prompt enhance code across all video models  </li>
<li>add job state tracking for video generation  </li>
<li>fix quantization not being applied on load for some models  </li>
<li>improve offloading for <strong>ltx</strong> and <strong>wan</strong>  </li>
<li>fix model selection in <strong>ltx</strong> tab  </li>
<li><strong>Experimental</strong></li>
<li><code>new</code> command line flag enables new <code>pydantic</code> and <code>albumentations</code> packages  </li>
<li><strong>modular pipelines</strong>: enable in <em>settings -&gt; model options</em><br />
    only compatible with some pipelines, invalidates preview generation  </li>
<li><strong>modular guiders</strong>: automatically used for compatible pipelines when <em>modular pipelines</em> is enabled<br />
    allows for using many different guidance methods:<br />
<em>CFG, CFGZero, PAG, APG, SLG, SEG, TCFG, FDG</em>  </li>
<li><strong>Wiki</strong></li>
<li>updates to <em>AMD-ROCm, ZLUDA, LoRA, DirectML, SDNQ, Quantization, Prompting, LoRA</em> pages  </li>
<li>new <em>Stability-Matrix</em> page  </li>
<li><strong>Fixes</strong></li>
<li><strong>Microsoft Florence 2</strong> both base and large variants<br />
<em>note</em> this will trigger download of the new variant of the model, feel free to delete older variant in <code>huggingface</code> folder  </li>
<li><strong>MiaoshouAI PromptGen</strong> 1.5/2.0 in both base and large variants  </li>
<li>fix prompt scheduling, thanks @nolbert82  </li>
<li>ui: fix image metadata display when switching selected image in control tab  </li>
<li>framepack: add explicit hf-login before framepack load  </li>
<li>framepack: patch solver for unsupported gpus  </li>
<li>benchmark: remove forced sampler from system info benchmark  </li>
<li>xyz-grid: fix xyz grid with random seeds  </li>
<li>reference: fix download for sd15/sdxl reference models  </li>
<li>fix checks in init/mask image decode  </li>
<li>fix hf token with extra chars  </li>
<li>image viewer refocus on gallery after returning from full screen mode  </li>
<li>fix attention guidance metadata save/restore  </li>
<li>vae preview add explicity cuda.sync  </li>
</ul>
<h2 id="update-for-2025-09-15">Update for 2025-09-15</h2>
<h3 id="highlights-for-2025-09-15">Highlights for 2025-09-15</h3>
<p><em>What's new</em>? Big one is that we're (<em>finally</em>) switching the default UI to <strong>ModernUI</strong>, for both desktop and mobile use!<br />
<strong>StandardUI</strong> is still available and can be selected in settings, but ModernUI is now the default for new installs  </p>
<p><em>What's else</em>? <strong>Chroma</strong> is in its final form, there are several new <strong>Qwen-Image</strong> variants and <strong>Nunchaku</strong> hit version 1.0!<br />
Also, there are quite a few offloading improvements and many quality-of-life changes to UI and overall workflows<br />
And check out new <strong>history</strong> tab in the right panel, it now shows visualization of entire processing timeline!  </p>
<p><img alt="Screenshot" src="https://github.com/user-attachments/assets/d6119a63-6ee5-4597-95f6-29ed0701d3b5" /></p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a> | <a href="https://github.com/sponsors/vladmandic">Sponsor</a>  </p>
<h3 id="details-for-2025-09-15">Details for 2025-09-15</h3>
<ul>
<li><strong>Models</strong></li>
<li><strong>Chroma</strong> final versions: <a href="https://huggingface.co/lodestones/Chroma1-HD">Chroma1-HD</a>, <a href="https://huggingface.co/lodestones/Chroma1-Base">Chroma1-Base</a> and <a href="https://huggingface.co/lodestones/Chroma1-Flash">Chroma1-Flash</a>  </li>
<li><strong>Qwen-Image</strong> <a href="https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union">InstantX ControlNet Union</a> support<br />
<em>note</em> qwen-image is already a very large model and controlnet adds 3.5GB on top of that so quantization and offloading are highly recommended!  </li>
<li><a href="https://huggingface.co/vladmandic/Qwen-Lightning-Edit">Qwen-Lightning-Edit</a> and <a href="https://huggingface.co/SahilCarterr/Qwen-Image-Distill-Full">Qwen-Image-Distill</a> variants  </li>
<li><strong>Nunchaku</strong> variants of <a href="https://huggingface.co/nunchaku-tech/nunchaku-qwen-image">Qwen-Image-Lightning</a>, <a href="https://huggingface.co/nunchaku-tech/nunchaku-qwen-image-edit">Qwen-Image-Edit</a>, <a href="https://huggingface.co/nunchaku-tech/nunchaku-qwen-image-edit">Nunchaku-Qwen-Image-Edit-Lightning</a></li>
<li><strong>Nunchaku</strong> variant of <a href="https://huggingface.co/nunchaku-tech/nunchaku-flux.1-krea-dev">Flux.1-Krea-Dev</a><br />
    if you have a compatible nVidia GPU, Nunchaku is the fastest quantization &amp; inference engine  </li>
<li><a href="https://huggingface.co/Tencent-Hunyuan/HYDiT-ControlNet-v1.2">HunyuanDiT ControlNet</a> Canny, Depth, Pose  </li>
<li><a href="https://huggingface.co/KBlueLeaf/HDM-xut-340M-anime">KBlueLeaf/HDM-xut-340M-anime</a><br />
    highly experimental: HDM <em>Home-made-Diffusion-Model</em> is a project to investigate specialized training recipe/scheme<br />
    for pre-training T2I model at home based on super-light architecture<br />
<em>requires</em>: generator=cpu, dtype=float16, offload=none, both positive and negative prompts are required and must be long &amp; detailed  </li>
<li><a href="https://huggingface.co/apple/FastVLM-0.5B">Apple FastVLM</a> in 0.5B, 1.5B and 7B variants<br />
    available in captioning tab  </li>
<li>updated <a href="https://vladmandic.github.io/sd-samples/compare.html">SD.Next Model Samples Gallery</a>  </li>
<li><strong>UI</strong></li>
<li>default to <strong>ModernUI</strong><br />
    standard ui is still available via <em>settings -&gt; user interface -&gt; theme type</em>  </li>
<li>mobile-friendly!  </li>
<li>new <strong>History</strong> section in the right panel<br />
    shows detailed job history plus timeline of the execution  </li>
<li>make hints touch-friendly: hold touch to display hint  </li>
<li>improved image scaling in img2img and control interfaces  </li>
<li>add base model type to networks display, thanks @Artheriax  </li>
<li>additional hints to ui, thanks @Artheriax  </li>
<li>add video support to gallery, thanks @CalamitousFelicitousness  </li>
<li>additional artwork for reference models in networks, thanks @liutyi  </li>
<li>improve ui hints display  </li>
<li>restyled all toolbuttons to be modernui native  </li>
<li>reordered system settings  </li>
<li>dynamic direction of dropdowns  </li>
<li>improve process tab layout  </li>
<li>improve detection of active tab  </li>
<li>configurable horizontal vs vertical panel layout<br />
    in settings -&gt; user interface -&gt; panel min width<br />
<em>example</em>: if panel width is less than specified value, layout switches to vertical  </li>
<li>configurable grid images size<br />
    in <em>settings -&gt; user interface -&gt; grid image size</em>  </li>
<li>gallery now includes reference model images  </li>
<li>reference models now include indicator if they are <em>ready</em> or <em>need download</em></li>
<li><strong>Offloading</strong></li>
<li><strong>balanced</strong><ul>
<li>enable offload during pre-forward by default  </li>
<li>improve offloading of models with multiple dits  </li>
<li>improve offloading of models with impliciy vae processing  </li>
<li>improve offloading of models with controlnet  </li>
<li>more aggressive offloading of controlnet with lowvram flag  </li>
</ul>
</li>
<li><strong>group</strong><ul>
<li>new offloading method, using <em>type=leaf</em> works on a similar level as sequential offloading<br />
  and can present significant savings on low-vram gpus, but comes at the higher performance cost  </li>
</ul>
</li>
<li><strong>Quantization</strong></li>
<li>option to specify models types not to quantize: <em>settings -&gt; quantization</em><br />
    allows for having quantization enabled, but skipping specific model types that do not need it<br />
<em>example</em>: <code>sd, sdxl</code>  </li>
<li><strong>sdnq</strong><ul>
<li>add quantized matmul support for all quantization types and group sizes  </li>
<li>improve the performance of low bit quants  </li>
</ul>
</li>
<li><strong>nunchaku</strong>: update to <code>nunchaku==1.0.0</code><br />
<em>note</em>: nunchaku updated the repo which will trigger re-download of nunchaku models when first used<br />
    nunchaku is currently available for: <em>Flux.1 Dev/Schnell/Kontext/Krea/Depth/Fill</em>, <em>Qwen-Image/Qwen-Lightning</em>, <em>SANA-1.6B</em>  </li>
<li><strong>tensorrt</strong>: new quantization engine from nvidia<br />
<em>experimental</em>: requires new pydantic package which <em>may</em> break other things, to enable start sdnext with <code>--new</code> flag<br />
<em>note</em>: this is model quantization only, no support for tensorRT inference yet  </li>
<li><strong>Other</strong></li>
<li><strong>LoRA</strong> allow specifying module to apply lora on<br />
<em>example</em>: <code>&lt;lora:mylora:1.0:module=unet&gt;</code> would apply lora <em>only</em> on unet regardless of lora content<br />
    this is particularly useful when you have multiple loras and you want to apply them on different parts of the model<br />
<em>example</em>: <code>&lt;lora:firstlora:1.0:high&gt;</code> and <code>&lt;lora:secondlora:1.0:low&gt;</code><br />
<em>note</em>: <code>low</code> is shorthand for <code>module=transformer_2</code> and <code>high</code> is shortcut for <code>module=transformer</code>  </li>
<li><strong>Detailer</strong> allow manually setting processing resolution<br />
<em>note</em>: this does not impact the actual image resolution, only the resolution at which detailer internally operates  </li>
<li>refactor reuse-seed and add functionality to all tabs  </li>
<li>refactor modernui js codebase  </li>
<li>move zluda flash attenion to <em>Triton Flash attention</em> option  </li>
<li>remove samplers filtering  </li>
<li>allow both flow-matching and discrete samplers for sdxl models  </li>
<li>cleanup command line parameters  </li>
<li>add <code>--new</code> command line flag to enable testing of new packages without breaking existing installs  </li>
<li>downgrade rocm to <code>torch==2.7.1</code>  </li>
<li>set the minimum supported rocm version on linux to <code>rocm==6.0</code>  </li>
<li>disallow <code>zluda</code> and <code>directml</code> on non-windows platforms  </li>
<li>update openvino to <code>openvino==2025.3.0</code>  </li>
<li>add deprecation warning for <code>python==3.9</code>  </li>
<li>allow setting denoise strength to 0 in control/img2img<br />
    this allows to run workflows which only refine or detail existing image without changing it   </li>
<li><strong>Fixes</strong></li>
<li>normalize path hanlding when deleting images  </li>
<li>unified compile upscalers  </li>
<li>fix OpenVINO with ControlNet  </li>
<li>fix hidden model tags in networks display  </li>
<li>fix networks reference models display on windows  </li>
<li>fix handling of pre-quantized <code>flux</code> models  </li>
<li>fix <code>wan</code> use correct pipeline for i2v models  </li>
<li>fix <code>qwen-image</code> with hires  </li>
<li>fix <code>omnigen-2</code> failure  </li>
<li>fix <code>auraflow</code> quantization  </li>
<li>fix <code>kandinsky-3</code> noise  </li>
<li>fix <code>infiniteyou</code> pipeline offloading  </li>
<li>fix <code>skyreels-v2</code> image-to-video  </li>
<li>fix <code>flex2</code> img2img denoising strength  </li>
<li>fix <code>flex2</code> contronet vs inpaint image selection, thanks @alerikaisattera  </li>
<li>fix some use cases with access via reverse-proxy  </li>
<li>fix segfault on startup with <code>rocm==6.4.3</code> and <code>torch==2.8</code>  </li>
<li>fix wildcards folders traversal, thanks @dymil  </li>
<li>fix zluda flash attention with enable_gqa  </li>
<li>fix <code>wan a14b</code> quantization  </li>
<li>fix reprocess workflow for control with hires  </li>
<li>fix samplers set timesteps vs sigmas  </li>
<li>fix <code>detailer</code> missing metadata  </li>
<li>fix <code>infiniteyou</code> lora load with  </li>
</ul>
<h2 id="update-for-2025-08-20">Update for 2025-08-20</h2>
<p>A quick service release with several important hotfixes, improved localization support and adding new <strong>Qwen</strong> model variants...</p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<ul>
<li><strong>Models</strong></li>
<li><a href="https://huggingface.co/Qwen/Qwen-Image-Edit">Qwen-Image-Edit</a><br />
    Image editing using natural language prompting, similar to <code>Flux.1-Kontext</code>, but based on larger 20B <code>Qwen-Image</code> model  </li>
<li><a href="https://huggingface.co/nunchaku-tech/nunchaku-qwen-image">Nunchaku-Qwen-Image</a><br />
    if you have a compatible nVidia GPU, Nunchaku is the fastest quantization engine, currently available for Flux.1, SANA and Qwen-Image models<br />
<em>note</em>: release version of <code>nunchaku==0.3.2</code> does NOT include support, so you need to build <a href="https://nunchaku.tech/docs/nunchaku/installation/installation.html">nunchaku</a> from source  </li>
<li><a href="https://vladmandic.github.io/sd-samples/compare.html">SD.Next Model Samples Gallery</a>  </li>
<li>updated with new models  </li>
<li><strong>Features</strong>  </li>
<li>new <em>setting -&gt; huggingface -&gt; download method</em><br />
    default is <code>rust</code> as new <code>xet</code> is known to cause issues  </li>
<li>support for <code>flux.1-kontext</code> lora  </li>
<li>support for <code>qwen-image</code> lora  </li>
<li>new <em>setting -&gt; quantization -&gt; modules dtype dict</em><br />
    used to manually override quant types per module  </li>
<li><strong>UI</strong>  </li>
<li>new artwork for reference models in networks<br />
    thanks @liutyi  </li>
<li>updated <a href="https://vladmandic.github.io/sdnext-docs/Locale/">localization</a> for all 8 languages  </li>
<li>localization support for ModernUI  </li>
<li>single-click on locale rotates current locale<br />
    double-click on locale resets locale to <code>en</code>  </li>
<li>exclude ModernUI from list of extensions<br />
    ModernUI is enabled in settings, not by manually enabling extension  </li>
<li><strong>Docs</strong>  </li>
<li>Models and Video pages updated with links to original model repos, model licenses and original release dates<br />
    thanks @alerikaisattera  </li>
<li><strong>Fixes</strong>  </li>
<li>nunchaku use new download links and default to <code>0.3.2</code><br />
    nunchaku wheels: <a href="https://huggingface.co/nunchaku-tech/nunchaku/tree/main">https://huggingface.co/nunchaku-tech/nunchaku/tree/main</a>  </li>
<li>fix OpenVINO with offloading  </li>
<li>add explicit offload calls on prompt encode  </li>
<li>error reporting on model load failure  </li>
<li>fix torch version checks  </li>
<li>remove extra cache clear  </li>
<li>enable explicit sync calls for <code>rocm</code> on windows  </li>
<li>note if restart-needed on initial startup import error  </li>
<li>bypass diffusers-lora-fuse on quantized models  </li>
<li>monkey-patch diffusers to use original weights shape when loading lora  </li>
<li>guard against null prompt  </li>
<li>install <code>hf_transfter</code> and <code>hf_xet</code> when needed  </li>
<li>fix ui cropped network tags  </li>
<li>enum reference models on startup  </li>
<li>dont report errors if agent scheduler is disabled  </li>
</ul>
<h2 id="update-for-2025-08-15">Update for 2025-08-15</h2>
<h3 id="highlights-for-2025-08-15">Highlights for 2025-08-15</h3>
<p>New release two weeks after the last one and its a big one with over 150 commits!
- Several new models: <a href="https://qwenlm.github.io/blog/qwen-image/">Qwen-Image</a> (plus <em>Lightning</em> variant) and <a href="https://www.krea.ai/blog/flux-krea-open-source-release">FLUX.1-Krea-Dev</a><br />
- Several updated models: <a href="https://huggingface.co/lodestones/Chroma">Chroma</a>, <a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P-Diffusers">SkyReels-V2</a>, <a href="https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers">Wan-VACE</a>, <a href="https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers-Distilled">HunyuanDiT</a><br />
- Plus continuing with major <strong>UI</strong> work with new embedded <strong>Docs/Wiki</strong> search, redesigned real-time <strong>hints</strong>, <strong>wildcards</strong> UI selector, built-in <strong>GPU monitor</strong>, <strong>CivitAI</strong> integration and more!<br />
- On the compute side, new profiles for high-vram GPUs, offloading improvements, parallel-load for large models, support for new <code>torch</code> release and improved quality when using low-bit quantization!    <br />
- <a href="https://vladmandic.github.io/sd-samples/compare.html">SD.Next Model Samples Gallery</a>: pre-generated image gallery with 60 models (45 base and 15 finetunes) and 40 different styles resulting in 2,400 high resolution images!<br />
  gallery additionally includes model details such as typical load and inference times as well as sizes and types of each model component (<em>e.g. unet, transformer, text-encoder, vae</em>)<br />
- And (<em>as always</em>) many bugfixes and improvements to existing features!  </p>
<p><img alt="sd-samples" src="https://github.com/user-attachments/assets/3efc8603-0766-4e4e-a4cb-d8c9b13d1e1d" /></p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<p><em>Note</em>: Change-in-behavior - locations of downloaded HuggingFace models and components are changed to allow for de-duplication of common modules and switched from using system default cache folder to <code>models/huggingface</code><br />
SD.Next will warn on startup on unused cache entries that can be removed. Also, to take advantage of de-duplication, you'll need to delete models from your <code>models/Diffusers</code> folder and let SD.Next re-download them!  </p>
<h3 id="details-for-2025-08-15">Details for 2025-08-15</h3>
<ul>
<li><strong>Models</strong>  </li>
<li><a href="https://qwenlm.github.io/blog/qwen-image/">Qwen-Image</a><br />
    new image foundational model with <em>20B</em> params DiT and using <em>Qwen2.5-VL-7B</em> as the text-encoder!<br />
    available via <em>networks -&gt; models -&gt; reference</em><br />
<em>note</em>: this model is almost 2x the size of Flux, quantization and offloading are highly recommended!<br />
<em>recommended</em> params: <em>steps=50, attention-guidance=4</em><br />
    also available is pre-packaged <a href="https://huggingface.co/vladmandic/Qwen-Lightning">Qwen-Lightning</a><br />
    which is an unofficial merge of <a href="https://qwenlm.github.io/blog/qwen-image/">Qwen-Image</a> with <a href="https://github.com/ModelTC/Qwen-Image-Lightning/">Qwen-Lightning-LoRA</a> to improve quality and allow for generating in 8-steps!  </li>
<li><a href="https://www.krea.ai/blog/flux-krea-open-source-release">FLUX.1-Krea-Dev</a><br />
    new 12B base model compatible with FLUX.1-Dev from <em>Black Forest Labs</em> with opinionated aesthetics and aesthetic preferences in mind<br />
    available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://huggingface.co/lodestones/Chroma">Chroma</a><br />
    great model based on FLUX.1 and then redesigned and retrained by <em>lodestones</em><br />
    update with latest <strong>HD</strong>, <strong>HD Flash</strong> and <strong>HD Annealed</strong> variants which are based on <em>v50</em> release<br />
    available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P-Diffusers">SkyReels-V2</a><br />
    SkyReels-V2 is a genarative video model based on Wan-2.1 but with heavily modified execution to allow for infinite-length video generation<br />
    supported variants are:  <ul>
<li>diffusion-forcing: <em>T2I DF 1.3B</em> for 540p videos, <em>T2I DF 14B</em> for 720p videos, <em>I2I DF 14B</em> for 720p videos  </li>
<li>standard: <em>T2I 14B</em> for 720p videos and <em>I2I 14B</em> for 720p videos  </li>
</ul>
</li>
<li><a href="https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers">Wan-VACE</a><br />
    basic support for <em>Wan 2.1 VACE 1.3B</em> and <em>14B</em> variants<br />
    optimized support with granular guidance control will follow soon  </li>
<li><a href="https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers-Distilled">HunyuanDiT-Distilled</a><br />
    variant of HunyuanDiT with reduced steps and improved performance<br />
<strong>Torch</strong>  </li>
<li>Set default to <code>torch==2.8.0</code> for <em>CUDA, ROCm and OpenVINO</em>  </li>
<li>Add support for <code>torch==2.9.0-nightly</code>  </li>
<li><strong>UI</strong>  </li>
<li>new embedded docs/wiki search!<br />
<strong>Docs</strong> search: fully-local and works in real-time on all document pages<br />
<strong>Wiki</strong> search: uses github api to search online wiki pages  </li>
<li>updated real-time hints, thanks @CalamitousFelicitousness  </li>
<li>add <strong>Wilcards</strong> UI<br />
    in networks display  </li>
<li>every heading element is collapsible!  </li>
<li>quicksettings reset button to restore all quicksettings to default values<br />
    because things do sometimes get wrong...  </li>
<li>configurable image fit in all image views  </li>
<li>rewritten <strong>CivitAI downloader</strong><br />
    in <em>models -&gt; civitai</em><br />
<em>hint</em>: you can enter model id in a search bar to pull information on specific model directly<br />
<em>hint</em>: you can download individual versions or batch-download all-at-once!  </li>
<li>redesigned <strong>GPU monitor</strong>  <ul>
<li>standard-ui: <em>system -&gt; gpu monitor</em>  </li>
<li>modern-ui: <em>aside -&gt; console -&gt; gpu monitor</em>  </li>
<li>supported for <em>nVidia CUDA</em> and <em>AMD ROCm</em> platforms  </li>
<li>configurable interval in <em>settings -&gt; user interface</em>  </li>
</ul>
</li>
<li>updated <em>models</em> tab<ul>
<li>updated <em>models -&gt; current</em> tab  </li>
<li>updated <em>models -&gt; list models</em> tab  </li>
<li>updated <em>models -&gt; metadata</em> tab  </li>
</ul>
</li>
<li>updated <em>extensions</em> tab</li>
<li>redesigned <em>settings -&gt; user interface</em>  </li>
<li>gallery bypass browser cache for thumbnails  </li>
<li>gallery safer delete operation  </li>
<li>networks display indicator for currently active items<br />
    applies to: <em>styles, loras</em>  </li>
<li>apply privacy blur to hf and civitai tokens  </li>
<li>image download will now use actual image filename  </li>
<li>increase default and maximum ui request timeout to 2min/5min  </li>
<li><em>hint</em>: card layout<br />
    card layout is used by networks, gallery, civitai search, etc.<br />
    you can change card size in <em>settings -&gt; user interface</em>  </li>
<li><strong>Offloading</strong>  </li>
<li>changed <strong>default</strong> values for offloading based on detected gpu memory<br />
    see <a href="https://vladmandic.github.io/sdnext-docs/Offload/">offloading docs</a> for details  </li>
<li>new feature to specify which modules to offload always or never<br />
    in <em>settings -&gt; model offloading -&gt; offload always/never</em>  </li>
<li>new <code>highvram</code> profile provides significant performance boost on gpus with more than 24gb  </li>
<li>new <code>offload during pre-forward</code> option<br />
    in <em>settings -&gt; model offloading</em><br />
    switches from explicit offloading to implicit offloading on module execution change  </li>
<li>new <code>diffusers_offload_nonblocking</code> exerimental setting<br />
    instructs torch to use non-blocking move operations when possible  </li>
<li><strong>Features</strong>  </li>
<li>new <code>T5: Use shared instance of text encoder</code> option<br />
    in <em>settings -&gt; text encoder</em><br />
    since a lot of new models use T5 text encoder, this option allows to share<br />
    the same instance across all models without duplicate downloads<br />
<em>note</em> this will not reduce size of your already downloaded models, but will reduce size of future downloads  </li>
<li><strong>Wan</strong> select which stage to run: <em>first/second/both</em> with configurable <em>boundary ration</em> when running both stages<br />
    in settings -&gt; model options  </li>
<li>prompt parser allow explict <code>BOS</code> and <code>EOS</code> tokens in prompt  </li>
<li><strong>Nunchaku</strong> support for <em>FLUX.1-Fill</em> and <em>FLUX.1-Depth</em> models  </li>
<li>update requirements/packages  </li>
<li>use model vae scale-factor for image width/heigt calculations  </li>
<li><strong>SDNQ</strong> add <code>modules_dtype_dict</code> to quantize <em>Qwen Image</em> with mixed dtype  </li>
<li><strong>prompt enhance</strong>
    add <code>allura-org/Gemma-3-Glitter-4B</code>, <code>Qwen/Qwen3-4B-Instruct-2507</code>, <code>Qwen/Qwen2.5-VL-3B-Instruct</code> model support<br />
    improve system prompt  </li>
<li><strong>schedulers</strong> add <strong>Flash FlowMatch</strong>  </li>
<li><strong>model loader</strong> add parallel loader option<br />
    enabled by default, selectable in <em>settings -&gt; model loading</em>  </li>
<li><strong>filename namegen</strong> use exact sequence number instead of next available<br />
    this allows for more predictable and consistent filename generation  </li>
<li><strong>network delete</strong> new feature that allows to delete network from disk<br />
    in <em>networks -&gt; show details -&gt; delete</em><br />
    this will also delete description, metadata and previews associated with the network<br />
    only applicable to safetensors networks, not downloaded diffuser models  </li>
<li><strong>Wiki</strong>  </li>
<li>Models page updated with links to original model repos and model licenses, thanks @alerikaisattera  </li>
<li>Updated Model-Support with newly supported models  </li>
<li>Updated Offload, Prompting, API pages  </li>
<li><strong>API</strong></li>
<li>add <code>/sdapi/v1/checkpoint</code> POST endpoint to simply load a model  </li>
<li>add <code>/sdapi/v1/modules</code> GET endpoint to get info on model components/modules  </li>
<li>all generate endpoints now support <code>sd_model_checkpoint</code> parameter<br />
    this allows to specify which model to use for generation without needing to use additional endpoints  </li>
<li><strong>Refactor</strong></li>
<li>change default huggingface cache folder from system default to <code>models/huggingface</code><br />
    sd.next will warn on startup on unused cache entries  </li>
<li>new unified pipeline component loader in <code>pipelines/generic</code>  </li>
<li>remove <strong>LDSR</strong>  </li>
<li>remove <code>api-only</code> cli option  </li>
<li><strong>Docker</strong>  </li>
<li>update cuda base image: <code>pytorch/pytorch:2.8.0-cuda12.8-cudnn9-runtime</code>  </li>
<li>update official builds: <a href="https://hub.docker.com/r/vladmandic/sdnext-cuda/tags">https://hub.docker.com/r/vladmandic/sdnext-cuda/tags</a>  </li>
<li><strong>Fixes</strong>  </li>
<li>refactor legacy processing loop  </li>
<li>fix settings components mismatch  </li>
<li>fix <em>Wan 2.2-5B I2V</em> workflow  </li>
<li>fix <em>Wan</em> T2I workflow  </li>
<li>fix OpenVINO  </li>
<li>fix video model vs pipeline mismatch  </li>
<li>fix video generic save frames  </li>
<li>fix inpaint image metadata  </li>
<li>fix processing image save loop  </li>
<li>fix progress bar with refine/detailer  </li>
<li>fix api progress reporting endpoint  </li>
<li>fix <code>openvino</code> backend failing to compile  </li>
<li>fix <code>zluda</code> with hip-sdk==6.4</li>
<li>fix <code>nunchaku</code> fallback on unsupported model  </li>
<li>fix <code>nunchaku</code> windows download links  </li>
<li>fix <em>Flux.1-Kontext-Dev</em> with variable resolution  </li>
<li>use <code>utf_16_be</code> as primary metadata decoding  </li>
<li>fix <code>sd35</code> width/height alignment  </li>
<li>fix <code>nudenet</code> api  </li>
<li>fix global state tracking  </li>
<li>fix ui tab detection for networks  </li>
<li>fix ui checkbox/radio styling for non-default themes  </li>
<li>fix loading custom transformers and t5 safetensors tunes  </li>
<li>add mtime to reference models  </li>
<li>patch torch version so 3rd party libraries can use expected format  </li>
<li>unified stat size/mtime calls  </li>
<li>reapply offloading on ipadapter load  </li>
<li>api set default script-name  </li>
<li>avoid forced gc and rely on thresholds  </li>
<li>add missing interrogate in output panel  </li>
</ul>
<h2 id="update-for-2025-07-29">Update for 2025-07-29</h2>
<h3 id="highlights-for-2025-07-29">Highlights for 2025-07-29</h3>
<p>This is a big one: simply looking at number of changes, probably the biggest release since the project started!  </p>
<p>Feature highlights include:<br />
- <a href="https://github.com/user-attachments/assets/6f156154-0b0a-4be2-94f0-979e9f679501">ModernUI</a> has quite some redesign which should make it more user friendly and easier to navigate plus several new UI themes<br />
  If you're still using <strong>StandardUI</strong>, give <a href="https://vladmandic.github.io/sdnext-docs/Themes/">ModernUI</a> a try!<br />
- New models such as <a href="https://wan.video/">WanAI 2.2</a> in 5B and A14B variants for both <em>text-to-video</em> and <em>image-to-video</em> workflows as well as <em>text-to-image</em> workflow!<br />
  and also <a href="https://huggingface.co/Freepik/F-Lite">FreePik F-Lite</a>, <a href="https://huggingface.co/briaai/BRIA-3.2">Bria 3.2</a> and <a href="https://civitai.com/models/1789765?modelVersionId=2025412">bigASP 2.5</a><br />
- Redesigned <a href="https://vladmandic.github.io/sdnext-docs/Video">Video</a> interface with support for general video models plus optimized <a href="https://vladmandic.github.io/sdnext-docs/FramePack">FramePack</a> and <a href="https://vladmandic.github.io/sdnext-docs/LTX">LTXVideo</a> support<br />
- Fully integrated nudity detection and optional censorship with <a href="https://vladmandic.github.io/sdnext-docs/NudeNet">NudeNet</a><br />
- New background replacement and relightning methods using <strong>Latent Bridge Matching</strong> and new <strong>PixelArt</strong> processing filter<br />
- Enhanced auto-detection of default sampler types/settings results in avoiding common mistakes<br />
- Additional <strong>LLM/VLM</strong> models available for captioning and prompt enhance<br />
- Number of workflow and general quality-of-life improvements, especially around <strong>Styles</strong>, <strong>Detailer</strong>, <strong>Preview</strong>, <strong>Batch</strong>, <strong>Control</strong><br />
- Compute improvements<br />
- <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a> &amp; <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> updates, especially new end-to-end <a href="https://vladmandic.github.io/sdnext-docs/Parameters/">Parameters</a> page  </p>
<p>In this release we finally break with legacy with the removal of the original <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/">A1111</a> codebase which has not been maintained for a while now<br />
This plus major cleanup of codebase and external dependencies resulted in ~55k LoC (<em>lines-of-code</em>) reduction and spread over <a href="https://github.com/vladmandic/sdnext/pull/4017">~750 files</a> in ~200 commits!  </p>
<p>We also switched project license to <a href="https://github.com/vladmandic/sdnext/blob/dev/LICENSE.txt">Apache-2.0</a> which means that SD.Next is now fully compatible with commercial and non-commercial use and redistribution regardless of modifications!  </p>
<p>And (<em>as always</em>) many bugfixes and improvements to existing features!<br />
For details, see <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a>  </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend clean install for this release due to sheer size of changes<br />
Although upgrades and existing installations are tested and should work fine!</p>
</div>
<p><img alt="Screenshot" src="https://github.com/user-attachments/assets/6f156154-0b0a-4be2-94f0-979e9f679501" /></p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-07-29">Details for 2025-07-29</h3>
<ul>
<li><strong>License</strong>  </li>
<li>SD.Next <a href="https://github.com/vladmandic/sdnext/blob/dev/LICENSE.txt">license</a> switched from <strong>aGPL-v3.0</strong> to <strong>Apache-v2.0</strong><br />
    this means that SD.Next is now fully compatible with commercial and non-commercial use and redistribution regardless of modifications!  </li>
<li><strong>Models</strong></li>
<li><a href="https://github.com/Wan-Video/Wan2.2">WanAI Wan 2.2</a> both 5B and A14B variants, for both T2V and I2V support<br />
    go to: <em>video -&gt; generic -&gt; wan -&gt; pick variant</em><br />
    optimized support with <em>VACE</em>, etc. will follow soon<br />
<em>caution</em> Wan2.2 on its own is ~68GB, but also includes optional second-stage for later low-noise processing which is absolutely massive at additional ~54GB<br />
    you can enable second stage processing in <em>settings -&gt; model options</em>, its disabled by default<br />
<em>note</em>: quantization and offloading are highly recommended regardless of first-stage only or both stages!  </li>
<li><a href="https://wan.video/">WanAI Wan</a> T2V models for T2I workflows<br />
    Wan is originally designed for <em>video</em> workflows, but now also be used for <em>text-to-image</em> workflows!<br />
    supports <em>Wan-2.1 in 1.3B</em> and 14B variants and <em>Wan-2.2 in 5B and A14B</em> variants<br />
    supports all standard features such as quantization, offloading, TAESD preview generation, LoRA support etc.<br />
    can also load unet/transformer fine-tunes in safetensors format using UNET loader<br />
    simply select in <em>networks -&gt; models -&gt; reference</em><br />
<em>note</em> 1.3B model is a bit too small for good results and 14B is very large at 78GB even without second-stage so aggressive quantization and offloading are recommended  </li>
<li><a href="https://huggingface.co/Freepik/F-Lite">FreePik F-Lite</a> in <em>7B, 10B and Texture</em> variants<br />
    F-Lite is a 7B/10B model trained exclusively on copyright-safe and SFW content, trained on internal dataset comprising approximately 80 million copyright-safe images<br />
    available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://huggingface.co/briaai/BRIA-3.2">Bria 3.2</a><br />
    Bria is a smaller 4B parameter model built entirely on licensed data and safe for commercial use<br />
<em>note</em>: this is a gated model, you need to <a href="https://huggingface.co/briaai/BRIA-3.2">accept terms</a> and set your <a href="https://vladmandic.github.io/sdnext-docs/Gated/">huggingface token</a><br />
    available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://civitai.com/models/1789765">bigASP 2.5</a><br />
    bigASP is an experimental SDXL finetune using Flow matching method<br />
    load as usual, and leave sampler set to <em>Default</em><br />
    or you can use following samplers: <em>UniPC, DPM, DEIS, SA</em><br />
    required sampler settings: <em>prediction-method=flow-prediction</em>, <em>sigma-method=flowmatch</em><br />
    recommended sampler settings: <em>flow-shift=1.0</em>  </li>
<li><a href="https://github.com/gojasper/LBM">LBM: Latent Bridge Matching</a><br />
    very fast automatic image background replacement methods with relightning!<br />
<em>simple</em>: automatic background replacement using <a href="https://github.com/ZhengPeng7/BiRefNet">BiRefNet</a><br />
<em>relighting</em>: automatic background replacement with reglighting so source image fits desired background<br />
    with optional composite blending<br />
    available in <em>img2img or control -&gt; scripts</em>  </li>
<li>add <strong>FLUX.1-Kontext-Dev</strong> inpaint workflow  </li>
<li>add <strong>FLUX.1-Kontext-Dev</strong> <strong>Nunchaku</strong> support<br />
<em>note</em>: FLUX.1 Kontext is about 2-3x faster with Nunchaku vs standard execution!  </li>
<li>support <strong>FLUX.1</strong> all-in-one safetensors  </li>
<li>support for <a href="https://huggingface.co/google/gemma-3n-E4B-it">Google Gemma 3n</a> E2B and E4B LLM/VLM models<br />
    available in <strong>prompt enhance</strong> and process <strong>captioning</strong>  </li>
<li>support for <a href="https://huggingface.co/HuggingFaceTB/SmolLM3-3B">HuggingFace SmolLM3</a> 3B LLM model<br />
    available in <strong>prompt enhance</strong>  </li>
<li>add <a href="https://huggingface.co/fal/AuraFlow-v0.2">fal AuraFlow 0.2</a> in addition to existing <a href="https://huggingface.co/fal/AuraFlow-v0.3">fal AuraFlow 0.3</a> due to large differences in model behavior<br />
    available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li>add integrated <a href="https://vladmandic.github.io/sdnext-docs/NudeNet">NudeNet</a> as built-in functionality<br />
<em>note</em>: used to be available as a separate <a href="https://github.com/vladmandic/sd-extension-nudenet">extension</a>  </li>
<li><strong>Video</strong></li>
<li>redesigned <strong>Video</strong> interface  </li>
<li>support for <strong>Generic</strong> video models<br />
    includes support for many video models without specific per-model optimizations<br />
    included: <em>Hunyuan, LTX, WAN, Mochi, Latte, Allegro, Cog</em><br />
    supports quantization, offloading, frame interpolation, etc.  </li>
<li>support for optimized <a href="https://vladmandic.github.io/sdnext-docs/FramePack">FramePack</a><br />
    with <em>t2i, i2i, flf2v</em> workflows<br />
    LoRA support, prompt enhance, etc.<br />
    now fully integrated instead of being a separate extension  </li>
<li>support for optmized <a href="https://vladmandic.github.io/sdnext-docs/LTX">LTXVideo</a><br />
    with <em>t2i, i2i, v2v</em> workflows<br />
    optional native upsampling and video refine workflows<br />
    LoRA support with different conditioning types such as Canny/Depth/Pose, etc.  </li>
<li>support for post load quantization  </li>
<li><strong>UI</strong>  </li>
<li>major update to modernui layout  </li>
<li>add new Windows-like <em>Blocks</em> UI theme  </li>
<li>redesign of the <em>Flat</em> UI theme  </li>
<li>enhanced look&amp;feel for <em>Gallery</em> tab with better search and collapsible sections, thanks to @CalamitousFelicitousness</li>
<li><strong>WIKI</strong>  </li>
<li>new <a href="https://vladmandic.github.io/sdnext-docs/Parameters/">Parameters</a> page that lists and explains all generation parameters<br />
    massive thanks to @CalamitousFelicitousness for bringing this to life!  </li>
<li>updated <em>Models, Video, LTX, FramePack, Styles</em>, etc.</li>
<li><strong>Compute</strong>  </li>
<li>support for <a href="https://github.com/thu-ml/SageAttention">SageAttention2++</a><br />
    provides 10-15% performance improvement over default SDPA for transformer-based models!<br />
    enable in <em>settings -&gt; compute settings -&gt; sdp options</em><br />
<em>note</em>: SD.Next will use either SageAttention v1/v2/v2++, depending which one is installed<br />
    until authors provide pre-build wheels for v2++, you need to install it manually or SD.Next will auto-install v1  </li>
<li>support for <code>torch.compile</code> for LLM: captioning/prompt-enhannce  </li>
<li>support for <code>torch.compile</code> with repeated-blocks<br />
    reduces time-to-compile 5x without loss of performance!<br />
    enable in <em>settings -&gt; model compile -&gt; repeated</em><br />
<em>note</em>: torch.compile is not compatible with balanced offload  </li>
<li><strong>Other</strong>  </li>
<li><strong>Styles</strong> can now include both generation params and server settings<br />
    see <a href="https://vladmandic.github.io/sdnext-docs/Styles/">Styles docs</a> for details  </li>
<li><strong>TAESD</strong> is now default preview type since its the only one that supports most new models  </li>
<li>support <strong>TAESD</strong> preview and remote VAE for <strong>HunyuanDit</strong>  </li>
<li>support <strong>TAESD</strong> preview and remote VAE for <strong>AuraFlow</strong>  </li>
<li>support <strong>TAESD</strong> preview for <strong>WanAI</strong>  </li>
<li>SD.Next now starts with <em>locked</em> state preventing model loading until startup is complete  </li>
<li>warn when modifying legacy settings that are no longer supported, but available for compatibilty  </li>
<li>warn on incompatible sampler and automatically restore default sampler  </li>
<li><strong>XYZ grid</strong> can now work with control tab:<br />
    if controlnet or processor are selected in xyz grid, they will overwrite settings from first unit in control tab,<br />
    when using controlnet/processor selected in xyz grid, behavior is forced as control-only<br />
    also freely selectable are control strength, start and end values  </li>
<li><strong>Batch</strong> warn on unprocessable images and skip operations on errors so that other images can still be processed  </li>
<li><strong>Metadata</strong> improved parsing and detect foreign metadata<br />
    detect ComfyUI images<br />
    detect InvokeAI images  </li>
<li><strong>Detailer</strong> add <code>expert</code> mode where list of detailer models can be converted to textbox for manual editing<br />
    see <a href="https://vladmandic.github.io/sdnext-docs/Detailer/">docs</a> for more information  </li>
<li><strong>Detailer</strong> add option to merge multiple results from each detailer model<br />
    for example, hands model can result in two hands each being processed separately or both hands can be merged into one composite job  </li>
<li><strong>Control</strong> auto-update width/height on image upload  </li>
<li><strong>Control</strong> auto-determine image save path depending on operations performed  </li>
<li>autodetect <strong>V-prediction</strong> models and override default sampler prediction type as needed  </li>
<li><strong>SDNQ</strong>  </li>
<li>use inference context during quantization  </li>
<li>use static compile  </li>
<li>rename quantization type for text encoders <code>default</code> option to <code>Same as model</code>  </li>
<li><strong>API</strong>  </li>
<li>add <code>/sdapi/v1/lock-checkpoint</code> endpoint that can be used to lock/unlock model changes<br />
    if model is locked, it cannot be changed using normal load or unload methods  </li>
<li><strong>Fixes</strong></li>
<li>allow theme type <code>None</code> to be set in config  </li>
<li>installer dont cache installed state  </li>
<li>fix Cosmos-Predict2 retrying TAESD download  </li>
<li>better handle startup import errors  </li>
<li>fix traceback width preventing copy&amp;paste  </li>
<li>fix ansi controle output from scripts/extensions  </li>
<li>fix diffusers models non-unique hash  </li>
<li>fix loading of manually downloaded diffuser models  </li>
<li>fix api <code>/sdapi/v1/embeddings</code> endpoint  </li>
<li>fix incorrect reporting of deleted and modified files  </li>
<li>fix SD3.x loader and TAESD preview  </li>
<li>fix xyz with control enabled  </li>
<li>fix control order of image save operations  </li>
<li>fix control batch-input processing  </li>
<li>fix modules merge save model  </li>
<li>fix torchvision bicubic upsample with ipex  </li>
<li>fix instantir pipeline  </li>
<li>fix prompt encoding if prompts within batch have different segment counts  </li>
<li>fix detailer min/max size  </li>
<li>fix loopback script  </li>
<li>fix networks tags display  </li>
<li>fix yolo refresh models  </li>
<li>cleanup control infotext  </li>
<li>allow upscaling with models that have implicit VAE processing  </li>
<li>framepack improve offloading  </li>
<li>improve prompt parser tokenizer loader  </li>
<li>improve scripts error handling  </li>
<li>improve infotext param parsing  </li>
<li>improve extensions ui search  </li>
<li>improve model type autodetection  </li>
<li>improve model auth check for hf repos  </li>
<li>improve Chroma prompt padding as per recommendations  </li>
<li>lock directml torch to <code>torch-directml==0.2.4.dev240913</code>  </li>
<li>lock directml transformers to <code>transformers==4.52.4</code>  </li>
<li>improve install of <code>sentencepiece</code> tokenizer  </li>
<li>add int8 matmul fallback for ipex with onednn qlinear  </li>
<li><strong>Refactoring</strong><br />
<em>note</em>: none of the removals result in loss-of-functionality since all those features are already re-implemented<br />
  goal here is to remove legacy code, code duplication and reduce code complexity  </li>
<li>obsolete <strong>original backend</strong>  </li>
<li>remove majority of legacy <strong>a1111</strong> codebase  </li>
<li>remove legacy ldm codebase: <code>/repositories/ldm</code>  </li>
<li>remove legacy blip codebase: <code>/repositories/blip</code>  </li>
<li>remove legacy codeformer codebase: <code>/repositories/codeformer</code>  </li>
<li>remove legacy clip patch model: <code>/models/karlo</code>  </li>
<li>remove legacy model configs: <code>/configs/*.yaml</code>  </li>
<li>remove legacy submodule: <code>/modules/k-diffusion</code>  </li>
<li>remove legacy hypernetworks support: <code>/modules/hypernetworks</code>  </li>
<li>remove legacy lora support: <code>/extensions-builtin/Lora</code>  </li>
<li>remove legacy clip/blip interrogate module  </li>
<li>remove modern-ui remove <code>only-original</code> vs <code>only-diffusers</code> code paths  </li>
<li>refactor control processing and separate preprocessing and image save ops  </li>
<li>refactor modernui layouts to rely on accordions more than individual controls  </li>
<li>refactore pipeline apply/unapply optional components &amp; features  </li>
<li>split monolithic <code>shared.py</code>  </li>
<li>cleanup <code>/modules</code>: move pipeline loaders to <code>/pipelines</code> root  </li>
<li>cleanup <code>/modules</code>: move code folders used by pipelines to <code>/pipelines/&lt;pipeline&gt;</code> folder  </li>
<li>cleanup <code>/modules</code>: move code folders used by scripts to <code>/scripts/&lt;script&gt;</code> folder  </li>
<li>cleanup <code>/modules</code>: global rename <code>modules.scripts</code> to avoid conflict with <code>/scripts</code>  </li>
<li>override <code>gradio</code> installer  </li>
<li>major refactoring of requirements and dependencies to unblock <code>numpy&gt;=2.1.0</code>  </li>
<li>patch <code>insightface</code>  </li>
<li>patch <code>facelib</code>  </li>
<li>patch <code>numpy</code>  </li>
<li>stronger lint rules<br />
    add separate <code>npm run lint</code>, <code>npm run todo</code>, <code>npm run test</code>, <code>npm run format</code> macros  </li>
</ul>
<h2 id="update-for-2025-06-30">Update for 2025-06-30</h2>
<h3 id="highlights-for-2025-06-30">Highlights for 2025-06-30</h3>
<p>New release with ~100 commits...So what's new? Well, its been a busy few weeks with new models coming out quite frequently:<br />
- New T2I/I2I models: <strong>OmniGen-2, Cosmos-Predict2, FLUX.1-Kontext, Chroma</strong><br />
- Additional VLM models: <strong>JoyCaption Beta, MoonDream 2</strong><br />
- Additional upscalers: <strong>UltraSharp v2</strong>  </p>
<p>And (as always) many bugfixes and improvements to existing features!  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-06-30">Details for 2025-06-30</h3>
<ul>
<li><strong>Models</strong></li>
<li><a href="https://vladmandic.github.io/sdnext-docs/Models/">Models Wiki page</a> is updated will all new models<br />
<em>note</em> all new image models larger than 30GB, so <a href="https://vladmandic.github.io/sdnext-docs/Offload/">offloading</a> and <a href="https://vladmandic.github.io/sdnext-docs/Quantization/">quantization</a> are necessary!  </li>
<li><a href="https://huggingface.co/OmniGen2/OmniGen2">OmniGen2</a>  <ul>
<li>OmniGen2 is a powerful unified multimodal model that supports t2i and i2i workflows and uses 4B transformer with Qwen-VL-2.5 4B VLM  </li>
<li>available via <em>networks -&gt; models -&gt; reference</em>  </li>
</ul>
</li>
<li><a href="https://research.nvidia.com/labs/dir/cosmos-predict2/">nVidia Cosmos-Predict2 T2I</a> <em>2B and 14B</em>  <ul>
<li>Cosmos-Predict2 T2I is a new foundational model from Nvidia in two variants: small 2B and large 14B</li>
<li>available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><em>note</em>: 14B variant is a very large model at 36GB</li>
<li><em>note</em>: this is a gated model, you need to <a href="https://huggingface.co/nvidia/Cosmos-Predict2-2B-Text2Image">accept terms</a> and set your <a href="https://vladmandic.github.io/sdnext-docs/Gated/">huggingface token</a>  </li>
</ul>
</li>
<li><a href="https://bfl.ai/announcements/flux-1-kontext-dev">Black Forest Labs FLUX.1 Kontext I2I</a> <em>Dev</em> variant  <ul>
<li>FLUX.1-Kontext is a 12B model billion parameter capable of editing images based on text instructions  </li>
<li>model is primarily designed for image editing workflows, but also works for text-to-image workflows  </li>
<li>requirements are similar to regular FLUX.1 although 2x slower  </li>
<li>available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><em>note</em>: this is a gated model, you need to <a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev">accept terms</a> and set your <a href="https://vladmandic.github.io/sdnext-docs/Gated/">huggingface token</a>  </li>
</ul>
</li>
<li><a href="https://huggingface.co/lodestones/Chroma">lodestones Chroma</a>  <ul>
<li>Chroma is a 8.9B parameter model based on <em>FLUX.1-schnell</em> and fully Apache 2.0 licensed  </li>
<li>available via <em>networks -&gt; models -&gt; reference</em>  </li>
<li><em>note</em>: model is still in training so future updates will trigger re-download  </li>
<li>large credits to @Trojaner for work on bringing Chroma support to SD.Next and all the optimizations around it!  </li>
</ul>
</li>
<li><a href="https://huggingface.co/fancyfeast/llama-joycaption-beta-one-hf-llava">JoyCaption Beta</a> support (in addition to existing JoyCaption Alpha)  <ul>
<li>new version of highly popular captioning model  </li>
<li>available via <em>caption -&gt; vlm caption</em>  </li>
</ul>
</li>
<li><a href="https://huggingface.co/vikhyatk/moondream2">MoonDream 2</a> support (updated)  <ul>
<li>really good 2B captioning model that can work on different levels of detail  </li>
<li>available via <em>caption -&gt; vlm caption</em>  </li>
</ul>
</li>
<li><a href="https://huggingface.co/Kim2091/UltraSharpV2">UltraSharp v2</a> support  <ul>
<li>one of the best upscalers (traditional, non-diffusion) available today!  </li>
<li>available via <em>process -&gt; upscale -&gt; chainner</em>  </li>
</ul>
</li>
<li><strong>Changes</strong>  </li>
<li>Update all core requirements  </li>
<li>Support Remote VAE with <em>Omnigen, Lumina 2 and PixArt</em>  </li>
<li>Enable quantization for captioning: <em>Gemma, Qwen, SMOL, Florence, JoyCaption</em>  </li>
<li>Add <code>--trace</code> command line param that enables trace logging  </li>
<li>Use Diffusers version of <em>OmniGen</em>  </li>
<li>Control move global settings to control elements -&gt; control settings tab  </li>
<li>Control add setting to run hires with or without control  </li>
<li>Update OpenVINO to 2025.2.0  </li>
<li>Simplified and unified quantization enabled for options  </li>
<li>Add <strong>PixelArt</strong> filter to processing tab  </li>
<li><strong>SDNQ Quantization</strong>  </li>
<li>Add <code>auto</code> quantization mode  </li>
<li>Add <code>modules_to_not_convert</code> support for post mode  </li>
<li>Improve offload compatibility  </li>
<li>Fix Qwen 2.5 with int8 matmul  </li>
<li>Fix Dora loading  </li>
<li>Remove per layer GC  </li>
<li>Add support for XYZ grid to test quantization modes<br />
<em>note</em>: you need to enable quantization and choose what it applies on, then xyz grid can change quantization mode<br />
<em>note</em>: you can also enable 'add time info' to compare performance of different quantization modes  </li>
<li><strong>API</strong></li>
<li>Add <code>/sdapi/v1/network?page=&lt;page_name&gt;&amp;item=&lt;item_name&gt;</code> endpoint that returns full network info  </li>
<li>Add <code>/sdapi/v1/lora?lora=&lt;lora_name&gt;</code> endpoint that returns full lora info and metadata  </li>
<li>Add <code>/sdapi/v1/controlnets?model_type=&lt;model_type|all|None&gt;</code> endpoints that returns list of available controlnets for specific model type  </li>
<li>Set default sampler to <code>Default</code>  </li>
<li><strong>Fixes</strong>  </li>
<li>IPEX with DPM2++ FlowMatch samplers  </li>
<li>Invalid attention processor with ControlNet  </li>
<li>LTXVideo default scheduler  </li>
<li>Balanced offload with OmniGen  </li>
<li>Quantization with OmniGen  </li>
<li>Do not save empty <code>params.txt</code> file  </li>
<li>Override <code>params.txt</code> using <code>SD_PATH_PARAMS</code> env variable  </li>
<li>Add <code>wheel</code> to requirements due to <code>pip</code> change  </li>
<li>Case-insensitive sampler name matching  </li>
<li>Fix delete file with gallery views  </li>
<li>Add <code>SD_SAVE_DEBUG</code> env variable to report all params and metadata save operations as they happen  </li>
<li>Fix TAESD model type detection  </li>
<li>Fix LoRA loader incorrectly reporting errors  </li>
<li>Fix hypertile for img2img and inpaint operations  </li>
<li>Fix prompt parser batch size  </li>
<li>Fix process batch with batch count  </li>
<li>Fix process batch double image save  </li>
<li>Fix unapply texture tiling  </li>
<li>Fix nunchaku batch support  </li>
<li>Fix LoRA change detection on pipeline type change  </li>
<li>Fix LoRA load order when it includes text-encoder data  </li>
<li>Suppress torch empty logging  </li>
<li>Improve TAESD live preview downscale handling  </li>
</ul>
<h2 id="update-for-2025-06-16">Update for 2025-06-16</h2>
<ul>
<li><strong>Feature</strong>  </li>
<li>Support for Python 3.13  </li>
<li>TeaCache support for Lumina 2  </li>
<li>Custom UNet and VAE loading support for Lumina 2  </li>
<li><strong>Changes</strong>  </li>
<li>Increase the medvram mode threshold from 8GB to 12GB  </li>
<li>Set CPU backend to use FP32 by default  </li>
<li>Relax Python version checks for Zluda  </li>
<li>Make VAE options not require model reload  </li>
<li>Add warning about incompatible attention processors  </li>
<li><strong>Torch</strong>  </li>
<li>Set default to <code>torch==2.7.1</code>  </li>
<li>Force upgrade pip when installing Torch  </li>
<li><strong>ROCm</strong>  </li>
<li>Support ROCm 6.4 with <code>--use-nightly</code>  </li>
<li>Don't override user set gfx version  </li>
<li>Don't override gfx version with RX 9000  </li>
<li>Fix flash-atten repo  </li>
<li><strong>SDNQ Quantization</strong>  </li>
<li>Add group size support for convolutional layers  </li>
<li>Add quantized matmul support for for convolutional layers  </li>
<li>Add 7-bit, 5-bit and 3-bit quantization support  </li>
<li>Add separate quant mode option for Text Encoders  </li>
<li>Fix forced FP32 with tensorwise FP8 matmul  </li>
<li>Fix PyTorch &lt;= 2.4 compatibility with FP8 matmul  </li>
<li>Fix VAE with conv quant  </li>
<li>Don't ignore the Quantize with GPU option with offload mode <code>none</code> and <code>model</code>  </li>
<li>High VRAM usage with Lumina 2  </li>
<li><strong>Fixes</strong>  </li>
<li>Meissonic with multiple generators  </li>
<li>OmniGen with new transformers  </li>
<li>Invalid attention processors  </li>
<li>PixArt Sigma Small and Large loading  </li>
<li>TAESD previews with PixArt and Lumina 2  </li>
<li>VAE Tiling with non-default tile sizes  </li>
<li>Lumina 2 with IPEX  </li>
<li>Nunchaku updated repo  </li>
<li>Double loading of models with custom UNets  </li>
</ul>
<h2 id="update-for-2025-06-02">Update for 2025-06-02</h2>
<h3 id="highlights-for-2025-06-02">Highlights for 2025-06-02</h3>
<p>This release is all about quantization: with new SD.Next own quantization method: <strong>SDNQ</strong><br />
<strong>SDNQ</strong> is based on <strong>NNCF</strong>, but has been re-implemented, optimized and evolved enough to become its own quantization method!<br />
It's fully cross-platform, supports all GPUs and includes tons of quantization methods:
- <em>8-bit, 6-bit, 4-bit, 2-bit and 1-bit int and uint</em>
- <em>8-bit e5, e4 and fnuz float</em></p>
<p>Also unlike most traditional methods, its also applicable to nearly all model types  </p>
<p><em>Hint</em>: Even if you may not need quantization for your current model, it may be worth trying it out as it can significantly improve performance or capabilities of your existing workflow! For example, you may not have issues with SD15 or SDXL, but you may have been limited running at high resolutions or with multiple ControlNet due to VRAM requirements - this will significantly reduce memory requirements. And on-the-fly quantization takes just few seconds during model load, there is no need to have multiple quant models permanently saved.  </p>
<p>On a different topic, <strong>SD.Next Wiki &amp; Docs</strong> and its <strong>UI Hints</strong> and <strong>UI Localization</strong> system are community efforts and any contributions are welcome!<br />
You dont need any coding experience, but if you learned something and you find documentation either wrong or insufficient, please do suggest edits!<br />
Take a look at <a href="https://github.com/vladmandic/sdnext/wiki/Docs">Docs</a>, <a href="https://github.com/vladmandic/sdnext/wiki/Hints">Hints</a> and <a href="https://github.com/vladmandic/sdnext/wiki/Locale">Localization</a> contribution guides</p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-06-02">Details for 2025-06-02</h3>
<ul>
<li><strong>SDNQ Quantization</strong>  </li>
<li>Renamed <code>NNCF</code> to <code>SDNQ</code>  </li>
<li>Renamed quantization scheme names to the underlying dtype names instead of NNCF names  <ul>
<li><code>INT8_SYM</code> -&gt; <code>int8</code>  </li>
<li><code>INT8</code> -&gt; <code>uint8</code>  </li>
<li><code>INT4_SYM</code> -&gt; <code>int4</code>  </li>
<li><code>INT4</code> -&gt; <code>uint4</code>  </li>
</ul>
</li>
<li>Add <code>float8_e4m3fn</code>, <code>float8_e5m2</code>, <code>float8_e4m3fnuz</code>, <code>float8_e5m2fnuz</code>, <code>int6</code>, <code>uint6</code>, <code>int2</code>, <code>uint2</code> and <code>uint1</code> support  </li>
<li>Add quantized matmul support for <code>float8_e4m3fn</code> and <code>float8_e5m2</code>  </li>
<li>Set the default quant mode to <code>pre</code>  </li>
<li>Use per token input quant with int8 and fp8 quantized matmul  </li>
<li>Implement better layer hijacks  </li>
<li>Add an option to toggle quantize with GPU  </li>
<li>Fix conv quant and add support for conv quant with asym modes  </li>
<li>Fix lora weight change  </li>
<li>Fix high RAM usage with pre mode  </li>
<li>Fix scale and zero_point not being offloaded  </li>
<li><strong>IPEX</strong>  </li>
<li>Disabe Dynamic Attention by default on PyTorch 2.7  </li>
<li>Remove GradScaler hijack and use <code>torch.amp.GradScaler</code> instead  </li>
<li><strong>Feature</strong>  </li>
<li>TeaCache support for HiDream I1  </li>
<li><strong>Changes</strong>  </li>
<li>Set the default attention optimizer to Scaled-Dot-Product on all backends  </li>
<li>Enable Dynamic attention for Scaled-Dot-Product with ROCm, DirectML, MPS and CPU backends  </li>
<li><strong>Fixes</strong></li>
<li>Gallery duplicate entries  </li>
<li>Prompt enhancement args mismatch  </li>
</ul>
<h2 id="update-for-2025-05-17">Update for 2025-05-17</h2>
<p><em>Curious how your system is performing?</em><br />
Run a built-in benchmark and compare to over 15k unique results world-wide: <a href="https://vladmandic.github.io/sd-extension-system-info/pages/benchmark.html">Benchmark data</a>!<br />
From slowest 0.02 it/s running on 6th gen CPU without acceleration up to 275+ it/s running on tuned GH100 system!  </p>
<p>Also, since quantization is becoming a necessity for almost all new models, see comparison of different quantization methods available in SD.Next: <a href="https://vladmandic.github.io/sdnext-docs/Quantization/">Quantization</a><br />
<em>Hint</em>: Even if you may not need quantization for your current model, it may be worth trying it out as it can significantly improve performance!  </p>
<p>For ZLUDA users, this update adds <a href="https://github.com/vladmandic/sdnext/issues/3918">compatibility</a> with with latest AMD Adrenaline drivers  </p>
<p>Btw, last few releases have been smaller, but more regular so do check posts about previous releases as features do quickly add up!  </p>
<ul>
<li><strong>Wiki</strong>  </li>
<li>Updates for: <em>Quantization, NNCF, WSL, ZLUDA, ROCm</em>  </li>
<li><strong>Models</strong>  </li>
<li><a href="https://huggingface.co/IndexTeam/Index-anisora">Index AniSora v1 5B</a> I2V<br />
    Based on CogVideoX architecture, trained as animated video generation model: This Project presenting Bilibili's gift to the anime world!  </li>
<li><a href="https://github.com/bilibili/Index-anisora?tab=readme-ov-file#anisorav10_rl">Index AniSora v1 RL 5B</a> I2V<br />
    RL-optimized AniSoraV1.0 for enhanced anime-style output  </li>
<li><strong>Compute</strong>  </li>
<li>ZLUDA: update to <code>zluda==3.9.5</code> with <code>torch==2.7.0</code><br />
<em>Note</em>: delete <code>.zluda</code> folder so that newest zluda will be installed if you are using the latest AMD Adrenaline driver  </li>
<li>NNCF: added experimental support for direct INT8 MatMul  </li>
<li><strong>Feature</strong>  </li>
<li>Prompt Enhance: option to allow/disallow NSFW content  </li>
<li><strong>Fixes</strong>  </li>
<li>OpenVINO: force cpu device  </li>
<li>Gradio: major cleanup and fixing defaults and ranges  </li>
<li>Pydantic: update to api types  </li>
<li>UI defaults: match correct prompt components  </li>
<li>NNCF with ControlNet  </li>
<li>NNCF with CogVideo</li>
<li>IPEX with CogVideo  </li>
<li>JXL image format metadata handling  </li>
</ul>
<h2 id="update-for-2025-05-12">Update for 2025-05-12</h2>
<h3 id="highlights-for-2025-05-12">Highlights for 2025-05-12</h3>
<p>First of all NNCF quantization engine has gone through some major enhancements and its now much faster, both in quantization as well as actual inference!<br />
And its a only truly cross-platform solution for quantization as all other methods are platform specific.  </p>
<p><em>Note</em> if you're a ZLUDA user, see notes on GPU driver compatibility as recent Andrenaline drivers do cause problems!<br />
And if you're a ROCm user, this release brings much faster compile times on Linux as well as first (experimental) builds for Windows!  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-05-12">Details for 2025-05-12</h3>
<ul>
<li><strong>Compute</strong></li>
<li><strong>NNCF</strong>  <ul>
<li>Faster quantization  </li>
<li>Faster inference with support for <code>torch.triton</code><br />
  up to 3.5x faster with INT4 and 2x faster with INT8  </li>
<li>New settings: <em>NNCF -&gt; Group size</em><br />
  default is a balance between performance (higher size) and quality (lower size)<br />
  0 is default at 64, -1 disables grouping  </li>
</ul>
</li>
<li><strong>ZLUDA</strong>:<ul>
<li><em>warning</em>: AMD Adrenaline 25.5.1 drivers are NOT COMPATIBLE with ZLUDA
  see <a href="https://github.com/vladmandic/sdnext/issues/3918">issue</a> for details</li>
</ul>
</li>
<li><strong>ROCm</strong><ul>
<li>first working builds of <strong>Torch with ROCm on Windows</strong><br />
  highly experimental<br />
  reach out on Discord if you want to test it  </li>
</ul>
</li>
<li><strong>Features</strong></li>
<li>Prompt Enhancer: support for <em>img2img</em> workflows<br />
    in img2img prompt enhancer will first analyze input image and then incorporate user prompt to create enhanced prompt  </li>
<li><strong>FramePack</strong><ul>
<li>improve LoRA compatibility  </li>
<li>add metadata to video  </li>
</ul>
</li>
<li><strong>UI</strong><ul>
<li>ModernUI: support for History tab  </li>
<li>ModernUI: support for FramePack tab  </li>
</ul>
</li>
<li><strong>API</strong>  <ul>
<li>add <code>/sdapi/v1/framepack</code> endpoint with full support for FramePack including all optional settings<br />
  see example: <code>sd-extension-framepack/create-video.py</code>  </li>
<li>add <code>/sdapi/v1/checkpoint</code> endpoint to get info on currently loaded model/checkpoint<br />
  see example: <code>cli/api-checkpoint.py</code>  </li>
<li>add <code>/sdapi/v1/prompt-enhance</code> endpoint to enhance prompt using LLM<br />
  see example: <code>cli/api-enhance.py</code><br />
  supports text, image and video prompts with or without input image<br />
<em>note</em>: if input image is provided, model should be left at default <code>gemma-3-4b-it</code> as most other LLMs do not support hybrid workflows  </li>
</ul>
</li>
<li><strong>Fixes</strong></li>
<li>Latent Diffusion Upscale</li>
<li>Model load: support SDXL safetensors packaged without VAE  </li>
<li>ROCm: disable cuDNN benchmark, fixes slow MIOpen tuning with <code>torch==2.7</code>  </li>
<li>Extensions: use in-process installer for extensions-builtin, improves startup performance  </li>
<li>FramePack: monkey-patch for dynamically installed <code>av</code>  </li>
<li>Logging: reduce spam while progress is active  </li>
<li>LoRA: legacy handler enable/disable  </li>
<li>LoRA: force clear-cache on model unload  </li>
<li>ADetailer: fix enable/disable  </li>
<li>ZLUDA: improve compatibility with older GPUs  </li>
</ul>
<h2 id="update-for-2025-05-06">Update for 2025-05-06</h2>
<p>Minor refesh with several bugfixes and updates to core libraries<br />
Plus new features with <strong>FramePack</strong> and <strong>HiDream-E1</strong></p>
<ul>
<li><strong>Features</strong>  </li>
<li><a href="https://vladmandic.github.io/sdnext-docs/FramePack">FramePack</a><br />
    add <strong>T2V</strong> mode in addition to <strong>I2V</strong> and <strong>FLF2V</strong><br />
    support for new <strong>F1: forward-only</strong> model variant in addition to regular <strong>bi-directional</strong><br />
    add <strong>prompt enhance</strong> using VLM: it will analyze input image and then create enhanced prompt based on user prompt and image<br />
    add <strong>prompt interpolation</strong>, section prompts do not need to match exact video section count<br />
    and improved performance<br />
<a href="https://vladmandic.github.io/sdnext-docs/FramePack">Docs</a> rewrite!  </li>
<li><strong>Prompt-Enhhance</strong><br />
    add <strong>Qwen3</strong> <em>0.6B/1.7B/4B</em> models<br />
    add thinking mode support (for models that have it)  </li>
<li><a href="https://huggingface.co/HiDream-ai/HiDream-E1-Full">HiDream-E1</a> natural language image-editing model built on HiDream-I1<br />
    available via  <em>networks -&gt; models -&gt; reference</em><br />
<em>note</em>: right now HiDream-E1 is limited to 768x768 images, so you must force resize image before running it  </li>
<li><strong>Other</strong>  </li>
<li>CUDA: set default to <code>torch==2.7.0</code> with <code>cuda==12.8</code>  </li>
<li>ZLUDA: update to <code>zluda==3.9.4</code> and <code>flash-attn-2</code>  </li>
<li>Docker: pre-install <code>ffmpeg</code>  </li>
<li>Wiki: updated pages: <em>FramePack, Video, ROCm, ZLUDA, Quantization</em>  </li>
<li>Gallery: support JXL image format  </li>
<li>Scheduler: add sigmoid beta scheduler  </li>
<li>GitHub: updated issue template  </li>
<li><strong>Fixes</strong>  </li>
<li>FramePack: correct dtype  </li>
<li>NNCF: check dependencies and register quant type  </li>
<li>API: refresh checkpoint list  </li>
<li>API: vlm-api endpoint  </li>
<li>Styles: save style with prompt  </li>
<li>Texture tiling: fix apply when switching models  </li>
<li>Diffusers: slow initial startup  </li>
<li>Gated access: obfuscate and log token used for access  </li>
<li>SDXL refiner workflow  </li>
<li>Control: t2i-adapter workflow  </li>
<li>Control: xs-controlnet workflow  </li>
<li>Control: lllite-workflow  </li>
<li>Control: refiner workflow with multiple control elements  </li>
</ul>
<h2 id="highlights-for-2025-04-28">Highlights for 2025-04-28</h2>
<p>Another major release with <em>over 120 commits</em>!<br />
Highlights include new <a href="https://github.com/vladmandic/sdnext/wiki/Nunchaku">Nunchaku Wiki</a> inference engine that allows running FLUX.1 with <strong>3-5x</strong> higher performance!<br />
And a new <a href="https://github.com/vladmandic/sd-extension-framepack">FramePack</a> extension for high-quality <em>I2V</em> and <em>FLF2V</em> video generation with unlimited duration!  </p>
<p>What else?
- New UI <strong>History</strong> tab<br />
- New models: <strong>Flex.2, LTXVideo-0.9.6, WAN-2.1-14B-FLF2V</strong>, schedulers: <strong>UniPC and LCM FlowMatch</strong>, features: <strong>CFGZero</strong><br />
- Major updates to: <strong>NNCF, OpenVINO, ROCm, ZLUDA</strong><br />
- Cumulative fixes since last release  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h2 id="details-for-2025-04-28">Details for 2025-04-28</h2>
<ul>
<li><strong>Features</strong></li>
<li><a href="https://github.com/mit-han-lab/nunchaku">Nunchaku</a> inference engine with custom <strong>SVDQuant</strong> 4-bit execution<br />
    highly experimental and with limited support, but when it works, its magic: <strong>Flux.1 at 6.0 it/s</strong> <em>(not sec/it)</em>!<br />
    basically, it can speed up supported models by 2-5x by using custom quantization and execution engine<br />
    see <a href="https://github.com/vladmandic/sdnext/wiki/Nunchaku">Nunchaku Wiki</a> for installation guide and list of supported models &amp; features  </li>
<li><a href="https://github.com/vladmandic/sd-extension-framepack">FramePack</a> based on <strong>HunyuanVideo-I2V</strong><br />
    full support and much more for <strong>Lllyasviel</strong> <a href="https://lllyasviel.github.io/frame_pack_gitpage/">FramePack</a><br />
    implemented as an extension for <strong>SD.Next</strong> (for the moment while dev is ongoing)<br />
    generate high-quality videos with pretty much unlimited duration and with limited VRAM!<br />
    install as any other extension and for details see extension <a href="https://github.com/vladmandic/sd-extension-framepack/blob/main/README.md">README</a>  <ul>
<li>I2V &amp; FLF2V support with explicit strength controls  </li>
<li>complex actions: modify prompts for each section of the video  </li>
<li>LoRA support: use normal <strong>HunyuanVideo</strong> LoRAs  </li>
<li>decode: use local, tiny or remote VAE  </li>
<li>custom models: e.g. replace llama with one of your choice  </li>
<li>video: multiple codecs and with hw acceleration, raw export, frame export, frame interpolation  </li>
<li>compute: quantization support, new offloading, more configuration options, cross-platform, etc.  </li>
</ul>
</li>
<li><a href="https://huggingface.co/ostris/Flex.2-preview">Ostris Flex.2 Preview</a><br />
    more than a FLUX.1 finetune, FLEX.2 is created from <em>Flux.1 Schnell -&gt; OpenFlux.1 -&gt; Flex.1-alpha -&gt; Flex.2-preview</em><br />
    and it has universal control and inpainting support built in!<br />
    supported for text and control workflows<br />
    when using in control mode, simply choose preprocessor and do not load actual controlnet<br />
    supported control modes are: <em>line, pose and depth</em><br />
    available via  <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://github.com/Lightricks/LTX-Video?tab=readme-ov-file">LTXVideo 0.9.6</a> <strong>T2V</strong> and <strong>I2V</strong><br />
    in both <strong>Standard</strong> and <strong>Distilled</strong> variants<br />
    available in <em>video tab</em></li>
<li><a href="https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P">WAN 2.1 14B 720P</a> <strong>FLF2V</strong><br />
    new first-to-last image video model from WAN-AI<br />
    available in <em>video tab</em></li>
<li><a href="https://github.com/WeichenFan/CFG-Zero-star">CFG-Zero</a> new guidance method optimized for flow-matching models<br />
    implemented for <strong>FLUX.1, HiDream-I1, SD3.x, CogView4, HunyuanVideo, WanAI</strong><br />
    enable and configure in <em>settings -&gt; pipeline modifiers -&gt; cfg zero</em><br />
    experiment with CFGZero support in XYZ-grid  </li>
<li><strong>Optimizations</strong></li>
<li><strong>NNCF</strong> update to 2.16.0<br />
    major refactoring of NNCF quantization code<br />
    new quant types: <code>INT8_SYM</code> (new default), <code>INT4</code> and <code>INT4_SYM</code><br />
    quantization support for the convolutional layers on unet models with sym methods<br />
    pre-load quantization support<br />
    LoRA support<br />
<em>if you're low on VRAM, NNCF is as close as a catch-all solution</em>  </li>
<li><strong>OpenVINO</strong> update to 2025.1.0 and Torch to 2.7  </li>
<li><strong>IPEX</strong> update to Torch 2.7  </li>
<li><strong>ROCm</strong> update to Torch 2.7  </li>
<li><strong>HiDream-I1</strong> optimized offloading and prompt-encode caching<br />
    it now works in 12GB VRAM / 26GB RAM!  </li>
<li><strong>CogView3</strong> and <strong>CogView4</strong> model loader optimizations  </li>
<li><strong>Sana</strong> model loader optimizations</li>
<li>add explicit offload after encode prompt<br />
    configure in <em>settings -&gt; text encoder -&gt; offload</em>  </li>
<li><strong>UI</strong>  </li>
<li>new History tab where you can see all jobs since the server startup<br />
    and optionally download any of the previously generated images/videos<br />
    access via <em>system -&gt; history</em>  </li>
<li>server restart from ui now replaces currently running process<br />
    instead of trying to reload python modules in-place  </li>
<li>add option to enable/disable clip skip<br />
    disabled by default to avoid issues with frequent incorrect recommendations<br />
    in <em>settings -&gt; pipeline modifiers</em></li>
<li>configurable restore metadata from image to settings and to params<br />
    in <em>settings -&gt; image metadata</em>  </li>
<li><strong>API</strong>  </li>
<li>new <a href="https://github.com/vladmandic/sdnext/wiki/API">API Wiki</a>  </li>
<li>server will now maintain job history which can be queried via API<br />
    so you can check previous jobs as well as request any previously generated images/videos  </li>
<li>history endpoint: <code>/sdapi/v1/history?id={id}</code>  </li>
<li>download endpoint: <code>/file={filename}</code>  </li>
<li>progress api <code>/sdapi/v1/progress</code> now also include task id in the response  </li>
<li><strong>Other</strong></li>
<li><strong>OMI</strong> support for sd15/sdxl omi-standard LoRAs</li>
<li>text/image/control/video pipeline vs task compatibility check  </li>
<li><strong>HiDream-I1, FLUX.1, SD3.x</strong> add HF gated access auth check  </li>
<li><strong>HiDream-I1</strong> LoRA support<br />
    currently limited to diffusers-only LoRAs, CivitAI LoRA support is TBD  </li>
<li><strong>HiDream-I1</strong> add LLM info to image metadata  </li>
<li>add <code>model_type</code> as option for image filename pattern  </li>
<li>add <strong>UniPC FlowMatch</strong> scheduler  </li>
<li>add <strong>LCM FlowMatch</strong> scheduler  </li>
<li>networks: set which networks to skip when scanning civitai<br />
    in <em>settings -&gt; networks -&gt; network scan</em><br />
    comma-separate list of regex patterns to skip  </li>
<li>ui display reference models with subdued color  </li>
<li>xyz grid support bool  </li>
<li>do not force gc at end of processing  </li>
<li>add <code>SD_LORA_DUMP</code> env variable for dev/diag to dump lora/model keys  </li>
<li><strong>Wiki</strong>  </li>
<li>new <em>Nunchaku</em>, <em>API</em> pages  </li>
<li>updated <em>HiDream, Quantization, NNCF, Video, Docker, WSL, ZLUDA</em> pages  </li>
<li><strong>Fixes</strong></li>
<li>HunyuanVideo-I2V with latest transformers  </li>
<li>NNCF with TE-only quant  </li>
<li>ONNX init fix  </li>
<li>Quanto with TE/LLM quant  </li>
<li>HiDream live preview  </li>
<li>FLUX.1 controlnet i2i  </li>
<li>SD35 InstantX IP-adapter  </li>
<li>OpenVINO device selection</li>
<li>xyz grid restore settings  </li>
<li>config save unnecessary keys  </li>
<li>recursive wildcards  </li>
<li>extension installer handling of PYTHONPATH  </li>
<li>trace logging  </li>
<li>api logging  </li>
<li>sd/sdxl-inpaint model loader  </li>
<li>settings list display only visible items  </li>
<li>checkpoint match when searching for model to load  </li>
<li>video vae selection load correct vae</li>
</ul>
<h2 id="update-for-2025-04-12">Update for 2025-04-12</h2>
<h3 id="highlights-for-2025-04-12">Highlights for 2025-04-12</h3>
<p>Last release was just over a week ago and here we are again with another update as a new high-end image model, <a href="https://github.com/vladmandic/sdnext/wiki/HiDream">HiDream-I1</a> jumped out and generated a lot of buzz!<br />
There are quite a few other performance and quality-of-life improvements in this release and 40 commits, so please take a look at the full <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a>  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-04-12">Details for 2025-04-12</h3>
<ul>
<li><strong>Models</strong>  </li>
<li><a href="https://huggingface.co/HiDream-ai/HiDream-I1-Full">HiDream-I1</a> in fast, dev and full variants!<br />
    new absolutely massive image generative foundation model with <strong>17B</strong> parameters and 4 text-encoders with additional <strong>8.3B</strong> parameters<br />
    simply select from <em>networks -&gt; models -&gt; reference</em><br />
    due to size (over 25B params in 58GB), offloading and on-the-fly quantization are pretty much a necessity<br />
    see <a href="https://github.com/vladmandic/sdnext/wiki/HiDream">HiDream Wiki page</a> for details  </li>
<li><strong>Features</strong>  </li>
<li>Custom model loader<br />
      can be used to load any known diffusion model with default or custom model components<br />
      in models -&gt; custom tab<br />
      see docs for details: <a href="https://vladmandic.github.io/sdnext-docs/Loader/">https://vladmandic.github.io/sdnext-docs/Loader/</a>  <ul>
<li>Pipe: <a href="https://github.com/zacheryvaughn/softfill-pipelines">SoftFill</a>  </li>
</ul>
</li>
<li><strong>Caching</strong>  </li>
<li>add <code>TeaCache</code> support to <em>Flux, CogVideoX, Mochi, LTX</em>  </li>
<li>add <code>FasterCache</code> support to <em>WanAI, LTX</em> (other video models already supported)  </li>
<li>add <code>PyramidAttentionBroadcast</code> support to <em>WanAI, LTX</em> (other video models already supported)  </li>
<li><strong>UI</strong>  </li>
<li>client polling speeds up and slows down depending if client page is visible or not<br />
    client polling does not ask for live preview if page is not visible<br />
    significantly reduces server load if you hide or minimize the page  </li>
<li>progress: use batch-count for progress  </li>
<li>grid: add of max-rows and max-columns in settings to control grid format  </li>
<li>gallery: add max-columns in settings for gradio gallery components  </li>
<li><strong>Other</strong>  </li>
<li>ZLUDA: add more GPUs to recognized list<br />
    select in scripts, available for sdxl in inpaint model  </li>
<li>LoRA: add option to force-reload LoRA on every generate  </li>
<li>settings: add <strong>Model options</strong> sections as placeholder for per-model settings</li>
<li>video: update <em>LTXVideo-0.9.5</em> pipeline  </li>
<li>te loader: allow free-form input in which case sdnext will attempt to load it as hf repo  </li>
<li>diag: add get-server-status to UI generate context menu  </li>
<li>diag: memory monitor detect gpu swapping  </li>
<li>use <a href="https://huggingface.co/blog/xet-on-the-hub">hf-xet</a> for huggingface downloads where possible  </li>
<li>quant: update &amp; fix <code>optimum-quanto</code> for transformers  </li>
<li>quant: update &amp; fix <code>torchao</code>  </li>
<li>model load: new setting for model load initial device map<br />
    can be used to force gpu vs cpu when loading model to avoid oom before model offloading is even activated after load  </li>
<li><strong>Changes</strong>  </li>
<li>params: Reset default guidance-rescale from 0.7 to 0.0  </li>
<li>progress: add additional fields to progress API  </li>
<li><strong>Fixes</strong>  </li>
<li>styles: resize and bring quick-ui to forward on hover  </li>
<li>LoRA: obey configured device when performing calculations  </li>
<li>ZLUDA: startup issues  </li>
<li>offload: balanced offload remove non-blocking move op  </li>
<li>logging: debug causes invalid import  </li>
<li>logging: cleanup  </li>
<li>ROCm: flash attention repo with navi rotary fix  </li>
<li>prompt: prompt scheduling with te caching  </li>
<li>ui: progress allow for longer timeouts  </li>
<li>internal: cleanup defined pipelines</li>
</ul>
<h2 id="update-for-2025-04-03">Update for 2025-04-03</h2>
<h3 id="highlights-for-2025-04-03">Highlights for 2025-04-03</h3>
<p>Time for another major release with ~120 commits and <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> that spans several pages!</p>
<p><em>Highlights?</em><br />
Video...Brand new Video processing module with support for all latest models: <strong>WAN21, Hunyuan, LTX, Cog, Allegro, Mochi1, Latte1</strong> in both <em>T2V</em> and <em>I2V</em> workflows<br />
And combined with <em>on-the-fly quantization</em>, support for <em>Local/Tiny/Remote</em> VAE, acceleration modules such as <em>FasterCache or PAB</em>, and more!<br />
Models...And support for new models: <strong>CogView-4</strong>, <strong>SANA 1.5</strong>,  </p>
<p><em>Plus...</em><br />
- New <strong>Prompt Enhance</strong> using LLM,
- New pipelines such as <strong>InfiniteYou</strong><br />
- New <strong>CLiP</strong> models, improvements to <strong>remote VAE</strong>, additional wiki/docs/guides<br />
- More quantization options and granular control<br />
- Pretty big performance updates to a) Any model using DiT based architecture due to new caching methods, b) ZLUDA with new attention methods, c) LoRA with much lower memory usage  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2025-04-03">Details for 2025-04-03</h3>
<ul>
<li><strong>Video tab</strong>  </li>
<li>see <a href="https://github.com/vladmandic/sdnext/wiki/Video">Video Wiki</a> for details!  </li>
<li>new top-level tab, replaces previous <em>video</em> script in text/image tabs<br />
    old scripts are still present, but will be removed in the future  </li>
<li>support for all latest models:  <ul>
<li><a href="https://huggingface.co/Tencent/HunyuanVideo">Hunyuan</a>: <em>HunyuanVideo, FastHunyuan, SkyReels</em> | <em>T2V, I2V</em>  </li>
<li><a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B-Diffusers">WAN21</a>: <em>1.3B, 14B</em> | <em>T2V, I2V</em>  </li>
<li><a href="https://huggingface.co/Lightricks/LTX-Video">LTXVideo</a>: <em>0.9.0, 0.9.1, 0.9.5</em> | <em>T2V, I2V</em>  </li>
<li><a href="https://huggingface.co/THUDM/CogVideoX-5b">CogVideoX</a>: <em>2B, 5B</em> | <em>T2V, I2V</em>  </li>
<li><a href="https://huggingface.co/rhymes-ai/Allegro">Allegro</a>: <em>T2V</em>  </li>
<li><a href="https://huggingface.co/genmo/mochi-1-preview">Mochi1</a>: <em>T2V</em>  </li>
<li><a href="https://huggingface.co/maxin-cn/Latte-1">Latte1</a>: *T2V  </li>
</ul>
</li>
<li>decoding:  <ul>
<li><strong>Default</strong>: use vae from model  </li>
<li><strong>Tiny VAE</strong>: support for <em>Hunyuan, WAN, Mochi</em>  </li>
<li><strong>Remote VAE</strong>: support for <em>Hunyuan</em>  </li>
</ul>
</li>
<li><strong>LoRA</strong><ul>
<li>support for <em>Hunyuan, LTX, WAN, Mochi, Cog</em>  </li>
<li>add option to apply LoRA directly on GPU or use CPU first in low-memory scenarios  </li>
<li>improve metadata and preview parallel fetch  </li>
<li>support for mp4 so first frame is extracted as used as lora preview  </li>
</ul>
</li>
<li>additional key points:  <ul>
<li>all models are auto-downloaded upon first use<br />
  uses <em>system paths -&gt; huggingface</em> folder  </li>
<li>support for many video types  </li>
<li>optional video interpolation while creating video files  </li>
<li>optional video preview in ui<br />
  present if video output is selected  </li>
<li>support for balanced offloading and model offloading<br />
  uses system settings  </li>
<li>on-the-fly quantization: <em>BnB, Quanto, TorchAO</em><br />
  uses system settings, granular for <em>transformer</em> and <em>text-encoder</em> separately  </li>
<li>different video models support different video resolutions, frame counts, etc.<br />
  and may require specific settings - see model links for details  </li>
<li>see <em>ToDo/Limitations</em> section for additional notes  </li>
</ul>
</li>
<li><strong>Models &amp; Pipelines</strong>  </li>
<li><a href="https://huggingface.co/THUDM/CogView4-6B">THUDM CogView 4</a> <strong>6B</strong> variant<br />
    new foundation model for image generation based o GLM-4 text encoder and a flow-based diffusion transformer<br />
    fully supports offloading and on-the-fly quantization<br />
    simply select from <em>networks -&gt; models -&gt; reference</em><br />
<em>note</em> cogview4 is compatible with flowmatching samplers  </li>
<li><a href="https://huggingface.co/Efficient-Large-Model/SANA1.5_4.8B_1024px_diffusers">NVLabs SANA 1.5</a> in <strong>1.6B</strong>, <strong>4.8B</strong> and <a href="https://huggingface.co/Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers">Sprint</a> variations<br />
    big update to previous SANA model<br />
    fully supports offloading and on-the-fly quantization<br />
    simply select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://github.com/bytedance/InfiniteYou/">ByteDance InfiniteYou</a>: Flexible Photo Recrafting While Preserving Your Identity<br />
    face-transfer model for FLUX.1<br />
    select from <em>Scripts -&gt; InfiniteYou</em><br />
    its large, ~12GB on top of FLUX.1 base model so make sure you have offloading and quantization setup<br />
<em>note</em> model will be auto-downloaded on first use  </li>
<li>New <a href="https://huggingface.co/zer0int/CLIP-Registers-Gated_MLP-ViT-L-14">zer0int CLiP-L</a> models:<br />
    download text encoders into folder set in settings -&gt; system paths -&gt; text encoders (default is <em>models/Text-encoder</em>)<br />
    load using <em>settings -&gt; text encoder</em><br />
<em>tip</em>: add <em>sd_text_encoder</em> to your <em>settings -&gt; user interface -&gt; quicksettings</em> list to have it appear at the top of the ui  </li>
<li><strong>Prompt Enhance</strong>  </li>
<li>see <a href="https://github.com/vladmandic/sdnext/wiki/Prompt-Enhance">Prompt Enhance Wiki</a> for details!  </li>
<li>new built-in extension available in text/image/control tabs  </li>
<li>can be used to manually or automatically enhance prompts using LLM  </li>
<li>built-in presets for <strong>Gemma-3, Qwen-2.5, Phi-4, Llama-3.2, SmolLM2, Dolphin-3</strong>  </li>
<li>support for custom models<br />
    load any models hosted on huggingface<br />
    load either model in huggingface format or <code>gguf</code> format<br />
<em>note</em>: any hf model in <code>transformers.AutoModelForCausalLM</code> standard should work<br />
<em>note</em>: not all model architecture are supported for <code>gguf</code> format  </li>
<li>models are auto-downloaded on first use  </li>
<li>support quantization and offloading  </li>
<li>auto-detect censored output  </li>
<li>debug using <code>SD_LLM_DEBUG=true</code> env variable  </li>
<li><strong>Acceleration</strong>  </li>
<li>Support for most DiT-based models, for example: <em>FLUX.1, SD35, Hunyuan, Mochi, Latte, Allegro, Cog</em>  </li>
<li>Enable and configure in <em>Settings -&gt; Pipeline modifiers</em>  </li>
<li><a href="https://huggingface.co/papers/2410.19355">FasterCache</a>  </li>
<li><a href="https://huggingface.co/papers/2408.12588">PyramidAttentionBroadcast</a>  </li>
<li><strong>Remote VAE</strong>  </li>
<li>add support for remote vae encode in addition to remote vae decode  </li>
<li>used by <em>img2img, inpaint, hires, detailer</em>  </li>
<li>remote vae encode is disabled by default, you can enable it in <em>settings -&gt; variable auto-encoder</em>  </li>
<li>add remote vae info to metadata, thanks @iDeNoh  </li>
<li>remote vae use <code>scaling_factor</code> and <code>shift_factor</code>  </li>
<li><strong>Caption/VLM</strong>  </li>
<li><a href="https://huggingface.co/google/gemma-3-4b-it">Google Gemma 3</a> 4B<br />
    simply select from list of available models in caption tab  </li>
<li><a href="https://huggingface.co/ByteDance/Sa2VA-1B">ByteDance/Sa2VA</a> 1B, 4B<br />
    simply select from list of available models in caption tab  </li>
<li>add option to set system prompt for vlm models that support it: <em>Gemma, Smol, Qwen</em>  </li>
<li><a href="https://github.com/vladmandic/sd-extension-nudenet/">NudeNet</a> extension updates  </li>
<li>add detection of prompt language and alphabet and filter based on those values  </li>
<li>add image policy checks using <code>LlavaGuard</code> VLM to detect policy violations (and reasons)<br />
    against top-10 standard harmful content categories  </li>
<li>add banned words/expressions check against prompt variations  </li>
<li><strong>LoRA</strong></li>
<li>enable memory cache by default  </li>
<li>significantly reduce memory usage  </li>
<li>improve performance  </li>
<li>improve detection of lora changes  </li>
<li>unload lora only when changes are detected  </li>
<li>refactor code for modularity  </li>
<li><strong>IPEX</strong>  </li>
<li>add <code>--upgrade</code> to torch_command when using <code>--use-nightly</code>  </li>
<li>add xpu to profiler  </li>
<li>fix untyped_storage, torch.eye and torch.cuda.device ops  </li>
<li>fix torch 2.7 compatibility  </li>
<li>fix performance with balanced offload  </li>
<li>fix triton and torch.compile  </li>
<li><strong>ROCm</strong></li>
<li>add <code>--upgrade</code> to torch_command when using <code>--use-nightly</code>  </li>
<li>disable fp16 for gfx1102 (rx 7600 and rx 7500 series) gpus  </li>
<li><strong>ZLUDA</strong>  </li>
<li><a href="https://github.com/vladmandic/sdnext/wiki/ZLUDA#how-to-enable-triton">triton for ZLUDA v3.9.2</a>  <ul>
<li><code>torch.compile</code> is now available  </li>
<li>Flash Attention 2 is now available  </li>
</ul>
</li>
<li><strong>Other</strong>  </li>
<li><strong>Command line</strong> new option <code>--monitor PERIOD</code> to monitor CPU and GPU memory ever n seconds  </li>
<li><strong>Upscale</strong> new <a href="https://huggingface.co/Heasterian/AsymmetricAutoencoderKLUpscaler_v2">asymmetric vae v2</a> upscaling method  </li>
<li><strong>Upscale</strong> new experimental support for <code>libvips</code> upscaling  </li>
<li><strong>Quantization</strong> add support for <code>optimum-quanto</code> on-the-fly quantization during load for all models<br />
    note: previous method for quanto is still valid and is noted in settings as post-load quantization  </li>
<li><strong>Quantization</strong> add support to <strong>CogView-3Plus</strong>  </li>
<li><strong>Default values</strong> rename vae, unet and text-encoder settings <em>None</em> to <em>Default</em> to avoid confusion  </li>
<li><strong>Detailer</strong>: add <em>renoise</em> option to increase/decrease noise during detailer pass<br />
    which can help with improving level of details</li>
<li><strong>CLI</strong>: add <code>cli/api-grid.py</code> which can generate grids using params-from-file for x/y axis  </li>
<li><strong>Samplers</strong> add ability to set sigma adjustment for each sampler  </li>
<li><strong>ModernUI</strong> updates  </li>
<li><strong>CSS</strong> updates  </li>
<li><strong>Video</strong> interpolate do not skip duplicate frames  </li>
<li><strong>Settings UI</strong> full refactor  </li>
<li><strong>Settings UI</strong> vertical/dirty indicator restores to default setting instead to previous value  </li>
<li>update <code>diffusers</code> and other requirements  </li>
<li><strong>Wiki/Docs</strong>  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/Models">Models</a> info  </li>
<li>new <a href="https://github.com/vladmandic/sdnext/wiki/Video">Video</a> guide  </li>
<li>new <a href="https://github.com/vladmandic/sdnext/wiki/Caption">Caption</a> guide  </li>
<li>new <a href="https://github.com/vladmandic/sdnext/wiki/VAE">VAE</a> guide  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/SD3">SD3</a> guide  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/ZLUDA">ZLUDA</a> guide  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/OpenVINO">OpenVINO</a> guide  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/AMD-ROCm">AMD-ROCm</a> guide  </li>
<li>updated <a href="https://github.com/vladmandic/sdnext/wiki/Intel-ARC">Intel-ARC</a> guide  </li>
<li><strong>Fixes</strong>  </li>
<li>fix installer not starting when older version of <code>rich</code> is installed  </li>
<li>fix circular imports when debug flags are enabled  </li>
<li>fix cuda errors with <em>directml</em>  </li>
<li>fix memory stats not displaying the ram usage  </li>
<li>fix <strong>RunPod</strong> memory limit reporting  </li>
<li>fix flux ipadapter with start/stop values  </li>
<li>fix progress api <code>eta_relative</code>  </li>
<li>fix <code>insightface</code> loader  </li>
<li>fix remove vae for flux.1  </li>
<li>guard against git returining invalid timestamp  </li>
<li>fix hires with latent upscale  </li>
<li>fix legacy diffusion latent upscalers  </li>
<li>fix upscaler selection in postprocessing  </li>
<li>fix sd35 with batch processing  </li>
<li>fix extra networks cover and inline views  </li>
<li>fix token counter error style with modernui  </li>
<li>fix sampler metadata when using default sampler  </li>
<li>fix paste incorrect float to int cast  </li>
<li>fix server restart from ui  </li>
<li>fix style apply params  </li>
<li>fix <code>wan22-i2v</code>  </li>
<li>do not allow edit of built-in styles  </li>
<li>improve lora compatibility with balanced offload  </li>
</ul>
<h2 id="update-for-2025-02-28">Update for 2025-02-28</h2>
<p>Primarily a hotfix/service release plus few UI improvements and one exciting new feature: Remote-VAE!</p>
<ul>
<li><strong>Remote Decode</strong>  </li>
<li>final step of image generate, VAE decode, is by far the most memory intensive operation and can easily result in out-of-memory errors<br />
    what can be done? Well, <em>Huggingface</em> is now providing <em>free-of-charge</em> <strong>remote-VAE-decode</strong> service!  </li>
<li>how to use? previous <em>Full quality</em> option in UI is replaced with VAE type selector: <em>Full, Tiny, Remote</em><br />
    currently supports SD15, SDXL and FLUX.1 with more models expected in the near future<br />
    depending on your bandwidth select mode in <em>settings -&gt; vae -&gt; raw/png/jpg</em><br />
    if remote processing fails SD.Next will fallback to using normal VAE decode process<br />
<em>privacy note</em>: only passed item is final latent itself without any user or generate information and latent is not stored in the cloud  </li>
<li><strong>UI</strong></li>
<li>modern ui reorg main tab<br />
    improve styling, improve scripts/extensions interface and separate ipadapters  </li>
<li>additional ui hints  </li>
<li><strong>Other</strong>  </li>
<li>add <code>--extensions-dir</code> cli arg and <code>SD_EXTENSIONSDIR</code> env variable to specify extensions directory  </li>
<li>update <code>zluda==3.9.0</code></li>
<li><strong>Fixes</strong>  </li>
<li>skip trying to register legacy/incompatibile extensions in control ui  </li>
<li>add additional scripts/extensions callbacks  </li>
<li>remove ui splash screen on auth fail  </li>
<li>log full config path, full log path, system name, extensions path</li>
<li>zluda hotfixes  </li>
<li>zluda force sync  </li>
<li>fix torch import on compile  </li>
<li>infotext parser force delimiter before params  </li>
<li>handle pipeline class switch errors  </li>
<li>improve extensions options compatibility  </li>
<li>fix flux on ipex  </li>
<li>disable fp64 emulation on ipex  </li>
</ul>
<h2 id="update-for-2025-02-18">Update for 2025-02-18</h2>
<h3 id="highlight-for-2025-02-18">Highlight for 2025-02-18</h3>
<p>We're back with another update with nearly 100 commits!<br />
- Starting with massive UI update with full <a href="https://vladmandic.github.io/sdnext-docs/Locale/">localization</a> for 8 languages<br />
  and 100+ new <a href="https://vladmandic.github.io/sdnext-docs/Hints/">hints</a><br />
- Big update to <a href="https://vladmandic.github.io/sdnext-docs/Docker/">Docker</a> containers<br />
  with support for all major compute platforms<br />
- A lot of <a href="https://vladmandic.github.io/sdnext-docs/Outpaint/">outpainting</a> goodies<br />
- Support for new models: <a href="https://github.com/Alpha-VLLM/Lumina-Image-2.0">AlphaVLLM Lumina 2</a> and <a href="https://huggingface.co/ostris/Flex.1-alpha">Ostris Flex.1-Alpha</a><br />
- And new <strong>Mixture-of-Diffusers</strong> regional prompting &amp; tiling pipeline<br />
- Follow-up to last weeks <strong>interrogate/captioning</strong> rewrite<br />
  now with redesigned captioning UI, batch support, and much more<br />
  plus <strong>JoyTag</strong>, <strong>JoyCaption</strong>, <strong>PaliGemma</strong>, <strong>ToriiGate</strong>, <strong>Ovis2</strong> added to list of supported models<br />
- Some changes to <strong>prompt parsing</strong> to allow more control as well as<br />
  more flexibility when mouting SDNext server to custom URL<br />
- Of course, cumulative fixes...  </p>
<p><em>...and more</em> - see <a href="https://github.com/vladmandic/sdnext/blob/dev/CHANGELOG.md">changelog</a> for full details!  </p>
<h3 id="details-for-2025-02-20">Details for 2025-02-20</h3>
<ul>
<li><strong>User Interface</strong>  </li>
<li><strong>Hints</strong>  <ul>
<li>added/updated 100+ ui hints!  </li>
<li><a href="https://vladmandic.github.io/sdnext-docs/Hints/">hints</a> documentation and contribution guide  </li>
</ul>
</li>
<li><strong>Localization</strong>  <ul>
<li>full ui localization!<br />
<em>english, croatian, spanish, french, italian, portuguese, chinese, japanese, korean, russian</em>  </li>
<li>set in <em>settings -&gt; user interface -&gt; language</em>  </li>
<li><a href="https://vladmandic.github.io/sdnext-docs/Locale/">localization</a> documentation  </li>
</ul>
</li>
<li><strong>UI</strong>  <ul>
<li>force browser cache-invalidate on page load  </li>
<li>configurable request timeout  </li>
<li>modernui improve gallery styling  </li>
<li>modernui improve networks styling  </li>
<li>modernui support variable card size  </li>
</ul>
</li>
<li><strong>Docs</strong>  </li>
<li>New <a href="https://vladmandic.github.io/sdnext-docs/Outpaint/">Outpaint</a> step-by-step guide  </li>
<li>Updated <a href="https://github.com/vladmandic/sdnext/wiki/Docker">Docker</a> guide<br />
    includes build and publish and both local and cloud examples  </li>
<li><strong>Models</strong>  </li>
<li><a href="https://github.com/Alpha-VLLM/Lumina-Image-2.0">AlphaVLLM Lumina 2</a><br />
    new foundation model for image generation based o Gemma-2-2B text encoder and a flow-based diffusion transformer<br />
    fully supports offloading and on-the-fly quantization<br />
    simply select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li><a href="https://huggingface.co/ostris/Flex.1-alpha">Ostris Flex.1-Alpha</a><br />
    originally based on <em>Flux.1-Schnell</em>, but retrained and with different architecture<br />
    result is model smaller than <em>Flux.1-Dev</em>, but with similar capabilities<br />
    fully supports offloading and on-the-fly quantization<br />
    simply select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li><strong>Functions</strong>  </li>
<li><a href="https://huggingface.co/posts/elismasilva/251775641926329">Mixture-of-Diffusers</a><br />
    Regional tiling type of a solution for SDXL models<br />
    select from <em>scripts -&gt; mixture of diffusers</em>  </li>
<li>[Automatic Color Inpaint]<br />
    Automatically creates mask based on selected color and triggers inpaint<br />
    simply select in <em>scripts -&gt; automatic color inpaint</em> when in img2img mode  </li>
<li><a href="https://github.com/microsoft/RAS">RAS: Region-Adaptive Sampling</a> <em>experimental</em><br />
    Speeds up SD3.5 models by sampling only regions of interest<br />
    Enable in <em>settings -&gt; pipeline modifiers -&gt; ras</em>  </li>
<li><strong>Interrogate/Captioning</strong>  </li>
<li>Redesigned captioning UI<br />
    split from Process tab into separate tab<br />
    split <code>clip</code> vs <code>vlm</code> models processing<br />
    direct <em>send-to</em> buttons on all tabs: txt/img/ctrl-&gt;process/caption, process/caption-&gt;txt/img/ctrl  </li>
<li>Advanced params:
    VLM: <em>max-tokens, num-beams, temperature, top-k, top-p, do-sample</em><br />
    CLiP: <em>min-length, max-length, chunk-size, min-flavors, max-flavors, flavor-count, num-beams</em><br />
    params are auto-saved in <code>config.json</code> and used when using quick interrogate<br />
    params that are set to 0 mean use model defaults  </li>
<li>Batch processing: VLM and CLiP<br />
    for example, can be used to caption your training dataset in one go<br />
    add option to append to captions file, can be used to run multiple captioning models in sequence<br />
    add option to run recursively on all subfolders<br />
    add progress bar  </li>
<li>Add additional VLM models:<br />
<a href="https://huggingface.co/fancyfeast/joytag">JoyTag</a><br />
<a href="https://huggingface.co/fancyfeast/llama-joycaption-alpha-two-hf-llava">JoyCaption 2</a><br />
<a href="https://huggingface.co/google/paligemma2-3b-pt-224">Google PaliGemma 2</a> 3B<br />
<a href="https://huggingface.co/Minthy/ToriiGate-v0.4-7B">ToriiGate 0.4</a> 7B<br />
<a href="https://huggingface.co/AIDC-AI/Ovis2-1B">AIDC Ovis2</a> 1B/2B/4B  </li>
<li><em>Note</em> some models require <code>flash-attn</code> to be installed<br />
    due to binary/build dependencies, it should not be done automatically,<br />
    see <a href="https://github.com/Dao-AILab/flash-attention">flash-attn</a> for installation instructions  </li>
<li><strong>Docker</strong>  </li>
<li>updated <strong>CUDA</strong> receipe to <code>torch==2.6.0</code> with <code>cuda==12.6</code> and add prebuilt image  </li>
<li>added <strong>ROCm</strong> receipe and prebuilt image  </li>
<li>added <strong>IPEX</strong> receipe and add prebuilt image  </li>
<li>added <strong>OpenVINO</strong> receipe and prebuilt image  </li>
<li><strong>System</strong>  </li>
<li>improve <strong>python==3.12</strong> compatibility  </li>
<li><strong>Torch</strong>  <ul>
<li>for <strong>zluda</strong> set default to <code>torch==2.6.0+cu118</code>  </li>
<li>for <strong>openvino</strong> set default to <code>torch==2.6.0+cpu</code>  </li>
</ul>
</li>
<li><strong>OpenVINO</strong>  <ul>
<li>update to <code>openvino==2025.0.0</code>  </li>
<li>improve upscaler compatibility  </li>
<li>enable upscaler compile by default  </li>
<li>fix shape mismatch errors on too many resolution changes  </li>
</ul>
</li>
<li><strong>ZLUDA</strong>  <ul>
<li>update to <code>zluda==3.8.8</code>  </li>
</ul>
</li>
<li><strong>Other</strong>  </li>
<li><strong>Asymmetric tiling</strong><br />
    allows for configurable image tiling for x/y axis separately<br />
    enable in <em>scripts -&gt; asymmetric tiling</em><br />
<em>note</em>: traditional symmetric tiling is achieved by setting circular mode for both x and y  </li>
<li><strong>Styles</strong><br />
    ability to save and/or restore prompts before or after parsing of wildcards<br />
    set in <em>settings -&gt; networks -&gt; styles</em>  </li>
<li><strong>Access tokens</strong><br />
    persist <em>models -&gt; hugginface -&gt; token</em><br />
    persist <em>models -&gt; civitai -&gt; token</em>  </li>
<li>global switch to lancosz method for all interal resize ops and bicubic for interpolation ops  </li>
<li><strong>Text encoder</strong><br />
    add advanced per-model options for text encoder<br />
    set in <em>settings -&gt; text encoder -&gt; Optional</em>  </li>
<li><strong>Subpath</strong><br />
    allow setting additional mount subpath over which server url will be accessible<br />
    set in <em>settings -&gt; user interface</em>  </li>
<li><strong>Prompt parsing</strong><br />
    better handling of prompt parsing when using masking char <code>\</code>  </li>
<li><strong>Fixes</strong>  </li>
<li>update torch nightly urls  </li>
<li>docs/wiki always use relative links  </li>
<li>ui use correct timezone for log display  </li>
<li>ui improve settings search behavior  </li>
<li>ui log scroll to bottom  </li>
<li>ui fix send to inpaint/sketch  </li>
<li>modernui add control init image toggle  </li>
<li>modernui fix sampler advanced options  </li>
<li>outpaint fixes  </li>
<li>validate output before hires/refine  </li>
<li>scheduler fix sigma index out of bounds  </li>
<li>force pydantic version reinstall/reload  </li>
<li>multi-unit when using controlnet-union  </li>
<li>pulid with hidiffusion  </li>
<li>api: stricter access control  </li>
<li>api: universal handle mount subpaths  </li>
</ul>
<h2 id="update-for-2025-02-05">Update for 2025-02-05</h2>
<ul>
<li>refresh dev/master branches</li>
</ul>
<h2 id="update-for-2025-02-04">Update for 2025-02-04</h2>
<h3 id="highlights-for-2025-02-04">Highlights for 2025-02-04</h3>
<p>Just one week after latest release and what a week it was with over 50 commits!  </p>
<p><em>What's New?</em><br />
- Rehosted core repo to new <a href="https://github.com/vladmandic/sdnext">home</a><br />
- Switched to using <code>torch==2.6.0</code> and added support for <code>nightly</code> builds required for <strong>nVidia Blackwell</strong> GPUs<br />
- Completely new <strong>interrogate/captioning</strong>, now supporting 150+ <strong>OpenCLiP</strong> models and 20+ built-in <strong>VLMs</strong><br />
- Support for <strong>new VLMs</strong>, New SOTA <strong>background removal</strong><br />
- Other: <em>torch tunable ops, extra networks search/filter, balanced offload, prompt parser, configurable tracebacks, etc.</em><br />
- Cumulative fixes...  </p>
<h3 id="details-for-2025-02-04">Details for 2025-02-04</h3>
<ul>
<li><strong>GitHub</strong></li>
<li>rename core repo from <a href="https://github.com/vladmandic/automatic">https://github.com/vladmandic/automatic</a> to <a href="https://github.com/vladmandic/sdnext">https://github.com/vladmandic/sdnext</a><br />
    old repo url should automatically redirect to new one for seamless transition and in-place upgrades<br />
    all internal links have been updated<br />
    wiki content and docs site have been updated  </li>
<li><strong>Docs</strong>:</li>
<li>Updated <a href="https://github.com/vladmandic/automatic/wiki/Debug">Debugging guide</a>  </li>
<li><strong>Torch</strong>:</li>
<li>for <strong>cuda</strong> set default to <code>torch==2.6.0+cu126</code><br />
    for <strong>rocm</strong> set default to <code>torch==2.6.0+rocm6.2.4</code><br />
    for <strong>ipex</strong> set default to <code>torch==2.6.0+xpu</code><br />
<em>note</em>: to avoid disruptions sdnext does not perform torch install during in-place upgrades<br />
    to force torch upgrade, either start with new installation or use <code>--reinstall</code> flag  </li>
<li>support for torch <strong>nightly</strong> builds and nvidia <strong>blackwell</strong> gpus!<br />
    use <code>--use-nightly</code> flag to install torch nightly builds<br />
    current defaults to <code>torch==2.7.0+cu128</code> prerelease<br />
<em>note</em>: nightly builds are required for blackwell gpus  </li>
<li>add support for torch <strong>tunable ops</strong>, this can speed up operations by up to <em>10-30%</em> on some platforms<br />
    set in <em>settings -&gt; backend settings -&gt; torch options</em> and <em>settings -&gt; system paths -&gt; tunable ops cache</em>  </li>
<li>add support for stream-loading, this can speed up model loading when models are located on network drives<br />
    set in <em>settings -&gt; models &amp; loading -&gt; model load using streams</em>  </li>
<li>enhanced error logging  </li>
<li><strong>Interrogate/Captioning</strong>  </li>
<li>single interrogate button for every input or output image  </li>
<li>behavior of interrogate configurable in <em>settings -&gt; interrogate</em><br />
    with detailed defaults for each model type also configurable  </li>
<li>select between 150+ <em>OpenCLiP</em> supported models, 20+ built-in <em>VLMs</em>, <em>DeepDanbooru</em>  </li>
<li><strong>VLM</strong>: now that we can use VLMs freely, we've also added support for few more out-of-the-box<br />
<a href="https://huggingface.co/Qwen/Qwen2-VL-2B">Alibaba Qwen VL2</a>, <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct">Huggingface Smol VL2</a>, <a href="https://huggingface.co/Minthy/ToriiGate-v0.4-2B">ToriiGate 0.4</a>  </li>
<li><strong>Postprocess</strong>  </li>
<li>new sota remove background model: <a href="https://huggingface.co/PramaLLC/BEN2">BEN2</a><br />
    select in <em>process -&gt; remove background</em> or enable postprocessing for txt2img/img2img operations  </li>
<li><strong>Other</strong>:</li>
<li><strong>networks</strong>: imporove search/filter and add visual indicators for types  </li>
<li><strong>balanced offload</strong> new defaults: <em>lowvram/4gb min threshold: 0, medvram/8gb min threshold: 0, default min threshold 0.25</em>  </li>
<li><strong>prompt parser</strong>: log stats with tokens, sections and min/avg/max weights  </li>
<li><strong>prompt parser</strong>: add setting to ignore line breaks in prompt<br />
    set in <em>settings -&gt; text encoder -&gt; use line breaks</em>  </li>
<li><strong>visual query</strong>: add list of predefined system prompts  </li>
<li><strong>onnx</strong>: allow manually specifying <code>onnxruntime</code> package
    set env variable <code>ONNXRUNTIME_COMMAND</code> to override default package installation  </li>
<li><strong>nvml cli</strong>: run nvidia-management-lib interrogate from cli<br />
    already available in ui in generate -&gt; right click -&gt; nvidia<br />
    &gt; python modules/api/nvml.py  </li>
<li><strong>Refactor</strong>:</li>
<li>unified trace handler with configurable tracebacks  </li>
<li>refactor interrogate/analyze/vqa code  </li>
<li><strong>Fixes</strong>:  </li>
<li>photomaker with offloading  </li>
<li>photomaker with refine  </li>
<li>detailer with faceid modules  </li>
<li>detailer restore pipeline before run  </li>
<li>fix <code>python==3.9</code> compatibility  </li>
<li>improve <code>python&gt;=3.12.3</code> compatibility</li>
<li>handle invalid <code>triton</code> on Linux  </li>
<li>correct library import order  </li>
<li>update requirements  </li>
<li>calculate dyn atten bmm slice rate  </li>
<li>dwpose update and patch <code>mmengine</code> installer  </li>
<li>ipex device wrapper with adetailer  </li>
<li>openvino error handling  </li>
<li>relax python version checks for rocm  </li>
<li>simplify and improve file wildcard matching  </li>
<li>fix <code>rich</code> version  </li>
<li>add cn active label</li>
</ul>
<h2 id="update-for-2025-01-29">Update for 2025-01-29</h2>
<h3 id="highlights-for-2025-01-29">Highlights for 2025-01-29</h3>
<p>Two weeks since last release, time for update!  </p>
<p><em>What's New?</em><br />
- New <strong>Detailer</strong> functionality including ability to use several new<br />
  face-restore models: <em>RestoreFormer, CodeFormer, GFPGan, GPEN-BFR</em>
- Support for new models/pipelines:<br />
  face-swapper with <strong>Photomaker-v2</strong> and video with <strong>Fast-Hunyuan</strong><br />
- Support for several new optimizations and accelerations:<br />
  Many <strong>IPEX</strong> improvements, native <em>torch fp8</em> support,<br />
  support for <strong>PAB:Pyramid-attention-broadcast</strong>, <strong>ParaAttention</strong> and <strong>PerFlow</strong><br />
- Fully built-in both model <strong>merge weights</strong> as well as model <strong>merge component</strong><br />
  Finally replace that pesky VAE in your favorite model with a fixed one!<br />
- Improved remote access control and reliability as well as running inside containers<br />
- And of course, hotfixes for all reported issues...  </p>
<h3 id="details-for-2025-01-29">Details for 2025-01-29</h3>
<ul>
<li><strong>Contributing</strong>:  </li>
<li>if you'd like to contribute, please see updated <a href="https://github.com/vladmandic/automatic/blob/dev/CONTRIBUTING">contributing</a> guidelines</li>
<li><strong>Model Merge</strong></li>
<li>replace model components and merge LoRAs<br />
    in addition to existing model weights merge support<br />
    now also having ability to replace model components and merge LoRAs<br />
    you can also test merges in-memory without needing to save to disk at all<br />
    and you can also use it to convert diffusers to safetensors if you want<br />
<em>example</em>: replace vae in your favorite model with a fixed one? replace text encoder? etc.<br />
<em>note</em>: limited to sdxl for now, additional models can be added depending on popularity  </li>
<li><strong>Detailer</strong>:  </li>
<li>in addition as standard behavior of detect &amp; run-generate, it can now also run face-restore models  </li>
<li>included models are: <em>CodeFormer, RestoreFormer, GFPGan, GPEN-BFR</em>  </li>
<li><strong>Face</strong>:  </li>
<li>new <a href="https://huggingface.co/TencentARC/PhotoMaker-V2">PhotoMaker v2</a> and reimplemented <a href="https://huggingface.co/TencentARC/PhotoMaker">PhotoMaker v1</a><br />
    compatible with sdxl models, generates pretty good results and its faster than most other methods<br />
    select under <em>scripts -&gt; face -&gt; photomaker</em>  </li>
<li>new <a href="https://github.com/somanchiu/ReSwapper">ReSwapper</a><br />
    todo: experimental-only and unfinished, only noting in changelog for future reference  </li>
<li><strong>Video</strong>  </li>
<li><strong>hunyuan video</strong> support for <a href="https://huggingface.co/FastVideo/FastHunyuan">FastHunyuan</a><br />
    simply select model variant and set appropriate parameters<br />
    recommended: sampler-shift=17, steps=6, resolution=720x1280, frames=125, guidance&gt;6.0  </li>
<li><a href="https://oahzxl.github.io/PAB/">PAB: Pyramid Attention Broadcast</a>  </li>
<li>speed up generation by caching attention results between steps  </li>
<li>enable in <em>settings -&gt; pipeline modifiers -&gt; pab</em>  </li>
<li>adjust settings as needed: wider timestep range means more acceleration, but higher accuracy drop  </li>
<li>compatible with most <code>transformer</code> based models: e.g. flux.1, hunyuan-video, lyx-video, mochi, etc.</li>
<li><a href="https://github.com/chengzeyi/ParaAttention">ParaAttention</a></li>
<li>first-block caching that can significantly speed up generation by dynamically reusing partial outputs between steps  </li>
<li>available for: flux, hunyuan-video, ltx-video, mochi  </li>
<li>enable in <em>settings -&gt; pipeline modifiers -&gt; para-attention</em>  </li>
<li>adjust residual diff threshold to balance the speedup and the accuracy:<br />
    higher values leads to more cache hits and speedups, but might also lead to a higher accuracy drop  </li>
<li><strong>IPEX</strong></li>
<li>enable force attention slicing, fp64 emulation, jit cache  </li>
<li>use the us server by default on linux  </li>
<li>use pytorch test branch on windows  </li>
<li>extend the supported python versions  </li>
<li>improve sdpa dynamic attention  </li>
<li><strong>Torch FP8</strong></li>
<li>uses torch <code>float8_e4m3fn</code> or <code>float8_e5m2</code> as data storage and performs dynamic upcasting to compute <code>dtype</code> as needed  </li>
<li>compatible with most <code>unet</code> and <code>transformer</code> based models: e.g. <em>sd15, sdxl, sd35, flux.1, hunyuan-video, ltx-video, etc.</em><br />
    this is alternative to <code>bnb</code>/<code>quanto</code>/<code>torchao</code> quantization on models/platforms/gpus where those libraries are not available  </li>
<li>enable in <em>settings -&gt; quantization -&gt; layerwise casting</em>  </li>
<li><a href="https://github.com/magic-research/piecewise-rectified-flow">PerFlow</a>  </li>
<li>piecewise rectified flow as model acceleration  </li>
<li>use <code>perflow</code> scheduler combined with one of the available pre-trained <a href="https://huggingface.co/hansyan">models</a>  </li>
<li><strong>Other</strong>:  </li>
<li><strong>upscale</strong>: new <a href="https://huggingface.co/Heasterian/AsymmetricAutoencoderKLUpscaler">asymmetric vae</a> upscaling method</li>
<li><strong>gallery</strong>: add http fallback for slow/unreliable links  </li>
<li><strong>splash</strong>: add legacy mode indicator on splash screen  </li>
<li><strong>network</strong>: extract thumbnail from model metadata if present  </li>
<li><strong>network</strong>: setting value to disable use of reference models  </li>
<li><strong>Refactor</strong>:  </li>
<li><strong>upscale</strong>: code refactor to unify latent, resize and model based upscalers  </li>
<li><strong>loader</strong>: ability to run in-memory models  </li>
<li><strong>schedulers</strong>: ability to create model-less schedulers  </li>
<li><strong>quantization</strong>: code refactor into dedicated module  </li>
<li><strong>dynamic attention sdpa</strong>: more correct implementation and new trigger rate control  </li>
<li><strong>Remote access</strong>:  </li>
<li>perform auth check on ui startup  </li>
<li>unified standard and modern-ui authentication method &amp; cleanup auth logging  </li>
<li>detect &amp; report local/external/public ip addresses if using <code>listen</code> mode  </li>
<li>detect <em>docker</em> enforced limits instead of system limits if running in a container  </li>
<li>warn if using public interface without authentication  </li>
<li><strong>Fixes</strong>:  </li>
<li>non-full vae decode  </li>
<li>send-to image transfer  </li>
<li>sana vae tiling  </li>
<li>increase gallery timeouts  </li>
<li>update ui element ids  </li>
<li>modernui use local font  </li>
<li>unique font family registration  </li>
<li>mochi video number of frames  </li>
<li>mark large models that should offload  </li>
<li>avoid repeated optimum-quanto installation  </li>
<li>avoid reinstalling bnb if not cuda  </li>
<li>image metadata civitai compatibility  </li>
<li>xyz grid handle invalid values  </li>
<li>omnigen pipeline handle float seeds  </li>
<li>correct logging of docker status on logs, thanks @kmscode  </li>
<li>fix omnigen  </li>
<li>fix docker status reporting  </li>
<li>vlm/vqa with moondream2  </li>
<li>rocm do not override triton installation  </li>
<li>port streaming model load to diffusers  </li>
</ul>
<h2 id="update-for-2025-01-15">Update for 2025-01-15</h2>
<h3 id="highlights-for-2025-01-15">Highlights for 2025-01-15</h3>
<p>Two weeks since last release, time for update!<br />
This time a bit shorter highligh reel as this is primarily a service release, but still there is more than few updates<br />
<em>(actually, there are ~60 commits, so its not that tiny)</em>  </p>
<p><em>What's New?"<br />
- Large <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a>/<a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> updates<br />
- New models: </em><em>Allegro Video</em><em>, new pipelines: </em><em>PixelSmith</em><em>, updates: </em><em>Hunyuan-Video</em><em>, </em><em>LTX-Video</em><em>, </em><em>Sana 4k</em><em><br />
- New version for </em><em>ZLUDA</em><em><br />
- New features in </em><em>Detailer</em><em>, </em><em>XYZ grid</em><em>, </em><em>Sysinfo</em><em>, </em><em>Logging</em><em>, </em><em>Schedulers</em><em>, </em><em>Video save/create</em>*<br />
- And a tons of hotfixes...  </p>
<h3 id="details-for-2025-01-15">Details for 2025-01-15</h3>
<ul>
<li><a href="https://vladmandic.github.io/sdnext-docs/">Wiki/Docs</a>:</li>
<li>updated: Detailer, Install, Update, Debug, Control-HowTo, ZLUDA  </li>
<li><a href="https://huggingface.co/rhymes-ai/Allegro">Allegro Video</a>  </li>
<li>optimizations: full offload and quantization support  </li>
<li><em>reference values</em>: width 1280 height 720 frames 88 steps 100 guidance 7.5  </li>
<li><em>note</em>: allegro model is really sensitive to input width/height/frames/steps<br />
    and may result in completely corrupt output if those are not within expected range  </li>
<li><a href="https://github.com/Thanos-DB/Pixelsmith/">PixelSmith</a></li>
<li>available for SD-XL in txt2img and img2img workflows</li>
<li>select from <em>scripts -&gt; pixelsmith</em>  </li>
<li><a href="https://github.com/Tencent/HunyuanVideo">Hunyuan Video</a> LoRA support</li>
<li>example: <a href="https://huggingface.co/Cseti/HunyuanVideo-LoRA-Arcane_Jinx-v1">https://huggingface.co/Cseti/HunyuanVideo-LoRA-Arcane_Jinx-v1</a></li>
<li><a href="https://github.com/Lightricks/LTX-Video">LTX Video</a> framewise decoding  </li>
<li>enabled by default, allows generating longer videos with reduced memory requirements  </li>
<li><a href="https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers">Sana 4k</a>  </li>
<li>new Sana variation with support of directly generating 4k images  </li>
<li>simply select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li>tip: enable vae tiling when generating very large images  </li>
<li><strong>Logging</strong>:</li>
<li>reverted enable debug by default  </li>
<li>updated <a href="https://github.com/vladmandic/automatic/wiki/debug">debug wiki</a>  </li>
<li>sort logged timers by duration  </li>
<li>allow min duration env variable for timers: <code>SD_MIN_TIMER=0.1</code> (default)  </li>
<li>update installer messages  </li>
<li><strong>Refactor</strong>:</li>
<li>refactored progress monitoring, job updates and live preview  </li>
<li>improved metadata save and restore  </li>
<li>startup tracing and optimizations  </li>
<li>threading load locks on model loads  </li>
<li>refactor native vs legacy model loader  </li>
<li>video save/create</li>
<li><strong>Schedulers</strong>:</li>
<li><a href="https://github.com/RedAIGC/Target-Driven-Distillation">TDD</a> new super-fast scheduler that can generate images in 4-8 steps<br />
    recommended to use with <a href="https://huggingface.co/RED-AIGC/TDD/tree/main">TDD LoRA</a>  </li>
<li><strong>Detailer</strong>:</li>
<li>add explicit detailer prompt and negative prompt  </li>
<li>add explicit detailer steps setting  </li>
<li>move steps, strength, prompt, negative from settings into ui params  </li>
<li>set/restore detailer metadata  </li>
<li>new <a href="https://github.com/vladmandic/automatic/wiki/Detailer">detailer wiki</a></li>
<li><strong>Preview</strong></li>
<li>since different TAESD versions produce different results and latest is not necessarily greatest<br />
    you can choose TAESD version in settings -&gt; live preview<br />
    also added is support for another finetuned version of TAESD <a href="https://huggingface.co/cqyan/hybrid-sd-tinyvae-xl">Hybrid TinyVAE</a>  </li>
<li><strong>Video</strong>  </li>
<li>all video create/save code is now unified  </li>
<li>add support for video formats: GIF, PNG, MP4/MP4V, MP4/AVC1, MP4/JVT3, MKV/H264, AVI/DIVX, AVI/RGBA, MJPEG/MJPG, MPG/MPG1, AVR/AVR1</li>
<li><em>note</em>: video format support is platform dependent and not all formats may be available on all platforms</li>
<li><em>note</em>: avc1 and h264 need custom opencv due to oss licensing issues  </li>
<li><strong>ZLUDA</strong> v3.8.7  </li>
<li>new runtime compiler implementation: complex types, JIT are now available  </li>
<li>fast fourier transformation is implemented  </li>
<li>experimental BLASLt support via nightly build  <ul>
<li>set <code>ZLUDA_NIGHTLY=1</code> to install nightly ZLUDA: newer torch such as 2.4.x (default) and 2.5.x are now available  </li>
<li>requirements: unofficial hipBLASLt  </li>
</ul>
</li>
<li><strong>Other</strong></li>
<li><strong>XYZ Grid</strong>: add prompt search&amp;replace options: <em>primary, refine, detailer, all</em></li>
<li><strong>SysInfo</strong>: update to collected data and benchmarks  </li>
<li><strong>Fixes</strong>:</li>
<li>explict clear caches on model load  </li>
<li>lock adetailer commit: <code>#a89c01d</code>  </li>
<li>xyzgrid progress calculation  </li>
<li>xyzgrid detailer</li>
<li>vae tiling use default value if not set  </li>
<li>sd35 img2img</li>
<li>samplers test for scale noise before using  </li>
<li>scheduler api  </li>
<li>sampler create error handling  </li>
<li>controlnet with hires  </li>
<li>controlnet with batch count  </li>
<li>apply settings skip hidden settings  </li>
<li>lora diffusers method apply only once  </li>
<li>lora diffusers method set prompt tags and metadata  </li>
<li>flux support on-the-fly quantization for bnb of unet only  </li>
<li>control restore pipeline before running hires  </li>
<li>restore args after batch run  </li>
<li>flux controlnet  </li>
<li>zluda installer  </li>
<li>control inherit parent pipe settings  </li>
<li>control logging  </li>
<li>hf cache folder settings  </li>
<li>fluxfill should not require base model</li>
</ul>
<h2 id="update-for-2024-12-31">Update for 2024-12-31</h2>
<p>NYE refresh release with quite a few optimizatios and bug fixes...<br />
Commit hash: <code>master: #dcfc9f3</code> <code>dev: #935cac6</code>  </p>
<ul>
<li><strong>LoRA</strong>:  </li>
<li>LoRA load/apply/unapply methods have been changed in 12/2024 Xmass release and further tuned in this release</li>
<li>for details on available methods, see <a href="https://github.com/vladmandic/automatic/wiki/Lora#lora-loader">https://github.com/vladmandic/automatic/wiki/Lora#lora-loader</a>  </li>
<li><strong>Sana</strong> support  </li>
<li>quantized models support  </li>
<li>add fuse support with on-demand apply/unapply (new default)  </li>
<li>add legacy option in <em>settings -&gt; networks</em>  </li>
<li><strong>HunyuanVideo</strong>:  </li>
<li>optimizations: full offload, quantization and tiling support  </li>
<li><strong>LTXVideo</strong>:  </li>
<li>optimizations: full offload, quantization and tiling support  </li>
<li><a href="https://github.com/ali-vilab/TeaCache/blob/main/TeaCache4LTX-Video/README.md">TeaCache</a> integration  </li>
<li><strong>VAE</strong>:  </li>
<li>tiling granular options in <em>settings -&gt; Variational Auto Encoder</em>  </li>
<li><strong>UI</strong>:  </li>
<li>live preview optimizations and error handling  </li>
<li>live preview high quality output, thanks @Disty0  </li>
<li>CSS optimizations when log view is disabled  </li>
<li><strong>Samplers</strong>:  </li>
<li>add flow shift options and separate dynamic thresholding from dynamic shifting  </li>
<li>autodetect matching sigma capabilities  </li>
<li><strong>API</strong>:  </li>
<li>better default values for generate  </li>
<li><strong>Refactor</strong>:  </li>
<li>remove all LDM imports if running in native mode  </li>
<li>startup optimizatios  </li>
<li><strong>Torch</strong>:  </li>
<li>support for <code>torch==2.6.0</code>  </li>
<li><strong>OpenVINO</strong>:  </li>
<li>disable re-compile on resolution change  </li>
<li>fix shape mismatch on resolution change  </li>
<li><strong>Fixes</strong>:  </li>
<li>flux pipeline switches: txt/img/inpaint  </li>
<li>flux custom unet loader for bnb  </li>
<li>flux do not requantize already quantized model</li>
<li>interrogate caption with T5  </li>
<li>on-the-fly quantization using TorchAO  </li>
<li>remove concurrent preview requests  </li>
<li>xyz grid recover on error  </li>
<li>hires batch  </li>
<li>sdxl refiner  </li>
<li>increase progress timeout</li>
<li>kandinsky matmul  </li>
<li>do not show disabled networks  </li>
<li>enable debug logging by default</li>
<li>image width/height calculation when doing img2img  </li>
<li>corrections with batch processing  </li>
<li>hires with refiner prompt and batch processing  </li>
<li>processing with nested calls  </li>
<li>ui networks initial sort  </li>
<li>esrgan on cpu devices  </li>
</ul>
<h2 id="update-for-2024-12-24">Update for 2024-12-24</h2>
<h3 id="highlights-for-2024-12-24">Highlights for 2024-12-24</h3>
<h3 id="sdnext-xmass-edition-whats-new">SD.Next Xmass edition: <em>What's new?</em></h3>
<p>While we have several new supported models, workflows and tools, this release is primarily about <em>quality-of-life improvements</em>:<br />
- New memory management engine<br />
  list of changes that went into this one is long: changes to GPU offloading, brand new LoRA loader, system memory management, on-the-fly quantization, improved gguf loader, etc.<br />
  but main goal is enabling modern large models to run on standard consumer GPUs<br />
  without performance hits typically associated with aggressive memory swapping and needs for constant manual tweaks<br />
- New <a href="https://vladmandic.github.io/sdnext-docs/">documentation website</a><br />
  with full search and tons of new documentation<br />
- New settings panel with simplified and streamlined configuration  </p>
<p>We've also added support for several new models such as highly anticipated <a href="https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px">NVLabs Sana</a> (see <a href="https://vladmandic.github.io/sdnext-docs/Model-Support/">supported models</a> for full list)<br />
And several new SOTA video models: <a href="https://huggingface.co/Lightricks/LTX-Video">Lightricks LTX-Video</a>, <a href="https://huggingface.co/tencent/HunyuanVideo">Hunyuan Video</a> and <a href="https://huggingface.co/genmo/mochi-1-preview">Genmo Mochi.1 Preview</a>  </p>
<p>And a lot of <strong>Control</strong> and <strong>IPAdapter</strong> goodies<br />
- for <strong>SDXL</strong> there is new <a href="https://huggingface.co/xinsir/controlnet-union-sdxl-1.0">ProMax</a>, improved <em>Union</em> and <em>Tiling</em> models<br />
- for <strong>FLUX.1</strong> there are <a href="https://blackforestlabs.ai/flux-1-tools/">Flux Tools</a> as well as official <em>Canny</em> and <em>Depth</em> models,<br />
  a cool <a href="https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev">Redux</a> model as well as <a href="https://huggingface.co/XLabs-AI/flux-ip-adapter-v2">XLabs</a> IP-adapter
- for <strong>SD3.5</strong> there are official <em>Canny</em>, <em>Blur</em> and <em>Depth</em> models in addition to existing 3rd party models<br />
  as well as <a href="https://huggingface.co/InstantX/SD3.5-Large-IP-Adapter">InstantX</a> IP-adapter  </p>
<p>Plus couple of new integrated workflows such as <a href="https://github.com/ali-vilab/FreeScale">FreeScale</a> and <a href="https://style-aligned-gen.github.io/">Style Aligned Image Generation</a>  </p>
<p>And it wouldn't be a <em>Xmass edition</em> without couple of custom themes: <em>Snowflake</em> and <em>Elf-Green</em>!<br />
All-in-all, we're around ~180 commits worth of updates, check the changelog for full list  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">ReadMe</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">ChangeLog</a> | <a href="https://vladmandic.github.io/sdnext-docs/">Docs</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h2 id="details-for-2024-12-24">Details for 2024-12-24</h2>
<h3 id="new-models-and-integrations">New models and integrations</h3>
<ul>
<li><a href="https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px">NVLabs Sana</a>
  support for 1.6B 2048px, 1.6B 1024px and 0.6B 512px models<br />
<strong>Sana</strong> can synthesize high-resolution images with strong text-image alignment by using <strong>Gemma2</strong> as text-encoder<br />
  and its <em>fast</em> - typically at least <strong>2x</strong> faster than sd-xl even for 1.6B variant and maintains performance regardless of resolution<br />
  e.g., rendering at 4k is possible in less than 8GB vram<br />
  to use, select from <em>networks -&gt; models -&gt; reference</em> and models will be auto-downloaded on first use<br />
<em>reference values</em>: sampler: default (or any flow-match variant), steps: 20, width/height: 1024, guidance scale: 4.5<br />
<em>note</em> like other LLM-based text-encoders, sana prefers long and descriptive prompts<br />
  any short prompt below 300 characters will be auto-expanded using built in Gemma LLM before encoding while long prompts will be passed as-is  </li>
<li><strong>ControlNet</strong></li>
<li>improved support for <strong>Union</strong> controlnets with granular control mode type</li>
<li>added support for latest <a href="https://huggingface.co/xinsir/controlnet-union-sdxl-1.0">Xinsir ProMax</a> all-in-one controlnet  </li>
<li>added support for multiple <strong>Tiling</strong> controlnets, for example <a href="https://huggingface.co/xinsir/controlnet-tile-sdxl-1.0">Xinsir Tile</a><br />
<em>note</em>: when selecting tiles in control settings, you can also specify non-square ratios<br />
    in which case it will use context-aware image resize to maintain overall composition<br />
<em>note</em>: available tiling options can be set in settings -&gt; control  </li>
<li><strong>IP-Adapter</strong>  </li>
<li>FLUX.1 <a href="https://huggingface.co/XLabs-AI/flux-ip-adapter-v2">XLabs</a> v1 and v2 IP-adapter  </li>
<li>FLUX.1 secondary guidance, enabled using <em>Attention guidance</em> in advanced menu  </li>
<li>SD 3.5 <a href="https://huggingface.co/InstantX/SD3.5-Large-IP-Adapter">InstantX</a> IP-adapter  </li>
<li><a href="https://blackforestlabs.ai/flux-1-tools/">Flux Tools</a><br />
<strong>Redux</strong> is actually a tool, <strong>Fill</strong> is inpaint/outpaint optimized version of <em>Flux-dev</em><br />
<strong>Canny</strong> &amp; <strong>Depth</strong> are optimized versions of <em>Flux-dev</em> for their respective tasks: they are <em>not</em> ControlNets that work on top of a model<br />
  to use, go to image or control interface and select <em>Flux Tools</em> in scripts<br />
  all models are auto-downloaded on first use<br />
<em>note</em>: All models are <a href="https://github.com/vladmandic/automatic/wiki/Gated">gated</a> and require acceptance of terms and conditions via web page<br />
<em>recommended</em>: Enable on-the-fly <a href="https://github.com/vladmandic/automatic/wiki/Quantization">quantization</a> or <a href="https://github.com/vladmandic/automatic/wiki/NNCF-Compression">compression</a> to reduce resource usage<br />
<em>todo</em>: support for Canny/Depth LoRAs  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev">Redux</a>: ~0.1GB<br />
    works together with existing model and basically uses input image to analyze it and use that instead of prompt<br />
<em>optional</em> can use prompt to combine guidance with input image<br />
<em>recommended</em>: low denoise strength levels result in more variety  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev">Fill</a>: ~23.8GB, replaces currently loaded model<br />
<em>note</em>: can be used in inpaint/outpaint mode only  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev">Canny</a>: ~23.8GB, replaces currently loaded model<br />
<em>recommended</em>: guidance scale 30  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev">Depth</a>: ~23.8GB, replaces currently loaded model<br />
<em>recommended</em>: guidance scale 10  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora">Flux ControlNet LoRA</a><br />
  alternative to standard ControlNets, FLUX.1 also allows LoRA to help guide the generation process<br />
  both <strong>Depth</strong> and <strong>Canny</strong> LoRAs are available in standard control menus  </li>
<li><a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-controlnets">StabilityAI SD35 ControlNets</a></li>
<li>In addition to previously released <code>InstantX</code> and <code>Alimama</code>, we now have <em>official</em> ones from StabilityAI  </li>
<li><a href="https://style-aligned-gen.github.io/">Style Aligned Image Generation</a><br />
  enable in scripts, compatible with sd-xl<br />
  enter multiple prompts in prompt field separated by new line<br />
  style-aligned applies selected attention layers uniformly to all images to achive consistency<br />
  can be used with or without input image in which case first prompt is used to establish baseline<br />
<em>note:</em> all prompts are processes as a single batch, so vram is limiting factor  </li>
<li><a href="https://github.com/ali-vilab/FreeScale">FreeScale</a><br />
  enable in scripts, compatible with sd-xl for text and img2img<br />
  run iterative generation of images at different scales to achieve better results<br />
  can render 4k sdxl images<br />
<em>note</em>: disable live preview to avoid memory issues when generating large images  </li>
</ul>
<h3 id="video-models">Video models</h3>
<ul>
<li><a href="https://huggingface.co/Lightricks/LTX-Video">Lightricks LTX-Video</a><br />
  model size: 27.75gb<br />
  support for 0.9.0, 0.9.1 and custom safetensor-based models with full quantization and offloading support<br />
  support for text-to-video and image-to-video, to use, select in <em>scripts -&gt; ltx-video</em><br />
<em>refrence values</em>: steps 50, width 704, height 512, frames 161, guidance scale 3.0  </li>
<li><a href="https://huggingface.co/tencent/HunyuanVideo">Hunyuan Video</a><br />
  model size: 40.92gb<br />
  support for text-to-video, to use, select in <em>scripts -&gt; hunyuan video</em><br />
  basic support only<br />
<em>refrence values</em>: steps 50, width 1280, height 720, frames 129, guidance scale 6.0  </li>
<li><a href="https://huggingface.co/genmo/mochi-1-preview">Genmo Mochi.1 Preview</a><br />
  support for text-to-video, to use, select in <em>scripts -&gt; mochi.1 video</em><br />
  basic support only<br />
<em>refrence values</em>: steps 64, width 848, height 480, frames 19, guidance scale 4.5  </li>
</ul>
<p><em>Notes</em>:
- all video models are very large and resource intensive!<br />
  any use on gpus below 16gb and systems below 48gb ram is experimental at best<br />
- sdnext support for video models is relatively basic with further optimizations pending community interest<br />
  any future optimizations would likely have to go into partial loading and excecution instead of offloading inactive parts of the model<br />
- new video models use generic llms for prompting and due to that requires very long and descriptive prompt<br />
- you may need to enable sequential offload for maximum gpu memory savings<br />
- optionally enable pre-quantization using bnb for additional memory savings<br />
- reduce number of frames and/or resolution to reduce memory usage  </p>
<h3 id="ui-and-workflow-improvements">UI and workflow improvements</h3>
<ul>
<li><strong>Docs</strong>:</li>
<li>New documentation site! <a href="https://vladmandic.github.io/sdnext-docs/">https://vladmandic.github.io/sdnext-docs/</a></li>
<li>Additional Wiki content: Styles, Wildcards, etc.</li>
<li><strong>LoRA</strong> handler rewrite:  </li>
<li>LoRA weights are no longer calculated on-the-fly during model execution, but are pre-calculated at the start<br />
    this results in perceived overhead on generate startup, but results in overall faster execution as LoRA does not need to be processed on each step<br />
    thanks @AI-Casanova  </li>
<li>LoRA weights can be applied/unapplied as on each generate or they can store weights backups for later use<br />
    this setting has large performance and resource implications, see <a href="https://github.com/vladmandic/automatic/wiki/Offload">Offload</a> wiki for details  </li>
<li>LoRA name in prompt can now also be an absolute path to a LoRA file, even if LoRA is not indexed<br />
    example: <code>&lt;lora:/test/folder/my-lora.safetensors:1.0&gt;</code></li>
<li>LoRA name in prompt can now also be path to a LoRA file op <code>huggingface</code><br />
    example: <code>&lt;lora:/huggingface.co/vendor/repo/my-lora.safetensors:1.0&gt;</code></li>
<li><strong>Model loader</strong> improvements:  </li>
<li>detect model components on model load fail  </li>
<li>allow passing absolute path to model loader  </li>
<li>Flux, SD35: force unload model  </li>
<li>Flux: apply <code>bnb</code> quant when loading <em>unet/transformer</em>  </li>
<li>Flux: all-in-one safetensors<br />
    example: <a href="https://civitai.com/models/646328?modelVersionId=1040235">https://civitai.com/models/646328?modelVersionId=1040235</a>  </li>
<li>Flux: do not recast quants  </li>
<li><strong>Memory</strong> improvements:  </li>
<li>faster and more compatible <em>balanced offload</em> mode  </li>
<li>balanced offload: units are now in percentage instead of bytes  </li>
<li>balanced offload: add both high and low watermark, defaults as below<br />
<code>0.25</code> for low-watermark: skip offload if memory usage is below 25%<br />
<code>0.70</code> high-watermark: must offload if memory usage is above 70%  </li>
<li>balanced offload will attempt to run offload as non-blocking and force gc at the end  </li>
<li>change-in-behavior:<br />
    low-end systems, triggered by either <code>lowvrwam</code> or by detection of &lt;=4GB will use <em>sequential offload</em><br />
    all other systems use <em>balanced offload</em> by default (can be changed in settings)<br />
    previous behavior was to use <em>model offload</em> on systems with &lt;=8GB and <code>medvram</code> and no offload by default  </li>
<li>VAE upcase is now disabled by default on all systems<br />
    if you have issues with image decode, you'll need to enable it manually  </li>
<li><strong>UI</strong>:  </li>
<li>improved stats on generate completion  </li>
<li>improved live preview display and performance  </li>
<li>improved accordion behavior  </li>
<li>auto-size networks height for sidebar  </li>
<li>control: hide preview column by default</li>
<li>control: optionn to hide input column</li>
<li>control: add stats</li>
<li>settings: reorganized and simplified  </li>
<li>browser -&gt; server logging framework  </li>
<li>add addtional themes: <code>black-reimagined</code>, thanks @Artheriax  </li>
<li><strong>Batch</strong></li>
<li>image batch processing will use caption files if they exist instead of default prompt  </li>
</ul>
<h3 id="updates">Updates</h3>
<ul>
<li><strong>Quantization</strong></li>
<li>Add <code>TorchAO</code> <em>pre</em> (during load) and <em>post</em> (during execution) quantization<br />
<strong>torchao</strong> supports 4 different int-based and 3 float-based quantization schemes<br />
  This is in addition to existing support for:  </li>
<li><code>BitsAndBytes</code> with 3 float-based quantization schemes  </li>
<li><code>Optimium.Quanto</code> with 3 int-based and 2 float-based quantizations schemes  </li>
<li><code>GGUF</code> with pre-quantized weights  </li>
<li>Switch <code>GGUF</code> loader from custom to diffuser native</li>
<li><strong>IPEX</strong>: update to IPEX 2.5.10+xpu  </li>
<li><strong>OpenVINO</strong>:  </li>
<li>update to 2024.6.0  </li>
<li>disable model caching by default  </li>
<li><strong>Sampler</strong> improvements  </li>
<li>UniPC, DEIS, SA, DPM-Multistep: allow FlowMatch sigma method and prediction type  </li>
<li>Euler FlowMatch: add sigma methods (<em>karras/exponential/betas</em>)  </li>
<li>Euler FlowMatch: allow using timestep presets to set sigmas  </li>
<li>DPM FlowMatch: update all and add sigma methods  </li>
<li>BDIA-DDIM: <em>experimental</em> new scheduler  </li>
<li>UFOGen: <em>experimental</em> new scheduler  </li>
</ul>
<h3 id="fixes">Fixes</h3>
<ul>
<li>add <code>SD_NO_CACHE=true</code> env variable to disable file/folder caching  </li>
<li>add settings -&gt; networks -&gt; embeddings -&gt; enable/disable</li>
<li>update <code>diffusers</code>  </li>
<li>fix README links  </li>
<li>fix sdxl controlnet single-file loader  </li>
<li>relax settings validator  </li>
<li>improve js progress calls resiliency  </li>
<li>fix text-to-video pipeline  </li>
<li>avoid live-preview if vae-decode is running  </li>
<li>allow xyz-grid with multi-axis s&amp;r  </li>
<li>fix xyz-grid with lora  </li>
<li>fix api script callbacks  </li>
<li>fix gpu memory monitoring  </li>
<li>simplify img2img/inpaint/sketch canvas handling  </li>
<li>fix prompt caching  </li>
<li>fix xyz grid skip final pass  </li>
<li>fix sd upscale script  </li>
<li>fix cogvideox-i2v  </li>
<li>lora auto-apply tags remove duplicates  </li>
<li>control load model on-demand if not already loaded  </li>
<li>taesd limit render to 2024px  </li>
<li>taesd downscale preview to 1024px max: configurable in settings -&gt; live preview  </li>
<li>uninstall conflicting <code>wandb</code> package  </li>
<li>dont skip diffusers version check if quick is specified  </li>
<li>notify on torch install  </li>
<li>detect pipeline fro diffusers folder-style model  </li>
<li>do not recast flux quants  </li>
<li>fix xyz-grid with lora none  </li>
<li>fix svd image2video  </li>
<li>fix gallery display during generate  </li>
<li>fix wildcards replacement to be unique  </li>
<li>fix animatediff-xl  </li>
<li>fix pag with batch count  </li>
</ul>
<h2 id="update-for-2024-11-21">Update for 2024-11-21</h2>
<h3 id="highlights-for-2024-11-21">Highlights for 2024-11-21</h3>
<p>Three weeks is a long time in Generative AI world - and we're back with ~140 commits worth of updates!</p>
<p><em>What's New?</em></p>
<p>First, a massive update to docs including new UI top-level <strong>info</strong> tab with access to <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">changelog</a> and <a href="https://github.com/vladmandic/automatic/wiki">wiki</a>, many updates and new articles AND full <strong>built-in documentation search</strong> capabilities</p>
<h4 id="new-integrations">New integrations</h4>
<ul>
<li><a href="https://github.com/ToTheBeginning/PuLID">PuLID</a>: Pure and Lightning ID Customization via Contrastive Alignment</li>
<li><a href="https://github.com/instantX-research/InstantIR">InstantX InstantIR</a>: Blind Image Restoration with Instant Generative Reference</li>
<li><a href="https://github.com/NVlabs/consistory">nVidia Labs ConsiStory</a>: Consistent Image Generation</li>
<li><a href="https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0">MiaoshouAI PromptGen v2.0</a> VQA captioning</li>
</ul>
<h4 id="workflow-improvements">Workflow Improvements</h4>
<ul>
<li>Native <strong>Docker</strong> support</li>
<li><strong>SD3x &amp; Flux.1</strong>: more ControlNets, all-in-one-safetensors, DPM samplers, skip-layer-guidance, etc.</li>
<li><strong>XYZ grid</strong>: benchmarking, video creation, etc.</li>
<li>Enhanced <strong>prompt</strong> parsing</li>
<li><strong>UI</strong> improvements</li>
<li><strong>Installer</strong> self-healing <code>venv</code></li>
</ul>
<p>And quite a few more improvements and fixes since the last update!
For full list and details see changelog...</p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2024-11-21">Details for 2024-11-21</h3>
<ul>
<li>Docs:  </li>
<li>new top-level <strong>info</strong> tab with access to <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">changelog</a> and <a href="https://github.com/vladmandic/automatic/wiki">wiki</a>  </li>
<li>UI built-in <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">changelog</a> search<br />
    since changelog is the best up-to-date source of info<br />
    go to info -&gt; changelog and search/highligh/navigate directly in UI!  </li>
<li>UI built-in <a href="https://github.com/vladmandic/automatic/wiki">wiki</a><br />
    go to info -&gt; wiki and search wiki pages directly in UI!  </li>
<li>major <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a> and <a href="https://github.com/vladmandic/automatic">Home</a> updates  </li>
<li>updated API swagger docs for at <code>/docs</code>  </li>
<li>Integrations:  </li>
<li><a href="https://github.com/ToTheBeginning/PuLID">PuLID</a>: Pure and Lightning ID Customization via Contrastive Alignment  <ul>
<li>advanced method of face id transfer with better quality as well as control over identity and appearance<br />
  try it out, likely the best quality available for sdxl models  </li>
<li>select in <em>scripts -&gt; pulid</em>  </li>
<li>compatible with <em>sdxl</em> for text-to-image, image-to-image, inpaint, refine, detailer workflows  </li>
<li>can be used in xyz grid  </li>
<li><em>note</em>: this module contains several advanced features on top of original implementation  </li>
</ul>
</li>
<li><a href="https://github.com/instantX-research/InstantIR">InstantIR</a>: Blind Image Restoration with Instant Generative Reference  <ul>
<li>alternative to traditional <code>img2img</code> with more control over restoration process  </li>
<li>select in <em>image -&gt; scripts -&gt; instantir</em>  </li>
<li>compatible with <em>sdxl</em>  </li>
<li><em>note</em>: after used once it cannot be unloaded without reloading base model  </li>
</ul>
</li>
<li><a href="https://github.com/NVlabs/consistory">ConsiStory</a>: Consistent Image Generation  <ul>
<li>create consistent anchor image and then generate images that are consistent with anchor  </li>
<li>select in <em>scripts -&gt; consistory</em>  </li>
<li>compatible with <em>sdxl</em>  </li>
<li><em>note</em>: very resource intensive and not compatible with model offloading  </li>
<li><em>note</em>: changing default parameters can lead to unexpected results and/or failures  </li>
<li><em>note</em>: after used once it cannot be unloaded without reloading base model  </li>
</ul>
</li>
<li>
<p><a href="https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0">MiaoshouAI PromptGen v2.0</a> base and large:  </p>
<ul>
<li><em>in process -&gt; visual query</em>  </li>
<li>caption modes:<br />
<code>&lt;GENERATE_TAGS&gt;</code> generate tags<br />
<code>&lt;CAPTION&gt;</code>, <code>&lt;DETAILED_CAPTION&gt;</code>, <code>&lt;MORE_DETAILED_CAPTION&gt;</code> caption image<br />
<code>&lt;ANALYZE&gt;</code> image composition<br />
<code>&lt;MIXED_CAPTION&gt;</code>, <code>&lt;MIXED_CAPTION_PLUS&gt;</code> detailed caption and tags with optional analyze  </li>
</ul>
</li>
<li>
<p>Model improvements:  </p>
</li>
<li>SD35: <strong>ControlNets</strong>:  <ul>
<li><em>InstantX Canny, Pose, Depth, Tile</em>  </li>
<li><em>Alimama Inpainting, SoftEdge</em>  </li>
<li><em>note</em>: that just like with FLUX.1 or any large model, ControlNet are also large and can push your system over the limit<br />
  e.g. SD3 controlnets vary from 1GB to over 4GB in size  </li>
</ul>
</li>
<li>SD35: <strong>All-in-one</strong> safetensors  <ul>
<li><em>examples</em>: <a href="https://civitai.com/models/882666/sd35-large-google-flan?modelVersionId=1003031">large</a>, <a href="https://civitai.com/models/900327">medium</a>  </li>
<li><em>note</em>: enable <em>bnb</em> on-the-fly quantization for even bigger gains  </li>
</ul>
</li>
<li>SD35: <strong>skip-layer-guidance</strong>  <ul>
<li>enable in <em>scripts -&gt; slg</em></li>
<li>allows for granular strength/start/stop control of guidance for each layer of the model  </li>
</ul>
</li>
<li>
<p><a href="https://huggingface.co/collections/Eugeoter/controlnext-673161eae023f413e0432799">NoobAI XL ControlNets</a>, thanks @lbeltrame</p>
</li>
<li>
<p>Workflow improvements:  </p>
</li>
<li>Native Docker support with pre-defined <a href="https://github.com/vladmandic/automatic/blob/dev/Dockerfile">Dockerfile</a></li>
<li>Samplers:<ul>
<li><strong>FlowMatch samplers</strong>:</li>
<li>Applicable to SD 3.x and Flux.1 models</li>
<li>Complete family: <em>DPM2, DPM2a, DPM2++, DPM2++ 2M, DPM2++ 2S, DPM2++ SDE, DPM2++ 2M SDE, DPM2++ 3M SDE</em></li>
<li><strong>Beta and Exponential</strong> sigma method enabled for all samplers</li>
</ul>
</li>
<li><strong>XYZ grid</strong>:  <ul>
<li>optional time benchmark info to individual images  </li>
<li>optional add params to individual images  </li>
<li>create video from generated grid images<br />
  supports all standard video types and interpolation  </li>
</ul>
</li>
<li><strong>Prompt parser</strong>:  <ul>
<li>support for prompt scheduling  </li>
<li>renamed parser options: <code>native</code>, <code>xhinker</code>, <code>compel</code>, <code>a1111</code>, <code>fixed</code>  </li>
<li>parser options are available in xyz grid  </li>
<li>improved caching  </li>
</ul>
</li>
<li><strong>UI</strong>:  <ul>
<li>better gallery and networks sidebar sizing  </li>
<li>add additional <a href="https://github.com/vladmandic/automatic/wiki/Hotkeys">hotkeys</a>  </li>
<li>add show networks on startup setting  </li>
<li>better mapping of networks previews  </li>
<li>optimize networks display load  </li>
</ul>
</li>
<li>Image2image:  <ul>
<li>integrated refine/upscale/hires workflow  </li>
</ul>
</li>
<li>Other:  </li>
<li><strong>Installer</strong>:  <ul>
<li>Log <code>venv</code> and package search paths  </li>
<li>Auto-remove invalid packages from <code>venv/site-packages</code><br />
  e.g. packages starting with <code>~</code> which are left-over due to windows access violation  </li>
<li>Requirements: update  </li>
</ul>
</li>
<li>Scripts:  <ul>
<li>More verbose descriptions for all scripts  </li>
</ul>
</li>
<li>Model loader:  <ul>
<li>Report modules included in safetensors when attempting to load a model  </li>
</ul>
</li>
<li>CLI:  <ul>
<li>refactor command line params<br />
  run <code>webui.sh</code>/<code>webui.bat</code> with <code>--help</code> to see all options  </li>
<li>added <code>cli/model-metadata.py</code> to display metadata in any safetensors file  </li>
<li>added <code>cli/model-keys.py</code> to quicky display content of any safetensors file  </li>
</ul>
</li>
<li>
<p>Internal:  </p>
<ul>
<li>Auto pipeline switching coveres wrapper classes and nested pipelines  </li>
<li>Full settings validation on load of <code>config.json</code>  </li>
<li>Refactor of all params in main processing classes  </li>
<li>Improve API scripts usage resiliency  </li>
</ul>
</li>
<li>
<p>Fixes:  </p>
</li>
<li>custom watermark add alphablending  </li>
<li>fix xyz grid include images  </li>
<li>fix xyz skip on interrupted  </li>
<li>fix vqa models ignoring hfcache folder setting  </li>
<li>fix network height in standard vs modern ui  </li>
<li>fix k-diff enum on startup  </li>
<li>fix text2video scripts  </li>
<li>multiple xyz-grid fixes  </li>
<li>dont uninstall flash-attn  </li>
<li>ui css fixes  </li>
</ul>
<h2 id="update-for-2024-11-01">Update for 2024-11-01</h2>
<p>Smaller release just 3 days after the last one, but with some important fixes and improvements.<br />
This release can be considered an LTS release before we kick off the next round of major updates.  </p>
<ul>
<li>Other:</li>
<li>Repo: move screenshots to GH pages</li>
<li>Update requirements</li>
<li>Fixes:</li>
<li>detailer min/max size as fractions of image size  </li>
<li>ipadapter load on-demand  </li>
<li>ipadapter face use correct yolo model  </li>
<li>list diffusers remove duplicates  </li>
<li>fix legacy extensions access to shared objects  </li>
<li>fix diffusers load from folder  </li>
<li>fix lora enum logging on windows  </li>
<li>fix xyz grid with batch count  </li>
<li>move dowwloads of some auxillary models to hfcache instead of models folder  </li>
</ul>
<h2 id="update-for-2024-10-29">Update for 2024-10-29</h2>
<h3 id="highlights-for-2024-10-29">Highlights for 2024-10-29</h3>
<ul>
<li>Support for <strong>all SD3.x variants</strong><br />
<em>SD3.0-Medium, SD3.5-Medium, SD3.5-Large, SD3.0-Large-Turbo</em></li>
<li>Allow quantization using <code>bitsandbytes</code> on-the-fly during models load
  Load any variant of SD3.x or FLUX.1 and apply quantization during load without the need for pre-quantized models  </li>
<li>Allow for custom model URL in standard model selector<br />
  Can be used to specify any model from <em>HuggingFace</em> or <em>CivitAI</em>  </li>
<li>Full support for <code>torch==2.5.1</code></li>
<li>New wiki articles: <a href="https://github.com/vladmandic/automatic/wiki/Gated">Gated Access</a>, <a href="https://github.com/vladmandic/automatic/wiki/Quantization">Quantization</a>, <a href="https://github.com/vladmandic/automatic/wiki/Offload">Offloading</a>  </li>
</ul>
<p>Plus tons of smaller improvements and cumulative fixes reported since last release  </p>
<p><a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2024-10-29">Details for 2024-10-29</h3>
<ul>
<li>model selector:</li>
<li>change-in-behavior</li>
<li>when typing, it will auto-load model as soon as exactly one match is found</li>
<li>allows entering model that are not on the list which triggers huggingface search<br />
    e.g. <code>stabilityai/stable-diffusion-xl-base-1.0</code><br />
    partial search hits are displayed in the log<br />
    if exact model is found, it will be auto-downloaded and loaded  </li>
<li>allows entering civitai direct download link which triggers model download<br />
    e.g. <code>https://civitai.com/api/download/models/72396?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16</code>  </li>
<li>auto-search-and-download can be disabled in settings -&gt; models -&gt; auto-download<br />
    this also disables reference models as they are auto-downloaded on first use as well  </li>
<li>sd3 enhancements:  </li>
<li>allow on-the-fly bnb quantization during load</li>
<li>report when loading incomplete model  </li>
<li>handle missing model components during load  </li>
<li>handle component preloading  </li>
<li>native lora handler  </li>
<li>support for all sd35 variants: <em>medium/large/large-turbo</em></li>
<li>gguf transformer loader (prototype)  </li>
<li>flux.1 enhancements:  </li>
<li>allow on-the-fly bnb quantization during load</li>
<li>samplers:</li>
<li>support for original k-diffusion samplers<br />
    select via <em>scripts -&gt; k-diffusion -&gt; sampler</em>  </li>
<li>ipadapter:</li>
<li>list available adapters based on loaded model type</li>
<li>add adapter <code>ostris consistency</code> for sd15/sdxl</li>
<li>detailer:</li>
<li>add <code>[prompt]</code> to refine/defailer prompts as placeholder referencing original prompt  </li>
<li>torch</li>
<li>use <code>torch==2.5.1</code> by default on supported platforms</li>
<li>CUDA set device memory limit
    in <em>settings -&gt; compute settings -&gt; torch memory limit</em><br />
    default=0 meaning no limit, if set torch will limit memory usage to specified fraction<br />
<em>note</em>: this is not a hard limit, torch will try to stay under this value  </li>
<li>compute backends:</li>
<li>OpenVINO: add accuracy option  </li>
<li>ZLUDA: guess GPU arch  </li>
<li>major model load refactor</li>
<li>wiki: new articles</li>
<li><a href="https://github.com/vladmandic/automatic/wiki/Gated">Gated Access Wiki</a>  </li>
<li><a href="https://github.com/vladmandic/automatic/wiki/Quantization">Quantization Wiki</a>  </li>
<li><a href="https://github.com/vladmandic/automatic/wiki/Offload">Offloading Wiki</a>  </li>
</ul>
<p>fixes:<br />
- fix send-to-control<br />
- fix k-diffusion<br />
- fix sd3 img2img and hires<br />
- fix ipadapter supported model detection<br />
- fix t2iadapter auto-download
- fix omnigen dynamic attention<br />
- handle a1111 prompt scheduling<br />
- handle omnigen image placeholder in prompt  </p>
<h2 id="update-for-2024-10-23">Update for 2024-10-23</h2>
<h3 id="highlights-for-2024-10-23">Highlights for 2024-10-23</h3>
<p>A month later and with nearly 300 commits, here is the latest <a href="https://github.com/vladmandic/automatic">SD.Next</a> update!  </p>
<h4 id="workflow-highlights-for-2024-10-23">Workflow highlights for 2024-10-23</h4>
<ul>
<li><strong>Reprocess</strong>: New workflow options that allow you to generate at lower quality and then<br />
  reprocess at higher quality for select images only or generate without hires/refine and then reprocess with hires/refine<br />
  and you can pick any previous latent from auto-captured history!  </li>
<li><strong>Detailer</strong> Fully built-in detailer workflow with support for all standard models  </li>
<li>Built-in <strong>model analyzer</strong><br />
  See all details of your currently loaded model, including components, parameter count, layer count, etc.  </li>
<li><strong>Extract LoRA</strong>: load any LoRA(s) and play with generate as usual<br />
  and once you like the results simply extract combined LoRA for future use!  </li>
</ul>
<h4 id="new-models-for-2024-10-23">New models for 2024-10-23</h4>
<ul>
<li>New fine-tuned <a href="https://huggingface.co/zer0int/CLIP-GmP-ViT-L-14">CLiP-ViT-L</a> 1st stage <strong>text-encoders</strong> used by most models (SD15/SDXL/SD3/Flux/etc.) brings additional details to your images  </li>
<li>New models:<br />
<a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large">Stable Diffusion 3.5 Large</a><br />
<a href="https://arxiv.org/pdf/2409.11340">OmniGen</a><br />
<a href="https://huggingface.co/THUDM/CogView3-Plus-3B">CogView 3 Plus</a><br />
<a href="https://github.com/viiika/Meissonic">Meissonic</a>  </li>
<li>Additional integration:<br />
<a href="https://github.com/genforce/ctrl-x">Ctrl+X</a> which allows for control of <strong>structure and appearance</strong> without the need for extra models,<br />
<a href="https://arxiv.org/pdf/2410.02416">APG: Adaptive Projected Guidance</a> for optimal <strong>guidance</strong> control,<br />
<a href="https://github.com/Huage001/LinFusion">LinFusion</a> for on-the-fly <strong>distillation</strong> of any sd15/sdxl model  </li>
</ul>
<h4 id="what-else-for-2024-10-23">What else for 2024-10-23</h4>
<ul>
<li>Tons of work on <strong>dynamic quantization</strong> that can be applied <em>on-the-fly</em> during model load to any model type (<em>you do not need to use pre-quantized models</em>)<br />
  Supported quantization engines include <code>BitsAndBytes</code>, <code>TorchAO</code>, <code>Optimum.quanto</code>, <code>NNCF</code> compression, and more...  </li>
<li>Auto-detection of best available <strong>device/dtype</strong> settings for your platform and GPU reduces neeed for manual configuration<br />
<em>Note</em>: This is a breaking change to default settings and its recommended to check your preferred settings after upgrade  </li>
<li>Full rewrite of <strong>sampler options</strong>, not far more streamlined with tons of new options to tweak scheduler behavior  </li>
<li>Improved <strong>LoRA</strong> detection and handling for all supported models  </li>
<li>Several of <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">Flux.1</a> optimizations and new quantization types  </li>
</ul>
<p>Oh, and we've compiled a full table with list of top-30 (<em>how many have you tried?</em>) popular text-to-image generative models,<br />
their respective parameters and architecture overview: <a href="https://github.com/vladmandic/automatic/wiki/Models">Models Overview</a>  </p>
<p>And there are also other goodies like multiple <em>XYZ grid</em> improvements, additional <em>Flux ControlNets</em>, additional <em>Interrogate models</em>, better <em>LoRA tags</em> support, and more...<br />
<a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a> | <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a> | <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a> | <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a></p>
<h3 id="details-for-2024-10-23">Details for 2024-10-23</h3>
<ul>
<li><strong>reprocess</strong></li>
<li>new top-level button: reprocess latent from your history of generated image(s)  </li>
<li>generate using full-quality:off and then reprocess using <em>full quality decode</em>  </li>
<li>generate without hires/refine and then <em>reprocess with hires/refine</em><br />
<em>note</em>: you can change hires/refine settings and run-reprocess again!  </li>
<li>
<p>reprocess using <em>detailer</em>  </p>
</li>
<li>
<p><strong>history</strong></p>
</li>
<li>by default, <strong>reprocess</strong> will pick last latent, but you can select any latent from history!  </li>
<li>history is under <em>networks -&gt; history</em><br />
    each history item includes info on operations that were used, timestamp and metadata  </li>
<li>any latent operation during workflow automatically adds one or more items to history<br />
    e.g. generate base + upscale + hires + detailer  </li>
<li>history size: <em>settings -&gt; execution -&gt; latent history size</em><br />
    memory usage is ~130kb of ram for 1mp image  </li>
<li>
<p><em>note</em> list of latents in history is not auto-refreshed, use refresh button  </p>
</li>
<li>
<p><strong>model analyzer</strong>  </p>
</li>
<li>see all details of your currently loaded model, including components, parameter count, layer count, etc.  </li>
<li>
<p>in models -&gt; current -&gt; analyze  </p>
</li>
<li>
<p><strong>text encoder</strong>:  </p>
</li>
<li>allow loading different custom text encoders: <em>clip-vit-l, clip-vit-g, t5</em><br />
    will automatically find appropriate encoder in the loaded model and replace it with loaded text encoder<br />
    download text encoders into folder set in settings -&gt; system paths -&gt; text encoders<br />
    default <code>models/Text-encoder</code> folder is used if no custom path is set<br />
    finetuned <em>clip-vit-l</em> models: <a href="https://huggingface.co/zer0int/CLIP-GmP-ViT-L-14">Detailed, Smooth</a>, <a href="https://huggingface.co/zer0int/LongCLIP-GmP-ViT-L-14">LongCLIP</a><br />
    reference <em>clip-vit-l</em> and <em>clip-vit-g</em> models: <a href="https://huggingface.co/collections/laion/openclip-laion-2b-64fcade42d20ced4e9389b30">OpenCLIP-Laion2b</a><br />
<em>note</em> sd/sdxl contain heavily distilled versions of reference models, so switching to reference model produces vastly different results  </li>
<li>xyz grid support for text encoder  </li>
<li>
<p>full prompt parser now correctly works with different prompts in batch  </p>
</li>
<li>
<p><strong>detailer</strong>:  </p>
</li>
<li>replaced <em>face-hires</em> with <em>detailer</em> which can run any number of standard detailing models  </li>
<li>includes <em>face/hand/person/eyes</em> predefined detailer models plus support for manually downloaded models<br />
    set path in <em>settings -&gt; system paths -&gt; yolo</em>  </li>
<li>select one or more models in detailer menu and thats it!  </li>
<li>to avoid duplication of ui elements, detailer will use following values from <strong>refiner</strong>:<br />
<em>sampler, steps, prompts</em>  </li>
<li>when using multiple detailers and prompt is <em>multi-line</em>, each line is applied to corresponding detailer  </li>
<li>adjustable settings:<br />
<em>strength, max detected objects, edge padding, edge blur, min detection confidence, max detection overlap, min and max size of detected object</em>  </li>
<li>image metadata includes info on used detailer models  </li>
<li><em>note</em> detailer defaults are not save in ui settings, they are saved in server settings<br />
    to apply your defaults, set ui values and apply via <em>system -&gt; settings -&gt; apply settings</em>  </li>
<li>
<p>if using models trained on multiple classes, you can specify which classes you want to detail<br />
    e.g. original yolo detection model is trained on coco dataset with 80 predefined classes<br />
    if you leave field blank, it will use any class found in the model<br />
    you can see classes defined in the model while model itself is loaded for the first time  </p>
</li>
<li>
<p><strong>extract lora</strong>: extract combined lora from current memory state, thanks @AI-Casanova<br />
  load any LoRA(s) and play with generate as usual and once you like the results simply extract combined LoRA for future use!<br />
  in <em>models -&gt; extract lora</em>  </p>
</li>
<li>
<p><strong>sampler options</strong>: full rewrite  </p>
</li>
</ul>
<p><em>sampler notes</em>:<br />
  - pick a sampler and then pick values, all values have "default" as a choice to make it simpler<br />
  - a lot of options are new, some are old but moved around<br />
    e.g. karras checkbox is replaced with a choice of different sigma methods<br />
  - not every combination of settings is valid<br />
  - some settings are specific to model types<br />
    e.g. sd15/sdxl typically use epsilon prediction<br />
  - quite a few well-known schedulers are just variations of settings, for example:<br />
    - <em>sampler sgm</em> is sampler with trailing spacing and sample prediction type<br />
    - <em>dpm 2m</em> or <em>3m</em> are <em>dpm 1s</em> with orders of 2 or 3<br />
    - <em>dpm 2m sde</em> is <em>dpm 2m</em> with <em>sde</em> as solver<br />
    - <em>sampler simple</em> is sampler with trailing spacing and linear beta schedule
  - xyz grid support for sampler options<br />
  - metadata updates for sampler options<br />
  - modernui updates for sampler options<br />
  - <em>note</em> sampler options defaults are not saved in ui settings, they are saved in server settings<br />
    to apply your defaults, set ui values and apply via <em>system -&gt; settings -&gt; apply settings</em>  </p>
<p><em>sampler options</em>:<br />
  - sigma method: <em>karas, beta, exponential</em><br />
  - timesteps spacing: <em>linspace, leading, trailing</em><br />
  - beta schedule: <em>linear, scaled, cosine</em><br />
  - prediction type: <em>epsilon, sample, v-prediction</em><br />
  - timesteps presents: <em>none, ays-sd15, ays-sdxl</em><br />
  - timesteps override: <custom><br />
  - sampler order: <em>0=default, 1-5</em><br />
  - options: <em>dynamic, low order, rescale</em>  </p>
<ul>
<li><a href="https://github.com/genforce/ctrl-x">Ctrl+X</a>:</li>
<li>control <strong>structure</strong> (<em>similar to controlnet</em>) and <strong>appearance</strong> (<em>similar to ipadapter</em>)<br />
    without the need for extra models, all via code feed-forwards!</li>
<li>can run in structure-only or appearance-only or both modes</li>
<li>when providing structure and appearance input images, its best to provide a short prompts describing them  </li>
<li>structure image can be <em>almost anything</em>: <em>actual photo, openpose-style stick man, 3d render, sketch, depth-map, etc.</em><br />
    just describe what it is in a structure prompt so it can be de-structured and correctly applied  </li>
<li>
<p>supports sdxl in both txt2img and img2img, simply select from scripts</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2410.02416">APG: Adaptive Projected Guidance</a></p>
</li>
<li>latest algo to provide better guidance for image generation, can be used instead of existing guidance rescale and/or PAG  </li>
<li>in addtion to stronger guidance and reduction of burn at high guidance values, it can also increase image details  </li>
<li>compatible with <em>sd15/sdxl/sc</em>  </li>
<li>select in scripts -&gt; apg  </li>
<li>for low    cfg scale, use positive momentum: e.g. cfg=2 =&gt; momentum=0.6</li>
<li>for normal cfg scale, use negative momentum: e.g. cfg=6 =&gt; momentum=-0.3</li>
<li>
<p>for high   cfg scale, use neutral  momentum: e.g. cfg=10 =&gt; momentum=0.0</p>
</li>
<li>
<p><a href="https://github.com/Huage001/LinFusion">LinFusion</a>  </p>
</li>
<li>apply liner distillation to during load to any sd15/sdxl model  </li>
<li>can reduce vram use for high resolutions and increase performance</li>
<li>
<p><em>note</em>: use lower cfg scales as typical for distilled models  </p>
</li>
<li>
<p><a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">Flux</a>  </p>
</li>
<li>see <a href="https://github.com/vladmandic/automatic/wiki/FLUX#quantization">wiki</a> for details on <code>gguf</code>  </li>
<li>support for <code>gguf</code> binary format for loading unet/transformer component  </li>
<li>support for <code>gguf</code> binary format for loading t5/text-encoder component: requires transformers pr  </li>
<li>additional controlnets: <a href="https://huggingface.co/collections/jasperai/flux1-dev-controlnets-66f27f9459d760dcafa32e08">JasperAI</a> <strong>Depth</strong>, <strong>Upscaler</strong>, <strong>Surface</strong>, thanks @EnragedAntelope  </li>
<li>additional controlnets: <a href="https://huggingface.co/XLabs-AI/flux-controlnet-hed-diffusers">XLabs-AI</a> <strong>Canny</strong>, <strong>Depth</strong>, <strong>HED</strong>  </li>
<li>mark specific unet as unavailable if load failed  </li>
<li>fix diffusers local model name parsing  </li>
<li>full prompt parser will auto-select <code>xhinker</code> for flux models  </li>
<li>controlnet support for img2img and inpaint (in addition to previous txt2img controlnet)  </li>
<li>allow separate vae load  </li>
<li>support for both kohya and onetrainer loras in native load mode for fp16/nf4/fp4, thanks @AI-Casanova  </li>
<li>support for differential diffusion  </li>
<li>added native load mode for qint8/qint4 models</li>
<li>
<p>avoid unet load if unchanged  </p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2409.11340">OmniGen</a>  </p>
</li>
<li>Radical new model with pure LLM architecture based on Phi-3  </li>
<li>Select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li>Can be used for text-to-image and image-to-image  </li>
<li>Image-to-image is <em>very</em> different, you need to specify in prompt what do you want to do<br />
    and add <code>|image|</code> placeholder where input image is used!<br />
    examples: <code>in |image| remove glasses from face</code>, <code>using depth map from |image|, create new image of a cute robot</code>  </li>
<li>
<p>Params used: prompt, steps, guidance scale for prompt guidance, refine guidance scale for image guidance<br />
    Recommended: guidance=3.0, refine-guidance=1.6  </p>
</li>
<li>
<p><a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large">Stable Diffusion 3.5 Large</a>  </p>
</li>
<li>New/improved variant of Stable Diffusion 3  </li>
<li>Select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li>Available in standard and turbo variations  </li>
<li>
<p><em>Note</em>: Access to to both variations of SD3.5 model is gated, you must accept the conditions and use HF login  </p>
</li>
<li>
<p><a href="https://huggingface.co/THUDM/CogView3-Plus-3B">CogView 3 Plus</a></p>
</li>
<li>Select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li>resolution width and height can be from 512px to 2048px and must be divisible by 32  </li>
<li>
<p>precision: bf16 or fp32<br />
    fp16 is not supported due to internal model overflows  </p>
</li>
<li>
<p><a href="https://github.com/viiika/Meissonic">Meissonic</a>  </p>
</li>
<li>Select from <em>networks -&gt; models -&gt; reference</em>  </li>
<li>Experimental as upstream implemenation code is unstable</li>
<li>
<p>Must set scheduler:default, generator:unset</p>
</li>
<li>
<p><a href="https://github.com/thu-ml/SageAttention">SageAttention</a>  </p>
</li>
<li>new 8-bit attention implementation on top of SDP that can provide acceleration for some models, thanks @Disty0  </li>
<li>enable in <em>settings -&gt; compute settings -&gt; sdp options -&gt; sage attention</em></li>
<li>compatible with DiT-based models: e.g. <em>Flux.1, AuraFlow, CogVideoX</em>  </li>
<li>
<p>not compatible with UNet-based models, e.g. <em>SD15, SDXL</em>  </p>
</li>
<li>
<p><strong>gpu</strong></p>
</li>
<li>previously <code>cuda_dtype</code> in settings defaulted to <code>fp16</code> if available  </li>
<li>now <code>cuda_type</code> defaults to <strong>Auto</strong> which executes <code>bf16</code> and <code>fp16</code> tests on startup and selects best available dtype<br />
    if you have specific requirements, you can still set to fp32/fp16/bf16 as desired<br />
    if you have gpu that incorrectly identifies bf16 or fp16 availablity, let us know so we can improve the auto-detection  </li>
<li>
<p>support for torch <strong>expandable segments</strong><br />
    enable in <em>settings -&gt; compute -&gt; torch expandable segments</em><br />
    can provide significant memory savings for some models<br />
    not enabled by default as its only supported on latest versions of torch and some gpus  </p>
</li>
<li>
<p><strong>xyz grid</strong> full refactor  </p>
</li>
<li>multi-mode: <em>selectable-script</em> and <em>alwayson-script</em>  </li>
<li>allow usage combined with other scripts  </li>
<li>allow <strong>unet</strong> selection  </li>
<li>allow passing <strong>model args</strong> directly:<br />
    allowed params will be checked against models call signature<br />
    example: <code>width=768; height=512, width=512; height=768</code>  </li>
<li>allow passing <strong>processing args</strong> directly:<br />
    params are set directly on main processing object and can be known or new params<br />
    example: <code>steps=10, steps=20; test=unknown</code>  </li>
<li>enable working with different resolutions<br />
    now you can adjust width/height in the grid just as any other param  </li>
<li>renamed options to include section name and adjusted cost of each option  </li>
<li>
<p>added additional metadata  </p>
</li>
<li>
<p><strong>interrogate</strong>  </p>
</li>
<li>add additional blip models: <em>blip-base, blip-large, blip-t5-xl, blip-t5-xxl, opt-2.7b, opt-6.7b</em>  </li>
<li>change default params for better memory utilization  </li>
<li>lock commits for miaoshouAI-promptgen  </li>
<li>add optional advanced params  </li>
<li>
<p>update logging  </p>
</li>
<li>
<p><strong>lora</strong> auto-apply tags to prompt  </p>
</li>
<li>controlled via <em>settings -&gt; networks -&gt; lora_apply_tags</em><br />
<em>0:disable, -1:all-tags, n:top-n-tags</em>  </li>
<li>uses tags from both model embedded data and civitai downloaded data  </li>
<li>if lora contains no tags, lora name itself will be used as a tag  </li>
<li>if prompt contains <code>_tags_</code> it will be used as placeholder for replacement, otherwise tags will be appended  </li>
<li>used tags are also logged and registered in image metadata  </li>
<li>loras are no longer filtered per detected type vs loaded model type as its unreliable  </li>
<li>loras display in networks now shows possible version in top-left corner  </li>
<li>correct using of <code>extra_networks_default_multiplier</code> if not scale is specified  </li>
<li>improve lora base model detection  </li>
<li>improve lora error handling and logging  </li>
<li>
<p>setting <code>lora_load_gpu</code> to load LoRA directly to GPU<br />
<em>default</em>: true unless lovwram  </p>
</li>
<li>
<p><strong>quantization</strong>  </p>
</li>
<li>new top level settings group as we have quite a few quantization options now!<br />
    configure in <em>settings -&gt; quantization</em>  </li>
<li>in addition to existing <code>optimum.quanto</code> and <code>nncf</code>, we now have <code>bitsandbytes</code> and <code>torchao</code>  </li>
<li><strong>bitsandbytes</strong>: fp8, fp4, nf4  <ul>
<li>quantization can be applied on-the-fly during model load  </li>
<li>currently supports <code>transformers</code> and <code>t5</code> in <strong>sd3</strong> and <strong>flux</strong>  </li>
</ul>
</li>
<li>
<p><strong>torchao</strong>: int8, int4, fp8, fp4, fpx  </p>
<ul>
<li>configure in settings -&gt; quantization  </li>
<li>can be applied to any model on-the-fly during load  </li>
</ul>
</li>
<li>
<p><strong>huggingface</strong>:  </p>
</li>
<li>force logout/login on token change  </li>
<li>
<p>unified handling of cache folder: set via <code>HF_HUB</code> or <code>HF_HUB_CACHE</code> or via settings -&gt; system paths  </p>
</li>
<li>
<p><strong>cogvideox</strong>:  </p>
</li>
<li>add support for <em>image2video</em> (in addition to previous <em>text2video</em> and <em>video2video</em>)  </li>
<li>
<p><em>note</em>: <em>image2video</em> requires separate 5b model variant  </p>
</li>
<li>
<p><strong>torch</strong>  </p>
</li>
<li>
<p>due to numerous issues with torch 2.5.0 which was just released as stable, we are sticking with 2.4.1 for now  </p>
</li>
<li>
<p><strong>backend=original</strong> is now marked as in maintenance-only mode  </p>
</li>
<li><strong>python 3.12</strong> improved compatibility, automatically handle <code>setuptools</code>  </li>
<li><strong>control</strong></li>
<li>persist/reapply units current state on server restart  </li>
<li>better handle size before/after metadata  </li>
<li><strong>video</strong> add option <code>gradio_skip_video</code> to avoid gradio issues with displaying generated videos  </li>
<li>add support for manually downloaded diffusers models from huggingface  </li>
<li><strong>ui</strong>  </li>
<li>move checkboxes <code>full quality, tiling, hidiffusion</code> to advanced section  </li>
<li>hide token counter until tokens are known  </li>
<li>minor ui optimizations  </li>
<li>fix update infotext on image select  </li>
<li>fix imageviewer exif parser  </li>
<li>selectable info view in image viewer, thanks @ZeldaMaster501  </li>
<li>setting to enable browser autolaunch, thanks @brknsoul  </li>
<li><strong>free-u</strong> check if device/dtype are fft compatible and cast as necessary  </li>
<li><strong>rocm</strong></li>
<li>additional gpu detection and auto-config code, thanks @lshqqytiger  </li>
<li>experimental triton backend for flash attention, thanks @lshqqytiger  </li>
<li>update to rocm 6.2, thanks @Disty0</li>
<li><strong>directml</strong>  </li>
<li>update <code>torch</code> to 2.4.1, thanks @lshqqytiger  </li>
<li><strong>extensions</strong>  </li>
<li>add mechanism to lock-down extension to specific working commit  </li>
<li>added <code>sd-webui-controlnet</code> and <code>adetailer</code> last-known working commits  </li>
<li><strong>upscaling</strong>  </li>
<li>interruptible operations</li>
<li><strong>refactor</strong>  </li>
<li>general lora apply/unapply process  </li>
<li>modularize main process loop  </li>
<li>massive log cleanup  </li>
<li>full lint pass  </li>
<li>improve inference mode handling  </li>
<li>unify quant lib loading  </li>
</ul>
<h2 id="update-for-2024-09-13">Update for 2024-09-13</h2>
<h3 id="highlights-for-2024-09-13">Highlights for 2024-09-13</h3>
<p>Major refactor of <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">FLUX.1</a> support:<br />
- Full <strong>ControlNet</strong> support, better <strong>LoRA</strong> support, full <strong>prompt attention</strong> implementation<br />
- Faster execution, more flexible loading, additional quantization options, and more...<br />
- Added <strong>image-to-image</strong>, <strong>inpaint</strong>, <strong>outpaint</strong>, <strong>hires</strong> modes<br />
- Added workflow where FLUX can be used as <strong>refiner</strong> for other models<br />
- Since both <em>Optimum-Quanto</em> and <em>BitsAndBytes</em> libraries are limited in their platform support matrix,<br />
  try enabling <strong>NNCF</strong> for quantization/compression on-the-fly!  </p>
<p>Few image related goodies...<br />
- <strong>Context-aware</strong> resize that allows for <em>img2img/inpaint</em> even at massively different aspect ratios without distortions!
- <strong>LUT Color grading</strong> apply professional color grading to your images using industry-standard <em>.cube</em> LUTs!
- Auto <strong>HDR</strong> image create for SD and SDXL with both 16ch true-HDR and 8-ch HDR-effect images ;)  </p>
<p>And few video related goodies...<br />
- <a href="https://huggingface.co/THUDM/CogVideoX-5b">CogVideoX</a> <strong>2b</strong> and <strong>5b</strong> variants<br />
  with support for <em>text-to-video</em> and <em>video-to-video</em>!<br />
- <a href="https://github.com/guoyww/animatediff/">AnimateDiff</a> <strong>prompt travel</strong> and <strong>long context windows</strong>!<br />
  create video which travels between different prompts and at long video lengths!  </p>
<p>Plus tons of other items and fixes - see <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">changelog</a> for details!<br />
Examples:
- Built-in prompt-enhancer, TAESD optimizations, new DC-Solver scheduler, global XYZ grid management, etc.<br />
- Updates to ZLUDA, IPEX, OpenVINO...</p>
<h3 id="details-for-2024-09-13">Details for 2024-09-13</h3>
<p><strong>Major refactor of FLUX.1 support:</strong>
- allow configuration of individual FLUX.1 model components: <em>transformer, text-encoder, vae</em><br />
  model load will load selected components first and then initialize model using pre-loaded components<br />
  components that were not pre-loaded will be downloaded and initialized as needed<br />
  as usual, components can also be loaded after initial model load<br />
<em>note</em>: use of transformer/unet is recommended as those are flux.1 finetunes<br />
<em>note</em>: manually selecting vae and text-encoder is not recommended<br />
<em>note</em>: mix-and-match of different quantizations for different components can lead to unexpected errors<br />
  - transformer/unet is list of manually downloaded safetensors<br />
  - vae is list of manually downloaded safetensors<br />
  - text-encoder is list of predefined and manually downloaded text-encoders<br />
- <strong>controlnet</strong> support:
  support for <strong>InstantX/Shakker-Labs</strong> models including <a href="https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union">Union-Pro</a>
  note that flux controlnet models are large, up to 6.6GB on top of already large base model!<br />
  as such, you may need to use offloading:sequential which is not as fast, but uses far less memory<br />
  when using union model, you must also select control mode in the control unit<br />
  flux does not yet support <em>img2img</em> so to use controlnet, you need to set contronet input via control unit override<br />
- model support loading <strong>all-in-one</strong> safetensors<br />
  not recommended due to massive duplication of components, but added due to popular demand<br />
  each such model is 20-32GB in size vs ~11GB for typical unet fine-tune<br />
- improve logging, warn when attempting to load unet as base model<br />
- <strong>refiner</strong> support<br />
  FLUX.1 can be used as refiner for other models such as sd/sdxl<br />
  simply load sd/sdxl model as base and flux model as refiner and use as usual refiner workflow<br />
- <strong>img2img</strong>, <strong>inpaint</strong> and <strong>outpaint</strong> support<br />
<em>note</em> flux may require higher denoising strength than typical sd/sdxl models<br />
<em>note</em>: img2img is not yet supported with controlnet<br />
- transformer/unet support <em>fp8/fp4</em> quantization<br />
  this brings supported quants to: <em>nf4/fp8/fp4/qint8/qint4</em>
- vae support <em>fp16</em><br />
- <strong>lora</strong> support additional training tools<br />
- <strong>face-hires</strong> support<br />
- support <strong>fuse-qkv</strong> projections<br />
  can speed up generate<br />
  enable via <em>settings -&gt; compute -&gt; fused projections</em>  </p>
<p><strong>Other improvements &amp; Fixes:</strong>
- <a href="https://huggingface.co/THUDM/CogVideoX-5b">CogVideoX</a><br />
  - support for both <strong>2B</strong> and <strong>5B</strong> variations<br />
  - support for both <strong>text2video</strong> and <strong>video2video</strong> modes
  - simply select in <em>scripts -&gt; cogvideox</em><br />
  - as with any video modules, includes additional frame interpolation using RIFE<br />
  - if init video is used, it will be automatically resized and interpolated to desired number of frames<br />
- <strong>AnimateDiff</strong>:<br />
  - <strong>prompt travel</strong><br />
     create video which travels between different prompts at different steps!<br />
     example prompt:
      &gt; 0: dog<br />
      &gt; 5: cat<br />
      &gt; 10: bird<br />
  - support for <strong>v3</strong> model (finally)<br />
  - support for <strong>LCM</strong> model<br />
  - support for <strong>free-noise</strong> rolling context window<br />
    allow for creation of much longer videos, automatically enabled if frames &gt; 16<br />
- <strong>Context-aware</strong> image resize, thanks @AI-Casanova!<br />
  based on <a href="https://github.com/li-plus/seam-carving">seam-carving</a><br />
  allows for <em>img2img/inpaint</em> even at massively different aspect ratios without distortions!<br />
  simply select as resize method when using <em>img2img</em> or <em>control</em> tabs<br />
- <strong>HDR</strong> high-dynamic-range image create for SD and SDXL<br />
  create hdr images from in multiple exposures by latent-space modifications during generation<br />
  use via <em>scripts -&gt; hdr</em><br />
  option <em>save hdr images</em> creates images in standard 8bit/channel (hdr-effect) <em>and</em> 16bit/channel (full-hdr) PNG format<br />
  ui result is always 8bit/channel hdr-effect image plus grid of original images used to create hdr<br />
  grid image can be disabled via settings -&gt; user interface -&gt; show grid<br />
  actual full-hdr image is not displayed in ui, only optionally saved to disk<br />
- new scheduler: <a href="https://github.com/wl-zhao/DC-Solver">DC Solver</a><br />
- <strong>color grading</strong> apply professional color grading to your images<br />
  using industry-standard <em>.cube</em> LUTs!
  enable via <em>scripts -&gt; color-grading</em><br />
- <strong>hires</strong> workflow now allows for full resize options<br />
  not just limited width/height/scale<br />
- <strong>xyz grid</strong> is now availabe as both local and global script!
- <strong>prompt enhance</strong>: improve quality and/or verbosity of your prompts<br />
  simply select in <em>scripts -&gt; prompt enhance</em>
  uses <a href="https://huggingface.co/gokaygokay/Flux-Prompt-Enhance">gokaygokay/Flux-Prompt-Enhance</a> model<br />
- <strong>decode</strong>
  - auto-set upcast if first decode fails<br />
  - restore dtype on upcast<br />
- <strong>taesd</strong> configurable number of layers<br />
  can be used to speed-up taesd decoding by reducing number of ops<br />
  e.g. if generating 1024px image, reducing layers by 1 will result in preview being 512px<br />
  set via <em>settings -&gt; live preview -&gt; taesd decode layers</em><br />
- <strong>xhinker</strong> prompt parser handle offloaded models<br />
- <strong>control</strong> better handle offloading<br />
- <strong>upscale</strong> will use resize-to if set to non-zero values over resize-by<br />
  applies to any upscale options, including refine workflow<br />
- <strong>networks</strong> add option to choose if mouse-over on network should attempt to fetch additional info<br />
  option:<code>extra_networks_fetch</code> enable/disable in <em>settings -&gt; networks</em><br />
- speed up some garbage collection ops<br />
- sampler settings add <strong>dynamic shift</strong><br />
  used by flow-matching samplers to adjust between structure and details<br />
- sampler settings force base shift<br />
  improves quality of the flow-matching samplers<br />
- <strong>t5</strong> support manually downloaded models<br />
  applies to all models that use t5 transformer<br />
- <strong>modern-ui</strong> add override field<br />
- full <strong>lint</strong> updates<br />
- use <code>diffusers</code> from main branch, no longer tied to release<br />
- improve diffusers/transformers/huggingface_hub progress reporting<br />
- use unique identifiers for all ui components<br />
- <strong>visual query</strong> (a.ka vqa or vlm) added support for several models
  - <a href="https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v1.5">MiaoshouAI PromptGen 1.5 Base</a>
  - <a href="https://huggingface.co/MiaoshouAI/Florence-2-large-PromptGen-v1.5">MiaoshouAI PromptGen 1.5 Large</a>
  - <a href="https://huggingface.co/thwri/CogFlorence-2.2-Large">CogFlorence 2.2 Large</a>
- <strong>modernui</strong> update<br />
- <strong>zluda</strong> update to 3.8.4, thanks @lshqqytiger!
- <strong>ipex</strong> update to 2.3.110+xpu on linux, thanks @Disty0!
- <strong>openvino</strong> update to 2024.3.0, thanks @Disty0!
- update <code>requirements</code>
- fix <strong>AuraFlow</strong><br />
- fix handling of model configs if offline config is not available<br />
- fix vae decode in backend original<br />
- fix model path typos<br />
- fix guidance end handler<br />
- fix script sorting<br />
- fix vae dtype during load<br />
- fix all ui labels are unique</p>
<h2 id="update-for-2024-08-31">Update for 2024-08-31</h2>
<h3 id="highlights-for-2024-08-31">Highlights for 2024-08-31</h3>
<p>Summer break is over and we are back with a massive update!  </p>
<p>Support for all of the new models:<br />
- <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">Black Forest Labs FLUX.1</a><br />
- <a href="https://huggingface.co/fal/AuraFlow">AuraFlow 0.3</a><br />
- <a href="https://huggingface.co/Alpha-VLLM/Lumina-Next-SFT-diffusers">AlphaVLLM Lumina-Next-SFT</a><br />
- <a href="https://huggingface.co/Kwai-Kolors/Kolors">Kwai Kolors</a><br />
- <a href="https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers">HunyuanDiT 1.2</a>  </p>
<p>What else? Just a bit... ;)  </p>
<p>New <strong>fast-install</strong> mode, new <strong>Optimum Quanto</strong> and <strong>BitsAndBytes</strong> based quantization modes, new <strong>balanced offload</strong> mode that dynamically offloads GPU&lt;-&gt;CPU as needed, and more...<br />
And from previous service-pack: new <strong>ControlNet-Union</strong> <em>all-in-one</em> model, support for <strong>DoRA</strong> networks, additional <strong>VLM</strong> models, new <strong>AuraSR</strong> upscaler  </p>
<p><strong>Breaking Changes...</strong></p>
<p>Due to internal changes, you'll need to reset your <strong>attention</strong> and <strong>offload</strong> settings!<br />
But...For a good reason, new <em>balanced offload</em> is magic when it comes to memory utilization while sacrificing minimal performance!</p>
<h3 id="details-for-2024-08-31">Details for 2024-08-31</h3>
<p><strong>New Models...</strong></p>
<p>To use and of the new models, simply select model from <em>Networks -&gt; Reference</em> and it will be auto-downloaded on first use  </p>
<ul>
<li><a href="https://blackforestlabs.ai/announcing-black-forest-labs/">Black Forest Labs FLUX.1</a><br />
  FLUX.1 models are based on a hybrid architecture of multimodal and parallel diffusion transformer blocks, scaled to 12B parameters and builing on flow matching<br />
  This is a very large model at ~32GB in size, its recommended to use a) offloading, b) quantization<br />
  For more information on variations, requirements, options, and how to donwload and use FLUX.1, see <a href="https://github.com/vladmandic/automatic/wiki/FLUX">Wiki</a><br />
  SD.Next supports:  </li>
<li><a href="https://huggingface.co/black-forest-labs/FLUX.1-dev">FLUX.1 Dev</a> and <a href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">FLUX.1 Schnell</a> original variations  </li>
<li>additional <a href="https://huggingface.co/Disty0/FLUX.1-dev-qint8">qint8</a> and <a href="https://huggingface.co/Disty0/FLUX.1-dev-qint4">qint4</a> quantized variations  </li>
<li>additional <a href="https://huggingface.co/sayakpaul/flux.1-dev-nf4">nf4</a> quantized variation  </li>
<li><a href="https://huggingface.co/fal/AuraFlow">AuraFlow</a><br />
  AuraFlow v0.3 is the fully open-sourced largest flow-based text-to-image generation model<br />
  This is a very large model at 6.8B params and nearly 31GB in size, smaller variants are expected in the future<br />
  Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  </li>
<li><a href="https://huggingface.co/Alpha-VLLM/Lumina-Next-SFT-diffusers">AlphaVLLM Lumina-Next-SFT</a><br />
  Lumina-Next-SFT is a Next-DiT model containing 2B parameters, enhanced through high-quality supervised fine-tuning (SFT)<br />
  This model uses T5 XXL variation of text encoder (previous version of Lumina used Gemma 2B as text encoder)<br />
  Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  </li>
<li><a href="https://huggingface.co/Kwai-Kolors/Kolors">Kwai Kolors</a><br />
  Kolors is a large-scale text-to-image generation model based on latent diffusion<br />
  This is an SDXL style model that replaces standard CLiP-L and CLiP-G text encoders with a massive <code>chatglm3-6b</code> encoder supporting both English and Chinese prompting  </li>
<li><a href="https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers">HunyuanDiT 1.2</a><br />
  Hunyuan-DiT is a powerful multi-resolution diffusion transformer (DiT) with fine-grained Chinese understanding  </li>
<li><a href="https://github.com/guoyww/animatediff/">AnimateDiff</a><br />
  support for additional models: <strong>SD 1.5 v3</strong> (Sparse), <strong>SD Lightning</strong> (4-step), <strong>SDXL Beta</strong>  </li>
</ul>
<p><strong>New Features...</strong></p>
<ul>
<li>support for <strong>Balanced Offload</strong>, thanks @Disty0!<br />
  balanced offload will dynamically split and offload models from the GPU based on the max configured GPU and CPU memory size<br />
  model parts that dont fit in the GPU will be dynamically sliced and offloaded to the CPU<br />
  see <em>Settings -&gt; Diffusers Settings -&gt; Max GPU memory and Max CPU memory</em><br />
<em>note</em>: recommended value for max GPU memory is ~80% of your total GPU memory<br />
<em>note</em>: balanced offload will force loading LoRA with Diffusers method<br />
<em>note</em>: balanced offload is not compatible with Optimum Quanto  </li>
<li>support for <strong>Optimum Quanto</strong> with 8 bit and 4 bit quantization options, thanks @Disty0 and @Trojaner!<br />
  to use, go to Settings -&gt; Compute Settings and enable "Quantize Model weights with Optimum Quanto" option<br />
<em>note</em>: Optimum Quanto requires PyTorch 2.4  </li>
<li>new prompt attention mode: <strong>xhinker</strong> which brings support for prompt attention to new models such as FLUX.1 and SD3<br />
  to use, enable in <em>Settings -&gt; Execution -&gt; Prompt attention</em></li>
<li>use <a href="https://huggingface.co/docs/peft/main/en/index">PEFT</a> for <strong>LoRA</strong> handling on all models other than SD15/SD21/SDXL<br />
  this improves LoRA compatibility for SC, SD3, AuraFlow, Flux, etc.  </li>
</ul>
<p><strong>Changes &amp; Fixes...</strong></p>
<ul>
<li>default resolution bumped from 512x512 to 1024x1024, time to move on ;)</li>
<li>convert <strong>Dynamic Attention SDP</strong> into a global SDP option, thanks @Disty0!<br />
<em>note</em>: requires reset of selected attention option</li>
<li>update default <strong>CUDA</strong> version from 12.1 to 12.4</li>
<li>update <code>requirements</code></li>
<li>samplers now prefers the model defaults over the diffusers defaults, thanks @Disty0!  </li>
<li>improve xyz grid for lora handling and add lora strength option  </li>
<li>don't enable Dynamic Attention by default on platforms that support Flash Attention, thanks @Disty0!  </li>
<li>convert offload options into a single choice list, thanks @Disty0!<br />
<em>note</em>: requires reset of selected offload option  </li>
<li>control module allows reszing of indivudual process override images to match input image<br />
  for example: set size-&gt;before-&gt;method:nearest, mode:fixed or mode:fill  </li>
<li>control tab includes superset of txt and img scripts</li>
<li>automatically offload disabled controlnet units  </li>
<li>prioritize specified backend if <code>--use-*</code> option is used, thanks @lshqqytiger</li>
<li>ipadapter option to auto-crop input images to faces to improve efficiency of face-transfter ipadapters  </li>
<li>update <strong>IPEX</strong> to 2.1.40+xpu on Linux, thanks @Disty0!  </li>
<li>general <strong>ROCm</strong> fixes, thanks @lshqqytiger!  </li>
<li>support for HIP SDK 6.1 on ZLUDA backend, thanks @lshqqytiger!</li>
<li>fix full vae previews, thanks @Disty0!  </li>
<li>fix default scheduler not being applied, thanks @Disty0!  </li>
<li>fix Stable Cascade with custom schedulers, thanks @Disty0!  </li>
<li>fix LoRA apply with force-diffusers</li>
<li>fix LoRA scales with force-diffusers</li>
<li>fix control API</li>
<li>fix VAE load refrerencing incorrect configuration</li>
<li>fix NVML gpu monitoring</li>
</ul>
<h2 id="update-for-2024-07-08">Update for 2024-07-08</h2>
<p>This release is primary service release with cumulative fixes and several improvements, but no breaking changes.</p>
<p><strong>New features...</strong>
- massive updates to <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a><br />
  with over 20 new pages and articles, now includes guides for nearly all major features<br />
<em>note</em>: this is work-in-progress, if you have any feedback or suggestions, please let us know!
  thanks @GenesisArtemis!<br />
- support for <strong>DoRA</strong> networks, thanks @AI-Casanova!
- support for <a href="https://pypi.org/project/uv/">uv</a>, extremely fast installer, thanks @Yoinky3000!<br />
  to use, simply add <code>--uv</code> to your command line params<br />
- <a href="https://huggingface.co/xinsir/controlnet-union-sdxl-1.0">Xinsir ControlNet++ Union</a><br />
  new SDXL <em>all-in-one</em> controlnet that can process any kind of preprocessors!
- <a href="https://huggingface.co/thwri/CogFlorence-2-Large-Freeze">CogFlorence 2 Large</a> VLM model<br />
  to use, simply select in process -&gt; visual query<br />
- <a href="https://huggingface.co/fal/AuraSR">AuraSR</a> high-quality 4x GAN-style upscaling model<br />
  note: this is a large upscaler at 2.5GB  </p>
<p><strong>And fixes...</strong>
- enable <strong>Florence VLM</strong>  for all platforms, thanks @lshqqytiger!<br />
- improve ROCm detection under WSL2, thanks @lshqqytiger!<br />
- add SD3 with FP16 T5 to list of detected models<br />
- fix executing extensions with zero params<br />
- add support for embeddings bundled in LoRA, thanks @AI-Casanova!<br />
- fix executing extensions with zero params<br />
- fix nncf for lora, thanks @Disty0!<br />
- fix diffusers version detection for SD3<br />
- fix current step for higher order samplers<br />
- fix control input type video<br />
- fix reset pipeline at the end of each iteration<br />
- fix faceswap when no faces detected<br />
- fix civitai search
- multiple ModernUI fixes</p>
<h2 id="update-for-2024-06-23">Update for 2024-06-23</h2>
<h3 id="highlights-for-2024-06-23">Highlights for 2024-06-23</h3>
<p>Following zero-day <strong>SD3</strong> release, a 10 days later heres a refresh with 10+ improvements<br />
including full prompt attention, support for compressed weights, additional text-encoder quantization modes.  </p>
<p>But theres more than SD3:<br />
- support for quantized <strong>T5</strong> text encoder <em>FP16/FP8/FP4/INT8</em> in all models that use T5: SD3, PixArt-, etc.<br />
- support for <strong>PixArt-Sigma</strong> in small/medium/large variants<br />
- support for <strong>HunyuanDiT 1.1</strong><br />
- additional <strong>NNCF weights compression</strong> support: SD3, PixArt, ControlNet, Lora<br />
- integration of <strong>MS Florence</strong> VLM/VQA <em>Base</em> and <em>Large</em> models<br />
- (finally) new release of <strong>Torch-DirectML</strong><br />
- additional efficiencies for users with low VRAM GPUs<br />
- over 20 overall fixes  </p>
<h3 id="model-improvements-for-2024-06-23">Model Improvements for 2024-06-23</h3>
<ul>
<li><strong>SD3</strong>: enable tiny-VAE (TAESD) preview and non-full quality mode  </li>
<li>SD3: enable base LoRA support  </li>
<li>SD3: add support for FP4 quantized T5 text encoder<br />
  simply select in <em>settings -&gt; model -&gt; text encoder</em><br />
<em>note</em> for SD3 with T5, set SD.Next to use FP16 precision, not BF16 precision  </li>
<li>SD3: add support for INT8 quantized T5 text encoder, thanks @Disty0!  </li>
<li>SD3: enable cpu-offloading for T5 text encoder, thanks @Disty0!  </li>
<li>SD3: simplified loading of model in single-file safetensors format<br />
  model load can now be performed fully offline  </li>
<li>SD3: full support for prompt parsing and attention, thanks @AI-Casanova!</li>
<li>SD3: ability to target different prompts to each of text-encoders, thanks @AI-Casanova!<br />
  example: <code>dog TE2: cat TE3: bird</code></li>
<li>SD3: add support for sampler shift for Euler FlowMatch<br />
  see <em>settings -&gt; samplers</em>, also available as param in xyz grid<br />
  higher shift means model will spend more time on structure and less on details  </li>
<li>SD3: add support for selecting T5 text encoder variant in XYZ grid</li>
<li><strong>Pixart-</strong>: Add <em>small</em> (512px) and <em>large</em> (2k) variations, in addition to existing <em>medium</em> (1k)  </li>
<li>Pixart-: Add support for 4/8bit quantized t5 text encoder<br />
<em>note</em> by default pixart- uses full fp16 t5 encoder with large memory footprint<br />
  simply select in <em>settings -&gt; model -&gt; text encoder</em> before or after model load  </li>
<li><strong>HunyuanDiT</strong>: support for model version 1.1  </li>
<li><strong>MS Florence</strong>: integration of Microsoft Florence VLM/VQA Base and Large models<br />
  simply select in <em>process -&gt; visual query</em>!</li>
</ul>
<h3 id="general-improvements-for-2024-06-23">General Improvements for 2024-06-23</h3>
<ul>
<li>support FP4 quantized T5 text encoder, in addition to existing FP8 and FP16</li>
<li>support for T5 text-encoder loader in <strong>all</strong> models that use T5<br />
<em>example</em>: load FP4 or FP8 quantized T5 text-encoder into PixArt Sigma!</li>
<li>support for <code>torch-directml</code> <strong>0.2.2</strong>, thanks @lshqqytiger!<br />
<em>note</em>: new directml is finally based on modern <code>torch</code> 2.3.1!  </li>
<li>xyz grid: add support for LoRA selector</li>
<li>vae load: store original vae so it can be restored when set to none</li>
<li>extra networks: info display now contains link to source url if model if its known<br />
  works for civitai and huggingface models  </li>
<li>force gc for lowvram users and improve gc logging</li>
<li>improved google.colab support</li>
<li>css tweaks for standardui</li>
<li>css tweaks for modernui</li>
<li>additional torch gc checks, thanks @Disty0!</li>
</ul>
<p><strong>Improvements: NNCF</strong>, thanks @Disty0!<br />
- SD3 and PixArt support<br />
- moved the first compression step to CPU<br />
- sequential cpu offload (lowvram) support<br />
- Lora support without reloading the model<br />
- ControlNet compression support  </p>
<h3 id="fixes-for-2024-06-23">Fixes for 2024-06-23</h3>
<ul>
<li>fix unsaturated outputs, force apply vae config on model load  </li>
<li>fix hidiffusion handling of non-square aspect ratios, thanks @ShenZhang-Shin!</li>
<li>fix control second pass resize  </li>
<li>fix hunyuandit set attention processor</li>
<li>fix civitai download without name</li>
<li>fix compatibility with latest adetailer</li>
<li>fix invalid sampler warning</li>
<li>fix starting from non git repo</li>
<li>fix control api negative prompt handling</li>
<li>fix saving style without name provided</li>
<li>fix t2i-color adapter</li>
<li>fix sdxl "has been incorrectly initialized"</li>
<li>fix api face-hires</li>
<li>fix api ip-adapter</li>
<li>fix memory exceptions with ROCm, thanks @Disty0!</li>
<li>fix face-hires with lowvram, thanks @Disty0!</li>
<li>fix pag incorrectly resetting pipeline</li>
<li>cleanup image metadata</li>
<li>restructure api examples: <code>cli/api-*</code></li>
<li>handle theme fallback when invalid theme is specified</li>
<li>remove obsolete training code leftovers</li>
</ul>
<h2 id="update-for-2024-06-13">Update for 2024-06-13</h2>
<h3 id="highlights-for-2024-06-13">Highlights for 2024-06-13</h3>
<p>First, yes, it is here and supported: <a href="https://stability.ai/news/stable-diffusion-3-medium"><strong>StabilityAI Stable Diffusion 3 Medium</strong></a><br />
for details on how to download and use, see <a href="https://github.com/vladmandic/automatic/wiki/SD3">Wiki</a></p>
<h4 id="what-else-2024-06-13">What else 2024-06-13?</h4>
<p>A lot of work on state-of-the-art multi-lingual models with both <a href="https://github.com/Tencent/HunyuanDiT">Tenecent HunyuanDiT</a> and <a href="https://github.com/mulanai/MuLan">MuLan</a><br />
Plus tons of minor features such as optimized initial install experience, <strong>T-Gate</strong> and <strong>ResAdapter</strong>, additional ModernUI themes (both light and dark) and fixes since the last release which was only 2 weeks ago!</p>
<h3 id="full-changelog-for-2024-06-13">Full Changelog for 2024-06-13</h3>
<h4 id="new-models-for-2024-06-23">New Models for 2024-06-23</h4>
<ul>
<li><a href="https://stability.ai/news/stable-diffusion-3-medium">StabilityAI Stable Diffusion 3 Medium</a><br />
  yup, supported!<br />
  quote: <em>"Stable Diffusion 3 Medium is a multimodal diffusion transformer (MMDiT) model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency"</em><br />
  sdnext also supports switching optional T5 text encoder on-the-fly as well as loading model from either diffusers repo or safetensors single-file<br />
  for details, see <a href="https://github.com/vladmandic/automatic/wiki/SD3">Wiki</a></li>
<li><a href="https://github.com/Tencent/HunyuanDiT">Tenecent HunyuanDiT</a> bilingual english/chinese diffusion transformer model<br />
  note: this is a very large model at ~17GB, but can be used with less VRAM using model offloading<br />
  simply select from networks -&gt; models -&gt; reference, model will be auto-downloaded on first use  </li>
</ul>
<h4 id="new-functionality-for-2024-06-23">New Functionality for 2024-06-23</h4>
<ul>
<li><a href="https://github.com/mulanai/MuLan">MuLan</a> Multi-language prompts
  write your prompts in ~110 auto-detected languages!<br />
  compatible with <em>SD15</em> and <em>SDXL</em><br />
  enable in scripts -&gt; MuLan and set encoder to <code>InternVL-14B-224px</code> encoder<br />
<em>note</em>: right now this is more of a proof-of-concept before smaller and/or quantized models are released<br />
  model will be auto-downloaded on first use: note its huge size of 27GB<br />
  even executing it in FP16 will require ~16GB of VRAM for text encoder alone<br />
  examples:  </li>
<li>English: photo of a beautiful woman wearing a white bikini on a beach with a city skyline in the background</li>
<li>Croatian: fotografija lijepe ene u bijelom bikiniju na plai s gradskim obzorom u pozadini</li>
<li>Italian: Foto di una bella donna che indossa un bikini bianco su una spiaggia con lo skyline di una citt sullo sfondo</li>
<li>Spanish: Foto de una hermosa mujer con un bikini blanco en una playa con un horizonte de la ciudad en el fondo</li>
<li>German: Foto einer schnen Frau in einem weien Bikini an einem Strand mit einer Skyline der Stadt im Hintergrund</li>
<li>Arabic:             </li>
<li>Japanese: </li>
<li>Chinese: , </li>
<li>Korean:           </li>
<li><a href="https://github.com/HaozheLiu-ST/T-GATE">T-Gate</a> Speed up generations by gating at which step cross-attention is no longer needed<br />
  enable via scripts -&gt; t-gate<br />
  compatible with <em>SD15</em>  </li>
<li><strong>PCM LoRAs</strong> allow for fast denoising using less steps with standard <em>SD15</em> and <em>SDXL</em> models<br />
  download from <a href="https://huggingface.co/Kijai/converted_pcm_loras_fp16/tree/main">https://huggingface.co/Kijai/converted_pcm_loras_fp16/tree/main</a></li>
<li><a href="https://github.com/bytedance/res-adapter">ByteDance ResAdapter</a> resolution-free model adapter<br />
  allows to use resolutions from 0.5 to 2.0 of original model resolution, compatible with <em>SD15</em> and <em>SDXL</em>
  enable via scripts -&gt; resadapter and select desired model</li>
<li><strong>Kohya HiRes Fix</strong> allows for higher resolution generation using standard <em>SD15</em> models<br />
  enable via scripts -&gt; kohya-hires-fix<br />
<em>note</em>: alternative to regular hidiffusion method, but with different approach to scaling  </li>
<li>additional built-in 4 great custom trained <strong>ControlNet SDXL</strong> models from Xinsir: OpenPose, Canny, Scribble, AnimePainter<br />
  thanks @lbeltrame</li>
<li>add torch <strong>full deterministic mode</strong>
  enable in settings -&gt; compute -&gt; use deterministic mode<br />
  typical differences are not large and its disabled by default as it does have some performance impact  </li>
<li>new sampler: <strong>Euler FlowMatch</strong>  </li>
</ul>
<h4 id="improvements-fixes-2024-06-13">Improvements Fixes 2024-06-13</h4>
<ul>
<li>additional modernui themes</li>
<li>reintroduce prompt attention normalization, disabled by default, enable in settings -&gt; execution<br />
  this can drastically help with unbalanced prompts  </li>
<li>further work on improving python 3.12 functionality and remove experimental flag<br />
  note: recommended version remains python 3.11 for all users, except if you are using directml in which case its python 3.10  </li>
<li>improved <strong>installer</strong> for initial installs<br />
  initial install will do single-pass install of all required packages with correct versions<br />
  subsequent runs will check package versions as necessary  </li>
<li>add env variable <code>SD_PIP_DEBUG</code> to write <code>pip.log</code> for all pip operations<br />
  also improved installer logging  </li>
<li>add python version check for <code>torch-directml</code>  </li>
<li>do not install <code>tensorflow</code> by default  </li>
<li>improve metadata/infotext parser<br />
  add <code>cli/image-exif.py</code> that can be used to view/extract metadata from images  </li>
<li>lower overhead on generate calls  </li>
<li>auto-synchronize modernui and core branches  </li>
<li>add option to pad prompt with zeros, thanks @Disty</li>
</ul>
<h4 id="fixes-2024-06-13">Fixes 2024-06-13</h4>
<ul>
<li>cumulative fixes since the last release  </li>
<li>fix apply/unapply hidiffusion for sd15  </li>
<li>fix controlnet reference enabled check  </li>
<li>fix face-hires with control batch count  </li>
<li>install pynvml on-demand  </li>
<li>apply rollback-vae option to latest torch versions, thanks @Iaotle  </li>
<li>face hires skip if strength is 0  </li>
<li>restore all sampler configuration on sampler change  </li>
</ul>
<h2 id="update-for-2024-06-02">Update for 2024-06-02</h2>
<ul>
<li>fix textual inversion loading</li>
<li>fix gallery mtime display</li>
<li>fix extra network scrollable area when using modernui</li>
<li>fix control prompts list handling</li>
<li>fix restore variation seed and strength</li>
<li>fix negative prompt parsing from metadata</li>
<li>fix stable cascade progress monitoring</li>
<li>fix variation seed with hires pass</li>
<li>fix loading models trained with onetrainer</li>
<li>add variation seed info to metadata</li>
<li>workaround for scale-by when using modernui</li>
<li>lock torch-directml version</li>
<li>improve xformers installer</li>
<li>improve ultralytics installer (face-hires)</li>
<li>improve triton installer (compile)</li>
<li>improve insightface installer (faceip)</li>
<li>improve mim installer (dwpose)</li>
<li>add dpm++ 1s and dpm++ 3m aliases for dpm++ 2m scheduler with different orders</li>
</ul>
<h2 id="update-for-2024-05-28">Update for 2024-05-28</h2>
<h3 id="highlights-for-2024-05-28">Highlights for 2024-05-28</h3>
<p>New <a href="https://github.com/vladmandic/automatic">SD.Next</a> release has been baking in <code>dev</code> for a longer than usual, but changes are massive - about 350 commits for core and 300 for UI...</p>
<p>Starting with the new UI - yup, this version ships with a <em>preview</em> of the new <a href="https://github.com/BinaryQuantumSoul/sdnext-modernui">ModernUI</a><br />
For details on how to enable and use it, see <a href="https://github.com/BinaryQuantumSoul/sdnext-modernui">Home</a> and <a href="https://github.com/vladmandic/automatic/wiki/Themes">WiKi</a><br />
<strong>ModernUI</strong> is still in early development and not all features are available yet, please report <a href="https://github.com/BinaryQuantumSoul/sdnext-modernui/issues">issues and feedback</a><br />
Thanks to @BinaryQuantumSoul for his hard work on this project!  </p>
<p><em>What else?</em></p>
<h4 id="new-built-in-features">New built-in features</h4>
<ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps">PWA</a> SD.Next is now installable as a web-app</li>
<li><strong>Gallery</strong>: extremely fast built-in gallery viewer<br />
  List, preview, search through all your images and videos!  </li>
<li><strong>HiDiffusion</strong> allows generating very-high resolution images out-of-the-box using standard models  </li>
<li><strong>Perturbed-Attention Guidance</strong> (PAG) enhances sample quality in addition to standard CFG scale  </li>
<li><strong>LayerDiffuse</strong> simply create transparent (foreground-only) images  </li>
<li><strong>IP adapter masking</strong> allows to use multiple input images for each segment of the input image  </li>
<li>IP adapter <strong>InstantStyle</strong> implementation  </li>
<li><strong>Token Downsampling</strong> (ToDo) provides significant speedups with minimal-to-none quality loss  </li>
<li><strong>Samplers optimizations</strong> that allow normal samplers to complete work in 1/3 of the steps!<br />
  Yup, even popular DPM++2M can now run in 10 steps with quality equaling 30 steps using <strong>AYS</strong> presets  </li>
<li>Native <strong>wildcards</strong> support  </li>
<li>Improved built-in <strong>Face HiRes</strong>  </li>
<li>Better <strong>outpainting</strong>  </li>
<li>And much more...<br />
  For details of above features and full list, see <a href="https://github.com/vladmandic/automatic/blob/dev/CHANGELOG.md">Changelog</a></li>
</ul>
<h4 id="new-models">New models</h4>
<p>While still waiting for <em>Stable Diffusion 3.0</em>, there have been some significant models released in the meantime:</p>
<ul>
<li><a href="https://pixart-alpha.github.io/PixArt-sigma-project/">PixArt-</a>, high end diffusion transformer model (<em>DiT</em>) capable of directly generating images at 4K resolution  </li>
<li><a href="https://github.com/IDKiro/sdxs">SDXS</a>, extremely fast 1-step generation consistency model  </li>
<li><a href="https://huggingface.co/ByteDance/Hyper-SD">Hyper-SD</a>, 1-step, 2-step, 4-step and 8-step optimized models  </li>
</ul>
<p><em>Note</em><br />
<a href="https://github.com/vladmandic/automatic">SD.Next</a> is no longer marked as a fork of <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/">A1111</a> and github project has been fully detached<br />
Given huge number of changes with <em>+3443/-3342</em> commits diff (at the time of fork detach) over the past year,<br />
a completely different backend/engine and a change of focus, it is time to give credit to original <a href="https://github.com/auTOMATIC1111">author</a>,  and move on!  </p>
<h3 id="full-changelog-for-2024-05-28">Full ChangeLog for 2024-05-28</h3>
<ul>
<li><strong>Features</strong>:</li>
<li><strong>ModernUI</strong> preview of the new <a href="https://github.com/BinaryQuantumSoul/sdnext-modernui">ModernUI</a>  </li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps">PWA</a> SD.Next is now installable as a web-app and includes verified manifest  </li>
<li>**Gallery</li>
<li><strong>Gallery</strong>: list, preview, search through all your images and videos!<br />
    Implemented as infinite-scroll with client-side-caching and lazy-loading while being fully async and non-blocking<br />
    Search or sort by path, name, size, width, height, mtime or any image metadata item, also with extended syntax like <em>width &gt; 1000</em><br />
<em>Settings</em>: optional additional user-defined folders, thumbnails in fixed or variable aspect-ratio  </li>
<li><a href="https://github.com/megvii-research/HiDiffusion">HiDiffusion</a>:<br />
    Generate high-resolution images using your standard models without duplicates/distorsions AND improved performance<br />
    For example, <em>SD15</em> can now go up to <em>2024x2048</em> and <em>SDXL</em> up to <em>4k</em> natively
    Simply enable checkbox in advanced menu and set desired resolution<br />
    Additional settings are available in <em>settings -&gt; inference settings -&gt; hidiffusion</em><br />
    And can also be set and used via <em>xyz grid</em><br />
<em>Note</em>: HiDiffusion resolution sensitive, so if you get error, set resolution to be multiples of 128  </li>
<li><a href="https://github.com/KU-CVLAB/Perturbed-Attention-Guidance">Perturbed-Attention Guidance</a><br />
    PAG enhances sample quality by utilizing self-attention in formation of latent in addition to standard CFG scale<br />
    Simply set <em>advanced -&gt; attention guidance</em> and <em>advanced -&gt; adaptive scaling</em><br />
    Additional options are available in <em>settings -&gt; inference settings -&gt; pag</em><br />
<em>Note</em>: PAG has replaced SAG as attention guidance method in SD.Next  </li>
<li><a href="https://github.com/rootonchair/diffuser_layerdiffuse">LayerDiffuse</a>
    Create transparent images with foreground-only being generated<br />
    Simply select from scripts -&gt; apply to current model<br />
    All necessary files will be auto-downloaded on first use  </li>
<li><strong>IP Adapter Masking</strong>:<br />
    Powerful method of using masking with ip-adapters<br />
    When combined with multiple ip-adapters, it allows for different inputs guidance for each segment of the input image<br />
<em>Hint</em>: to create masks, you can use manually created masks or control-&gt;mask module with auto-segment to create masks and later upload them  </li>
<li><strong>IP Adapter advanced layer configuration</strong>:<br />
    Allows for more control over how each layer of ip-adapter is applied, requires a valid dict to be passed as input<br />
    See <a href="https://github.com/InstantStyle/InstantStyle">InstantStyle</a> for details  </li>
<li><strong>OneDiff</strong>: new optimization/compile engine, thanks @aifartist<br />
    As with all other compile engines, enable via <em>settings -&gt; compute settings -&gt; compile</em>  </li>
<li><a href="https://arxiv.org/html/2402.13573v2">ToDo</a> Token Downsampling for Efficient Generation of High-Resolution Images<br />
    Newer alternative method to <a href="https://github.com/dbolya/tomesd">ToMe</a> that can provide speed-up with minimal quality loss<br />
    Enable in <em>settings -&gt; inference settings -&gt; token merging</em><br />
    Also available in XYZ grid  </li>
<li><strong>Outpaint</strong>:<br />
    New method of outpainting that uses a combination of auto-masking and edge generation to create seamless transitions between original and generated image<br />
    Use on control tab:<ul>
<li><em>input -&gt; denoising strength: 0.5 or higher</em></li>
<li><em>select image -&gt; outpaint -&gt; expand edges or zoom out to desired size</em></li>
<li><em>size -&gt; mode: outpaint, method: nearest</em></li>
<li><em>mask -&gt; inpaint masked only (if you want to keep original image)</em></li>
</ul>
</li>
<li><strong>Wildcards</strong>:<ul>
<li>native support of standard file-based wildcards in prompt  </li>
<li>enabled by default, can be disabled in <em>settings -&gt; extra networks</em> if you want to use 3rd party extension  </li>
<li>wildcards folder is set in <em>settings -&gt; system paths</em> and can be flat-file list or complex folder structure  </li>
<li>matches strings <code>"__*__"</code> in positive and negative prompts  </li>
<li>supports filename and path-based wildcards  </li>
<li>supports nested wildcards (wildcard can refer to another wildcard, etc.)  </li>
<li>supports wildcards files in one-choice per line or multiple choices per line separated by <code>|</code> format  </li>
<li><em>note</em>: this is in addition to previously released style-based wildcards  </li>
</ul>
</li>
<li><strong>Models</strong>:</li>
<li><strong>Load UNET</strong>: ability to override/load external UNET to a selected model<br />
    Works similar to how VAE is selected and loaded: Set UNet folder and UNet model in settings<br />
    Can be replaced on-the-fly, not just during initial model load<br />
    Enables usage of fine-tunes such as <a href="https://huggingface.co/mhdang/dpo-sd1.5-text2image-v1">DPO-SD15</a> or <a href="https://huggingface.co/mhdang/dpo-sdxl-text2image-v1">DPO-SDXL</a><br />
<em>Note</em>: if there is a <code>JSON</code> file with the same name as the model it will be used as Unet config, otherwise Unet config from currently loaded model will be used  </li>
<li><a href="https://pixart-alpha.github.io/PixArt-sigma-project/">PixArt-</a>
    pixart- is a high end diffusion Transformer model (DiT) with a T5 encoder/decoder capable of directly generating images at 4K resolution<br />
    to use, simply select from <em>networks -&gt; models -&gt; reference -&gt; PixArt-</em><br />
<em>note</em>: this is a very large model at ~22GB<br />
    set parameters: <em>sampler: Default</em>  </li>
<li><a href="https://github.com/IDKiro/sdxs">SDXS</a>
    sdxs is an extremely fast 1-step generation consistency model that also uses TAESD as quick VAE out-of-the-box<br />
    to use, simply select from <em>networks -&gt; models -&gt; reference -&gt; SDXS</em><br />
    set parameters: <em>sampler: CMSI, steps: 1, cfg_scale: 0.0</em></li>
<li><a href="https://huggingface.co/ByteDance/Hyper-SD">Hyper-SD</a><br />
    sd15 and sdxl 1-step, 2-step, 4-step and 8-step optimized models using lora<br />
    set parameters: <em>sampler: TCD or LCM, steps: 1/2/4/8, cfg_scale: 0.0</em>  </li>
<li><strong>UI</strong>:</li>
<li>Faster <strong>UI</strong> load times</li>
<li>Theme types:<br />
<strong>Standard</strong> (built-in themes), <strong>Modern</strong> (experimental nextgen ui), <strong>None</strong> (used for Gradio and Huggingface 3rd party themes)<br />
    Specifying a theme type updates list of available themes<br />
    For example, <em>Gradio</em> themes will not appear as available if theme type is set to <em>Standard</em>  </li>
<li>Redesign of base txt2img interface  </li>
<li>Minor tweaks to styles: refresh/apply/save</li>
<li>See details in <a href="https://github.com/vladmandic/automatic/wiki/Themes">WiKi</a></li>
<li><strong>API</strong>:</li>
<li>Add API endpoint <code>/sdapi/v1/control</code> and CLI util <code>cli/simple-control.py</code><br />
    (in addition to previously added <code>/sdapi/v1/preprocessors</code> and <code>/sdapi/v1/masking</code>)<br />
    example:
    &gt; simple-control.py --prompt 'woman in the city' --sampler UniPC --steps 20<br />
    &gt; --input \~/generative/Samples/cutie-512.png --output /tmp/test.png --processed /tmp/proc.png<br />
    &gt; --control 'Canny:Canny FP16:0.7, OpenPose:OpenPose FP16:0.8' --type controlnet<br />
    &gt; --ipadapter 'Plus:~/generative/Samples/cutie-512.png:0.5'  </li>
<li>Add API endpoint <code>/sdapi/v1/vqa</code> and CLI util <code>cli/simple-vqa.py</code></li>
<li><strong>Changes</strong>:</li>
<li>Due to change in Diffusers model loading<br />
    initial model load will now fetch config files required for the model<br />
    from the Huggingface site instead of using predefined YAML files</li>
<li>Removed built-in extensions: <em>ControlNet</em> and <em>Image-Browser</em><br />
    as both <em>image-browser</em> and <em>controlnet</em> have native built-in equivalents<br />
    both can still be installed by user if desired  </li>
<li>Different defaults depending on available GPU, thanks @Disty0<ul>
<li>4GB and below: <em>lowvram</em></li>
<li>8GB and below: <em>medvram</em></li>
<li>Cross-attention: Dynamic Attention SDP with <em>medvram</em> or <em>lowvram</em>, otherwise SDP  </li>
<li>VAE Tiling enabled with <em>medvram</em> and <em>lowvram</em></li>
<li>Disable Extract EMA by default</li>
<li>Disable forced VAE Slicing for <em>lowvram</em></li>
</ul>
</li>
<li>Upscaler compile disabled by default with OpenVINO backend  </li>
<li>Hypernetwork support disabled by default, can be enabled in settings  </li>
<li><strong>Improvements</strong>:</li>
<li>Faster server startup  </li>
<li>Styles apply wildcards to params</li>
<li>Face HiRes fully configurable and higher quality when using high-resolution models  </li>
<li>Extra networks persistent sort order in settings  </li>
<li>Add option to make batch generations use fully random seed vs sequential  </li>
<li>Make metadata in full screen viewer optional</li>
<li>Add VAE civitai scan metadata/preview</li>
<li>More efficient in-browser callbacks</li>
<li>Updated all system requirements  </li>
<li>UI log monitor will auto-reconnect to server on server restart  </li>
<li>UI styles includes indicator for active styles  </li>
<li>UI reduce load on browser  </li>
<li>Secondary sampler add option "same as primary"  </li>
<li>Change attention mechanism on-the-fly without model reload, thanks @Disty0  </li>
<li>Update stable-fast with support for torch 2.2.2 and 2.3.0, thanks @Aptronymist</li>
<li>Add torch <em>cudaMallocAsync</em> in compute options<br />
    Can improve memory utilization on compatible GPUs (RTX and newer)  </li>
<li>Torch dynamic profiling<br />
    You can enable/disable full torch profiling in settings top menu on-the-fly  </li>
<li>Prompt caching - if you use the same prompt multiple times, no need to re-parse and encode it<br />
    Useful for batches as prompt processing is ~0.1sec on each pass  </li>
<li>Enhance <code>SD_PROMPT_DEBUG</code> to show actual tokens used</li>
<li>Support controlnet manually downloads models in both standalone and diffusers format<br />
    For standalone, simply copy safetensors file to <code>models/control/controlnet</code> folder<br />
    For diffusers format, create folder with model name in <code>models/control/controlnet/</code><br />
    and copy <code>model.json</code> and <code>diffusion_pytorch_model.safetensors</code> to that folder  </li>
<li><strong>Samplers</strong></li>
<li>Add <em>Euler SGM</em> variation (e.g. SGM Uniform), optimized for SDXL-Lightning models<br />
<em>note</em>: you can use other samplers as well with SDXL-Lightning models  </li>
<li>Add <em>CMSI</em> sampler, optimized for consistency models  </li>
<li>Add option <em>timestep spacing</em> to sampler settings and sampler section in main ui
    Note: changing timestep spacing changes behavior of sampler and can help to make any sampler turbo/lightning compatibile</li>
<li>Add option <em>timesteps</em> to manually set timesteps instead of relying on steps+spacing<br />
    Additionally, presets from nVidias align-you-steps reasearch are provided<br />
    Result is that perfectly aligned steps can drastically reduce number of steps needed!<br />
    For example, <strong>AYS</strong> preset alows DPM++2M to run in ~10 steps with quality equallying ~30 steps!  </li>
<li><strong>IPEX</strong>, thanks @Disty0</li>
<li>Update to <em>IPEX 2.1.20</em> on Linux<br />
    requires removing the venv folder to update properly  </li>
<li>Removed 1024x1024 workaround  </li>
<li>Disable ipexrun by default, set <code>IPEXRUN=True</code> if you want to use <code>ipexrun</code>  </li>
<li><strong>ROCm</strong>, thanks @Disty0  </li>
<li>Add support for ROCm 6.1 nighthly builds  </li>
<li>Switch to stable branch of PyTorch  </li>
<li>Compatibility improvenments  </li>
<li>Add <strong>MIGraphX</strong> torch compile engine  </li>
<li><strong>ZLUDA</strong>, thanks @lshqqytiger</li>
<li>Rewrite ZLUDA installer</li>
<li>ZLUDA <strong>v3.8</strong> updates: Runtime API support</li>
<li>Add <code>--reinstall-zluda</code> (to download the latest ZLUDA)</li>
<li><strong>Fixes</strong>:</li>
<li>Update requirements</li>
<li>Installer automatically handle detached git states  </li>
<li>Prompt params parser</li>
<li>Allowing forcing LoRA loading method for some or all models</li>
<li>Image save without metadata</li>
<li>API generate save metadata</li>
<li>Face/InstantID faults</li>
<li>CivitAI update model info for all models</li>
<li>FP16/BF16 test on model load</li>
<li>Variation seed possible NaNs</li>
<li>Enumerate diffusers model with multiple variants</li>
<li>Diffusers skip non-models on enum</li>
<li>Face-HiRes compatibility with control modules</li>
<li>Face-HiRes avoid doule save in some scenarios</li>
<li>Loading safetensors embeddings</li>
<li>CSS fixes</li>
<li>Check if attention processor is compatible with model</li>
<li>SD Upscale when used with control module</li>
<li>Noise sampler seed, thanks @leppie</li>
<li>Control module with ADetailer and active ControlNet</li>
<li>Control module restore button full functionality</li>
<li>Control improved handling with multiple control units and different init images</li>
<li>Control add correct metadata to image</li>
<li>Time embeddings load part of model load</li>
<li>A1111 update OptionInfo properties</li>
<li>MOTD exception handling</li>
<li>Notifications not triggering</li>
<li>Prompt cropping on copy</li>
</ul>
<h2 id="update-for-2024-03-19">Update for 2024-03-19</h2>
<h3 id="highlights-2024-03-19">Highlights 2024-03-19</h3>
<p>New models:
- <a href="https://github.com/Stability-AI/StableCascade">Stable Cascade</a> <em>Full</em> and <em>Lite</em>
- <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">Playground v2.5</a>
- <a href="https://github.com/youngwanLEE/sdxl-koala">KOALA 700M</a>
- <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1">Stable Video Diffusion XT 1.1</a>
- <a href="https://huggingface.co/ali-vilab/i2vgen-xl">VGen</a>  </p>
<p>New pipelines and features:
- Img2img using <a href="https://leditsplusplus-project.static.hf.space/index.html">LEdit++</a>, context aware method with image analysis and positive/negative prompt handling
- Trajectory Consistency Distillation <a href="https://mhh0318.github.io/tcd">TCD</a> for processing in even less steps
- Visual Query &amp; Answer using <a href="https://github.com/vikhyat/moondream">moondream2</a> as an addition to standard interrogate methods
- <strong>Face-HiRes</strong>: simple built-in detailer for face refinements
- Even simpler outpaint: when resizing image, simply pick outpaint method and if image has different aspect ratio, blank areas will be outpainted!
- UI aspect-ratio controls and other UI improvements
- User controllable invisibile and visible watermarking
- Native composable LoRA</p>
<p>What else?</p>
<ul>
<li><strong>Reference models</strong>: <em>Networks -&gt; Models -&gt; Reference</em>: All reference models now come with recommended settings that can be auto-applied if desired  </li>
<li><strong>Styles</strong>: Not just for prompts! Styles can apply <em>generate parameters</em> as templates and can be used to <em>apply wildcards</em> to prompts<br />
improvements, Additional API endpoints  </li>
<li>Given the high interest in <a href="https://github.com/vosen/ZLUDA">ZLUDA</a> engine introduced in last release weve updated much more flexible/automatic install procedure (see <a href="https://github.com/vladmandic/automatic/wiki/ZLUDA">wiki</a> for details)  </li>
<li>Plus Additional Improvements such as: Smooth tiling, Refine/HiRes workflow improvements, Control workflow  </li>
</ul>
<p>Further details:<br />
- For basic instructions, see <a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a><br />
- For more details on all new features see full <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a><br />
- For documentation, see <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a>
- <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a> server  </p>
<h3 id="full-changelog-2024-03-19">Full Changelog 2024-03-19</h3>
<ul>
<li><a href="https://github.com/Stability-AI/StableCascade">Stable Cascade</a> <em>Full</em> and <em>Lite</em></li>
<li>large multi-stage high-quality model from warp-ai/wuerstchen team and released by stabilityai  </li>
<li>download using networks -&gt; reference</li>
<li>see <a href="https://github.com/vladmandic/automatic/wiki/Stable-Cascade">wiki</a> for details</li>
<li><a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">Playground v2.5</a></li>
<li>new model version from Playground: based on SDXL, but with some cool new concepts</li>
<li>download using networks -&gt; reference</li>
<li>set sampler to <em>DPM++ 2M EDM</em> or <em>Euler EDM</em></li>
<li><a href="https://github.com/youngwanLEE/sdxl-koala">KOALA 700M</a></li>
<li>another very fast &amp; light sdxl model where original unet was compressed and distilled to 54% of original size  </li>
<li>download using networks -&gt; reference</li>
<li><em>note</em> to download fp16 variant (recommended), set settings -&gt; diffusers -&gt; preferred model variant  </li>
<li><a href="https://leditsplusplus-project.static.hf.space/index.html">LEdit++</a></li>
<li>context aware img2img method with image analysis and positive/negative prompt handling  </li>
<li>enable via img2img -&gt; scripts -&gt; ledit</li>
<li>uses following params from standard img2img: cfg scale (recommended ~3), steps (recommended ~50), denoise strength (recommended ~0.7)</li>
<li>can use postive and/or negative prompt to guide editing process<ul>
<li>positive prompt: what to enhance, strength and threshold for auto-masking</li>
<li>negative prompt: what to remove, strength and threshold for auto-masking  </li>
</ul>
</li>
<li><em>note</em>: not compatible with model offloading</li>
<li><strong>Second Pass / Refine</strong></li>
<li>independent upscale and hires options: run hires without upscale or upscale without hires or both</li>
<li>upscale can now run 0.1-8.0 scale and will also run if enabled at 1.0 to allow for upscalers that simply improve image quality</li>
<li>update ui section to reflect changes</li>
<li><em>note</em>: behavior using backend:original is unchanged for backwards compatibilty</li>
<li><strong>Visual Query</strong> visual query &amp; answer in process tab  </li>
<li>go to process -&gt; visual query  </li>
<li>ask your questions, e.g. "describe the image", "what is behind the subject", "what are predominant colors of the image?"</li>
<li>primary model is <a href="https://github.com/vikhyat/moondream">moondream2</a>, a <em>tiny</em> 1.86B vision language model<br />
<em>note</em>: its still 3.7GB in size, so not really tiny  </li>
<li>additional support for multiple variations of several base models: <em>GIT, BLIP, ViLT, PIX</em>, sizes range from 0.3 to 1.7GB  </li>
<li><strong>Video</strong></li>
<li><strong>Image2Video</strong><ul>
<li>new module for creating videos from images  </li>
<li>simply enable from <em>img2img -&gt; scripts -&gt; image2video</em>  </li>
<li>model is auto-downloaded on first use</li>
<li>based on <a href="https://huggingface.co/ali-vilab/i2vgen-xl">VGen</a>  </li>
</ul>
</li>
<li><strong>Stable Video Diffusion</strong><ul>
<li>updated with <em>SVD 1.0, SVD XT 1.0 and SVD XT 1.1</em></li>
<li>models are auto-downloaded on first use</li>
<li>simply enable from <em>img2img -&gt; scripts -&gt; stable video diffusion</em>  </li>
<li>for svd 1.0, use frames=~14, for xt models use frames=~25</li>
</ul>
</li>
<li><strong>Composable LoRA</strong>, thanks @AI-Casanova</li>
<li>control lora strength for each step
    for example: <code>&lt;xxx:0.1@0,0.9@1&gt;</code> means strength=0.1 for step at 0% and intepolate towards strength=0.9 for step at 100%</li>
<li><em>note</em>: this is a very experimental feature and may not work as expected</li>
<li><strong>Control</strong></li>
<li>added <em>refiner/hires</em> workflows</li>
<li>added resize methods to before/after/mask: fixed, crop, fill</li>
<li><strong>Styles</strong>: styles are not just for prompts!</li>
<li>new styles editor: <em>networks -&gt; styles -&gt; edit</em></li>
<li>styles can apply generate parameters, for example to have a style that enables and configures hires:<br />
    parameters=<code>enable_hr: True, hr_scale: 2, hr_upscaler: Latent Bilinear antialias, hr_sampler_name: DEIS, hr_second_pass_steps: 20, denoising_strength: 0.5</code></li>
<li>styles can apply wildcards to prompts, for example:<br />
    wildcards=<code>movie=mad max, dune, star wars, star trek; intricate=realistic, color sketch, pencil sketch, intricate</code></li>
<li>as usual, you can apply any number of styles so you can choose which settings are applied and in which order and which wildcards are used</li>
<li><strong>UI</strong></li>
<li><em>aspect-ratio</em><em> add selector and lock to width/height control<br />
    allowed aspect ration can be configured via </em>settings -&gt; user interface*  </li>
<li><em>interrogate</em> tab is now merged into <em>process</em> tab  </li>
<li><em>image viewer</em> now displays image metadata</li>
<li><em>themes</em> improve on-the-fly switching</li>
<li><em>log monitor</em> flag server warnings/errors and overall improve display</li>
<li><em>control</em> separate processor settings from unit settings</li>
<li><strong>Face HiRes</strong></li>
<li>new <em>face restore</em> option, works similar to well-known <em>adetailer</em> by running an inpaint on detected faces but with just a checkbox to enable/disable  </li>
<li>set as default face restorer in settings -&gt; postprocessing  </li>
<li>disabled by default, to enable simply check <em>face restore</em> in your generate advanced settings  </li>
<li>strength, steps and sampler are set using by hires section in refine menu  </li>
<li>strength can be overriden in settings -&gt; postprocessing  </li>
<li>will use secondary prompt and secondary negative prompt if present in refine  </li>
<li><strong>Watermarking</strong></li>
<li>SD.Next disables all known watermarks in models, but does allow user to set custom watermark  </li>
<li>see <em>settings -&gt; image options -&gt; watermarking</em>  </li>
<li>invisible watermark: using steganogephy  </li>
<li>image watermark: overlaid on top of image  </li>
<li><strong>Reference models</strong></li>
<li>additional reference models available for single-click download &amp; run:
    <em>Stable Cascade, Stable Cascade lite, Stable Video Diffusion XT 1.1</em>  </li>
<li>reference models will now download <em>fp16</em> variation by default  </li>
<li>reference models will print recommended settings to log if present</li>
<li>new setting in extra network: <em>use reference values when available</em><br />
    disabled by default, if enabled will force use of reference settings for models that have them</li>
<li><strong>Samplers</strong></li>
<li><a href="https://mhh0318.github.io/tcd/">TCD</a>: Trajectory Consistency Distillation<br />
    new sampler that produces consistent results in a very low number of steps (comparable to LCM but without reliance on LoRA)<br />
    for best results, use with TCD LoRA: <a href="https://huggingface.co/h1t/TCD-SDXL-LoRA">https://huggingface.co/h1t/TCD-SDXL-LoRA</a></li>
<li><em>DPM++ 2M EDM</em> and <em>Euler EDM</em><br />
    EDM is a new solver algorithm currently available for DPM++2M and Euler samplers<br />
    Note that using EDM samplers with non-EDM optimized models will provide just noise and vice-versa  </li>
<li><strong>Improvements</strong></li>
<li><strong>FaceID</strong> extend support for LoRA, HyperTile and FreeU, thanks @Trojaner</li>
<li><strong>Tiling</strong> now extends to both Unet and VAE producing smoother outputs, thanks @AI-Casanova</li>
<li>new setting in image options: <em>include mask in output</em></li>
<li>improved params parsing from from prompt string and styles</li>
<li>default theme updates and additional built-in theme <em>black-gray</em></li>
<li>support models with their own YAML model config files</li>
<li>support models with their own JSON per-component config files, for example: <code>playground-v2.5_vae.config</code></li>
<li>prompt can have comments enclosed with <code>/*</code> and <code>*/</code><br />
    comments are extracted from prompt and added to image metadata  </li>
<li><strong>ROCm</strong>  </li>
<li>add <strong>ROCm</strong> 6.0 nightly option to installer, thanks @jicka</li>
<li>add <em>flash attention</em> support for rdna3, thanks @Disty0<br />
    install flash_attn package for rdna3 manually and enable <em>flash attention</em> from <em>compute settings</em><br />
    to install flash_attn, activate the venv and run <code>pip install -U git+https://github.com/ROCm/flash-attention@howiejay/navi_support</code>  </li>
<li><strong>IPEX</strong></li>
<li>disabled IPEX Optimize by default  </li>
<li><strong>API</strong></li>
<li>add preprocessor api endpoints<br />
    GET:<code>/sdapi/v1/preprocessors</code>, POST:<code>/sdapi/v1/preprocess</code>, sample script:<code>cli/simple-preprocess.py</code></li>
<li>add masking api endpoints<br />
    GET:<code>/sdapi/v1/masking</code>, POST:<code>/sdapi/v1/mask</code>, sample script:<code>cli/simple-mask.py</code></li>
<li><strong>Internal</strong></li>
<li>improved vram efficiency for model compile, thanks @Disty0</li>
<li><strong>stable-fast</strong> compatibility with torch 2.2.1  </li>
<li>remove obsolete textual inversion training code</li>
<li>remove obsolete hypernetworks training code</li>
<li><strong>Refiner</strong> validated workflows:</li>
<li>Fully functional: SD15 + SD15, SDXL + SDXL, SDXL + SDXL-R</li>
<li>Functional, but result is not as good: SD15 + SDXL, SDXL + SD15, SD15 + SDXL-R</li>
<li><strong>SDXL Lightning</strong> models just-work, just makes sure to set CFG Scale to 0<br />
    and choose a best-suited sampler, it may not be the one youre used to (e.g. maybe even basic Euler)  </li>
<li><strong>Fixes</strong></li>
<li>improve <em>model cpu offload</em> compatibility</li>
<li>improve <em>model sequential offload</em> compatibility</li>
<li>improve <em>bfloat16</em> compatibility</li>
<li>improve <em>xformers</em> installer to match cuda version and install triton</li>
<li>fix extra networks refresh</li>
<li>fix <em>sdp memory attention</em> in backend original</li>
<li>fix autodetect sd21 models</li>
<li>fix api info endpoint</li>
<li>fix <em>sampler eta</em> in xyz grid, thanks @AI-Casanova</li>
<li>fix <em>requires_aesthetics_score</em> errors</li>
<li>fix t2i-canny</li>
<li>fix <em>differenital diffusion</em> for manual mask, thanks @23pennies</li>
<li>fix ipadapter apply/unapply on batch runs</li>
<li>fix control with multiple units and override images</li>
<li>fix control with hires</li>
<li>fix control-lllite</li>
<li>fix font fallback, thanks @NetroScript</li>
<li>update civitai downloader to handler new metadata</li>
<li>improve control error handling</li>
<li>use default model variant if specified variant doesnt exist</li>
<li>use diffusers lora load override for <em>lcm/tcd/turbo loras</em></li>
<li>exception handler around vram memory stats gather</li>
<li>improve ZLUDA installer with <code>--use-zluda</code> cli param, thanks @lshqqytiger</li>
</ul>
<h2 id="update-for-2024-02-22">Update for 2024-02-22</h2>
<p>Only 3 weeks since last release, but heres another feature-packed one!
This time release schedule was shorter as we wanted to get some of the fixes out faster.</p>
<h3 id="highlights-2024-02-22">Highlights 2024-02-22</h3>
<ul>
<li><strong>IP-Adapters</strong> &amp; <strong>FaceID</strong>: multi-adapter and multi-image suport  </li>
<li>New optimization engines: <a href="https://github.com/horseee/DeepCache">DeepCache</a>, <a href="https://github.com/vosen/ZLUDA">ZLUDA</a> and <strong>Dynamic Attention Slicing</strong>  </li>
<li>New built-in pipelines: <a href="https://github.com/exx8/differential-diffusion">Differential diffusion</a> and <a href="https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#regional-prompting-pipeline">Regional prompting</a>  </li>
<li>Big updates to: <strong>Outpainting</strong> (noised-edge-extend), <strong>Clip-skip</strong> (interpolate with non-integrer values!), <strong>CFG end</strong> (prevent overburn on high CFG scales), <strong>Control</strong> module masking functionality  </li>
<li>All reported issues since the last release are addressed and included in this release  </li>
</ul>
<p>Further details:<br />
- For basic instructions, see <a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a><br />
- For more details on all new features see full <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a><br />
- For documentation, see <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a>
- <a href="https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867">Discord</a> server  </p>
<h3 id="full-changelog-for-2024-02-22">Full ChangeLog for 2024-02-22</h3>
<ul>
<li><strong>Improvements</strong>:</li>
<li><strong>IP Adapter</strong> major refactor  <ul>
<li>support for <strong>multiple input images</strong> per each ip adapter  </li>
<li>support for <strong>multiple concurrent ip adapters</strong><br />
<em>note</em>: you cannot mix &amp; match ip adapters that use different <em>CLiP</em> models, for example <code>Base</code> and <code>Base ViT-G</code>  </li>
<li>add <strong>adapter start/end</strong> to settings, thanks @AI-Casanova<br />
  having adapter start late can help with better control over composition and prompt adherence<br />
  having adapter end early can help with overal quality and performance  </li>
<li>unified interface in txt2img, img2img and control  </li>
<li>enhanced xyz grid support  </li>
</ul>
</li>
<li><strong>FaceID</strong> now also works with multiple input images!  </li>
<li><a href="https://github.com/exx8/differential-diffusion">Differential diffusion</a><br />
    img2img generation where you control strength of each pixel or image area<br />
    can be used with manually created masks or with auto-generated depth-maps
    uses general denoising strength value<br />
    simply enable from <em>img2img -&gt; scripts -&gt; differential diffusion</em><br />
<em>note</em>: supports sd15 and sdxl models  </li>
<li><a href="https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#regional-prompting-pipeline">Regional prompting</a> as a built-in solution<br />
    usage is same as original implementation from @hako-mikan<br />
    click on title to open docs and see examples of full syntax on how to use it<br />
    simply enable from <em>scripts -&gt; regional prompting</em><br />
<em>note</em>: supports sd15 models only  </li>
<li><a href="https://github.com/horseee/DeepCache">DeepCache</a> model acceleration<br />
    it can produce massive speedups (2x-5x) with no overhead, but with some loss of quality<br />
<em>settings -&gt; compute -&gt; model compile -&gt; deep-cache</em> and <em>settings -&gt; compute -&gt; model compile -&gt; cache interval</em>  </li>
<li><a href="https://github.com/vosen/ZLUDA">ZLUDA</a> experimental support, thanks @lshqqytiger  <ul>
<li>ZLUDA is CUDA wrapper that can be used for GPUs without native support</li>
<li>best use case is <em>AMD GPUs on Windows</em>, see <a href="https://github.com/vladmandic/automatic/wiki/ZLUDA">wiki</a> for details  </li>
</ul>
</li>
<li><strong>Outpaint</strong> control outpaint now uses new alghorithm: noised-edge-extend<br />
    new method allows for much larger outpaint areas in a single pass, even outpaint 512-&gt;1024 works well<br />
    note that denoise strength should be increased for larger the outpaint areas, for example outpainting 512-&gt;1024 works well with denoise 0.75<br />
    outpaint can run in <em>img2img</em> mode (default) and <em>inpaint</em> mode where original image is masked (if inpaint masked only is selected)  </li>
<li><strong>Clip-skip</strong> reworked completely, thanks @AI-Casanova &amp; @Disty0<br />
    now clip-skip range is 0-12 where previously lowest value was 1 (default is still 1)<br />
    values can also be decimal to interpolate between different layers, for example <code>clip-skip: 1.5</code>, thanks @AI-Casanova  </li>
<li><strong>CFG End</strong> new param to control image generation guidance, thanks @AI-Casanova<br />
    sometimes you want strong control over composition, but you want it to stop at some point<br />
    for example, when used with ip-adapters or controlnet, high cfg scale can overpower the guided image  </li>
<li><strong>Control</strong><ul>
<li>when performing inpainting, you can specify processing resolution using <strong>size-&gt;mask</strong>  </li>
<li>units now have extra option to re-use current preview image as processor input  </li>
</ul>
</li>
<li><strong>Cross-attention</strong> refactored cross-attention methods, thanks @Disty0  <ul>
<li>for backend:original, its unchanged: SDP, xFormers, Doggettxs, InvokeAI, Sub-quadratic, Split attention  </li>
<li>for backend:diffuers, list is now: SDP, xFormers, Batch matrix-matrix, Split attention, Dynamic Attention BMM, Dynamic Attention SDP<br />
  note: you may need to update your settings! Attention Slicing is renamed to Split attention  </li>
<li>for ROCm, updated default cross-attention to Scaled Dot Product  </li>
</ul>
</li>
<li><strong>Dynamic Attention Slicing</strong>, thanks @Disty0  <ul>
<li>dynamically slices attention queries in order to keep them under the slice rate<br />
  slicing gets only triggered if the query size is larger than the slice rate to gain performance<br />
<em>Dynamic Attention Slicing BMM</em> uses <em>Batch matrix-matrix</em><br />
<em>Dynamic Attention Slicing SDP</em> uses <em>Scaled Dot Product</em>  </li>
<li><em>settings -&gt; compute settings -&gt; attention -&gt; dynamic attention slicing</em>  </li>
</ul>
</li>
<li><strong>ONNX</strong>:  <ul>
<li>allow specify onnx default provider and cpu fallback<br />
<em>settings -&gt; diffusers</em>  </li>
<li>allow manual install of specific onnx flavor<br />
<em>settings -&gt; onnx</em>  </li>
<li>better handling of <code>fp16</code> models/vae, thanks @lshqqytiger  </li>
</ul>
</li>
<li><strong>OpenVINO</strong> update to <code>torch 2.2.0</code>, thanks @Disty0  </li>
<li><strong>HyperTile</strong> additional options thanks @Disty0  <ul>
<li>add swap size option  </li>
<li>add use only for hires pass option  </li>
</ul>
</li>
<li>add <code>--theme</code> cli param to force theme on startup  </li>
<li>add <code>--allow-paths</code> cli param to add additional paths that are allowed to be accessed via web, thanks @OuticNZ  </li>
<li><strong>Wiki</strong>:</li>
<li>added benchmark notes for IPEX, OpenVINO and Olive  </li>
<li>added ZLUDA wiki page  </li>
<li><strong>Internal</strong></li>
<li>update dependencies  </li>
<li>refactor txt2img/img2img api  </li>
<li>enhanced theme loader  </li>
<li>add additional debug env variables  </li>
<li>enhanced sdp cross-optimization control<br />
    see <em>settings -&gt; compute settings</em>  </li>
<li>experimental support for <em>python 3.12</em>  </li>
<li><strong>Fixes</strong>:  </li>
<li>add variation seed to diffusers txt2img, thanks @AI-Casanova  </li>
<li>add cmd param <code>--skip-env</code> to skip setting of environment parameters during sdnext load  </li>
<li>handle extensions that install conflicting versions of packages<br />
<code>onnxruntime</code>, <code>opencv2-python</code>  </li>
<li>installer refresh package cache on any install  </li>
<li>fix embeddings registration on server startup, thanks @AI-Casanova  </li>
<li>ipex handle dependencies, thanks @Disty0  </li>
<li>insightface handle dependencies  </li>
<li>img2img mask blur and padding  </li>
<li>xyz grid handle ip adapter name and scale  </li>
<li>lazy loading of image may prevent metadata from being loaded on time  </li>
<li>allow startup without valid models folder  </li>
<li>fix interrogate api endpoint  </li>
<li>control fix resize causing runtime errors  </li>
<li>control fix processor override image after processor change  </li>
<li>control fix display grid with batch  </li>
<li>control restore pipeline before running scripts/extensions  </li>
<li>handle pipelines that return dict instead of object  </li>
<li>lora use strict name matching if preferred option is by-filename  </li>
<li>fix inpaint mask only for diffusers  </li>
<li>fix vae dtype mismatch, thanks @Disty0  </li>
<li>fix controlnet inpaint mask  </li>
<li>fix theme list refresh  </li>
<li>fix extensions update information in ui  </li>
<li>fix taesd with bfloat16</li>
<li>fix model merge manual merge settings, thanks @AI-Casanova  </li>
<li>fix gradio instant update issues for textboxes in quicksettings  </li>
<li>fix rembg missing dependency  </li>
<li>bind controlnet extension to last known working commit, thanks @Aptronymist  </li>
<li>prompts-from-file fix resizable prompt area  </li>
</ul>
<h2 id="update-for-2024-02-07">Update for 2024-02-07</h2>
<p>Another big release just hit the shelves!</p>
<h3 id="highlights-2024-02-07">Highlights 2024-02-07</h3>
<ul>
<li>A lot more functionality in the <strong>Control</strong> module:</li>
<li>Inpaint and outpaint support, flexible resizing options, optional hires  </li>
<li>Built-in support for many new processors and models, all auto-downloaded on first use  </li>
<li>Full support for scripts and extensions  </li>
<li>Complete <strong>Face</strong> module<br />
  implements all variations of <strong>FaceID</strong>, <strong>FaceSwap</strong> and latest <strong>PhotoMaker</strong> and <strong>InstantID</strong>  </li>
<li>Much enhanced <strong>IPAdapter</strong> modules  </li>
<li>Brand new <strong>Intelligent masking</strong>, manual or automatic<br />
  Using ML models (<em>LAMA</em> object removal, <em>REMBG</em> background removal, <em>SAM</em> segmentation, etc.) and with live previews<br />
  With granular blur, erode and dilate controls  </li>
<li>New models and pipelines:<br />
<strong>Segmind SegMoE</strong>, <strong>Mixture Tiling</strong>, <strong>InstaFlow</strong>, <strong>SAG</strong>, <strong>BlipDiffusion</strong>  </li>
<li>Massive work integrating latest advances with <a href="https://github.com/vladmandic/automatic/wiki/OpenVINO">OpenVINO</a>, <a href="https://github.com/vladmandic/automatic/wiki/Intel-ARC">IPEX</a> and <a href="https://github.com/vladmandic/automatic/wiki/ONNX-Runtime-&amp;-Olive">ONNX Olive</a></li>
<li>Full control over brightness, sharpness and color shifts and color grading during generate process directly in latent space  </li>
<li><strong>Documentation</strong>! This was a big one, with a lot of new content and updates in the <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a>  </li>
</ul>
<p>Plus welcome additions to <strong>UI performance, usability and accessibility</strong> and flexibility of deployment as well as <strong>API</strong> improvements<br />
And it also includes fixes for all reported issues so far  </p>
<p>As of this release, default backend is set to <strong>diffusers</strong> as its more feature rich than <strong>original</strong> and supports many additional models (original backend does remain as fully supported)  </p>
<p>Also, previous versions of <strong>SD.Next</strong> were tuned for balance between performance and resource usage.<br />
With this release, focus is more on performance.<br />
See <a href="https://github.com/vladmandic/automatic/wiki/Benchmark">Benchmark</a> notes for details, but as a highlight, we are now hitting <strong>~110-150 it/s</strong> on a standard nVidia RTX4090 in optimal scenarios!  </p>
<p>Further details:<br />
- For basic instructions, see <a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a><br />
- For more details on all new features see full <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a><br />
- For documentation, see <a href="https://github.com/vladmandic/automatic/wiki">WiKi</a></p>
<h3 id="full-changelog-2024-02-07">Full ChangeLog 2024-02-07</h3>
<ul>
<li>Heavily updated <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a>  </li>
<li><strong>Control</strong>:  </li>
<li>new docs:<ul>
<li><a href="https://github.com/vladmandic/automatic/wiki/Control">Control overview</a>  </li>
<li><a href="https://github.com/vladmandic/automatic/wiki/Control-Guide">Control guide</a>, thanks @Aptronymist  </li>
</ul>
</li>
<li>add <strong>inpaint</strong> support<br />
    applies to both <em>img2img</em> and <em>controlnet</em> workflows  </li>
<li>add <strong>outpaint</strong> support<br />
    applies to both <em>img2img</em> and <em>controlnet</em> workflows<br />
<em>note</em>: increase denoising strength since outpainted area is blank by default  </li>
<li>new <strong>mask</strong> module  <ul>
<li>granular blur (gaussian), erode (reduce or remove noise) and dilate (pad or expand)  </li>
<li>optional <strong>live preview</strong>  </li>
<li>optional <strong>auto-segmentation</strong> using ml models<br />
  auto-segmentation can be done using <strong>segment-anything</strong> models or <strong>rembg</strong> models<br />
<em>note</em>: auto segmentation will automatically expand user-masked area to segments that include current user mask  </li>
<li>optional <strong>auto-mask</strong><br />
  if you dont provide mask or mask is empty, you can instead use auto-mask to automatically generate mask<br />
  this is especially useful if you want to use advanced masking on batch or video inputs and dont want to manually mask each image<br />
<em>note</em>: such auto-created mask is also subject to all other selected settings such as auto-segmentation, blur, erode and dilate  </li>
<li>optional <strong>object removal</strong> using LaMA model<br />
  remove selected objects from images with a single click<br />
  works best when combined with auto-segmentation to remove smaller objects  </li>
<li>masking can be combined with control processors in which case mask is applied before processor  </li>
<li>unmasked part of can is optionally applied to final image as overlay, see settings <code>mask_apply_overlay</code>  </li>
</ul>
</li>
<li>support for many additional controlnet models<br />
    now built-in models include 30+ SD15 models and 15+ SDXL models  </li>
<li>allow <strong>resize</strong> both <em>before</em> and <em>after</em> generate operation<br />
    this allows for workflows such as: <em>image -&gt; upscale or downscale -&gt; generate -&gt; upscale or downscale -&gt; output</em><br />
    providing more flexibility and than standard hires workflow<br />
<em>note</em>: resizing before generate can be done using standard upscalers or latent</li>
<li>implicit <strong>hires</strong><br />
    since hires is only used for txt2img, control reuses existing resize functionality
    any image size is used as txt2img target size<br />
    but if resize scale is also set its used to additionally upscale image after initial txt2img and for hires pass  </li>
<li>add support for <strong>scripts</strong> and <strong>extensions</strong><br />
    you can now combine control workflow with your favorite script or extension<br />
<em>note</em> extensions that are hard-coded for txt2img or img2img tabs may not work until they are updated  </li>
<li>add <strong>depth-anything</strong> depth map processor and trained controlnet  </li>
<li>add <strong>marigold</strong> depth map processor<br />
    this is state-of-the-art depth estimation model, but its quite heavy on resources  </li>
<li>add <strong>openpose xl</strong> controlnet  </li>
<li>add blip/booru <strong>interrogate</strong> functionality to both input and output images  </li>
<li>configurable output folder in settings  </li>
<li>auto-refresh available models on tab activate  </li>
<li>add image preview for override images set per-unit  </li>
<li>more compact unit layout  </li>
<li>reduce usage of temp files  </li>
<li>add context menu to action buttons  </li>
<li>move ip-adapter implementation to control tabs  </li>
<li>resize by now applies to input image or frame individually<br />
    allows for processing where input images are of different sizes  </li>
<li>support controlnets with non-default yaml config files  </li>
<li>implement resize modes for override images  </li>
<li>allow any selection of units  </li>
<li>dynamically install depenencies required by specific processors  </li>
<li>fix input image size  </li>
<li>fix video color mode  </li>
<li>fix correct image mode  </li>
<li>fix batch/folder/video modes  </li>
<li>fix processor switching within same unit  </li>
<li>fix pipeline switching between different modes  </li>
<li><strong>Face</strong> module<br />
  implements all variations of <strong>FaceID</strong>, <strong>FaceSwap</strong> and latest <strong>PhotoMaker</strong> and <strong>InstantID</strong><br />
  simply select from scripts and choose your favorite method and model<br />
<em>note</em>: all models are auto-downloaded on first use  </li>
<li><a href="https://huggingface.co/h94/IP-Adapter-FaceID">FaceID</a>  <ul>
<li>faceid guides image generation given the input image  </li>
<li>full implementation for <em>SD15</em> and <em>SD-XL</em>, to use simply select from <em>Scripts</em><br />
<strong>Base</strong> (93MB) uses <em>InsightFace</em> to generate face embeds and <em>OpenCLIP-ViT-H-14</em> (2.5GB) as image encoder<br />
<strong>Plus</strong> (150MB) uses <em>InsightFace</em> to generate face embeds and <em>CLIP-ViT-H-14-laion2B</em> (3.8GB) as image encoder<br />
<strong>SDXL</strong> (1022MB) uses <em>InsightFace</em> to generate face embeds and <em>OpenCLIP-ViT-bigG-14</em> (3.7GB) as image encoder  </li>
</ul>
</li>
<li><a href="https://github.com/deepinsight/insightface/blob/master/examples/in_swapper/README.md">FaceSwap</a>  <ul>
<li>face swap performs face swapping at the end of generation  </li>
<li>based on InsightFace in-swapper  </li>
</ul>
</li>
<li><a href="https://github.com/TencentARC/PhotoMaker">PhotoMaker</a>  <ul>
<li>for <em>SD-XL</em> only  </li>
<li>new model from TenencentARC using similar concept as IPAdapter, but with different implementation and<br />
  allowing full concept swaps between input images and generated images using trigger words  </li>
<li>note: trigger word must match exactly one term in prompt for model to work  </li>
</ul>
</li>
<li><a href="https://github.com/InstantID/InstantID">InstantID</a>  <ul>
<li>for <em>SD-XL</em> only  </li>
<li>based on custom trained ip-adapter and controlnet combined concepts  </li>
<li>note: controlnet appears to be heavily watermarked  </li>
</ul>
</li>
<li>enable use via api, thanks @trojaner  </li>
<li><a href="https://huggingface.co/h94/IP-Adapter">IPAdapter</a>  </li>
<li>additional models for <em>SD15</em> and <em>SD-XL</em>, to use simply select from <em>Scripts</em>:<br />
<strong>SD15</strong>: Base, Base ViT-G, Light, Plus, Plus Face, Full Face<br />
<strong>SDXL</strong>: Base SDXL, Base ViT-H SDXL, Plus ViT-H SDXL, Plus Face ViT-H SDXL  </li>
<li>enable use via api, thanks @trojaner  </li>
<li><a href="https://github.com/segmind/segmoe">Segmind SegMoE</a>  </li>
<li>initial support for reference models<br />
    download&amp;load via network -&gt; models -&gt; reference -&gt; <strong>SegMoE SD 4x2</strong> (3.7GB), <strong>SegMoE XL 2x1</strong> (10GB), <strong>SegMoE XL 4x2</strong>  </li>
<li>note: since segmoe is basically sequential mix of unets from multiple models, it can get large<br />
    SD 4x2 is ~4GB, XL 2x1 is ~10GB and XL 4x2 is 18GB  </li>
<li>supports lora, thanks @AI-Casanova</li>
<li>support for create and load custom mixes will be added in the future  </li>
<li><a href="https://arxiv.org/abs/2302.02412">Mixture Tiling</a>  </li>
<li>uses multiple prompts to guide different parts of the grid during diffusion process  </li>
<li>can be used ot create complex scenes with multiple subjects  </li>
<li>simply select from scripts  </li>
<li><a href="https://github.com/SusungHong/Self-Attention-Guidance">Self-attention guidance</a>  </li>
<li>simply select scale in advanced menu  </li>
<li>can drastically improve image coherence as well as reduce artifacts  </li>
<li>note: only compatible with some schedulers  </li>
<li><a href="https://tianxingwu.github.io/pages/FreeInit/">FreeInit</a> for <strong>AnimateDiff</strong></li>
<li>greatly improves temporal consistency of generated outputs  </li>
<li>all options are available in animateddiff script  </li>
<li><a href="https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion">SalesForce BlipDiffusion</a>  </li>
<li>model can be used to place subject in a different context  </li>
<li>requires input image  </li>
<li>last word in prompt and negative prompt will be used as source and target subjects  </li>
<li>sampler must be set to default before loading the model  </li>
<li><a href="https://github.com/gnobitab/InstaFlow">InstaFlow</a>  </li>
<li>another take on super-fast image generation in a single step  </li>
<li>set <em>sampler:default, steps:1, cfg-scale:0</em>  </li>
<li>load from networks -&gt; models -&gt; reference  </li>
<li><strong>Improvements</strong>  </li>
<li><strong>ui</strong>  <ul>
<li>check version and <strong>update</strong> SD.Next via UI<br />
  simply go to: settings -&gt; update</li>
<li>globally configurable <strong>font size</strong><br />
  will dynamically rescale ui depending on settings -&gt; user interface  </li>
<li>built-in <strong>themes</strong> can be changed on-the-fly<br />
  this does not work with gradio-default themes as css is created by gradio itself  </li>
<li>two new <strong>themes</strong>: <em>simple-dark</em> and <em>simple-light</em>  </li>
<li>modularized blip/booru interrogate<br />
  now appears as toolbuttons on image/gallery output  </li>
<li>faster browser page load  </li>
<li>update hints, thanks @brknsoul  </li>
<li>cleanup settings  </li>
</ul>
</li>
<li><strong>server</strong><ul>
<li>all move/offload options are disable by default for optimal performance<br />
  enable manually if low on vram  </li>
</ul>
</li>
<li><strong>server startup</strong>: performance  <ul>
<li>reduced module imports<br />
  ldm support is now only loaded when running in backend=original  </li>
<li>faster extension load  </li>
<li>faster json parsing  </li>
<li>faster lora indexing  </li>
<li>lazy load optional imports  </li>
<li>batch embedding load, thanks @midcoastal and @AI-Casanova<br />
  10x+ faster embeddings load for large number of embeddings, now works for 1000+ embeddings  </li>
<li>file and folder list caching, thanks @midcoastal
  if you have a lot of files and and/or are using slower or non-local storage, this speeds up file access a lot  </li>
<li>add <code>SD_INSTALL_DEBUG</code> env variable to trace all <code>git</code> and <code>pip</code> operations</li>
</ul>
</li>
<li><strong>extra networks</strong>  <ul>
<li>4x faster civitai metadata and previews lookup  </li>
<li>better display and selection of tags &amp; trigger words<br />
  if hashes are calculated, trigger words will only be displayed for actual model version  </li>
<li>better matching of previews  </li>
<li>better search, including searching for multiple keywords or using full regex<br />
  see wiki page for more details on syntax<br />
  thanks @NetroScript  </li>
<li>reduce html overhead  </li>
</ul>
</li>
<li><strong>model compression</strong>, thanks @Disty0  <ul>
<li>using built-in NNCF model compression, you can reduce the size of your models significantly<br />
  example: up to 3.4GB of VRAM saved for SD-XL model!  </li>
<li>see <a href="https://github.com/vladmandic/automatic/wiki/Model-Compression-with-NNCF">wiki</a> for details  </li>
</ul>
</li>
<li><strong>embeddings</strong><br />
    you can now use sd 1.5 embeddings with your sd-xl models!, thanks @AI-Casanova<br />
    conversion is done on-the-fly, is completely transparent and result is an approximation of embedding<br />
    to enable: settings-&gt;extra networks-&gt;auto-convert embeddings  </li>
<li><strong>offline deployment</strong>: allow deployment without git clone<br />
    for example, you can now deploy a zip of the sdnext folder  </li>
<li><strong>latent upscale</strong>: updated latent upscalers (some are new)<br />
<em>nearest, nearest-exact, area, bilinear, bicubic, bilinear-antialias, bicubic-antialias</em></li>
<li><strong>scheduler</strong>: added <code>SA Solver</code>  </li>
<li><strong>model load to gpu</strong><br />
    new option in settings-&gt;diffusers allowing models to be loaded directly to GPU while keeping RAM free<br />
    this option is not compatible with any kind of model offloading as model is expected to stay in GPU<br />
    additionally, all model-moves can now be traced with env variable <code>SD_MOVE_DEBUG</code>  </li>
<li><strong>xyz grid</strong><ul>
<li>range control<br />
  example: <code>5.0-6.0:3</code> will generate 3 images with values <code>5.0,5.5,6.0</code><br />
  example: <code>10-20:4</code> will generate 4 images with values <code>10,13,16,20</code>  </li>
<li>continue on error<br />
  now you can use xyz grid with different params and test which ones work and which dont  </li>
<li>correct font scaling, thanks @nCoderGit  </li>
</ul>
</li>
<li><strong>hypertile</strong>  <ul>
<li>enable vae tiling  </li>
<li>add autodetect optimial value<br />
  set tile size to 0 to use autodetected value  </li>
</ul>
</li>
<li><strong>cli</strong>  <ul>
<li><code>sdapi.py</code> allow manual api invoke<br />
  example: <code>python cli/sdapi.py /sdapi/v1/sd-models</code>  </li>
<li><code>image-exif.py</code> improve metadata parsing  </li>
<li><code>install-sf</code> helper script to automatically find best available stable-fast package for the platform  </li>
</ul>
</li>
<li><strong>memory</strong>: add ram usage monitoring in addition to gpu memory usage monitoring  </li>
<li><strong>vae</strong>: enable taesd batch decode<br />
    enable/disable with settings -&gt; diffusers &gt; vae slicing  </li>
<li><strong>compile</strong></li>
<li>new option: <strong>fused projections</strong><br />
    pretty much free 5% performance boost for compatible models<br />
    enable in settings -&gt; compute settings  </li>
<li>new option: <strong>dynamic quantization</strong> (experimental)<br />
    reduces memory usage and increases performance<br />
    enable in settings -&gt; compute settings<br />
    best used together with torch compile: <em>inductor</em><br />
    this feature is highly experimental and will evolve over time<br />
    requires nightly versions of <code>torch</code> and <code>torchao</code><br />
    &gt; <code>pip install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121</code><br />
    &gt; <code>pip install -U git+https://github.com/pytorch-labs/ao</code>  </li>
<li>new option: <strong>compile text encoder</strong> (experimental)  </li>
<li><strong>correction</strong>  </li>
<li>new section in generate, allows for image corrections during generataion directly in latent space  </li>
<li>adds <em>brightness</em>, <em>sharpness</em> and <em>color</em> controls, thanks @AI-Casanova</li>
<li>adds <em>color grading</em> controls, thanks @AI-Casanova</li>
<li>replaces old <strong>hdr</strong> section</li>
<li><strong>IPEX</strong>, thanks @disty0  </li>
<li>see <a href="https://github.com/vladmandic/automatic/wiki/Intel-ARC">wiki</a> for details  </li>
<li>rewrite ipex hijacks without CondFunc<br />
    improves compatibilty and performance<br />
    fixes random memory leaks  </li>
<li>out of the box support for Intel Data Center GPU Max Series  </li>
<li>remove IPEX / Torch 2.0 specific hijacks  </li>
<li>add <code>IPEX_SDPA_SLICE_TRIGGER_RATE</code>, <code>IPEX_ATTENTION_SLICE_RATE</code> and <code>IPEX_FORCE_ATTENTION_SLICE</code> env variables  </li>
<li>disable 1024x1024 workaround if the GPU supports 64 bit  </li>
<li>fix lock-ups at very high resolutions  </li>
<li><strong>OpenVINO</strong>, thanks @disty0  </li>
<li>see <a href="https://github.com/vladmandic/automatic/wiki/OpenVINO">wiki</a> for details  </li>
<li><strong>quantization support with NNCF</strong><br />
    run 8 bit directly without autocast<br />
    enable <em>OpenVINO Quantize Models with NNCF</em> from <em>Compute Settings</em>  </li>
<li><strong>4-bit support with NNCF</strong><br />
    enable <em>Compress Model weights with NNCF</em> from <em>Compute Settings</em> and set a 4-bit NNCF mode<br />
    select both CPU and GPU from the device selection if you want to use 4-bit or 8-bit modes on GPU  </li>
<li>experimental support for <em>Text Encoder</em> compiling<br />
    OpenVINO is faster than IPEX now  </li>
<li>update to OpenVINO 2023.3.0  </li>
<li>add device selection to <code>Compute Settings</code><br />
    selecting multiple devices will use <code>HETERO</code> device  </li>
<li>remove <code>OPENVINO_TORCH_BACKEND_DEVICE</code> env variable  </li>
<li>reduce system memory usage after compile  </li>
<li>fix cache loading with multiple models  </li>
<li><strong>Olive</strong> support, thanks @lshqqytiger</li>
<li>fully merged in in <a href="https://github.com/vladmandic/automatic/wiki/ONNX-Runtime-&amp;-Olive">wiki</a>, see wiki for details  </li>
<li>as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  </li>
<li><strong>fixes</strong>  </li>
<li>civitai model download: enable downloads of embeddings</li>
<li>ipadapter: allow changing of model/image on-the-fly  </li>
<li>ipadapter: fix fallback of cross-attention on unload  </li>
<li>rebasin iterations, thanks @AI-Casanova</li>
<li>prompt scheduler, thanks @AI-Casanova</li>
<li>python: fix python 3.9 compatibility  </li>
<li>sdxl: fix positive prompt embeds</li>
<li>img2img: clip and blip interrogate  </li>
<li>img2img: sampler selection offset  </li>
<li>img2img: support variable aspect ratio without explicit resize  </li>
<li>cli: add <code>simple-upscale.py</code> script  </li>
<li>cli: fix cmd args parsing  </li>
<li>cli: add <code>run-benchmark.py</code> script  </li>
<li>api: add <code>/sdapi/v1/version</code> endpoint</li>
<li>api: add <code>/sdapi/v1/platform</code> endpoint</li>
<li>api: return current image in progress api if requested  </li>
<li>api: sanitize response object  </li>
<li>api: cleanup error logging  </li>
<li>api: fix api-only errors  </li>
<li>api: fix image to base64</li>
<li>api: fix upscale  </li>
<li>refiner: fix use of sd15 model as refiners in second pass  </li>
<li>refiner: enable none as option in xyz grid  </li>
<li>sampler: add sampler options info to metadata</li>
<li>sampler: guard against invalid sampler index  </li>
<li>sampler: add img2img_extra_noise option</li>
<li>config: reset default cfg scale to 6.0  </li>
<li>hdr: fix math, thanks @AI-Casanova</li>
<li>processing: correct display metadata  </li>
<li>processing: fix batch file names  </li>
<li>live preview: fix when using <code>bfloat16</code>  </li>
<li>live preview: add thread locking  </li>
<li>upscale: fix ldsr</li>
<li>huggingface: handle fallback model variant on load  </li>
<li>reference: fix links to models and use safetensors where possible  </li>
<li>model merge: unbalanced models where not all keys are present, thanks @AI-Casanova</li>
<li>better sdxl model detection</li>
<li>global crlf-&gt;lf switch  </li>
<li>model type switch if there is loaded submodels  </li>
<li>cleanup samplers use of compute devices, thanks @Disty0  </li>
<li><strong>other</strong>  </li>
<li>extensions <code>sd-webui-controlnet</code> is locked to commit <code>ecd33eb</code> due to breaking changes  </li>
<li>extension <code>stable-diffusion-webui-images-browser</code> is locked to commit <code>27fe4a7</code> due to breaking changes  </li>
<li>updated core requirements  </li>
<li>fully dynamic pipelines<br />
    pipeline switch is now done on-the-fly and does not require manual initialization of individual components<br />
    this allows for quick implementation of new pipelines<br />
    see <code>modules/sd_models.py:switch_pipe</code> for details  </li>
<li>major internal ui module refactoring<br />
    this may cause compatibility issues if an extension is doing a direct import from <code>ui.py</code><br />
    in which case, report it so we can add a compatibility layer  </li>
<li>major public api refactoring<br />
    this may cause compatibility issues if an extension is doing a direct import from <code>api.py</code> or <code>models.py</code><br />
    in which case, report it so we can add a compatibility layer  </li>
</ul>
<h2 id="update-for-2023-12-29">Update for 2023-12-29</h2>
<p>To wrap up this amazing year, were releasing a new version of <a href="https://github.com/vladmandic/automatic">SD.Next</a>, this one is absolutely massive!  </p>
<h3 id="highlights-2023-12-29">Highlights 2023-12-29</h3>
<ul>
<li>Brand new Control module for <em>text, image, batch and video</em> processing<br />
  Native implementation of all control methods for both <em>SD15</em> and <em>SD-XL</em><br />
   <strong>ControlNet | ControlNet XS | Control LLLite | T2I Adapters | IP Adapters</strong><br />
  For details, see <a href="https://github.com/vladmandic/automatic/wiki/Control">Wiki</a> documentation:  </li>
<li>Support for new models types out-of-the-box<br />
  This brings number of supported t2i/i2i model families to 13!<br />
   <strong>Stable Diffusion 1.5/2.1 | SD-XL | LCM | Segmind | Kandinsky | Pixart- | Wrstchen | aMUSEd | DeepFloyd IF | UniDiffusion | SD-Distilled | BLiP Diffusion | etc.</strong>  </li>
<li>New video capabilities:<br />
   <strong>AnimateDiff | SVD | ModelScope | ZeroScope</strong>  </li>
<li>Enhanced platform support<br />
   <strong>Windows | Linux | MacOS</strong> with <strong>nVidia | AMD | IntelArc | DirectML | OpenVINO | ONNX+Olive</strong> backends  </li>
<li>Better onboarding experience (first install)<br />
  with all model types available for single click download &amp; load (networks -&gt; reference)  </li>
<li>Performance optimizations!
  For comparisment of different processing options and compile backends, see <a href="https://github.com/vladmandic/automatic/wiki/Benchmark">Wiki</a><br />
  As a highlight, were reaching <strong>~100 it/s</strong> (no tricks, this is with full features enabled and end-to-end on a standard nVidia RTX4090)  </li>
<li>New <a href="https://github.com/vladmandic/automatic/blob/dev/scripts/example.py">custom pipelines</a> framework for quickly porting any new pipeline  </li>
</ul>
<p>And others improvements in areas such as: Upscaling (up to 8x now with 40+ available upscalers), Inpainting (better quality), Prompt scheduling, new Sampler options, new LoRA types, additional UI themes, better HDR processing, built-in Video interpolation, parallel Batch processing, etc.  </p>
<p>Plus some nifty new modules such as <strong>FaceID</strong> automatic face guidance using embeds during generation and <strong>Depth 3D</strong> image to 3D scene</p>
<h3 id="full-changelog-2023-12-29">Full ChangeLog 2023-12-29</h3>
<ul>
<li><strong>Control</strong>  </li>
<li>native implementation of all image control methods:<br />
<strong>ControlNet</strong>, <strong>ControlNet XS</strong>, <strong>Control LLLite</strong>, <strong>T2I Adapters</strong> and <strong>IP Adapters</strong>  </li>
<li>top-level <strong>Control</strong> next to <strong>Text</strong> and <strong>Image</strong> generate  </li>
<li>supports all variations of <strong>SD15</strong> and <strong>SD-XL</strong> models  </li>
<li>supports <em>Text</em>, <em>Image</em>, <em>Batch</em> and <em>Video</em> processing  </li>
<li>for details and list of supported models and workflows, see Wiki documentation:<br />
<a href="https://github.com/vladmandic/automatic/wiki/Control">https://github.com/vladmandic/automatic/wiki/Control</a>  </li>
<li><strong>Diffusers</strong>  </li>
<li><a href="https://huggingface.co/segmind/Segmind-Vega">Segmind Vega</a> model support  <ul>
<li>small and fast version of <strong>SDXL</strong>, only 3.1GB in size!  </li>
<li>select from <em>networks -&gt; reference</em>  </li>
</ul>
</li>
<li><a href="https://huggingface.co/amused/amused-256">aMUSEd 256</a> and <a href="https://huggingface.co/amused/amused-512">aMUSEd 512</a> model support  <ul>
<li>lightweigt models that excel at fast image generation  </li>
<li><em>note</em>: must select: settings -&gt; diffusers -&gt; generator device: unset</li>
<li>select from <em>networks -&gt; reference</em></li>
</ul>
</li>
<li><a href="https://huggingface.co/playgroundai/playground-v1">Playground v1</a>, <a href="https://huggingface.co/playgroundai/playground-v2-256px-base">Playground v2 256</a>, <a href="https://huggingface.co/playgroundai/playground-v2-512px-base">Playground v2 512</a>, <a href="https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic">Playground v2 1024</a> model support  <ul>
<li>comparable to SD15 and SD-XL, trained from scratch for highly aesthetic images  </li>
<li>simply select from <em>networks -&gt; reference</em> and use as usual  </li>
</ul>
</li>
<li><a href="https://dxli94.github.io/BLIP-Diffusion-website/">BLIP-Diffusion</a>  <ul>
<li>img2img model that can replace subjects in images using prompt keywords  </li>
<li>download and load by selecting from <em>networks -&gt; reference -&gt; blip diffusion</em></li>
<li>in image tab, select <code>blip diffusion</code> script</li>
</ul>
</li>
<li><a href="https://github.com/PRIS-CV/DemoFusion">DemoFusion</a> run your SDXL generations at any resolution!  <ul>
<li>in <strong>Text</strong> tab select <em>script</em> -&gt; <em>demofusion</em>  </li>
<li><em>note</em>: GPU VRAM limits do not automatically go away so be careful when using it with large resolutions<br />
  in the future, expect more optimizations, especially related to offloading/slicing/tiling,<br />
  but at the moment this is pretty much experimental-only  </li>
</ul>
</li>
<li><a href="https://github.com/guoyww/animatediff/">AnimateDiff</a>  <ul>
<li>overall improved quality  </li>
<li>can now be used with <em>second pass</em> - enhance, upscale and hires your videos!  </li>
</ul>
</li>
<li><a href="https://github.com/tencent-ailab/IP-Adapter">IP Adapter</a>  <ul>
<li>add support for <strong>ip-adapter-plus_sd15, ip-adapter-plus-face_sd15 and ip-adapter-full-face_sd15</strong>  </li>
<li>can now be used in <em>xyz-grid</em>  </li>
</ul>
</li>
<li><strong>Text-to-Video</strong>  <ul>
<li>in text tab, select <code>text-to-video</code> script  </li>
<li>supported models: <strong>ModelScope v1.7b, ZeroScope v1, ZeroScope v1.1, ZeroScope v2, ZeroScope v2 Dark, Potat v1</strong><br />
<em>if you know of any other t2v models youd like to see supported, let me know!</em>  </li>
<li>models are auto-downloaded on first use  </li>
<li><em>note</em>: current base model will be unloaded to free up resources  </li>
</ul>
</li>
<li><strong>Prompt scheduling</strong> now implemented for Diffusers backend, thanks @AI-Casanova</li>
<li><strong>Custom pipelines</strong> contribute by adding your own custom pipelines!  <ul>
<li>for details, see fully documented example:<br />
<a href="https://github.com/vladmandic/automatic/blob/dev/scripts/example.py">https://github.com/vladmandic/automatic/blob/dev/scripts/example.py</a>  </li>
</ul>
</li>
<li><strong>Schedulers</strong>  <ul>
<li>add timesteps range, changing it will make scheduler to be over-complete or under-complete  </li>
<li>add rescale betas with zero SNR option (applicable to Euler, Euler a and DDIM, allows for higher dynamic range)  </li>
</ul>
</li>
<li><strong>Inpaint</strong>  <ul>
<li>improved quality when using mask blur and padding  </li>
</ul>
</li>
<li><strong>UI</strong>  <ul>
<li>3 new native UI themes: <strong>orchid-dreams</strong>, <strong>emerald-paradise</strong> and <strong>timeless-beige</strong>, thanks @illu_Zn</li>
<li>more dynamic controls depending on the backend (original or diffusers)<br />
  controls that are not applicable in current mode are now hidden  </li>
<li>allow setting of resize method directly in image tab<br />
  (previously via settings -&gt; upscaler_for_img2img)  </li>
</ul>
</li>
<li><strong>Optional</strong></li>
<li><strong>FaceID</strong> face guidance during generation  <ul>
<li>also based on IP adapters, but with additional face detection and external embeddings calculation  </li>
<li>calculates face embeds based on input image and uses it to guide generation  </li>
<li>simply select from <em>scripts -&gt; faceid</em>  </li>
<li><em>experimental module</em>: requirements must be installed manually:<br />
    &gt; pip install insightface ip_adapter  </li>
</ul>
</li>
<li><strong>Depth 3D</strong> image to 3D scene<ul>
<li>delivered as an extension, install from extensions tab<br />
<a href="https://github.com/vladmandic/sd-extension-depth3d">https://github.com/vladmandic/sd-extension-depth3d</a>  </li>
<li>creates fully compatible 3D scene from any image by using depth estimation<br />
  and creating a fully populated mesh  </li>
<li>scene can be freely viewed in 3D in the UI itself or downloaded for use in other applications  </li>
</ul>
</li>
<li><a href="https://github.com/vladmandic/automatic/wiki/ONNX-Olive">ONNX/Olive</a>  <ul>
<li>major work continues in olive branch, see wiki for details, thanks @lshqqytiger<br />
  as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  </li>
</ul>
</li>
<li><strong>General</strong>  </li>
<li>new <strong>onboarding</strong>  <ul>
<li>if no models are found during startup, app will no longer ask to download default checkpoint<br />
  instead, it will show message in UI with options to change model path or download any of the reference checkpoints  </li>
<li><em>extra networks -&gt; models -&gt; reference</em> section is now enabled for both original and diffusers backend  </li>
</ul>
</li>
<li>support for <strong>Torch 2.1.2</strong> (release) and <strong>Torch 2.3</strong> (dev)  </li>
<li><strong>Process</strong> create videos from batch or folder processing<br />
      supports <em>GIF</em>, <em>PNG</em> and <em>MP4</em> with full interpolation, scene change detection, etc.  </li>
<li><strong>LoRA</strong>  <ul>
<li>add support for block weights, thanks @AI-Casanova<br />
  example <code>&lt;lora:SDXL_LCM_LoRA:1.0:in=0:mid=1:out=0&gt;</code>  </li>
<li>add support for LyCORIS GLora networks  </li>
<li>add support for LoRA PEFT (<em>Diffusers</em>) networks  </li>
<li>add support for Lora-OFT (<em>Kohya</em>) and Lyco-OFT (<em>Kohaku</em>) networks  </li>
<li>reintroduce alternative loading method in settings: <code>lora_force_diffusers</code>  </li>
<li>add support for <code>lora_fuse_diffusers</code> if using alternative method<br />
  use if you have multiple complex loras that may be causing performance degradation<br />
  as it fuses lora with model during load instead of interpreting lora on-the-fly  </li>
</ul>
</li>
<li><strong>CivitAI downloader</strong> allow usage of access tokens for download of gated or private models  </li>
<li><strong>Extra networks</strong> new <em>settting -&gt; extra networks -&gt; build info on first access</em><br />
    indexes all networks on first access instead of server startup  </li>
<li><strong>IPEX</strong>, thanks @disty0  <ul>
<li>update to <strong>Torch 2.1</strong><br />
  if you get file not found errors, set <code>DISABLE_IPEXRUN=1</code> and run the webui with <code>--reinstall</code>  </li>
<li>built-in <em>MKL</em> and <em>DPCPP</em> for IPEX, no need to install OneAPI anymore  </li>
<li><strong>StableVideoDiffusion</strong> is now supported with IPEX  </li>
<li><strong>8 bit support with NNCF</strong> on Diffusers backend  </li>
<li>fix IPEX Optimize not applying with Diffusers backend  </li>
<li>disable 32bit workarounds if the GPU supports 64bit  </li>
<li>add <code>DISABLE_IPEXRUN</code> and <code>DISABLE_IPEX_1024_WA</code> environment variables  </li>
<li>performance and compatibility improvements  </li>
</ul>
</li>
<li><strong>OpenVINO</strong>, thanks @disty0  <ul>
<li><strong>8 bit support for CPUs</strong>  </li>
<li>reduce System RAM usage  </li>
<li>update to Torch 2.1.2  </li>
<li>add <em>Directory for OpenVINO cache</em> option to <em>System Paths</em>  </li>
<li>remove Intel ARC specific 1024x1024 workaround  </li>
</ul>
</li>
<li><strong>HDR controls</strong>  <ul>
<li>batch-aware for enhancement of multiple images or video frames  </li>
<li>available in image tab  </li>
</ul>
</li>
<li><strong>Logging</strong><ul>
<li>additional <em>TRACE</em> logging enabled via specific env variables<br />
  see <a href="https://github.com/vladmandic/automatic/wiki/Debug">https://github.com/vladmandic/automatic/wiki/Debug</a> for details  </li>
<li>improved profiling<br />
  use with <code>--debug --profile</code>  </li>
<li>log output file sizes  </li>
</ul>
</li>
<li><strong>Other</strong>  <ul>
<li><strong>API</strong> several minor but breaking changes to API behavior to better align response fields, thanks @Trojaner</li>
<li><strong>Inpaint</strong> add option <code>apply_overlay</code> to control if inpaint result should be applied as overlay or as-is<br />
  can remove artifacts and hard edges of inpaint area but also remove some details from original  </li>
<li><strong>chaiNNer</strong> fix <code>NaN</code> issues due to autocast  </li>
<li><strong>Upscale</strong> increase limit from 4x to 8x given the quality of some upscalers  </li>
<li><strong>Networks</strong> fix sort  </li>
<li>reduced default <strong>CFG scale</strong> from 6 to 4 to be more out-of-the-box compatibile with LCM/Turbo models</li>
<li>disable google fonts check on server startup  </li>
<li>fix torchvision/basicsr compatibility  </li>
<li>fix styles quick save  </li>
<li>add hdr settings to metadata  </li>
<li>improve handling of long filenames and filenames during batch processing  </li>
<li>do not set preview samples when using via api  </li>
<li>avoid unnecessary resizes in img2img and inpaint  </li>
<li>safe handling of config updates avoid file corruption on I/O errors  </li>
<li>updated <code>cli/simple-txt2img.py</code> and <code>cli/simple-img2img.py</code> scripts  </li>
<li>save <code>params.txt</code> regardless of image save status  </li>
<li>update built-in log monitor in ui, thanks @midcoastal  </li>
<li>major CHANGELOG doc cleanup, thanks @JetVarimax  </li>
<li>major INSTALL doc cleanup, thanks JetVarimax  </li>
</ul>
</li>
</ul>
<h2 id="update-for-2023-12-04">Update for 2023-12-04</h2>
<p>Whats new? Native video in SD.Next via both <strong>AnimateDiff</strong> and <strong>Stable-Video-Diffusion</strong> - and including native MP4 encoding and smooth video outputs out-of-the-box, not just animated-GIFs.<br />
Also new is support for <strong>SDXL-Turbo</strong> as well as new <strong>Kandinsky 3</strong> models and cool latent correction via <strong>HDR controls</strong> for any <em>txt2img</em> workflows, best-of-class <strong>SDXL model merge</strong> using full ReBasin methods and further mobile UI optimizations.  </p>
<ul>
<li><strong>Diffusers</strong></li>
<li><strong>IP adapter</strong><ul>
<li>lightweight native implementation of T2I adapters which can guide generation towards specific image style  </li>
<li>supports most T2I models, not limited to SD 1.5  </li>
<li>models are auto-downloaded on first use</li>
</ul>
</li>
<li><strong>AnimateDiff</strong><ul>
<li>lightweight native implementation of AnimateDiff models:<br />
<em>AnimateDiff 1.4, 1.5 v1, 1.5 v2, AnimateFace</em></li>
<li>supports SD 1.5 only  </li>
<li>models are auto-downloaded on first use  </li>
<li>for video saving support, see video support section</li>
<li>can be combined with IP-Adapter for even better results!  </li>
</ul>
</li>
<li><strong>HDR latent control</strong>, based on <a href="https://huggingface.co/blog/TimothyAlexisVass/explaining-the-sdxl-latent-space#long-prompts-at-high-guidance-scales-becoming-possible">article</a>  <ul>
<li>in <em>Advanced</em> params</li>
<li>allows control of <em>latent clamping</em>, <em>color centering</em> and <em>range maximization</em>  </li>
<li>supported by <em>XYZ grid</em>  </li>
</ul>
</li>
<li><a href="https://huggingface.co/stabilityai/sd-turbo">SD21 Turbo</a> and <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDXL Turbo</a> support  <ul>
<li>just set CFG scale (0.0-1.0) and steps (1-3) to a very low value  </li>
<li>compatible with original StabilityAI SDXL-Turbo or any of the newer merges</li>
<li>download safetensors or select from networks -&gt; reference</li>
</ul>
</li>
<li><a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid">Stable Video Diffusion</a> and <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt">Stable Video Diffusion XT</a> support  <ul>
<li>download using built-in model downloader or simply select from <em>networks -&gt; reference</em><br />
  support for manually downloaded safetensors models will be added later  </li>
<li>for video saving support, see video support section</li>
<li>go to <em>image</em> tab, enter input image and select <em>script</em> -&gt; <em>stable video diffusion</em></li>
</ul>
</li>
<li><a href="https://huggingface.co/kandinsky-community/kandinsky-3">Kandinsky 3</a> support  <ul>
<li>download using built-in model downloader or simply select from <em>networks -&gt; reference</em>  </li>
<li>this model is absolutely massive at 27.5GB at fp16, so be patient  </li>
<li>model params count is at 11.9B (compared to SD-XL at 3.3B) and its trained on mixed resolutions from 256px to 1024px  </li>
<li>use either model offload or sequential cpu offload to be able to use it  </li>
</ul>
</li>
<li>better autodetection of <em>inpaint</em> and <em>instruct</em> pipelines  </li>
<li>support long seconary prompt for refiner  </li>
<li><strong>Video support</strong></li>
<li>applies to any model that supports video generation, e.g. AnimateDiff and StableVideoDiffusion  </li>
<li>support for <strong>animated-GIF</strong>, <strong>animated-PNG</strong> and <strong>MP4</strong>  </li>
<li>GIF and PNG can be looped  </li>
<li>MP4 can have additional padding at the start/end as well as motion-aware interpolated frames for smooth playback<br />
    interpolation is done using <a href="https://arxiv.org/abs/2011.06294">RIFE</a> with native implementation in SD.Next<br />
    And its fast - interpolation from 16 frames with 10x frames to target 160 frames results takes 2-3sec</li>
<li>output folder for videos is in <em>settings -&gt; image paths -&gt; video</em>  </li>
<li><strong>General</strong>  </li>
<li>redesigned built-in profiler  <ul>
<li>now includes both <code>python</code> and <code>torch</code> and traces individual functions  </li>
<li>use with <code>--debug --profile</code>  </li>
</ul>
</li>
<li><strong>model merge</strong> add <strong>SD-XL ReBasin</strong> support, thanks @AI-Casanova  </li>
<li>further UI optimizations for <strong>mobile devices</strong>, thanks @iDeNoh  </li>
<li>log level defaults to info for console and debug for log file  </li>
<li>better prompt display in process tab  </li>
<li>increase maximum lora cache values  </li>
<li>fix extra networks sorting</li>
<li>fix controlnet compatibility issues in original backend  </li>
<li>fix img2img/inpaint paste params  </li>
<li>fix save text file for manually saved images  </li>
<li>fix python 3.9 compatibility issues  </li>
</ul>
<h2 id="update-for-2023-11-23">Update for 2023-11-23</h2>
<p>New release, primarily focused around three major new features: full <strong>LCM</strong> support, completely new <strong>Model Merge</strong> functionality and <strong>Stable-fast</strong> compile support<br />
Also included are several other improvements and large number of hotfixes - see full changelog for details  </p>
<ul>
<li><strong>Diffusers</strong>  </li>
<li><strong>LCM</strong> support for any <em>SD 1.5</em> or <em>SD-XL</em> model!  <ul>
<li>download <a href="https://huggingface.co/latent-consistency/lcm-lora-sdv1-5/tree/main">lcm-lora-sd15</a> and/or <a href="https://huggingface.co/latent-consistency/lcm-lora-sdxl/tree/main">lcm-lora-sdxl</a>  </li>
<li>load for favorite <em>SD 1.5</em> or <em>SD-XL</em> model <em>(original LCM was SD 1.5 only, this is both)</em>  </li>
<li>load <strong>lcm lora</strong> <em>(note: lcm lora is processed differently than any other lora)</em>  </li>
<li>set <strong>sampler</strong> to <strong>LCM</strong>  </li>
<li>set number of steps to some low number, for SD-XL 6-7 steps is normally sufficient<br />
  note: LCM scheduler does not support steps higher than 50  </li>
<li>set CFG to between 1 and 2  </li>
</ul>
</li>
<li>Add <code>cli/lcm-convert.py</code> script to convert any SD 1.5 or SD-XL model to LCM model<br />
    by baking in LORA and uploading to Huggingface, thanks @Disty0  </li>
<li>Support for <a href="https://github.com/chengzeyi/stable-fast">Stable Fast</a> model compile on <em>Windows/Linux/WSL2</em> with <em>CUDA</em><br />
    See <a href="https://github.com/vladmandic/automatic/wiki/Benchmark">Wiki:Benchmark</a> for details and comparison<br />
    of different backends, precision modes, advanced settings and compile modes<br />
<em>Hint</em>: <strong>70+ it/s</strong> is possible on <em>RTX4090</em> with no special tweaks  </li>
<li>Add additional pipeline types for manual model loads when loading from <code>safetensors</code>  </li>
<li>Updated logic for calculating <strong>steps</strong> when using base/hires/refiner workflows  </li>
<li>Improve <strong>model offloading</strong> for both model and sequential cpu offload when dealing with meta tensors</li>
<li>Safe model offloading for non-standard models  </li>
<li>Fix <strong>DPM SDE</strong> scheduler  </li>
<li>Better support for SD 1.5 <strong>inpainting</strong> models  </li>
<li>Add support for <strong>OpenAI Consistency decoder VAE</strong></li>
<li>Enhance prompt parsing with long prompts and support for <em>BREAK</em> keyword<br />
    Change-in-behavior: new line in prompt now means <em>BREAK</em>  </li>
<li>Add alternative Lora loading algorithm, triggered if <code>SD_LORA_DIFFUSERS</code> is set  </li>
<li><strong>Models</strong></li>
<li><strong>Model merge</strong><ul>
<li>completely redesigned, now based on best-of-class <code>meh</code> by @s1dlx<br />
  and heavily modified for additional functionality and fully integrated by @AI-Casanova (thanks!)  </li>
<li>merge SD or SD-XL models using <em>simple merge</em> (12 methods),<br />
  using one of <em>presets</em> (20 built-in presets) or custom block merge values  </li>
<li>merge with ReBasin permutations and/or clipping protection  </li>
<li>fully multithreaded for fastest merge possible  </li>
</ul>
</li>
<li><strong>Model update</strong>  <ul>
<li>under UI -&gt; Models - Update  </li>
<li>scan existing models for updated metadata on CivitAI and<br />
  provide download functionality for models with available  </li>
</ul>
</li>
<li><strong>Extra networks</strong>  </li>
<li>Use multi-threading for 5x load speedup  </li>
<li>Better Lora trigger words support  </li>
<li>Auto refresh styles on change  </li>
<li><strong>General</strong>  </li>
<li>Many <strong>mobile UI</strong> optimizations, thanks @iDeNoh</li>
<li>Support for <strong>Torch 2.1.1</strong> with CUDA 12.1 or CUDA 11.8  </li>
<li>Configurable location for HF cache folder<br />
    Default is standard <code>~/.cache/huggingface/hub</code>  </li>
<li>Reworked parser when pasting previously generated images/prompts<br />
    includes all <code>txt2img</code>, <code>img2img</code> and <code>override</code> params  </li>
<li>Reworked <strong>model compile</strong></li>
<li>Support custom upscalers in subfolders  </li>
<li>Add additional image info when loading image in process tab  </li>
<li>Better file locking when sharing config and/or models between multiple instances  </li>
<li>Handle custom API endpoints when using auth  </li>
<li>Show logged in user in log when accessing via UI and/or API  </li>
<li>Support <code>--ckpt none</code> to skip loading a model  </li>
<li><strong>XYZ grid</strong></li>
<li>Add refiner options to XYZ Grid  </li>
<li>Add option to create only subgrids in XYZ grid, thanks @midcoastal</li>
<li>Allow custom font, background and text color in settings</li>
<li><strong>Fixes</strong>  </li>
<li>Fix <code>params.txt</code> saved before actual image</li>
<li>Fix inpaint  </li>
<li>Fix manual grid image save  </li>
<li>Fix img2img init image save  </li>
<li>Fix upscale in txt2img for batch counts when no hires is used  </li>
<li>More uniform models paths  </li>
<li>Safe scripts callback execution  </li>
<li>Improved extension compatibility  </li>
<li>Improved BF16 support  </li>
<li>Match previews for reference models with downloaded models</li>
</ul>
<h2 id="update-for-2023-11-06">Update for 2023-11-06</h2>
<p>Another pretty big release, this time with focus on new models (3 new model types), new backends and optimizations
Plus quite a few fixes  </p>
<p>Also, <a href="https://github.com/vladmandic/automatic/wiki">Wiki</a> has been updated with new content, so check it out!<br />
Some highlights: <a href="https://github.com/vladmandic/automatic/wiki/OpenVINO">OpenVINO</a>, <a href="https://github.com/vladmandic/automatic/wiki/Intel-ARC">IntelArc</a>, <a href="https://github.com/vladmandic/automatic/wiki/DirectML">DirectML</a>, <a href="https://github.com/vladmandic/automatic/wiki/ONNX-Olive">ONNX/Olive</a></p>
<ul>
<li><strong>Diffusers</strong></li>
<li>since now <strong>SD.Next</strong> supports <strong>12</strong> different model types, weve added reference model for each type in<br />
<em>Extra networks -&gt; Reference</em> for easier select &amp; auto-download<br />
    Models can still be downloaded manually, this is just a convenience feature &amp; a showcase for supported models  </li>
<li>new model type: <a href="https://huggingface.co/segmind/SSD-1B">Segmind SSD-1B</a><br />
    its a <em>distilled</em> model trained at 1024px, this time 50% smaller and faster version of SD-XL!<br />
    (and quality does not suffer, its just more optimized)<br />
    test shows batch-size:4 with 1k images at full quality used less than 6.5GB of VRAM<br />
    and for further optimization, you can use built-in <strong>TAESD</strong> decoder,<br />
    which results in batch-size:16 with 1k images using 7.9GB of VRAM
    select from extra networks -&gt; reference or download using built-in <strong>Huggingface</strong> downloader: <code>segmind/SSD-1B</code>  </li>
<li>new model type: <a href="https://github.com/PixArt-alpha/PixArt-alpha">Pixart- XL 2</a><br />
    in medium/512px and large/1024px variations<br />
    comparable in quality to SD 1.5 and SD-XL, but with better text encoder and highly optimized training pipeline<br />
    so finetunes can be done in as little as 10% compared to SD/SD-XL (note that due to much larger text encoder, it is a large model)<br />
    select from extra networks -&gt; reference or download using built-in <strong>Huggingface</strong> downloader: <code>PixArt-alpha/PixArt-XL-2-1024-MS</code>  </li>
<li>new model type: <a href="https://github.com/openai/consistency_models">LCM: Latent Consistency Models</a><br />
    trained at 512px, but with near-instant generate in a as little as 3 steps!<br />
    combined with OpenVINO, generate on CPU takes less than 5-10 seconds: <a href="https://www.youtube.com/watch?v=b90ESUTLsRo">https://www.youtube.com/watch?v=b90ESUTLsRo</a><br />
    and absolute beast when combined with <strong>HyperTile</strong> and <strong>TAESD</strong> decoder resulting in <strong>28 FPS</strong><br />
    (on RTX4090 for batch 16x16 at 512px)<br />
    note: set sampler to <strong>Default</strong> before loading model as LCM comes with its own <em>LCMScheduler</em> sampler<br />
    select from extra networks -&gt; reference or download using built-in <strong>Huggingface</strong> downloader: <code>SimianLuo/LCM_Dreamshaper_v7</code>  </li>
<li>support for <strong>Custom pipelines</strong>, thanks @disty0<br />
    download using built-in <strong>Huggingface</strong> downloader<br />
    think of them as plugins for diffusers not unlike original extensions that modify behavior of <code>ldm</code> backend<br />
    list of community pipelines: <a href="https://github.com/huggingface/diffusers/blob/main/examples/community/README.md">https://github.com/huggingface/diffusers/blob/main/examples/community/README.md</a>  </li>
<li>new custom pipeline: <code>Disty0/zero123plus-pipeline</code>, thanks @disty0<br />
    generate 4 output images with different camera positions: front, side, top, back!<br />
    for more details, see <a href="https://github.com/vladmandic/automatic/discussions/2421">https://github.com/vladmandic/automatic/discussions/2421</a>  </li>
<li>new backend: <strong>ONNX/Olive</strong> <em>(experimental)</em>, thanks @lshqqytiger<br />
    for details, see <a href="https://github.com/vladmandic/automatic/wiki/ONNX-Runtime">WiKi</a></li>
<li>extend support for <a href="https://github.com/ChenyangSi/FreeU">Free-U</a><br />
    improve generations quality at no cost (other than finding params that work for you)  </li>
<li><strong>General</strong>  </li>
<li>attempt to auto-fix invalid samples which occur due to math errors in lower precision<br />
    example: <code>RuntimeWarning: invalid value encountered in cast: sample = sample.astype(np.uint8)</code><br />
    begone <strong>black images</strong> <em>(note: if it proves as working, this solution will need to be expanded to cover all scenarios)</em>  </li>
<li>add <strong>Lora OFT</strong> support, thanks @antis0007 and @ai-casanova  </li>
<li><strong>Upscalers</strong>  <ul>
<li><strong>compile</strong> option, thanks @disty0  </li>
<li><strong>chaiNNer</strong> add high quality models from <a href="https://openmodeldb.info/users/helaman">Helaman</a>  </li>
</ul>
</li>
<li>redesigned <strong>Progress bar</strong> with full details on current operation  </li>
<li>new option: <em>settings -&gt; images -&gt; keep incomplete</em><br />
    can be used to skip vae decode on aborted/skipped/interrupted image generations  </li>
<li>new option: <em>settings -&gt; system paths -&gt; models</em><br />
    can be used to set custom base path for <em>all</em> models (previously only as cli option)  </li>
<li>remove external clone of items in <code>/repositories</code>  </li>
<li><strong>Interrogator</strong> module has been removed from <code>extensions-builtin</code><br />
    and fully implemented (and improved) natively  </li>
<li><strong>UI</strong>  </li>
<li>UI tweaks for default themes  </li>
<li>UI switch core font in default theme to <strong>noto-sans</strong><br />
    previously default font was simply <em>system-ui</em>, but it lead to too much variations between browsers and platforms  </li>
<li>UI tweaks for mobile devices, thanks @iDeNoh  </li>
<li>updated <strong>Context menu</strong><br />
    right-click on any button in action menu (e.g. generate button)  </li>
<li><strong>Extra networks</strong>  </li>
<li>sort by name, size, date, etc.  </li>
<li>switch between <em>gallery</em> and <em>list</em> views  </li>
<li>add tags from user metadata (in addition to tags in model metadata) for <strong>lora</strong>  </li>
<li>added <strong>Reference</strong> models for diffusers backend  </li>
<li>faster enumeration of all networks on server startup  </li>
<li><strong>Packages</strong></li>
<li>updated <code>diffusers</code> to 0.22.0, <code>transformers</code> to 4.34.1  </li>
<li>update <strong>openvino</strong>, thanks @disty0  </li>
<li>update <strong>directml</strong>, @lshqqytiger  </li>
<li><strong>Compute</strong>  </li>
<li><strong>OpenVINO</strong>:  <ul>
<li>updated to mainstream <code>torch</code> <em>2.1.0</em>  </li>
<li>support for <strong>ESRGAN</strong> upscalers  </li>
</ul>
</li>
<li><strong>Fixes</strong>  </li>
<li>fix <strong>freeu</strong> for backend original and add it to xyz grid  </li>
<li>fix loading diffuser models in huggingface format from non-standard location  </li>
<li>fix default styles looking in wrong location  </li>
<li>fix missing upscaler folder on initial startup  </li>
<li>fix handling of relative path for models  </li>
<li>fix simple live preview device mismatch  </li>
<li>fix batch img2img  </li>
<li>fix diffusers samplers: dpm++ 2m, dpm++ 1s, deis  </li>
<li>fix new style filename template  </li>
<li>fix image name template using model name  </li>
<li>fix image name sequence  </li>
<li>fix model path using relative path  </li>
<li>fix safari/webkit layour, thanks @eadnams22</li>
<li>fix <code>torch-rocm</code> and <code>tensorflow-rocm</code> version detection, thanks @xangelix  </li>
<li>fix <strong>chainner</strong> upscalers color clipping  </li>
<li>fix for base+refiner workflow in diffusers mode: number of steps, diffuser pipe mode  </li>
<li>fix for prompt encoder with refiner in diffusers mode  </li>
<li>fix prompts-from-file saving incorrect metadata  </li>
<li>fix add/remove extra networks to prompt</li>
<li>fix before-hires step  </li>
<li>fix diffusers switch from invalid model  </li>
<li>force second requirements check on startup  </li>
<li>remove <strong>lyco</strong>, multiple_tqdm  </li>
<li>enhance extension compatibility for extensions directly importing codeformers  </li>
<li>enhance extension compatibility for extensions directly accessing processing params  </li>
<li><strong>css</strong> fixes  </li>
<li>clearly mark external themes in ui  </li>
<li>update <code>typing-extensions</code>  </li>
</ul>
<h2 id="update-for-2023-10-17">Update for 2023-10-17</h2>
<p>This is a major release, with many changes and new functionality...  </p>
<p>Changelog is massive, but do read through or youll be missing on some very cool new functionality<br />
or even free speedups and quality improvements (regardless of which workflows youre using)!  </p>
<p>Note that for this release its recommended to perform a clean install (e.g. fresh <code>git clone</code>)<br />
Upgrades are still possible and supported, but clean install is recommended for best experience  </p>
<ul>
<li><strong>UI</strong>  </li>
<li>added <strong>change log</strong> to UI<br />
    see <em>System -&gt; Changelog</em>  </li>
<li>converted submenus from checkboxes to accordion elements<br />
    any ui state including state of open/closed menus can be saved as default!<br />
    see <em>System -&gt; User interface -&gt; Set menu states</em>  </li>
<li>new built-in theme <strong>invoked</strong><br />
    thanks @BinaryQuantumSoul  </li>
<li>add <strong>compact view</strong> option in settings -&gt; user interface  </li>
<li>small visual indicator bottom right of page showing internal server job state  </li>
<li><strong>Extra networks</strong>:  </li>
<li><strong>Details</strong>  <ul>
<li>new details interface to view and save data about extra networks<br />
  main ui now has a single button on each en to trigger details view  </li>
<li>details view includes model/lora metadata parser!  </li>
<li>details view includes civitai model metadata!  </li>
</ul>
</li>
<li><strong>Metadata</strong>:  <ul>
<li>you can scan <a href="https://civitai.com/">civitai</a><br />
  for missing metadata and previews directly from extra networks<br />
  simply click on button in top-right corner of extra networks page  </li>
</ul>
</li>
<li><strong>Styles</strong>  <ul>
<li>save/apply icons moved to extra networks  </li>
<li>can be edited in details view  </li>
<li>support for single or multiple styles per json  </li>
<li>support for embedded previews  </li>
<li>large database of art styles included by default<br />
  can be disabled in <em>settings -&gt; extra networks -&gt; show built-in</em>  </li>
<li>styles can also be used in a prompt directly: <code>&lt;style:style_name&gt;</code><br />
  if style if an exact match, it will be used<br />
  otherwise it will rotate between styles that match the start of the name<br />
  that way you can use different styles as wildcards when processing batches  </li>
<li>styles can have <strong>extra</strong> fields, not just prompt and negative prompt<br />
  for example: <em>"Extra: sampler: Euler a, width: 480, height: 640, steps: 30, cfg scale: 10, clip skip: 2"</em></li>
</ul>
</li>
<li><strong>VAE</strong>  <ul>
<li>VAEs are now also listed as part of extra networks  </li>
<li>Image preview methods have been redesigned: simple, approximate, taesd, full<br />
  please set desired preview method in settings  </li>
<li>both original and diffusers backend now support "full quality" setting<br />
  if you desired model or platform does not support FP16 and/or you have a low-end hardware and cannot use FP32<br />
  you can disable "full quality" in advanced params and it will likely reduce decode errors (infamous black images)  </li>
</ul>
</li>
<li><strong>LoRA</strong>  <ul>
<li>LoRAs are now automatically filtered based on compatibility with currently loaded model<br />
  note that if lora type cannot be auto-determined, it will be left in the list  </li>
</ul>
</li>
<li><strong>Refiner</strong>  <ul>
<li>you can load model from extra networks as base model or as refiner<br />
  simply select button in top-right of models page  </li>
</ul>
</li>
<li><strong>General</strong>  <ul>
<li>faster search, ability to show/hide/sort networks  </li>
<li>refactored subfolder handling<br />
<em>note</em>: this will trigger model hash recalculation on first model use  </li>
</ul>
</li>
<li><strong>Diffusers</strong>:  </li>
<li>better pipeline <strong>auto-detect</strong> when loading from safetensors  </li>
<li><strong>SDXL Inpaint</strong>  <ul>
<li>although any model can be used for inpainiting, there is a case to be made for<br />
  dedicated inpainting models as they are tuned to inpaint and not generate  </li>
<li>model can be used as base model for <strong>img2img</strong> or refiner model for <strong>txt2img</strong><br />
  To download go to <em>Models -&gt; Huggingface</em>:  </li>
<li><code>diffusers/stable-diffusion-xl-1.0-inpainting-0.1</code> <em>(6.7GB)</em>  </li>
</ul>
</li>
<li><strong>SDXL Instruct-Pix2Pix</strong>  <ul>
<li>model can be used as base model for <strong>img2img</strong> or refiner model for <strong>txt2img</strong><br />
  this model is massive and requires a lot of resources!<br />
  to download go to <em>Models -&gt; Huggingface</em>:  </li>
<li><code>diffusers/sdxl-instructpix2pix-768</code> <em>(11.9GB)</em>  </li>
</ul>
</li>
<li><strong>SD Latent Upscale</strong>  <ul>
<li>you can use <em>SD Latent Upscale</em> models as <strong>refiner models</strong><br />
  this is a bit experimental, but it works quite well!<br />
  to download go to <em>Models -&gt; Huggingface</em>:  </li>
<li><code>stabilityai/sd-x2-latent-upscaler</code> <em>(2.2GB)</em>  </li>
<li><code>stabilityai/stable-diffusion-x4-upscaler</code> <em>(1.7GB)</em>  </li>
</ul>
</li>
<li>better <strong>Prompt attention</strong><br />
    should better handle more complex prompts<br />
    for sdxl, choose which part of prompt goes to second text encoder - just add <code>TE2:</code> separator in the prompt<br />
    for hires and refiner, second pass prompt is used if present, otherwise primary prompt is used<br />
    new option in <em>settings -&gt; diffusers -&gt; sdxl pooled embeds</em><br />
    thanks @AI-Casanova  </li>
<li>better <strong>Hires</strong> support for SD and SDXL  </li>
<li>better <strong>TI embeddings</strong> support for SD and SDXL<br />
    faster loading, wider compatibility and support for embeddings with multiple vectors<br />
    information about used embedding is now also added to image metadata<br />
    thanks @AI-Casanova  </li>
<li>better <strong>Lora</strong> handling<br />
    thanks @AI-Casanova  </li>
<li>better <strong>SDXL preview</strong> quality (approx method)<br />
    thanks @BlueAmulet</li>
<li>new setting: <em>settings -&gt; diffusers -&gt; force inpaint</em><br />
    as some models behave better when in <em>inpaint</em> mode even for normal <em>img2img</em> tasks  </li>
<li><strong>Upscalers</strong>:</li>
<li>pretty much a rewrite and tons of new upscalers - built-in list is now at <strong>42</strong>  </li>
<li>fix long outstanding memory leak in legacy code, amazing this went undetected for so long  </li>
<li>more high quality upscalers available by default<br />
<strong>SwinIR</strong> (2), <strong>ESRGAN</strong> (12), <strong>RealESRGAN</strong> (6), <strong>SCUNet</strong> (2)  </li>
<li>if that is not enough, there is new <strong>chaiNNer</strong> integration:<br />
    adds 15 more upscalers from different families out-of-the-box:<br />
<strong>HAT</strong> (6), <strong>RealHAT</strong> (2), <strong>DAT</strong> (1), <strong>RRDBNet</strong> (1), <strong>SPSRNet</strong> (1), <strong>SRFormer</strong> (2), <strong>SwiftSR</strong> (2)<br />
    and yes, you can download and add your own, just place them in <code>models/chaiNNer</code>  </li>
<li>two additional latent upscalers based on SD upscale models when using Diffusers backend<br />
<strong>SD Upscale 2x</strong>, <strong>SD Upscale 4x</strong><em><br />
    note: Recommended usage for </em>SD Upscale* is by using second pass instead of upscaler<br />
    as it allows for tuning of prompt, seed, sampler settings which are used to guide upscaler  </li>
<li>upscalers are available in <strong>xyz grid</strong>  </li>
<li>simplified <em>settings-&gt;postprocessing-&gt;upscalers</em><br />
    e.g. all upsamplers share same settings for tiling  </li>
<li>allow upscale-only as part of <strong>txt2img</strong> and <strong>img2img</strong> workflows<br />
    simply set <em>denoising strength</em> to 0 so hires does not get triggered  </li>
<li>unified init/download/execute/progress code  </li>
<li>easier installation  </li>
<li><strong>Samplers</strong>:  </li>
<li>moved ui options to submenu  </li>
<li>default list for new installs is now all samplers, list can be modified in settings  </li>
<li>simplified samplers configuration in settings<br />
    plus added few new ones like sigma min/max which can highly impact sampler behavior  </li>
<li>note that list of samplers is now <em>different</em> since keeping a flat-list of all possible<br />
    combinations results in 50+ samplers which is not practical<br />
    items such as algorithm (e.g. karras) is actually a sampler option, not a sampler itself  </li>
<li><strong>CivitAI</strong>:</li>
<li>civitai model download is now multithreaded and resumable<br />
    meaning that you can download multiple models in parallel<br />
    as well as resume aborted/incomplete downloads  </li>
<li>civitai integration in <em>models -&gt; civitai</em> can now find most<br />
    previews AND metadata for most models (checkpoints, loras, embeddings)<br />
    metadata is now parsed and saved in <em>[model].json</em><br />
    typical hit rate is &gt;95% for models, loras and embeddings  </li>
<li>description from parsed model metadata is used as model description if there is no manual<br />
    description file present in format of <em>[model].txt</em>  </li>
<li>to enable search for models, make sure all models have set hash values<br />
<em>Models -&gt; Valida -&gt; Calculate hashes</em>  </li>
<li><strong>LoRA</strong></li>
<li>new unified LoRA handler for all LoRA types (lora, lyco, loha, lokr, locon, ia3, etc.)<br />
    applies to both original and diffusers backend<br />
    thanks @AI-Casanova for diffusers port  </li>
<li>for <em>backend:original</em>, separate lyco handler has been removed  </li>
<li><strong>Compute</strong>  </li>
<li><strong>CUDA</strong>:  <ul>
<li>default updated to <code>torch</code> <em>2.1.0</em> with cuda <em>12.1</em>  </li>
<li>testing moved to <code>torch</code> <em>2.2.0-dev/cu122</em>  </li>
<li>check out <em>generate context menu -&gt; show nvml</em> for live gpu stats (memory, power, temp, clock, etc.)</li>
</ul>
</li>
<li><strong>Intel Arc/IPEX</strong>:  <ul>
<li>tons of optimizations, built-in binary wheels for Windows<br />
  i have to say, intel arc/ipex is getting to be quite a player, especially with openvino<br />
  thanks @Disty0 @Nuullll  </li>
</ul>
</li>
<li><strong>AMD ROCm</strong>:  <ul>
<li>updated installer to support detect <code>ROCm</code> <em>5.4/5.5/5.6/5.7</em>  </li>
<li>support for <code>torch-rocm-5.7</code></li>
</ul>
</li>
<li><strong>xFormers</strong>:<ul>
<li>default updated to <em>0.0.23</em>  </li>
<li>note that latest xformers are still not compatible with cuda 12.1<br />
  recommended to use torch 2.1.0 with cuda 11.8<br />
  if you attempt to use xformers with cuda 12.1, it will force a full xformers rebuild on install<br />
  which can take a very long time and may/may-not work  </li>
<li>added cmd param <code>--use-xformers</code> to force usage of exformers  </li>
</ul>
</li>
<li><strong>GC</strong>:  <ul>
<li>custom garbage collect threshold to reduce vram memory usage, thanks @Disty0<br />
  see <em>settings -&gt; compute -&gt; gc</em>  </li>
</ul>
</li>
<li><strong>Inference</strong>  </li>
<li>new section in <strong>settings</strong>  <ul>
<li><a href="https://github.com/tfernd/HyperTile">HyperTile</a>: new!<br />
  available for <em>diffusers</em> and <em>original</em> backends<br />
  massive (up to 2x) speed-up your generations for free :)<br />
<em>note: hypertile is not compatible with any extension that modifies processing parameters such as resolution</em><br />
  thanks @tfernd</li>
<li><a href="https://github.com/ChenyangSi/FreeU">Free-U</a>: new!<br />
  available for <em>diffusers</em> and <em>original</em> backends<br />
  improve generations quality at no cost (other than finding params that work for you)<br />
<em>note: temporarily disabled for diffusers pending release of diffusers==0.22</em><br />
  thanks @ljleb  </li>
<li><a href="https://github.com/dbolya/tomesd">Token Merging</a>: not new, but updated<br />
  available for <em>diffusers</em> and <em>original</em> backends<br />
  speed-up your generations by merging redundant tokens<br />
  speed up will depend on how aggressive you want to be with token merging  </li>
<li><strong>Batch mode</strong><br />
  new option <em>settings -&gt; inference -&gt; batch mode</em><br />
  when using img2img process batch, optionally process multiple images in batch in parallel<br />
  thanks @Symbiomatrix</li>
</ul>
</li>
<li><strong>NSFW Detection/Censor</strong>  </li>
<li>install extension: <a href="https://github.com/vladmandic/sd-extension-nudenet">NudeNet</a><br />
    body part detection, image metadata, advanced censoring, etc...<br />
    works for <em>text</em>, <em>image</em> and <em>process</em> workflows<br />
    more in the extension notes  </li>
<li><strong>Extensions</strong></li>
<li>automatic discovery of new extensions on github<br />
    no more waiting for them to appear in index!</li>
<li>new framework for extension validation<br />
    extensions ui now shows actual status of extensions for reviewed extensions<br />
    if you want to contribute/flag/update extension status, reach out on github or discord  </li>
<li>better overall compatibility with A1111 extensions (up to a point)  </li>
<li><a href="https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111">MultiDiffusion</a><br />
    has been removed from list of built-in extensions<br />
    you can still install it manually if desired  </li>
<li>[LyCORIS]<a href="https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris">https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris</a><br />
    has been removed from list of built-in extensions<br />
    it is considered obsolete given that all functionality is now built-in  </li>
<li><strong>General</strong>  </li>
<li><strong>Startup</strong>  <ul>
<li>all main CLI parameters can now be set as environment variable as well<br />
  for example <code>--data-dir &lt;path&gt;</code> can be specified as <code>SD_DATADIR=&lt;path&gt;</code> before starting SD.Next  </li>
</ul>
</li>
<li><strong>XYZ Grid</strong><ul>
<li>more flexibility to use selection or strings  </li>
</ul>
</li>
<li><strong>Logging</strong>  <ul>
<li>get browser session info in server log  </li>
<li>allow custom log file destination<br />
  see <code>webui --log</code>  </li>
<li>when running with <code>--debug</code> flag, log is force-rotated<br />
  so each <code>sdnext.log.*</code> represents exactly one server run  </li>
<li>internal server job state tracking  </li>
</ul>
</li>
<li><strong>Launcher</strong>  <ul>
<li>new <code>webui.ps1</code> powershell launcher for windows (old <code>webui.bat</code> is still valid)<br />
  thanks @em411  </li>
</ul>
</li>
<li><strong>API</strong><ul>
<li>add end-to-end example how to use API: <code>cli/simple-txt2img.js</code><br />
  covers txt2img, upscale, hires, refiner  </li>
</ul>
</li>
<li><strong>train.py</strong><ul>
<li>wrapper script around built-in <strong>kohyas lora</strong> training script<br />
  see <code>cli/train.py --help</code><br />
  new support for sd and sdxl, thanks @evshiron<br />
  new support for full offline mode (without sdnext server running)  </li>
</ul>
</li>
<li><strong>Themes</strong></li>
<li>all built-in themes are fully supported:  <ul>
<li><em>black-teal (default), light-teal, black-orange, invoked, amethyst-nightfall, midnight-barbie</em>  </li>
</ul>
</li>
<li>if youre using any <strong>gradio default</strong> themes or a <strong>3rd party</strong> theme or  that are not optimized for SD.Next, you may experience issues<br />
    default minimal style has been updated for compatibility, but actual styling is completely outside of SD.Next control  </li>
</ul>
<h2 id="update-for-2023-09-13">Update for 2023-09-13</h2>
<p>Started as a mostly a service release with quite a few fixes, but then...<br />
Major changes how <strong>hires</strong> works as well as support for a very interesting new model <a href="https://huggingface.co/blog/wuertschen">Wuerstchen</a>  </p>
<ul>
<li>tons of fixes  </li>
<li>changes to <strong>hires</strong>  </li>
<li>enable non-latent upscale modes (standard upscalers)  </li>
<li>when using latent upscale, hires pass is run automatically  </li>
<li>when using non-latent upscalers, hires pass is skipped by default<br />
    enabled using <strong>force hires</strong> option in ui<br />
    hires was not designed to work with standard upscalers, but i understand this is a common workflow  </li>
<li>when using refiner, upscale/hires runs before refiner pass  </li>
<li>second pass can now also utilize full/quick vae quality  </li>
<li>note that when combining non-latent upscale, hires and refiner output quality is maximum,<br />
    but operations are really resource intensive as it includes: <em>base-&gt;decode-&gt;upscale-&gt;encode-&gt;hires-&gt;refine</em></li>
<li>all combinations of: decode full/quick + upscale none/latent/non-latent + hires on/off + refiner on/off<br />
    should be supported, but given the number of combinations, issues are possible  </li>
<li>all operations are captured in image metadata</li>
<li>diffusers:</li>
<li>allow loading of sd/sdxl models from safetensors without online connectivity</li>
<li>support for new model: <a href="https://huggingface.co/warp-ai/wuerstchen">wuerstchen</a><br />
    its a high-resolution model (1024px+) thats ~40% faster than sd-xl with a bit lower resource requirements<br />
    go to <em>models -&gt; huggingface -&gt; search "warp-ai/wuerstchen" -&gt; download</em><br />
    its nearly 12gb in size, so be patient :)</li>
<li>minor re-layout of the main ui  </li>
<li>updated <strong>ui hints</strong>  </li>
<li>updated <strong>models -&gt; civitai</strong>  </li>
<li>search and download loras  </li>
<li>find previews for already downloaded models or loras  </li>
<li>new option <strong>inference mode</strong>  </li>
<li>default is standard <code>torch.no_grad</code><br />
    new option is <code>torch.inference_only</code> which is slightly faster and uses less vram, but only works on some gpus  </li>
<li>new cmdline param <code>--no-metadata</code><br />
  skips reading metadata from models that are not already cached  </li>
<li>updated <strong>gradio</strong>  </li>
<li><strong>styles</strong> support for subfolders  </li>
<li><strong>css</strong> optimizations</li>
<li>clean-up <strong>logging</strong>  </li>
<li>capture system info in startup log  </li>
<li>better diagnostic output  </li>
<li>capture extension output  </li>
<li>capture ldm output  </li>
<li>cleaner server restart  </li>
<li>custom exception handling</li>
</ul>
<h2 id="update-for-2023-09-06">Update for 2023-09-06</h2>
<p>One week later, another large update!</p>
<ul>
<li>system:  </li>
<li>full <strong>python 3.11</strong> support<br />
    note that changing python version does require reinstall<br />
    and if youre already on python 3.10, really no need to upgrade  </li>
<li>themes:  </li>
<li>new default theme: <strong>black-teal</strong>  </li>
<li>new light theme: <strong>light-teal</strong>  </li>
<li>new additional theme: <strong>midnight-barbie</strong>, thanks @nyxia  </li>
<li>extra networks:  </li>
<li>support for <strong>tags</strong><br />
    show tags on hover, search by tag, list tags, add to prompt, etc.  </li>
<li><strong>styles</strong> are now also listed as part of extra networks<br />
    existing <code>styles.csv</code> is converted upon startup to individual styles inside <code>models/style</code><br />
    this is stage one of new styles functionality<br />
    old styles interface is still available, but will be removed in future  </li>
<li>cache file lists for much faster startup<br />
    speedups are 50+% for large number of extra networks  </li>
<li>ui refresh button now refreshes selected page, not all pages  </li>
<li>simplified handling of <strong>descriptions</strong><br />
    now shows on-mouse-over without the need for user interaction  </li>
<li><strong>metadata</strong> and <strong>info</strong> buttons only show if there is actual content  </li>
<li>diffusers:  </li>
<li>add full support for <strong>textual inversions</strong> (embeddings)<br />
    this applies to both sd15 and sdxl<br />
    thanks @ai-casanova for porting compel/sdxl code  </li>
<li>mix&amp;match <strong>base</strong> and <strong>refiner</strong> models (<em>experimental</em>):<br />
    most of those are "because why not" and can result in corrupt images, but some are actually useful<br />
    also note that if youre not using actual refiner model, you need to bump refiner steps<br />
    as normal models are not designed to work with low step count<br />
    and if youre having issues, try setting prompt parser to "fixed attention" as majority of problems<br />
    are due to token mismatches when using prompt attention  <ul>
<li>any sd15 + any sd15  </li>
<li>any sd15 + sdxl-refiner  </li>
<li>any sdxl-base + sdxl-refiner  </li>
<li>any sdxl-base + any sd15  </li>
<li>any sdxl-base + any sdxl-base  </li>
</ul>
</li>
<li>ability to <strong>interrupt</strong> (stop/skip) model generate  </li>
<li>added <strong>aesthetics score</strong> setting (for sdxl)<br />
    used to automatically guide unet towards higher pleasing images<br />
    highly recommended for simple prompts  </li>
<li>added <strong>force zeros</strong> setting<br />
    create zero-tensor for prompt if prompt is empty (positive or negative)  </li>
<li>general:  </li>
<li><code>rembg</code> remove backgrounds support for <strong>is-net</strong> model  </li>
<li><strong>settings</strong> now show markers for all items set to non-default values  </li>
<li><strong>metadata</strong> refactored how/what/when metadata is added to images<br />
    should result in much cleaner and more complete metadata  </li>
<li>pre-create all system folders on startup  </li>
<li>handle model load errors gracefully  </li>
<li>improved vram reporting in ui  </li>
<li>improved script profiling (when running in debug mode)  </li>
</ul>
<h2 id="update-for-2023-08-30">Update for 2023-08-30</h2>
<p>Time for a quite a large update that has been leaking bit-by-bit over the past week or so...<br />
<em>Note</em>: due to large changes, it is recommended to reset (delete) your <code>ui-config.json</code>  </p>
<ul>
<li>diffusers:  </li>
<li>support for <strong>distilled</strong> sd models<br />
    just go to models/huggingface and download a model, for example:<br />
<code>segmind/tiny-sd</code>, <code>segmind/small-sd</code>, <code>segmind/portrait-finetuned</code><br />
    those are lower quality, but extremely small and fast<br />
    up to 50% faster than sd 1.5 and execute in as little as 2.1gb of vram  </li>
<li>general:  </li>
<li>redesigned <strong>settings</strong>  <ul>
<li>new layout with separated sections:<br />
<em>settings, ui config, licenses, system info, benchmark, models</em>  </li>
<li><strong>system info</strong> tab is now part of settings<br />
  when running outside of sdnext, system info is shown in main ui  </li>
<li>all system and image paths are now relative by default  </li>
<li>add settings validation when performing load/save  </li>
<li>settings tab in ui now shows settings that are changed from default values  </li>
<li>settings tab switch to compact view  </li>
</ul>
</li>
<li>update <strong>gradio</strong> major version<br />
    this may result in some smaller layout changes since its a major version change<br />
    however, browser page load is now much faster  </li>
<li>optimizations:<ul>
<li>optimize model hashing  </li>
<li>add cli param <code>--skip-all</code> that skips all installer checks<br />
  use at personal discretion, but it can be useful for bulk deployments  </li>
<li>add model <strong>precompile</strong> option (when model compile is enabled)  </li>
<li><strong>extra network</strong> folder info caching<br />
  results in much faster startup when you have large number of extra networks  </li>
<li>faster <strong>xyz grid</strong> switching<br />
  especially when using different checkpoints  </li>
</ul>
</li>
<li>update <strong>second pass</strong> options for clarity</li>
<li>models:<ul>
<li>civitai download missing model previews</li>
</ul>
</li>
<li>add <strong>openvino</strong> (experimental) cpu optimized model compile and inference<br />
    enable with <code>--use-openvino</code><br />
    thanks @disty0  </li>
<li>enable batch <strong>img2img</strong> scale-by workflows<br />
    now you can batch process with rescaling based on each individual original image size  </li>
<li>fixes:<ul>
<li>fix extra networks previews  </li>
<li>css fixes  </li>
<li>improved extensions compatibility (e.g. <em>sd-cn-animation</em>)  </li>
<li>allow changing <strong>vae</strong> on-the-fly for both original and diffusers backend</li>
</ul>
</li>
</ul>
<h2 id="update-for-2023-08-20">Update for 2023-08-20</h2>
<p>Another release thats been baking in dev branch for a while...</p>
<ul>
<li>general:</li>
<li>caching of extra network information to enable much faster create/refresh operations<br />
    thanks @midcoastal</li>
<li>diffusers:</li>
<li>add <strong>hires</strong> support (<em>experimental</em>)<br />
    applies to all model types that support img2img, including <strong>sd</strong> and <strong>sd-xl</strong><br />
    also supports all hires upscaler types as well as standard params like steps and denoising strength<br />
    when used with <strong>sd-xl</strong>, it can be used with or without refiner loaded<br />
    how to enable - there are no explicit checkboxes other than second pass itself:<ul>
<li>hires: upscaler is set and target resolution is not at default  </li>
<li>refiner: if refiner model is loaded  </li>
</ul>
</li>
<li>images save options: <em>before hires</em>, <em>before refiner</em></li>
<li>redo <code>move model to cpu</code> logic in settings -&gt; diffusers to be more reliable<br />
    note that system defaults have also changed, so you may need to tweak to your liking  </li>
<li>update dependencies</li>
</ul>
<h2 id="update-for-2023-08-17">Update for 2023-08-17</h2>
<p>Smaller update, but with some breaking changes (to prepare for future larger functionality)...</p>
<ul>
<li>general:</li>
<li>update all metadata saved with images<br />
    see <a href="https://github.com/vladmandic/automatic/wiki/Metadata">https://github.com/vladmandic/automatic/wiki/Metadata</a> for details  </li>
<li>improved <strong>amd</strong> installer with support for <strong>navi 2x &amp; 3x</strong> and <strong>rocm 5.4/5.5/5.6</strong><br />
    thanks @evshiron  </li>
<li>fix <strong>img2img</strong> resizing (applies to <em>original, diffusers, hires</em>)  </li>
<li>config change: main <code>config.json</code> no longer contains entire configuration<br />
    but only differences from defaults (similar to recent change performed to <code>ui-config.json</code>)  </li>
<li>diffusers:</li>
<li>enable <strong>batch img2img</strong> workflows  </li>
<li>original:  </li>
<li>new samplers: <strong>dpm++ 3M sde</strong> (standard and karras variations)<br />
    enable in <em>settings -&gt; samplers -&gt; show samplers</em></li>
<li>expose always/never discard penultimate sigma<br />
    enable in <em>settings -&gt; samplers</em>  </li>
</ul>
<h2 id="update-for-2023-08-11">Update for 2023-08-11</h2>
<p>This is a big one thats been cooking in <code>dev</code> for a while now, but finally ready for release...</p>
<ul>
<li>diffusers:</li>
<li><strong>pipeline autodetect</strong>
    if pipeline is set to autodetect (default for new installs), app will try to autodetect pipeline based on selected model<br />
    this should reduce user errors such as loading <strong>sd-xl</strong> model when <strong>sd</strong> pipeline is selected  </li>
<li><strong>quick vae decode</strong> as alternative to full vae decode which is very resource intensive<br />
    quick decode is based on <code>taesd</code> and produces lower quality, but its great for tests or grids as it runs much faster and uses far less vram<br />
    disabled by default, selectable in <em>txt2img/img2img -&gt; advanced -&gt; full quality</em>  </li>
<li><strong>prompt attention</strong> for sd and sd-xl<br />
    supports both <code>full parser</code> and native <code>compel</code><br />
    thanks @ai-casanova  </li>
<li>advanced <strong>lora load/apply</strong> methods<br />
    in addition to standard lora loading that was recently added to sd-xl using diffusers, now we have  <ul>
<li><strong>sequential apply</strong> (load &amp; apply multiple loras in sequential manner) and  </li>
<li><strong>merge and apply</strong> (load multiple loras and merge before applying to model)<br />
see <em>settings -&gt; diffusers -&gt; lora methods</em><br />
thanks @hameerabbasi and @ai-casanova  </li>
</ul>
</li>
<li><strong>sd-xl vae</strong> from safetensors now applies correct config<br />
    result is that 3rd party vaes can be used without washed out colors  </li>
<li>options for optimized memory handling for lower memory usage<br />
    see <em>settings -&gt; diffusers</em></li>
<li>general:</li>
<li>new <strong>civitai model search and download</strong><br />
    native support for civitai, integrated into ui as <em>models -&gt; civitai</em>  </li>
<li>updated requirements<br />
    this time its a bigger change so upgrade may take longer to install new requirements</li>
<li>improved <strong>extra networks</strong> performance with large number of networks</li>
</ul>
<h2 id="update-for-2023-08-05">Update for 2023-08-05</h2>
<p>Another minor update, but it unlocks some cool new items...</p>
<ul>
<li>diffusers:</li>
<li>vaesd live preview (sd and sd-xl)  </li>
<li>fix inpainting (sd and sd-xl)  </li>
<li>general:</li>
<li>new torch 2.0 with ipex (intel arc)  </li>
<li>additional callbacks for extensions<br />
    enables latest comfyui extension  </li>
</ul>
<h2 id="update-for-2023-07-30">Update for 2023-07-30</h2>
<p>Smaller release, but IMO worth a post...</p>
<ul>
<li>diffusers:</li>
<li>sd-xl loras are now supported!</li>
<li>memory optimizations: Enhanced sequential CPU offloading, model CPU offload, FP16 VAE<ul>
<li>significant impact if running SD-XL (for example, but applies to any model) with only 8GB VRAM</li>
</ul>
</li>
<li>update packages</li>
<li>minor bugfixes</li>
</ul>
<h2 id="update-for-2023-07-26">Update for 2023-07-26</h2>
<p>This is a big one, new models, new diffusers, new features and updated UI...</p>
<p>First, <strong>SD-XL 1.0</strong> is released and yes, SD.Next supports it out of the box!</p>
<ul>
<li><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors">SD-XL Base</a></li>
<li><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/sd_xl_refiner_1.0.safetensors">SD-XL Refiner</a></li>
</ul>
<p>Also fresh is new <strong>Kandinsky 2.2</strong> model that does look quite nice:</p>
<ul>
<li><a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder">Kandinsky Decoder</a></li>
<li><a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-prior">Kandinsky Prior</a></li>
</ul>
<p>Actual changelog is:</p>
<ul>
<li>general:</li>
<li>new loading screens and artwork</li>
<li>major ui simplification for both txt2img and img2img<br />
    nothing is removed, but you can show/hide individual sections<br />
    default is very simple interface, but you can enable any sections and save it as default in settings  </li>
<li>themes: add additional built-in theme, <code>amethyst-nightfall</code></li>
<li>extra networks: add add/remove tags to prompt (e.g. lora activation keywords)</li>
<li>extensions: fix couple of compatibility items</li>
<li>firefox compatibility improvements</li>
<li>minor image viewer improvements</li>
<li>
<p>add backend and operation info to metadata</p>
</li>
<li>
<p>diffusers:</p>
</li>
<li>were out of experimental phase and diffusers backend is considered stable  </li>
<li>sd-xl: support for <strong>sd-xl 1.0</strong> official model</li>
<li>sd-xl: loading vae now applies to both base and refiner and saves a bit of vram  </li>
<li>sd-xl: denoising_start/denoising_end</li>
<li>sd-xl: enable dual prompts<br />
    dual prompt is used if set regardless if refiner is enabled/loaded<br />
    if refiner is loaded &amp; enabled, refiner prompt will also be used for refiner pass  <ul>
<li>primary prompt goes to <a href="https://huggingface.co/openai/clip-vit-large-patch14">OpenAI CLIP-ViT/L-14</a></li>
<li>refiner prompt goes to <a href="https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k">OpenCLIP-ViT/bigG-14</a></li>
</ul>
</li>
<li><strong>kandinsky 2.2</strong> support<br />
    note: kandinsky model must be downloaded using model downloader, not as safetensors due to specific model format  </li>
<li>refiner: fix batch processing</li>
<li>vae: enable loading of pure-safetensors vae files without config<br />
    also enable <em>automatic</em> selection to work with diffusers  </li>
<li>sd-xl: initial lora support<br />
    right now this applies to official lora released by <strong>stability-ai</strong>, support for <strong>kohyas</strong> lora is expected soon  </li>
<li>implement img2img and inpainting (experimental)<br />
    actual support and quality depends on model<br />
    it works as expected for sd 1.5, but not so much for sd-xl for now  </li>
<li>implement limited stop/interrupt for diffusers
    works between stages, not within steps  </li>
<li>add option to save image before refiner pass  </li>
<li>option to set vae upcast in settings  </li>
<li>
<p>enable fp16 vae decode when using optimized vae<br />
    this pretty much doubles performance of decode step (delay after generate is done)  </p>
</li>
<li>
<p>original</p>
</li>
<li>fix hires secondary sampler<br />
    this now fully obsoletes <code>fallback_sampler</code> and <code>force_hr_sampler_name</code>  </li>
</ul>
<h2 id="update-for-2023-07-18">Update for 2023-07-18</h2>
<p>While were waiting for official SD-XL release, heres another update with some fixes and enhancements...</p>
<ul>
<li><strong>global</strong></li>
<li>image save: option to add invisible image watermark to all your generated images<br />
    disabled by default, can be enabled in settings -&gt; image options<br />
    watermark information will be shown when loading image such as in process image tab<br />
    also additional cli utility <code>/cli/image-watermark.py</code> to read/write/strip watermarks from images  </li>
<li>batch processing: fix metadata saving, also allow to drag&amp;drop images for batch processing  </li>
<li>ui configuration: you can modify all ui default values from settings as usual,<br />
    but only values that are non-default will be written to <code>ui-config.json</code>  </li>
<li>startup: add cmd flag to skip all <code>torch</code> checks  </li>
<li>startup: force requirements check on each server start<br />
    there are too many misbehaving extensions that change system requirements  </li>
<li>internal: safe handling of all config file read/write operations<br />
    this allows sdnext to run in fully shared environments and prevents any possible configuration corruptions  </li>
<li><strong>diffusers</strong>:</li>
<li>sd-xl: remove image watermarks autocreated by 0.9 model  </li>
<li>vae: enable loading of external vae, documented in diffusers wiki<br />
    and mix&amp;match continues, you can even use sd-xl vae with sd 1.5 models!  </li>
<li>samplers: add concept of <em>default</em> sampler to avoid needing to tweak settings for primary or second pass<br />
    note that sampler details will be printed in log when running in debug level  </li>
<li>samplers: allow overriding of sampler beta values in settings  </li>
<li>refiner: fix refiner applying only to first image in batch  </li>
<li>refiner: allow using direct latents or processed output in refiner  </li>
<li>model: basic support for one more model: <a href="https://github.com/thu-ml/unidiffuser">UniDiffuser</a><br />
    download using model downloader: <code>thu-ml/unidiffuser-v1</code><br />
    and set resolution to 512x512  </li>
</ul>
<h2 id="update-for-2023-07-14">Update for 2023-07-14</h2>
<p>Trying to unify settings for both original and diffusers backend without introducing duplicates...</p>
<ul>
<li>renamed <strong>hires fix</strong> to <strong>second pass</strong><br />
  as that is what it actually is, name hires fix is misleading to start with  </li>
<li>actual <strong>hires fix</strong> and <strong>refiner</strong> are now options inside <strong>second pass</strong> section  </li>
<li>obsoleted settings -&gt; sampler -&gt; <strong>force_hr_sampler_name</strong><br />
  it is now part of <strong>second pass</strong> options and it works the same for both original and diffusers backend<br />
  which means you can use different scheduler settings for txt2img and hires if you want  </li>
<li>sd-xl refiner will run if its loaded and if second pass is enabled<br />
  so you can quickly enable/disable refiner by simply enabling/disabling second pass  </li>
<li>you can mix&amp;match <strong>model</strong> and <strong>refiner</strong><br />
  for example, you can generate image using sd 1.5 and still use sd-xl refiner as second pass  </li>
<li>reorganized settings -&gt; samplers to show which section refers to which backend  </li>
<li>added diffusers <strong>lmsd</strong> sampler  </li>
</ul>
<h2 id="update-for-2023-07-13">Update for 2023-07-13</h2>
<p>Another big one, but now improvements to both <strong>diffusers</strong> and <strong>original</strong> backends as well plus ability to dynamically switch between them!</p>
<ul>
<li>swich backend between diffusers and original on-the-fly</li>
<li>you can still use <code>--backend &lt;backend&gt;</code> and now that only means in which mode app will start,
    but you can change it anytime in ui settings</li>
<li>for example, you can even do things like generate image using sd-xl,<br />
    then switch to original backend and perform inpaint using a different model  </li>
<li>diffusers backend:</li>
<li>separate ui settings for refiner pass with sd-xl<br />
    you can specify: prompt, negative prompt, steps, denoise start  </li>
<li>fix loading from pure safetensors files<br />
    now you can load sd-xl from safetensors file or from huggingface folder format  </li>
<li>fix kandinsky model (2.1 working, 2.2 was just released and will be soon)  </li>
<li>original backend:</li>
<li>improvements to vae/unet handling as well as cross-optimization heads<br />
    in non-technical terms, this means lower memory usage and higher performance<br />
    and you should be able to generate higher resolution images without any other changes</li>
<li>other:</li>
<li>major refactoring of the javascript code<br />
    includes fixes for text selections and navigation  </li>
<li>system info tab now reports on nvidia driver version as well  </li>
<li>minor fixes in extra-networks  </li>
<li>installer handles origin changes for submodules  </li>
</ul>
<p>big thanks to @huggingface team for great communication, support and fixing all the reported issues asap!</p>
<h2 id="update-for-2023-07-10">Update for 2023-07-10</h2>
<p>Service release with some fixes and enhancements:</p>
<ul>
<li>diffusers:</li>
<li>option to move base and/or refiner model to cpu to free up vram  </li>
<li>model downloader options to specify model variant / revision / mirror  </li>
<li>now you can download <code>fp16</code> variant directly for reduced memory footprint  </li>
<li>basic <strong>img2img</strong> workflow (<em>sketch</em> and <em>inpaint</em> are not supported yet)<br />
    note that <strong>sd-xl</strong> img2img workflows are architecturaly different so it will take longer to implement  </li>
<li>updated hints for settings  </li>
<li>extra networks:</li>
<li>fix corrupt display on refesh when new extra network type found  </li>
<li>additional ui tweaks  </li>
<li>generate thumbnails from previews only if preview resolution is above 1k</li>
<li>image viewer:</li>
<li>fixes for non-chromium browsers and mobile users and add option to download image  </li>
<li>option to download image directly from image viewer</li>
<li>general</li>
<li>fix startup issue with incorrect config  </li>
<li>installer should always check requirements on upgrades</li>
</ul>
<h2 id="update-for-2023-07-08">Update for 2023-07-08</h2>
<p>This is a massive update which has been baking in a <code>dev</code> branch for a while now</p>
<ul>
<li>merge experimental diffusers support  </li>
</ul>
<p><em>TL;DR</em>: Yes, you can run <strong>SD-XL</strong> model in <strong>SD.Next</strong> now<br />
For details, see Wiki page: <a href="https://github.com/vladmandic/automatic/wiki/Diffusers">Diffusers</a><br />
Note this is still experimental, so please follow Wiki<br />
Additional enhancements and fixes will be provided over the next few days<br />
<em>Thanks to @huggingface team for making this possible and our internal @team for all the early testing</em></p>
<p>Release also contains number of smaller updates:</p>
<ul>
<li>add pan &amp; zoom controls (touch and mouse) to image viewer (lightbox)  </li>
<li>cache extra networks between tabs<br />
  this should result in neat 2x speedup on building extra networks  </li>
<li>add settings -&gt; extra networks -&gt; do not automatically build extra network pages<br />
  speeds up app start if you have a lot of extra networks and you want to build them manually when needed  </li>
<li>extra network ui tweaks  </li>
</ul>
<h2 id="update-for-2023-07-01">Update for 2023-07-01</h2>
<p>Small quality-of-life updates and bugfixes:</p>
<ul>
<li>add option to disallow usage of ckpt checkpoints</li>
<li>change lora and lyco dir without server restart</li>
<li>additional filename template fields: <code>uuid</code>, <code>seq</code>, <code>image_hash</code>  </li>
<li>image toolbar is now shown only when image is present</li>
<li>image <code>Zip</code> button gone and its not optional setting that applies to standard <code>Save</code> button</li>
<li>folder <code>Show</code> button is present only when working on localhost,<br />
  otherwise its replaced with <code>Copy</code> that places image URLs on clipboard so they can be used in other apps</li>
</ul>
<h2 id="update-for-2023-06-30">Update for 2023-06-30</h2>
<p>A bit bigger update this time, but contained to specific areas...</p>
<ul>
<li>change in behavior<br />
  extensions no longer auto-update on startup<br />
  using <code>--upgrade</code> flag upgrades core app as well as all submodules and extensions  </li>
<li><strong>live server log monitoring</strong> in ui<br />
  configurable via settings -&gt; live preview  </li>
<li>new <strong>extra networks interface</strong><br />
<em>note: if youre using a 3rd party ui extension for extra networks, it will likely need to be updated to work with new interface</em></li>
<li>display in front of main ui, inline with main ui or as a sidebar  </li>
<li>lazy load thumbnails<br />
    drastically reduces load times for large number of extra networks  </li>
<li>auto-create thumbnails from preview images in extra networks in a background thread<br />
    significant load time saving on subsequent restarts  </li>
<li>support for info files in addition to description files  </li>
<li>support for variable aspect-ratio thumbnails  </li>
<li>new folder view  </li>
<li><strong>extensions sort</strong> by trending  </li>
<li>add requirements check for training  </li>
</ul>
<h2 id="update-for-2023-06-26">Update for 2023-06-26</h2>
<ul>
<li>new training tab interface  </li>
<li>redesigned preprocess, train embedding, train hypernetwork  </li>
<li>new models tab interface  </li>
<li>new model convert functionality, thanks @akegarasu  </li>
<li>new model verify functionality  </li>
<li>lot of ipex specific fixes/optimizations, thanks @disty0  </li>
</ul>
<h2 id="update-for-2023-06-20">Update for 2023-06-20</h2>
<p>This one is less relevant for standard users, but pretty major if youre running an actual server<br />
But even if not, it still includes bunch of cumulative fixes since last release - and going by number of new issues, this is probably the most stable release so far...
(next one is not going to be as stable, but it will be fun :) )</p>
<ul>
<li>minor improvements to extra networks ui  </li>
<li>more hints/tooltips integrated into ui  </li>
<li>new dedicated api server  </li>
<li>but highly promising for high throughput server  </li>
<li>improve server logging and monitoring with  </li>
<li>server log file rotation  </li>
<li>ring buffer with api endpoint <code>/sdapi/v1/log</code>  </li>
<li>real-time status and load endpoint <code>/sdapi/v1/system-info/status</code></li>
</ul>
<h2 id="update-for-2023-06-14">Update for 2023-06-14</h2>
<p>Second stage of a jumbo merge from upstream plus few minor changes...</p>
<ul>
<li>simplify token merging  </li>
<li>reorganize some settings  </li>
<li>all updates from upstream: <strong>A1111</strong> v1.3.2 [df004be] <em>(latest release)</em><br />
  pretty much nothing major that i havent released in previous versions, but its still a long list of tiny changes  </li>
<li>skipped/did-not-port:<br />
    add separate hires prompt: unnecessarily complicated and spread over large number of commits due to many regressions<br />
    allow external scripts to add cross-optimization methods: dangerous and i dont see a use case for it so far<br />
    load extension info in threads: unnecessary as other optimizations ive already put place perform equally good  </li>
<li>broken/reverted:<br />
    sub-quadratic optimization changes  </li>
</ul>
<h2 id="update-for-2023-06-13">Update for 2023-06-13</h2>
<p>Just a day later and one <em>bigger update</em>...
Both some <strong>new functionality</strong> as well as <strong>massive merges</strong> from upstream  </p>
<ul>
<li>new cache for models/lora/lyco metadata: <code>metadata.json</code><br />
  drastically reduces disk access on app startup  </li>
<li>allow saving/resetting of <strong>ui default values</strong><br />
  settings -&gt; ui defaults</li>
<li>ability to run server without loaded model<br />
  default is to auto-load model on startup, can be changed in settings -&gt; stable diffusion<br />
  if disabled, model will be loaded on first request, e.g. when you click generate<br />
  useful when you want to start server to perform other tasks like upscaling which do not rely on model  </li>
<li>updated <code>accelerate</code> and <code>xformers</code></li>
<li>huge nubmer of changes ported from <strong>A1111</strong> upstream<br />
  this was a massive merge, hopefully this does not cause any regressions<br />
  and still a bit more pending...</li>
</ul>
<h2 id="update-for-2023-06-12">Update for 2023-06-12</h2>
<ul>
<li>updated ui labels and hints to improve clarity and provide some extra info<br />
  this is 1st stage of the process, more to come...<br />
  if you want to join the effort, see <a href="https://github.com/vladmandic/automatic/discussions/1246">https://github.com/vladmandic/automatic/discussions/1246</a></li>
<li>new localization and hints engine<br />
  how hints are displayed can be selected in settings -&gt; ui  </li>
<li>reworked <strong>installer</strong> sequence<br />
  as some extensions are loading packages directly from their preload sequence<br />
  which was preventing some optimizations to take effect  </li>
<li>updated <strong>settings</strong> tab functionality, thanks @gegell<br />
  with real-time monitor for all new and/or updated settings  </li>
<li><strong>launcher</strong> will now warn if application owned files are modified<br />
  you are free to add any user files, but do not modify app files unless youre sure in what youre doing  </li>
<li>add more profiling for scripts/extensions so you can see what takes time<br />
  this applies both to initial load as well as execution  </li>
<li>experimental <code>sd_model_dict</code> setting which allows you to load model dictionary<br />
  from one model and apply weights from another model specified in <code>sd_model_checkpoint</code><br />
  results? who am i to judge :)</li>
</ul>
<h2 id="update-for-2023-06-05">Update for 2023-06-05</h2>
<p>Few new features and extra handling for broken extensions<br />
that caused my phone to go crazy with notifications over the weekend...</p>
<ul>
<li>added extra networks to <strong>xyz grid</strong> options<br />
  now you can have more fun with all your embeddings and loras :)  </li>
<li>new <strong>vae decode</strong> method to help with larger batch sizes, thanks @bigdog  </li>
<li>new setting -&gt; lora -&gt; <strong>use lycoris to handle all lora types</strong><br />
  this is still experimental, but the goal is to obsolete old built-in lora module<br />
  as it doesnt understand many new loras and built-in lyco module can handle it all  </li>
<li>somewhat optimize browser page loading<br />
  still slower than id want, but gradio is pretty bad at this  </li>
<li>profiling of scripts/extensions callbacks<br />
  you can now see how much or pre/post processing is done, not just how long generate takes  </li>
<li>additional exception handling so bad exception does not crash main app  </li>
<li>additional background removal models  </li>
<li>some work on bfloat16 which nobody really should be using, but why not </li>
</ul>
<h2 id="update-for-2023-06-02">Update for 2023-06-02</h2>
<p>Some quality-of-life improvements while working on larger stuff in the background...</p>
<ul>
<li>redesign action box to be uniform across all themes  </li>
<li>add <strong>pause</strong> option next to stop/skip  </li>
<li>redesigned progress bar  </li>
<li>add new built-in extension: <strong>agent-scheduler</strong><br />
  very elegant way to getting full queueing capabilities, thank @artventurdev  </li>
<li>enable more image formats<br />
  note: not all are understood by browser so previews and images may appear as blank<br />
  unless you have some browser extensions that can handle them<br />
  but they are saved correctly. and cant beat raw quality of 32-bit <code>tiff</code> or <code>psd</code> :)  </li>
<li>change in behavior: <code>xformers</code> will be uninstalled on startup if they are not active<br />
  if you do have <code>xformers</code> selected as your desired cross-optimization method, then they will be used<br />
  reason is that a lot of libaries try to blindly import xformers even if they are not selected or not functional  </li>
</ul>
<h2 id="update-for-2023-05-30">Update for 2023-05-30</h2>
<p>Another bigger one...And more to come in the next few days...</p>
<ul>
<li>new live preview mode: taesd<br />
  i really like this one, so its enabled as default for new installs  </li>
<li>settings search feature  </li>
<li>new sampler: dpm++ 2m sde  </li>
<li>fully common save/zip/delete (new) options in all tabs<br />
  which (again) meant rework of process image tab  </li>
<li>system info tab: live gpu utilization/memory graphs for nvidia gpus  </li>
<li>updated controlnet interface  </li>
<li>minor style changes  </li>
<li>updated lora, swinir, scunet and ldsr code from upstream  </li>
<li>start of merge from a1111 v1.3  </li>
</ul>
<h2 id="update-for-2023-05-26">Update for 2023-05-26</h2>
<p>Some quality-of-life improvements...</p>
<ul>
<li>updated <a href="https://github.com/vladmandic/automatic/blob/master/README.md">README</a></li>
<li>created <a href="https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md">CHANGELOG</a><br />
  this will be the source for all info about new things moving forward<br />
  and cross-posted to <a href="https://github.com/vladmandic/automatic/discussions/99">Discussions#99</a> as well as discord <a href="https://discord.com/channels/1101998836328697867/1109953953396957286">announcements</a></li>
<li>optimize model loading on startup<br />
  this should reduce startup time significantly  </li>
<li>set default cross-optimization method for each platform backend<br />
  applicable for new installs only  </li>
<li><code>cuda</code> =&gt; Scaled-Dot-Product</li>
<li><code>rocm</code> =&gt; Sub-quadratic</li>
<li><code>directml</code> =&gt; Sub-quadratic</li>
<li><code>ipex</code> =&gt; invokeais</li>
<li><code>mps</code> =&gt; Doggettxs</li>
<li><code>cpu</code> =&gt; Doggettxs</li>
<li>optimize logging  </li>
<li>optimize profiling<br />
  now includes startup profiling as well as <code>cuda</code> profiling during generate  </li>
<li>minor lightbox improvements  </li>
<li>bugfixes...i dont recall when was a release with at least several of those  </li>
</ul>
<p>other than that - first stage of <a href="https://github.com/huggingface/diffusers">Diffusers</a> integration is now in master branch<br />
i dont recommend anyone to try it (and dont even think reporting issues for it)<br />
but if anyone wants to contribute, take a look at <a href="https://github.com/users/vladmandic/projects/1/views/1">project page</a></p>
<h2 id="update-for-2023-05-23">Update for 2023-05-23</h2>
<p>Major internal work with perhaps not that much user-facing to show for it ;)</p>
<ul>
<li>update core repos: <strong>stability-ai</strong>, <strong>taming-transformers</strong>, <strong>k-diffusion, blip</strong>, <strong>codeformer</strong><br />
  note: to avoid disruptions, this is applicable for new installs only</li>
<li>tested with <strong>torch 2.1</strong>, <strong>cuda 12.1</strong>, <strong>cudnn 8.9</strong><br />
  (production remains on torch2.0.1+cuda11.8+cudnn8.8)  </li>
<li>fully extend support of <code>--data-dir</code><br />
  allows multiple installations to share pretty much everything, not just models<br />
  especially useful if you want to run in a stateless container or cloud instance  </li>
<li>redo api authentication<br />
  now api authentication will use same user/pwd (if specified) for ui and strictly enforce it using httpbasicauth<br />
  new authentication is also fully supported in combination with ssl for both sync and async calls<br />
  if you want to use api programatically, see examples in <code>cli/sdapi.py</code>  </li>
<li>add dark/light theme mode toggle  </li>
<li>redo some <code>clip-skip</code> functionality  </li>
<li>better matching for vae vs model  </li>
<li>update to <code>xyz grid</code> to allow creation of large number of images without creating grid itself  </li>
<li>update <code>gradio</code> (again)  </li>
<li>more prompt parser optimizations  </li>
<li>better error handling when importing image settings which are not compatible with current install<br />
  for example, when upscaler or sampler originally used is not available  </li>
<li>fixes...amazing how many issues were introduced by porting a1111 v1.20 code without adding almost no new functionality<br />
  next one is v1.30 (still in dev) which does bring a lot of new features  </li>
</ul>
<h2 id="update-for-2023-05-17">Update for 2023-05-17</h2>
<p>This is a massive one due to huge number of changes,<br />
but hopefully it will go ok...</p>
<ul>
<li>new <strong>prompt parsers</strong><br />
  select in UI -&gt; Settings -&gt; Stable Diffusion  </li>
<li><strong>Full</strong>: my new implementation  </li>
<li><strong>A1111</strong>: for backward compatibility  </li>
<li><strong>Compel</strong>: as used in ComfyUI and InvokeAI (a.k.a <em>Temporal Weighting</em>)  </li>
<li><strong>Fixed</strong>: for really old backward compatibility  </li>
<li>monitor <strong>extensions</strong> install/startup and<br />
  log if they modify any packages/requirements<br />
  this is a <em>deep-experimental</em> python hack, but i think its worth it as extensions modifying requirements<br />
  is one of most common causes of issues</li>
<li>added <code>--safe</code> command line flag mode which skips loading user extensions<br />
  please try to use it before opening new issue  </li>
<li>reintroduce <code>--api-only</code> mode to start server without ui  </li>
<li>port <em>all</em> upstream changes from <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">A1111</a><br />
  up to today - commit hash <code>89f9faa</code>  </li>
</ul>
<h2 id="update-for-2023-05-15">Update for 2023-05-15</h2>
<ul>
<li>major work on <strong>prompt parsing</strong>
  this can cause some differences in results compared to what youre used to, but its all about fixes &amp; improvements</li>
<li>prompt parser was adding commas and spaces as separate words and tokens and/or prefixes</li>
<li>negative prompt weight using <code>[word:weight]</code> was ignored, it was always <code>0.909</code></li>
<li>bracket matching was anything but correct. complex nested attention brackets are now working.</li>
<li>btw, if you run with <code>--debug</code> flag, youll now actually see parsed prompt &amp; schedule</li>
<li>updated all scripts in <code>/cli</code>  </li>
<li>add option in settings to force different <strong>latent sampler</strong> instead of using primary only</li>
<li>add <strong>interrupt/skip</strong> capabilities to process images</li>
</ul>
<h2 id="update-for-2023-05-13">Update for 2023-05-13</h2>
<p>This is mostly about optimizations...</p>
<ul>
<li>improved <code>torch-directml</code> support<br />
  especially interesting for <strong>amd</strong> users on <strong>windows</strong>  where <strong>torch+rocm</strong> is not yet available<br />
  dont forget to run using <code>--use-directml</code> or default is <strong>cpu</strong>  </li>
<li>improved compatibility with <strong>nvidia</strong> rtx 1xxx/2xxx series gpus  </li>
<li>fully working <code>torch.compile</code> with <strong>torch 2.0.1</strong><br />
  using <code>inductor</code> compile takes a while on first run, but does result in 5-10% performance increase  </li>
<li>improved memory handling<br />
  for highest performance, you can also disable aggressive <strong>gc</strong> in settings  </li>
<li>improved performance<br />
  especially <em>after</em> generate as image handling has been moved to separate thread  </li>
<li>allow per-extension updates in extension manager  </li>
<li>option to reset configuration in settings  </li>
</ul>
<h2 id="update-for-2023-05-11">Update for 2023-05-11</h2>
<ul>
<li>brand new <strong>extension manager</strong><br />
  this is pretty much a complete rewrite, so new issues are possible</li>
<li>support for <code>torch</code> 2.0.1<br />
  note that if you are experiencing frequent hangs, this may be a worth a try  </li>
<li>updated <code>gradio</code> to 3.29.0</li>
<li>added <code>--reinstall</code> flag to force reinstall of all packages  </li>
<li>auto-recover &amp; re-attempt when <code>--upgrade</code> is requested but fails</li>
<li>check for duplicate extensions  </li>
</ul>
<h2 id="update-for-2023-05-08">Update for 2023-05-08</h2>
<p>Back online with few updates:</p>
<ul>
<li>bugfixes. yup, quite a lot of those  </li>
<li>auto-detect some cpu/gpu capabilities on startup<br />
  this should reduce need to tweak and tune settings like no-half, no-half-vae, fp16 vs fp32, etc  </li>
<li>configurable order of top level tabs  </li>
<li>configurable order of scripts in txt2img and img2img<br />
  for both, see sections in ui-&gt; settings -&gt; user interface</li>
</ul>
<h2 id="update-for-2023-05-04">Update for 2023-05-04</h2>
<p>Again, few days later...</p>
<ul>
<li>reviewed/ported <strong>all</strong> commits from <strong>A1111</strong> upstream<br />
  some a few are not applicable as i already have alternative implementations<br />
  and very few i choose not to implement (save/restore last-known-good-config is a bad hack)<br />
  otherwise, were fully up to date (it doesnt show on fork status as code merges were mostly manual due to conflicts)<br />
  but...due to sheer size of the updates, this may introduce some temporary issues  </li>
<li>redesigned server restart function<br />
  now available and working in ui<br />
  actually, since server restart is now a true restart and not ui restart, it can be used much more flexibly  </li>
<li>faster model load<br />
  plus support for slower devices via stream-load function (in ui settings)  </li>
<li>better logging<br />
  this includes new <code>--debug</code> flag for more verbose logging when troubleshooting  </li>
</ul>
<h2 id="update-for-2023-05-01">Update for 2023-05-01</h2>
<p>Been a bit quieter for last few days as changes were quite significant, but finally here we are...</p>
<ul>
<li>Updated core libraries: Gradio, Diffusers, Transformers</li>
<li>Added support for <strong>Intel ARC</strong> GPUs via Intel OneAPI IPEX (auto-detected)</li>
<li>Added support for <strong>TorchML</strong> (set by default when running on non-compatible GPU or on CPU)</li>
<li>Enhanced support for AMD GPUs with <strong>ROCm</strong></li>
<li>Enhanced support for Apple <strong>M1/M2</strong></li>
<li>Redesigned command params: run <code>webui --help</code> for details</li>
<li>Redesigned API and script processing</li>
<li>Experimental support for multiple <strong>Torch compile</strong> options</li>
<li>Improved sampler support</li>
<li>Google Colab: <a href="https://colab.research.google.com/drive/126cDNwHfifCyUpCCQF9IHpEdiXRfHrLN">https://colab.research.google.com/drive/126cDNwHfifCyUpCCQF9IHpEdiXRfHrLN</a><br />
  Maintained by <a href="https://github.com/Linaqruf/sd-notebook-collection">https://github.com/Linaqruf/sd-notebook-collection</a></li>
<li>Fixes, fixes, fixes...</li>
</ul>
<p>To take advantage of new out-of-the-box tunings, its recommended to delete your <code>config.json</code> so new defaults are applied. its not necessary, but otherwise you may need to play with UI Settings to get the best of Intel ARC, TorchML, ROCm or Apple M1/M2.</p>
<h2 id="update-for-2023-04-27">Update for 2023-04-27</h2>
<p>a bit shorter list as:</p>
<ul>
<li>ive been busy with bugfixing<br />
  there are a lot of them, not going to list each here.<br />
  but seems like critical issues backlog is quieting down and soon i can focus on new features development.  </li>
<li>ive started collaboration with couple of major projects,
  hopefully this will accelerate future development.</li>
</ul>
<p>whats new:</p>
<ul>
<li>ability to view/add/edit model description shown in extra networks cards  </li>
<li>add option to specify fallback sampler if primary sampler is not compatible with desired operation  </li>
<li>make clip skip a local parameter  </li>
<li>remove obsolete items from UI settings  </li>
<li>set defaults for AMD ROCm<br />
  if you have issues, you may want to start with a fresh install so configuration can be created from scratch</li>
<li>set defaults for Apple M1/M2<br />
  if you have issues, you may want to start with a fresh install so configuration can be created from scratch</li>
</ul>
<h2 id="update-for-2023-04-25">Update for 2023-04-25</h2>
<ul>
<li>update process image -&gt; info</li>
<li>add VAE info to metadata</li>
<li>update GPU utility search paths for better GPU type detection</li>
<li>update git flags for wider compatibility</li>
<li>update environment tuning</li>
<li>update ti training defaults</li>
<li>update VAE search paths</li>
<li>add compatibility opts for some old extensions</li>
<li>validate script args for always-on scripts<br />
  fixes: deforum with controlnet  </li>
</ul>
<h2 id="update-for-2023-04-24">Update for 2023-04-24</h2>
<ul>
<li>identify race condition where generate locks up while fetching preview</li>
<li>add pulldowns to x/y/z script</li>
<li>add VAE rollback feature in case of NaNs</li>
<li>use samples format for live preview</li>
<li>add token merging</li>
<li>use <strong>Approx NN</strong> for live preview</li>
<li>create default <code>styles.csv</code></li>
<li>fix setup not installing <code>tensorflow</code> dependencies</li>
<li>update default git flags to reduce number of warnings</li>
</ul>
<h2 id="update-for-2023-04-23">Update for 2023-04-23</h2>
<ul>
<li>fix VAE dtype<br />
  should fix most issues with NaN or black images  </li>
<li>add built-in Gradio themes  </li>
<li>reduce requirements  </li>
<li>more AMD specific work</li>
<li>initial work on Apple platform support</li>
<li>additional PR merges</li>
<li>handle torch cuda crashing in setup</li>
<li>fix setup race conditions</li>
<li>fix ui lightbox</li>
<li>mark tensorflow as optional</li>
<li>add additional image name templates</li>
</ul>
<h2 id="update-for-2023-04-22">Update for 2023-04-22</h2>
<ul>
<li>autodetect which system libs should be installed<br />
  this is a first pass of autoconfig for <strong>nVidia</strong> vs <strong>AMD</strong> environments  </li>
<li>fix parse cmd line args from extensions  </li>
<li>only install <code>xformers</code> if actually selected as desired cross-attention method</li>
<li>do not attempt to use <code>xformers</code> or <code>sdp</code> if running on cpu</li>
<li>merge tomesd token merging  </li>
<li>merge 23 PRs pending from a1111 backlog (!!)</li>
</ul>
<p><em>expect shorter updates for the next few days as ill be partially ooo</em></p>
<h2 id="update-for-2023-04-20">Update for 2023-04-20</h2>
<ul>
<li>full CUDA tuning section in UI Settings</li>
<li>improve exif/pnginfo metadata parsing<br />
  it can now handle 3rd party images or images edited in external software</li>
<li>optimized setup performance and logging</li>
<li>improve compatibility with some 3rd party extensions
  for example handle extensions that install packages directly from github urls</li>
<li>fix initial model download if no models found</li>
<li>fix vae not found issues</li>
<li>fix multiple git issues</li>
</ul>
<p>note: if you previously had command line optimizations such as --no-half, those are now ignored and moved to ui settings</p>
<h2 id="update-for-2023-04-19">Update for 2023-04-19</h2>
<ul>
<li>fix live preview</li>
<li>fix model merge</li>
<li>fix handling of user-defined temp folders</li>
<li>fix submit benchmark</li>
<li>option to override <code>torch</code> and <code>xformers</code> installer</li>
<li>separate benchmark data for system-info extension</li>
<li>minor css fixes</li>
<li>created initial merge backlog from pending prs on a1111 repo<br />
  see #258 for details</li>
</ul>
<h2 id="update-for-2023-04-18">Update for 2023-04-18</h2>
<ul>
<li>reconnect ui to active session on browser restart<br />
  this is one of most frequently asked for items, finally figured it out<br />
  works for text and image generation, but not for process as there is no progress bar reported there to start with  </li>
<li>force unload <code>xformers</code> when not used<br />
  improves compatibility with AMD/M1 platforms  </li>
<li>add <code>styles.csv</code> to UI settings to allow customizing path  </li>
<li>add <code>--skip-git</code> to cmd flags for power users that want<br />
  to skip all git checks and operations and perform manual updates</li>
<li>add <code>--disable-queue</code> to cmd flags that disables Gradio queues (experimental)
  this forces it to use HTTP instead of WebSockets and can help on unreliable network connections  </li>
<li>set scripts &amp; extensions loading priority and allow custom priorities<br />
  fixes random extension issues:<br />
<code>ScuNet</code> upscaler disappearing, <code>Additional Networks</code> not showing up on XYZ axis, etc.</li>
<li>improve html loading order</li>
<li>remove some <code>asserts</code> causing runtime errors and replace with user-friendly messages</li>
<li>update README.md</li>
</ul>
<h2 id="update-for-2023-04-17">Update for 2023-04-17</h2>
<ul>
<li><strong>themes</strong> are now dynamic and discovered from list of available gradio themes on huggingface<br />
  its quite a list of 30+ supported themes so far  </li>
<li>added option to see <strong>theme preview</strong> without the need to apply it or restart server</li>
<li>integrated <strong>image info</strong> functionality into <strong>process image</strong> tab and removed separate <strong>image info</strong> tab</li>
<li>more installer improvements</li>
<li>fix urls</li>
<li>updated github integration</li>
<li>make model download as optional if no models found</li>
</ul>
<h2 id="update-for-2023-04-16">Update for 2023-04-16</h2>
<ul>
<li>support for ui themes! to to <em>settings</em> -&gt; <em>user interface</em> -&gt; "ui theme*
  includes 12 predefined themes</li>
<li>ability to restart server from ui</li>
<li>updated requirements</li>
<li>removed <code>styles.csv</code> from repo, its now fully under user control</li>
<li>removed model-keyword extension as overly aggressive</li>
<li>rewrite of the fastapi middleware handlers</li>
<li>install bugfixes, hopefully new installer is now ok  \
  i really want to focus on features and not troubleshooting installer</li>
</ul>
<h2 id="update-for-2023-04-15">Update for 2023-04-15</h2>
<ul>
<li>update default values</li>
<li>remove <code>ui-config.json</code> from repo, its now fully under user control</li>
<li>updated extensions manager</li>
<li>updated locon/lycoris plugin</li>
<li>enable quick launch by default</li>
<li>add multidiffusion upscaler extensions</li>
<li>add model keyword extension</li>
<li>enable strong linting</li>
<li>fix circular imports</li>
<li>fix extensions updated</li>
<li>fix git update issues</li>
<li>update github templates</li>
</ul>
<h2 id="update-for-2023-04-14">Update for 2023-04-14</h2>
<ul>
<li>handle duplicate extensions</li>
<li>redo exception handler</li>
<li>fix generate forever</li>
<li>enable cmdflags compatibility</li>
<li>change default css font</li>
<li>fix ti previews on initial start</li>
<li>enhance tracebacks</li>
<li>pin transformers version to last known good version</li>
<li>fix extension loader</li>
</ul>
<h2 id="update-for-2023-04-12">Update for 2023-04-12</h2>
<p>This has been pending for a while, but finally uploaded some massive changes</p>
<ul>
<li>New launcher</li>
<li><code>webui.bat</code> and <code>webui.sh</code>:<br />
    Platform specific wrapper scripts that starts <code>launch.py</code> in Python virtual environment<br />
<em>Note</em>: Server can run without virtual environment, but it is recommended to use it<br />
    This is carry-over from original repo<br />
<strong>If youre unsure which launcher to use, this is the one you want</strong>  </li>
<li><code>launch.py</code>:<br />
    Main startup script<br />
    Can be used directly to start server in manually activated <code>venv</code> or to run it without <code>venv</code>  </li>
<li><code>installer.py</code>:<br />
    Main installer, used by <code>launch.py</code>  </li>
<li><code>webui.py</code>:<br />
    Main server script  </li>
<li>New logger</li>
<li>New exception handler</li>
<li>Built-in performance profiler</li>
<li>New requirements handling</li>
<li>Move of most of command line flags into UI Settings</li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../Debug/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Debugging">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Debugging
              </div>
            </div>
          </a>
        
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      SD.Next org
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="https://github.com/vladmandic/automatic/" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="https://discord.gg/VjvR2tabEX" target="_blank" rel="noopener" title="Discord" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.youtube.com/@SDNext" target="_blank" rel="noopener" title="Youtube" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.preview", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>