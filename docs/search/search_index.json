{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SD.Next","text":""},{"location":"#step-1-installation","title":"Step 1: Installation","text":"<ul> <li>Get started with SD.Next by following the installation instructions </li> <li>For more details, check out advanced installation guide  </li> <li>List and explanation of command line arguments</li> <li>Install walkthrough video</li> </ul> <p>Tip</p> <p>And for platform specific information, check out WSL | Intel Arc | DirectML | OpenVINO | ONNX &amp; Olive | ZLUDA | AMD ROCm | MacOS | nVidia | Docker</p> <p>Warning</p> <p>If you run into issues, check out troubleshooting and debugging guides  </p>"},{"location":"#step-2-experiment","title":"Step 2: Experiment","text":"<p>SD.Next supports broad range of models: supported models and model specs </p> <p>A good starting point is to use built-in Reference model list which includes a variety of models that will be auto-downloaded and configured on first access.</p> <p>Additionally, you can either download your own models or use built-in CivitAI and HuggingFace models downloader.</p> <p>Tip</p> <p>For additional model-specific information, check out Stable Diffusion XL | Stable Diffusion 3.x | Stable Cascade | FLUX.1 | LCM</p>"},{"location":"#step-3-learn-more","title":"Step 3: Learn more","text":""},{"location":"#guides-and-tutorials","title":"Guides and tutorials","text":"<ul> <li>Getting started </li> <li>Overview of main features </li> <li>Frequently Asked Questions FAQ </li> <li>Prompting guide: what is the syntax and how to get the best results  </li> <li>Networks interface overview and search </li> <li>What is LoRA and how to use it guide </li> <li>Control guide and overview: what is it and how to use it  </li> <li>Styles guide: how to use them  </li> <li>Wildcards how-to: use them to your advantage  </li> <li>Reprocess workflows: why and how to use them  </li> <li>XYZ grid how-to: expertiment with different parameters  </li> <li>Image processing overview: postprocess your existing images  </li> <li>Intro to IP Adapters: image guidance  </li> <li>Work with models: validate, convert, download, etc.  </li> <li>Overview of built-in scripts that provide additional functionality  </li> </ul>"},{"location":"#advanced-model-handling","title":"Advanced model handling","text":"<p>Some models may require additional access checks before they can be used: Gated access Learn how to optimize your environemnt: Performance tuning Learn how you can manage your resources better:</p> <ul> <li>Memory offloading</li> <li>Model Quantization</li> <li>Model Compression</li> </ul> <p>Overview of differnet compile/device settings impact on performance: Benchmarks</p>"},{"location":"#user-interface","title":"User interface","text":"<ul> <li>Changing look &amp; feel using themes </li> <li>List of keyboard shortcuts</li> </ul>"},{"location":"#behind-the-scenes","title":"Behind-the-scenes","text":"<p>If you want to use SD.Next via API or via CLI, check out tools examples</p> <p>If you want to understand more how Stable Diffusion works</p> <ul> <li>Diffusion Pipeline: How it Works </li> <li>List of Training Methods </li> </ul> <p>And two available backend modes in SD.Next: Diffusers &amp; Original </p>"},{"location":"#step-4-contribute","title":"Step 4: Contribute","text":"<ul> <li>Adding hints </li> <li>Building custom extensions </li> <li>Create user themes </li> </ul>"},{"location":"AMD-ROCm/","title":"AMD ROCm","text":""},{"location":"AMD-ROCm/#rocm-on-ubuntu","title":"ROCm on Ubuntu","text":""},{"location":"AMD-ROCm/#install-guide-for-ubuntu-2404","title":"Install Guide for Ubuntu 24.04","text":"<pre><code>sudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.2.2/ubuntu/noble/amdgpu-install_6.2.60202-1_all.deb\nsudo apt install ./amdgpu-install_6.2.60202-1_all.deb\nsudo amdgpu-install --usecase=rocm\nsudo usermod -a -G render,video $LOGNAME\n</code></pre>"},{"location":"AMD-ROCm/#install-guide-for-ubuntu-2204","title":"Install Guide for Ubuntu 22.04","text":"<p>Simply change the wget line from \"noble\" to \"jammy\" if using Ubuntu 22.04.</p>"},{"location":"AMD-ROCm/#rocm-windows-support","title":"ROCm Windows Support","text":"<p>This is a guide to build rocBLAS based on the ROCm Official Documentations.</p> <p>You may have an AMD GPU without official support on ROCm HIP SDK OR if you are using integrated AMD GPU (iGPU), and want it to be supported by HIP SDK on Windows. You may follow the guide below to build your rocBLAS.</p> <p>If you do not need to build ROCmLibs or already have the library, please skip this.</p> <p>Make sure you have the following software available on your PC. Otherwise, you may fail to build the ROCmLibs: 1. Visual Studio 2022 2. Python 3. Strawberry Perl 4. CMake 5. Git 6. HIP SDK (Mentioned in the first step) 7. Download rocBLAS and Tensile (Download Tensile 4.38.0 for ROCm 5.7.0 (latest) on Windows)</p> <p>Edit line 41 in file rdeps.py for rocBLAS. The old repo has an outdated vckpg, which will lead to failed build. Update the vcpkg by entering the following line in the terminal:</p> <pre><code>git clone -b 2024.02.14 https://github.com/microsoft/vcpkg\n</code></pre> <p>Download <code>Tensile 4.38.0</code> from the release page.</p> <p>Download Tensile-fix-fallback-arch-build.patch, and place in the <code>Tensile</code> folder. In this example, the path is: <code>C:\\ROCm\\Tensile-rocm-5.7.0</code>.</p> <p>Enter the following line in the terminal opened in <code>Tensile-rocm-5.7.0</code>:</p> <pre><code>git apply Tensile-fix-fallback-arch-build.patch\n</code></pre> <p>if your vckpkg version is built later than April, 2023, please replace the <code>CMakeLists.txt</code> in <code>Tensile/tree/develop/Tensile/Source/lib/CMakeLists.txt</code> with this CMakeLists.txt, and put in same folder. (For more information, please access ROCm Official Guide)</p> <p>In <code>C:\\ROCm\\rocBLAS-rocm-5.7.0</code>, run:</p> <pre><code>python rdeps.py\n</code></pre> <p>If you encounter any mistake, try to Google and fix it or try it again. Use <code>install.sh -d</code> in Linux.</p> <p>Once done, run:</p> <pre><code>python rmake.py -a \"gfx906;gfx1012\" --lazy-library-loading --no-merge-architectures -t \"C:\\ROCm\\Tensile-rocm-5.7.0\"\n</code></pre> <p>Change <code>gfx906;gfx1012</code> to your GPU LLVM Target. If you want to build multiple ones at a time, make sure to separate with <code>;</code>.</p> <p>Upon successful compilation, rocblas.dll will be generated. In this example, the file path is <code>C:\\ROCm\\rocBLAS-rocm-5.7.0\\build\\release\\staging\\rocblas.dll</code>. In addition, some Tensile data files will also be produced in <code>C:\\ROCm\\rocBLAS-rocm-5.7.0\\build\\release\\Tensile\\library</code>.</p> <p>To compile HIP SDK programs that use hipBLAS/rocBLAS, you need to replace the rocblas.dll file in the SDK with the one that you have just made yourself. Then, place <code>rocblas.dll</code>into <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin</code> and the Tensile data files into <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\library</code>.</p> <p>Your programs should run smooth as silk on the designated graphics card now.</p>"},{"location":"AMD-ROCm/#rocm-custom-build","title":"ROCm Custom Build","text":"<p>This guide will walk you through building rocBLAS using the official ROCm documentation.</p> <p>This guide is for users with AMD GPUs lacking official ROCm/HIP SDK support, or those wanting to enable HIP SDK support for hip sdk 5.7 and 6.1.2  on Windows for integrated AMD GPUs(iGPUs).\"</p> <p>If you already have the libraries, you can skip this section! </p> <p>To build your own rocBLAS, follow this guide. A simplified version is available on GitHub: ROCm-HIP-SDK-Windows-Support.</p> <p>Prerequisites: Ensure the following software is installed on your PC. <code>python</code>, <code>git</code>, and the <code>HIP SDK</code>are essential.  The script <code>rdeps.py</code> will automatically download any missing dependencies when you run it.</p> <ul> <li>Visual Studio 2022: (Download from https://visualstudio.microsoft.com/)</li> <li>Python: (Download from https://www.python.org/)</li> <li>Strawberry Perl:  (Download from https://strawberryperl.com/)</li> <li>CMake: (Download from https://cmake.org/download/)</li> <li>Git: (Download from https://git-scm.com/)</li> <li>HIP SDK: (Download from https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html)</li> </ul>"},{"location":"AMD-ROCm/#downloading-the-source-code","title":"Downloading the Source Code:","text":"<ol> <li>rocBLAS: Download the latest version (https://github.com/ROCm/rocBLAS).</li> <li> <p>ROCm 5.7.0:  Download <code>rocBLAS 3.1.0</code> rocBLAS 3.1.0 for ROCm 5.7.0</p> <ul> <li>ROCm 6.1.2: Download <code>rocBLAS 4.1.2</code> rocBLAS 4.1.2 for ROCm 6.1.2</li> </ul> </li> <li> <p>Tensile: Download the appropriate version:(https://github.com/ROCm/Tensile)</p> </li> <li> <p>ROCm 5.7.0:  Download <code>Tensile 4.38.0</code> Tensile 4.38.0 for ROCm 5.7.0</p> </li> <li> <p>ROCm 6.1.2: Download <code>Tensile 4.40.0</code> Tensile 4.40.0 for ROCm 6.1.2</p> </li> </ol>"},{"location":"AMD-ROCm/#patching-tensile-for-rocm-for-advanced-users-not-a-must-do","title":"Patching Tensile for ROCm (For Advanced Users, Not-a-must-Do)","text":"<p>These steps are necessary for specific configurations of ROCm and may not be required in all cases. If you had a optimized logic for you gpu arche or do the necessary edit as guide here,you may skip this steps.Especily build libs for xnack- features.</p>"},{"location":"AMD-ROCm/#determine-your-rocm-version","title":"Determine Your ROCm Version:","text":"<ul> <li>ROCm 5.7.0: Follow the instructions for \"For hip 5.7\" below.</li> <li>ROCm 6.1.2: Follow the instructions for \"For hip 6.1.2\" below.</li> </ul>"},{"location":"AMD-ROCm/#patches-for-tensile","title":"Patches for Tensile:","text":""},{"location":"AMD-ROCm/#for-hip-570","title":"For hip 5.7.0:","text":"<ol> <li> <p>Download Tensile-fix-fallback-arch-build.patch.</p> </li> <li> <p>Place the patch file in your <code>Tensile</code> folder (e.g., <code>C:\\ROCM\\Tensile-rocm-5.7.0</code>).</p> </li> <li> <p>Open a terminal within the <code>Tensile</code> folder.</p> </li> <li> <p>Apply the patch:    <pre><code>git apply Tensile-fix-fallback-arch-build.patch\n</code></pre></p> </li> <li>If nothing appears after applying, it's patched successfully. Otherwise, you may need to manually add the patch content to <code>TensileCreateLibrary.py</code>, you may also skip this steps if you have optimized logic available.</li> </ol>"},{"location":"AMD-ROCm/#for-hip-612","title":"For hip 6.1.2:","text":"<ol> <li> <p>Download Tensile-fix-fallback-arch-build-hip-6.1.2.patch.</p> </li> <li> <p>Place the patch file in your <code>Tensile</code> folder (e.g., <code>C:\\ROCM\\Tensile-rocm-6.1.2</code>).</p> </li> <li> <p>Open a terminal within the <code>Tensile</code> folder.</p> </li> <li> <p>Apply the patch:    <pre><code>git apply Tensile-fix-fallback-arch-build-hip-6.1.2.patch\n</code></pre></p> </li> <li> <p>If nothing appears after applying, it's patched successfully. Otherwise, you may need to manually add the patch content to <code>TensileCreateLibrary.py</code>.</p> </li> </ol>"},{"location":"AMD-ROCm/#skip-this-step-for-rocm-612","title":"( Skip this step for ROCm 6.1.2 )","text":"<p>Note: edit the line 41 in file rdeps.py for rocBLAS  ,The old repo has an outdated vckpg, which will lead to fail build.update the vcpkg ,by replace with the following line  <pre><code>git clone -b 2024.02.14 https://github.com/microsoft/vcpkg\n</code></pre> to udpate the vckpg version.</p> <ul> <li>vcpkg Version: If your vcpkg version was built after April 2023, replace <code>CMakeLists.txt</code> in <code>Tensile/tree/develop/Tensile/Source/lib/CMakeLists.txt</code> with this version and place it in the same folder (e.g., <code>rocm</code>).</li> <li>For more information, see the official ROCm guide.</li> </ul>"},{"location":"AMD-ROCm/#build-with-rdeps-and-rmake","title":"Build with rdeps and rmake:","text":"<ol> <li>Navigate to the <code>rocm/rocBLAS</code> directory in your terminal.</li> <li> <p>Run <code>python rdeps.py</code>. This script will configure your environment and download necessary packages.  <code>python rdeps.py</code> ( using <code>install.sh -d</code> in linux , if you encounter any mistakes , try to google and fix with it or try it again  ) after done . try next step</p> </li> <li> <p>After <code>rdeps.py</code> completes, run  <pre><code>python rmake.py -a \"gfx1101;gfx1103\" --lazy-library-loading--no-merge-architectures -t \"C:\\rocm\\Tensile-rocm-5.7.0\"\n</code></pre> (adjust paths and architectures as needed).</p> </li> </ol> <p>Important:</p> <ul> <li>Replace <code>\"gfx1101;gfx1103\"</code> with the correct GPU or APU architecture names for your system.Make sure sepearte with \";\"if you have more than one arches build .</li> <li>Make sure read the  Editing Tensile/Common.py and blow before to build .</li> <li>For ROCm 6.1.2, change the path to <code>C:\\rocm\\Tensile-rocm-6.1.2</code>.</li> <li>The specific commands and patch files may vary depending on your setup and ROCm version.</li> </ul> <p>After successfully building rocBLAS from source, you need to replace the default <code>rocblas.dll</code> with your compiled version for your HIP programs to utilize it. Here's how:</p> <ol> <li>Locate your Compiled Files:</li> <li><code>rocblas.dll</code>: Located in <code>C:\\ROCM\\rocBLAS-rocm-5.7.0\\build\\release\\staging\\</code> (or a similar path based on your build location).</li> <li> <p>Tensile data files: Found within <code>C:\\ROCM\\rocBLAS-rocm-5.7.0\\build\\release\\Tensile\\library\\</code> (adjust the path if needed).</p> </li> <li> <p>Replace the Default rocBLAS:</p> </li> <li> <p>Copy <code>rocblas.dll</code>  to <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin</code>. This is where the HIP SDK looks for it by default.( make sure to bakc up the origianl rocblas.dll )</p> </li> <li> <p>Place Tensile Data Files:</p> </li> <li> <p>Navigate to <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\</code></p> </li> <li> <p>Replace the <code>library</code> with new build ( back up the origianl library by rename to different name ,eg ,bklibrary).  This is where you should place all the Tensile data files from your build directory.</p> </li> <li> <p>Test Your HIP Program:</p> <ul> <li>Now, when you run your HIP program, it should use your newly compiled <code>rocblas.dll</code> and its associated Tensile data files.</li> </ul> </li> </ol> <p>Important Notes: * For ROCm 6.1.2, change the path to <code>C:\\Program Files\\AMD\\ROCm\\6.1\\bin\\</code>. * Always double-check the paths to ensure they match your installation configuration. * Make sure the ROCm version in the <code>bin</code> directory matches the version of rocBLAS you built.</p>"},{"location":"AMD-ROCm/#note-editing-tensilecommonpy","title":"Note: Editing Tensile/Common.py","text":"<p>This file contains general parameters used by the Tensile library. To ensure compatibility with your GPU, you need to update two specific settings.Update the value of <code>\" globalParameters[\"SupportedISA\"]\"</code>and <code>\"CACHED_ASM_CAPS\"</code> with your<code>gpu ISA and info</code> .and choose the simliar gpu achetecture. eg <code>RND2 for gfx1031 ,RND2 for gfx1032</code>, then copy and put below with your gpu number and others availble gpu data .For hip sdk 6.1.2 , <code>CACHED_ASM_CAPS</code> info move to tensile/AsmCaps.py, also edit architectureMap from line299 to 310 , add your arch infomation .map your arch information to correct logic file .however , some optimized logic don't exsit in the offoicial release. then we need to creat it.otherwilse ,it will creat a fallback no optimized rocblas and library.</p> <p>Here's a step-by-step guide:</p> <ol> <li>Choose Your Architecture:</li> <li>Select an existing architecture folder within <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic\\asm_full</code> (e.g., <code>navi21</code>). This will serve as a template for your new architecture.</li> <li> <p>Create a new folder with the name of your target architecture (e.g., <code>navi22</code>).</p> </li> <li> <p>Copy Files:</p> <ul> <li>Copy all the files from your chosen template folder into your new architecture folder.</li> </ul> </li> <li> <p>Modify Files:</p> </li> <li>Open the copied files in a code editor (like VS Code or Visual Studio).</li> <li>Search for instances of <code>navi21</code> and replace them with <code>navi22</code>.</li> <li>Update any <code>gfx1030</code> references to <code>gfx1031</code>  (or your target GPU's identifier).</li> <li>Find lines containing <code>ISA: [10, 3, 0]</code> and replace them with <code>ISA: [10, 3, 1]</code>. (Remember to adjust the ISA code according to your GPU)</li> <li>\"Rename all files within the new folder to reflect your architecture name (e.g., change 'navi21' to 'navi22'). You can use a file renaming tool like 'File Rename APP', a free application available in the Windows Store, for this task.\"</li> <li> <p>if build failed ,that's beacuse ROCm architectures have different capabilities. You need to ensure your <code>rocblas</code> is tailored to each architecture you're targeting:</p> <ul> <li> <p>gfx90c: Doesn't support <code>4x8II</code>.  Delete any logic or files related to <code>4x8II</code> within the <code>asm_full</code> folder under <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic</code>.</p> </li> <li> <p>gfx1010: Doesn't support <code>8II</code>. Do the same for files related to <code>8II</code> in the <code>asm_full</code> folder.</p> </li> <li>Checking Logic Files:  The \"new named logic file\" is likely a critical place where these operations are defined. Carefully review it and remove any unsupported calculations.</li> </ul> </li> <li> <p>Use Your New Architecture:</p> </li> <li>In <code>Tensile/Common.py</code>, update <code>\"CACHED_ASM_CAPS\"</code> or the relevant entries in  <code>architectureMap</code> to reference your new <code>navi22</code> folder.</li> </ol> <p>Important Notes:</p> <ul> <li>Carefully review the changes you make, as incorrect modifications can lead to errors.</li> </ul> <p>(Skip this for HIP 5.7, Necessary for HIP 6.1.2)</p> <p>Key Changes:</p> <ul> <li>Search for <code>gfx1030</code>: Begin by searching within both the Tensile and rocBLAS folders for instances of <code>gfx1030</code>. This identifier represents a gfx1030 GPU architecture.</li> <li>Replace with Your Target Architecture: Replace all occurrences of <code>gfx1030</code> with the corresponding code for your desired GPU architecture (e.g., <code>gfx1031</code>).</li> </ul> <p>Important Files to Modify:</p> <ul> <li> <p>Tensile: Within the Tensile folder, make changes to:</p> <ul> <li><code>CMakeLists.txt</code>: This file configures the build process and needs adjustments for new architectures.</li> <li><code>AMDGPU.hpp</code>: Defines the architecture-specific interface.</li> <li><code>PlaceholderLibrary.hpp</code>, <code>Predicaters.hpp</code>, <code>OclUtiles.cpp</code>: These files contain code related to specific functionalities, which might require modifications for your target GPU.</li> </ul> </li> <li> <p>rocBLAS: In the rocBLAS folder:</p> <ul> <li><code>CMakeLists.txt</code>: Similar to Tensile, update this file for your new architecture.</li> <li><code>handle.cpp</code>, <code>tensile_host.cpp</code>, <code>handle.hpp</code>: These files are likely involved in communication and interactions between rocBLAS and the GPU.</li> </ul> </li> </ul> <p>Caution:</p> <ul> <li>Modifying these core files can have unintended consequences.</li> </ul> <p>Advanced Usage:</p> <p>For maximum performance optimization, delve deeper into Tensile's logic files. Examples are provided in <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic\\asm_full</code>.</p> <p>For truly optimized libraries, you'll need to fine-tune these logic files specifically for your target hardware.The Tensile Tuning Guide provides practical guidance and techniques for start this process. Keep in mind that the tuning process requires patience, time, and a willingness to delve into Tensile's inner workings.</p> <p>More detail can be found in tuning , and tensile tuning .tex , A pdf version available in here</p> <p>Please feel welcome to edit this post and contribute optimized logic links. Remember to carefully consider the impact of any edits or additions.</p>"},{"location":"Advanced-Install/","title":"Advanced Install","text":""},{"location":"Advanced-Install/#start-scripts","title":"Start Scripts","text":"<p>Start scripts <code>webui.bat</code> or <code>webui.sh</code> are provided to create and activate VENV and immediately start launcher. No other work is performed in the shell scripts.  </p> <p>Actual launcher is started using <code>python launch.py</code> command.</p> <p>If you start launcher manually without creating &amp; activating VENV first, it will install packages system wide. This may be desired when running SD.Next in a dedicated container where there is no benefits of running additional isolation provided by VEVN.</p>"},{"location":"Advanced-Install/#venv","title":"VENV","text":"<p>SD.Next by default uses <code>venv</code> to install all dependencies Usage of <code>venv</code> is not required, but it is recommended to avoid library version conflicts with other applications</p> <p>You can also pre-create <code>venv</code> to use specific settings, for example:</p> <p>python -m venv venv --system-site-packages</p> <p>This will instruct VENV to use system site packages where available and only install missing/incorrect packages inside VENV</p>"},{"location":"Advanced-Install/#upgrades","title":"Upgrades","text":"<p>SD.Next has built-in upgrade mechanism when using <code>--upgrade</code> command line flag, but its fully supported to run manual upgrades using <code>git pull</code> as well.  </p>"},{"location":"Backend/","title":"Backend support","text":"<p>SD.Next supports two main backends: Diffusers and Original:</p> <ul> <li>Diffusers: Based on new Huggingface Diffusers implementation   Supports all models listed below   This backend is set as default for new installations  </li> <li>Original: Based on LDM reference implementation and significantly expanded on by A1111   This backend and is fully compatible with most existing functionality and extensions written for A1111 SDWebUI   Supports SD 1.x and SD 2.x models   All other model types such as SD-XL, LCM, Stable Cascade, PixArt, Playground, Segmind, Kandinsky, etc. require backend Diffusers </li> </ul>"},{"location":"Benchmark/","title":"Benchmark","text":"<p>To run standardized benchmark, you can use UI -&gt; System -&gt; Benchmark feature or via CLI using <code>cli/run-benchmark.py</code> script.</p> <p>It runs identical tests, but often CLI is faster due to lower overhead.</p>"},{"location":"Benchmark/#environment","title":"Environment","text":"<ul> <li>Hardware: nVidia RTX 4090 with i9-13900KF</li> <li>Packages: Torch 2.1.0 with CUDA 12.1 and cuDNN 8.9</li> <li>Params: model=SD15 | batch-size=4 | batch-count=4 | steps=50 | resolution=512px | sampler=Euler A</li> </ul>"},{"location":"Benchmark/#results","title":"Results","text":"<p>Basic tests using UI:</p> Diffusers Original Precision Params SDP xFormers SDP xFormers None FP32 Default 33.0 20.0 BF16 Default 73.0 45.5 FP16 Default 73.0 75.0 48.0 48.6 17.3 NHWC (channels last) 72.0 HyperTile (256) 79.0 ToMe (0.5) 77.0 Model no-move (medvram) 85.0 VAE no-slicing, no-tiling 73.8 Sequential offload (lowvram) 27.0"},{"location":"Benchmark/#notes-options","title":"Notes: Options","text":"<ul> <li>All numbers are in it/s and higher is better  </li> <li>Test matrix is not full as some options can be combined together (e.g. cuDNN + HyperTile)   while others cannot (e.g. HyperTile + ToMe)</li> <li>Results may differ on different GPU/CPU combinations   For example, pairing better CPU with older GPU may benefit from more processing done on CPU and leaving GPU to do only core ML tasks while paring high-end GPU with older CPU may result in lower results since CPU cannot feed enough tasks to GPU</li> <li>Diffusers perform significantly better than original backend on modern hardware since tasks remain on GPU for longer time   Equally, original backend may perform better on older hardware</li> <li>Running quick tasks such as single image generate at low steps may not be sufficient to fully saturate high-end GPU so results will be lower</li> <li>xFormers have a slight performance advantage over SDP   However, SDP is a built-in in Torch and \"just works\" while xFormers needs manual install and its highly version dependent  </li> <li>Some extensions can add significant overhead to pre/post processing even if they are not used</li> <li>Not worth consideration: cuDNN, NHWC, inference mode, eval</li> <li>cuDNN full bench finds best math algorithm for specific GPU, but default is nearly identical</li> <li>channels-last should better trigger utilization of tensor cores, but in practise result is nearly identical</li> <li>inference-mode should have more optimizations than default no_grad, but in practise result is nearly identical</li> <li>eval mode should allow for removal of some params in the model, but in pracise result is nearly identical</li> <li>Benefit of BF16 vs FP16 is not performance as much, its ability to run higher numerical ranges so it can perform calculations where FP16 may result in NaN</li> <li>Running in FP32 results in 60% performance drop - if you need FP32, you're leaving a lot on the table</li> <li>Cost of using lowvram is very high as it needs to swap parts of model in-memory. Even using medvram comes at noticeable cost</li> <li>Best: xFormers, FP16, HyperTile, no-model-move, no-slicing/tiling</li> </ul>"},{"location":"Benchmark/#compile","title":"Compile","text":"Compile type Performance Overhead cudnn/default 73.5 4 inductor/default 89.0 40 inductor/reduce-overhead 92.0 40 inductor/max-autotune 91.0 220 nvfuser/default 84.0 5 cudagraphs/reduce-overhead 85.0 14 stable-fast/sdp 96.0 76 stable-fast/xformers 96.0 101 stable-fast/full-graph 94.0 96"},{"location":"Benchmark/#notes-compile","title":"Notes: Compile","text":"<ul> <li>Performance numbers is in it/s and higher is better  </li> <li>Overhead is time in seconds needed to optimize a model with specific params and lower is better   Model needs compile on initial generate, but it may also need a recompile if params such as resolution of batch size change</li> <li>Model compile may not be compatible with any method that modifies underlying model,   including loading Lora weights on top of a model  </li> <li>stable-fast compile backend requires that package is manually installed on the system</li> </ul>"},{"location":"Benchmark/#intel-arc","title":"Intel ARC","text":""},{"location":"Benchmark/#environment_1","title":"Environment","text":"<ul> <li>Hardware: Intel ARC 770 LE 16GB with R7 5800X3D &amp; MSI B350M Mortar (PCI-E 3.0) &amp; 48 GB 3200 MHz CL18 RAM  </li> <li>OS: Arch Linux with this Docker environment: https://github.com/Disty0/docker-sdnext-ipex</li> <li>Packages: Torch 2.1.0a0+cxx11.abi with IPEX 2.1.10+xpu and MKL / DPCPP 2024.0.0</li> <li>Params: model=SD15 | batch-size=1 | batch-count=1 | steps=40 | resolution=512px | sampler=Euler a | CFG 6</li> </ul>"},{"location":"Benchmark/#results_1","title":"Results","text":"Diffusers Original Precision Params it/s it/s BF16 Default 8.54 7.75 FP16 Default 6.92 7.23 FP32 Default 3.73 3.74 BF16 HyperTile (256) 10.03 9.32 BF16 ToMe (0.5) 9.24 8.61 BF16 No IPEX Optimize 8.23 7.82 BF16 Model no-move (medvram) 9.04 BF16 VAE no-slicing, no-tiling 8.67 BF16 Sequential offload (lowvram) 1.60 0.67"},{"location":"Benchmark/#api-benchmarks","title":"API Benchmarks","text":"<pre><code>2024-02-07 22:52:56,406 INFO: {'run-benchmark'}\n2024-02-07 22:52:56,407 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 22:52:56,432 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-07 22:52:56,434 INFO: {'platform': {'arch': 'x86_64', 'cpu': '', 'system': 'Linux', 'release': '6.7.3-arch1-2', 'python': '3.11.6', 'torch': '2.1.0a0+cxx11.abi', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 22:52:56,437 INFO: {'model': 'SD1.5/SoteMixV3 [dcc16969a0]'}\n2024-02-07 22:52:56,441 INFO: {'system': {'cpu': {'free': 48901079040.00001, 'used': 1533939712, 'total': 50435018752.00001}, 'gpu': {'system': {'free': 17079205888, 'used': 0, 'total': 17079205888}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 22:52:56,441 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 22:53:10,362 INFO: {'warmup': 13.92}\n2024-02-07 22:53:18,182 INFO: {'batch': 1, 'its': 12.81, 'img': 3.9, 'wall': 3.9, 'peak': 2.61, 'oom': False}\n2024-02-07 22:53:31,723 INFO: {'batch': 2, 'its': 15.49, 'img': 3.23, 'wall': 6.45, 'peak': 3.07, 'oom': False}\n2024-02-07 22:53:55,512 INFO: {'batch': 4, 'its': 17.18, 'img': 2.91, 'wall': 11.64, 'peak': 3.07, 'oom': False}\n2024-02-07 22:54:39,504 INFO: {'batch': 8, 'its': 18.4, 'img': 2.72, 'wall': 21.74, 'peak': 3.07, 'oom': False}\n2024-02-07 22:55:43,500 INFO: {'batch': 12, 'its': 18.93, 'img': 2.64, 'wall': 31.7, 'peak': 3.07, 'oom': False}\n2024-02-07 22:56:58,086 INFO: {'batch': 16, 'its': 21.61, 'img': 2.31, 'wall': 37.01, 'peak': 3.07, 'oom': False}\n2024-02-07 22:58:48,560 INFO: {'batch': 24, 'its': 21.92, 'img': 2.28, 'wall': 54.74, 'peak': 3.64, 'oom': False}\n2024-02-07 23:01:09,184 INFO: {'batch': 32, 'its': 22.82, 'img': 2.19, 'wall': 70.12, 'peak': 4.06, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#openvino","title":"OpenVINO","text":""},{"location":"Benchmark/#environment_2","title":"Environment","text":"<ul> <li>Hardware: Intel ARC 770 LE 16GB with R7 5800X3D &amp; MSI B350M Mortar (PCI-E 3.0) &amp; 48 GB 3200 MHz CL18 RAM  </li> <li>OS: Arch Linux  </li> <li>Packages: Torch 2.1.2+cpu and OpenVINO 2023.2.0</li> <li>Params: model=SD15 | batch-size=1 | batch-count=1 | steps=20 | resolution=512px | sampler=Euler a | CFG 6</li> </ul>"},{"location":"Benchmark/#gpu-results","title":"GPU Results","text":"Diffusers Precision Params it/s Default Default 9.21"},{"location":"Benchmark/#cpu-results","title":"CPU Results","text":"Diffusers Precision Params s/it Default Default 3.00 Default LCM &amp; CFG 0 1.60 INT8 Default 3.30 INT4_SYM Default 4.00 INT4_ASYM Default 4.30 NF4 Default 5.25 FP32 Diffusers &amp; No OpenVINO 4.20"},{"location":"Benchmark/#directml","title":"DirectML","text":"<ul> <li>Hardware: Intel Core i9-14900K, SAPPHIRE AMD Radeon RX 7900 XTX NITRO+ Vapor-X 24GB, SAMSUNG DDR5 32GBx4</li> <li>Operating System: Windows 11 Build 22631</li> <li>Packages: PyTorch 2.0.0 (built with CPU), torch-directml 0.2.0.dev230426</li> <li>Performed using <code>cli/run-benchmark.py</code> script</li> </ul> <p>Peak: 9.36 with batch size 8.</p> <p>Possible max batch size: 12 (Slow with 12, OOM with 16)</p> <pre><code>2024-02-08 20:09:31,923 INFO: {'run-benchmark'}\n2024-02-08 20:09:31,924 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 20:09:32,005 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-08 20:09:32,007 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.11', 'torch': '2.0.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 20:09:32,013 INFO: {'system': {'cpu': {'free': 136382431232.00002, 'used': 708612096, 'total': 137091043328.00002}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 20:09:32,013 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 20:09:51,463 INFO: {'warmup': 19.45}\n2024-02-08 20:10:03,837 INFO: {'batch': 1, 'its': 8.06, 'img': 6.2, 'wall': 6.2, 'peak': 0.0, 'oom': False}\n2024-02-08 20:10:27,845 INFO: {'batch': 2, 'its': 9.02, 'img': 5.54, 'wall': 11.09, 'peak': 0.0, 'oom': False}\n2024-02-08 20:11:12,886 INFO: {'batch': 4, 'its': 9.04, 'img': 5.53, 'wall': 22.12, 'peak': 0.0, 'oom': False}\n2024-02-08 20:12:38,582 INFO: {'batch': 8, 'its': 9.36, 'img': 5.34, 'wall': 42.76, 'peak': 0.0, 'oom': False}\n2024-02-08 20:15:22,610 INFO: {'batch': 12, 'its': 7.31, 'img': 6.84, 'wall': 82.04, 'peak': 0.0, 'oom': False}\n2024-02-08 20:15:23,465 ERROR: {'requested': 16, 'received': 0}\n2024-02-08 20:15:24,161 ERROR: {'requested': 16, 'received': 0}\n2024-02-08 20:15:24,164 INFO: {'batch': 16, 'its': 1150.12, 'img': 0.04, 'wall': 0.7, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#onnx-runtime","title":"ONNX Runtime","text":"<ul> <li>Hardware: Intel Core i9-14900K, SAPPHIRE AMD Radeon RX 7900 XTX NITRO+ Vapor-X 24GB, SAMSUNG DDR5 32GBx4</li> <li>Operating System: Windows 11 Build 22631</li> <li>Packages: PyTorch 2.2.0 (built with CPU), onnxruntime 1.17.0, onnxruntime-directml 1.17.0</li> <li>Performed using <code>cli/run-benchmark.py</code> script</li> </ul> <p>Peak: 17.58</p> <p>Possible max batch size: 8 (Not OOM, but very slow with 12 or higher)</p> <pre><code>2024-02-08 19:20:45,235 INFO: {'run-benchmark'}\n2024-02-08 19:20:45,236 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 19:20:45,317 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-08 19:20:45,318 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.12', 'torch': '2.2.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 19:20:45,324 INFO: {'system': {'cpu': {'free': 136392728576.00002, 'used': 698314752, 'total': 137091043328.00002}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 19:20:45,324 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 19:21:03,553 INFO: {'warmup': 18.23}\n2024-02-08 19:21:12,036 INFO: {'batch': 1, 'its': 11.81, 'img': 4.23, 'wall': 4.23, 'peak': 0.0, 'oom': False}\n2024-02-08 19:21:26,618 INFO: {'batch': 2, 'its': 13.79, 'img': 3.62, 'wall': 7.25, 'peak': 0.0, 'oom': False}\n2024-02-08 19:21:54,400 INFO: {'batch': 4, 'its': 14.46, 'img': 3.46, 'wall': 13.83, 'peak': 0.0, 'oom': False}\n2024-02-08 19:22:40,407 INFO: {'batch': 8, 'its': 17.58, 'img': 2.84, 'wall': 22.75, 'peak': 0.0, 'oom': False}\n2024-02-08 19:30:30,903 INFO: {'batch': 12, 'its': 2.56, 'img': 19.57, 'wall': 234.8, 'peak': 0.0, 'oom': False}\n2024-02-08 19:40:08,391 INFO: {'batch': 16, 'its': 2.77, 'img': 18.05, 'wall': 288.86, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#with-optimized-model-using-olive","title":"With optimized model using Olive","text":"<ul> <li>Package: olive-ai 0.4.0</li> </ul> <p>Peak: 54.08</p> <p>Possible max batch size: Unknown (at least 48)</p> <pre><code>2024-02-08 18:51:28,096 INFO: {'run-benchmark'}\n2024-02-08 18:51:28,097 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 18:51:28,167 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-08 18:51:28,168 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.12', 'torch': '2.2.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 18:51:28,174 INFO: {'system': {'cpu': {'free': 136385822719.99998, 'used': 705220608, 'total': 137091043327.99998}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 18:51:28,174 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 18:51:42,445 INFO: {'warmup': 14.27}\n2024-02-08 18:51:46,603 INFO: {'batch': 1, 'its': 23.63, 'img': 2.12, 'wall': 2.12, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:00,527 INFO: {'batch': 2, 'its': 35.06, 'img': 1.43, 'wall': 2.85, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:18,711 INFO: {'batch': 4, 'its': 40.34, 'img': 1.24, 'wall': 4.96, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:42,958 INFO: {'batch': 8, 'its': 50.51, 'img': 0.99, 'wall': 7.92, 'peak': 0.0, 'oom': False}\n2024-02-08 18:53:13,677 INFO: {'batch': 12, 'its': 53.81, 'img': 0.93, 'wall': 11.15, 'peak': 0.0, 'oom': False}\n2024-02-08 18:53:51,700 INFO: {'batch': 16, 'its': 54.08, 'img': 0.92, 'wall': 14.79, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#api-benchmarks_1","title":"API Benchmarks","text":"<p>Using latest version of SD.Next with Torch 2.2.0, CUDA 12.1 Note: Usage of SD.Next via API is faster than via UI due to lower overhead.</p> <p>Environment: Intel i9-13900KF platform with nVidia RTX 4090 GPU </p> <p>As you can see, we're reaching peak performance of ~110 it/s using simple settings:  </p> <pre><code>vlado@wsl:~/dev/sdnext-dev $ python cli/run-benchmark.py --maxbatch 32\n2024-02-07 11:19:53,026 INFO: {'run-benchmark'}\n2024-02-07 11:19:53,027 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 11:19:53,046 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': 'd967bd03', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-07 11:19:53,048 INFO: {'platform': {'arch': 'x86_64', 'cpu': 'x86_64', 'system': 'Linux', 'release': '5.15.146.1-microsoft-standard-WSL2', 'python': '3.11.1', 'torch': '2.2.0+cu121', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 11:19:53,051 INFO: {'model': 'sd15/lyriel-v16 [ec6f68ea63]'}\n2024-02-07 11:19:53,054 INFO: {'system': {'cpu': {'free': 49020043264.0, 'used': 1495736320, 'total': 50515779584.0}, 'gpu': {'system': {'free': 24110956544, 'used': 1645740032, 'total': 25756696576}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 11:19:53,054 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 11:19:59,394 INFO: {'warmup': 6.34}\n2024-02-07 11:20:02,354 INFO: {'batch': 1, 'its': 33.63, 'img': 1.49, 'wall': 1.49, 'peak': 7.05, 'oom': False}\n2024-02-07 11:20:06,213 INFO: {'batch': 2, 'its': 64.3, 'img': 0.78, 'wall': 1.56, 'peak': 7.1, 'oom': False}\n2024-02-07 11:20:11,293 INFO: {'batch': 4, 'its': 90.87, 'img': 0.55, 'wall': 2.2, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:19,416 INFO: {'batch': 8, 'its': 104.6, 'img': 0.48, 'wall': 3.82, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:30,850 INFO: {'batch': 12, 'its': 111.96, 'img': 0.45, 'wall': 5.36, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:46,236 INFO: {'batch': 16, 'its': 110.37, 'img': 0.45, 'wall': 7.25, 'peak': 7.18, 'oom': False}\n2024-02-07 11:21:09,338 INFO: {'batch': 24, 'its': 109.75, 'img': 0.46, 'wall': 10.93, 'peak': 7.18, 'oom': False}\n2024-02-07 11:21:39,623 INFO: {'batch': 32, 'its': 111.38, 'img': 0.45, 'wall': 14.37, 'peak': 7.18, 'oom': False}\n</code></pre> <p>With a full optimizations and custom compiled Stable-Fast: We're reaching peak performance of ~150 it/s (and ~165 it/s using TAESD instead of full VAE):  </p> <pre><code>vlado@wsl:~/dev/sdnext-dev $ python cli/run-benchmark.py --maxbatch 32\n2024-02-07 11:29:23,431 INFO: {'run-benchmark'}\n2024-02-07 11:29:23,432 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 11:29:23,451 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': 'd967bd03', 'url': 'https://github.com/vladmandic/automatic/tree/dev'}}\n2024-02-07 11:29:23,453 INFO: {'platform': {'arch': 'x86_64', 'cpu': 'x86_64', 'system': 'Linux', 'release': '5.15.146.1-microsoft-standard-WSL2', 'python': '3.11.1', 'torch': '2.2.0+cu121', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 11:29:23,456 INFO: {'model': 'sd15/lyriel-v16 [ec6f68ea63]'}\n2024-02-07 11:29:23,459 INFO: {'system': {'cpu': {'free': 49373564927.99999, 'used': 1142214656, 'total': 50515779583.99999}, 'gpu': {'system': {'free': 24110956544, 'used': 1645740032, 'total': 25756696576}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 11:29:23,459 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 11:29:38,504 INFO: {'warmup': 15.04}\n2024-02-07 11:29:38,965 INFO: {'batch': 1, 'its': 78.16, 'img': 0.67, 'wall': 0.23, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:42,630 INFO: {'batch': 2, 'its': 98.91, 'img': 0.51, 'wall': 1.01, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:47,192 INFO: {'batch': 4, 'its': 117.92, 'img': 0.42, 'wall': 1.7, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:54,028 INFO: {'batch': 8, 'its': 142.42, 'img': 0.35, 'wall': 2.81, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:03,161 INFO: {'batch': 12, 'its': 153.29, 'img': 0.33, 'wall': 3.91, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:14,921 INFO: {'batch': 16, 'its': 153.41, 'img': 0.33, 'wall': 5.21, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:33,534 INFO: {'batch': 24, 'its': 144.65, 'img': 0.35, 'wall': 8.3, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:56,914 INFO: {'batch': 32, 'its': 150.59, 'img': 0.33, 'wall': 10.63, 'peak': 7.11, 'oom': False}\n</code></pre> <p>Additional performance may be reached by experimenting with different settings, but combination of such may lead to unstable results For example: channels-last, hyper-tile, tomesd, fused-projections </p>"},{"location":"CLI-Arguments/","title":"Command Line Arguments","text":"<p>Tip</p> <p>All command line arguments can also be set as environment flags, for example <code>--debug</code> is equivalent to <code>SD_DEBUG=True</code>  All options listed here are available as arguments to use from the command line or as environment variables, there's no need to do both.</p>"},{"location":"CLI-Arguments/#list","title":"List","text":"<p>webui --help</p> <pre><code>Configuration:\n  --backend {original,diffusers}                     force model pipeline type\n  --config CONFIG                                    Use specific server configuration file, default: config.json\n  --ui-config UI_CONFIG                              Use specific UI configuration file, default: ui-config.json\n  --medvram                                          Split model stages and keep only active part in VRAM, default: False\n  --lowvram                                          Split model components and keep only active part in VRAM, default: False\n  --freeze                                           Disable editing settings\n\nPaths:\n  --ckpt CKPT                                        Path to model checkpoint to load immediately, default: None\n  --data-dir DATA_DIR                                Base path where all user data is stored, default:\n  --models-dir MODELS_DIR                            Base path where all models are stored, default: models\n\nDiagnostics:\n  --no-hashing                                       Disable hashing of checkpoints, default: False\n  --no-metadata                                      Disable reading of metadata from models, default: False\n  --disable-queue                                    Disable queues, default: False\n  --device-id DEVICE_ID                              Select the default CUDA device to use, default: None\n\nHTTP:\n  --server-name SERVER_NAME                          Sets hostname of server, default: None\n  --tls-keyfile TLS_KEYFILE                          Enable TLS and specify key file, default: None\n  --tls-certfile TLS_CERTFILE                        Enable TLS and specify cert file, default: None\n  --tls-selfsign                                     Enable TLS with self-signed certificates, default: False\n  --cors-origins CORS_ORIGINS                        Allowed CORS origins as comma-separated list, default: None\n  --cors-regex CORS_REGEX                            Allowed CORS origins as regular expression, default: None\n  --subpath SUBPATH                                  Customize the URL subpath for usage with reverse proxy\n  --autolaunch                                       Open the UI URL in the system's default browser upon launch\n  --auth AUTH                                        Set access authentication like \"user:pwd,user:pwd\"\"\n  --auth-file AUTH_FILE                              Set access authentication using file, default: None\n  --api-only                                         Run in API only mode without starting UI\n  --allowed-paths ALLOWED_PATHS [ALLOWED_PATHS ...]  add additional paths to paths allowed for web access\n  --share                                            Enable UI accessible through Gradio site, default: False\n  --insecure                                         Enable extensions tab regardless of other options, default: False\n  --listen                                           Launch web server using public IP address, default: False\n  --port PORT                                        Launch web server with given server port, default: 7860\n\nSetup:\n  --reset                                            Reset main repository to latest version, default: False\n  --upgrade, --update                                Upgrade main repository to latest version, default: False\n  --requirements                                     Force re-check of requirements, default: False\n  --reinstall                                        Force reinstallation of all requirements, default: False\n  --uv                                               Use uv instead of pip to install the packages\n\nStartup:\n  --quick                                            Bypass version checks, default: False\n  --skip-requirements                                Skips checking and installing requirements, default: False\n  --skip-extensions                                  Skips running individual extension installers, default: False\n  --skip-git                                         Skips running all GIT operations, default: False\n  --skip-torch                                       Skips running Torch checks, default: False\n  --skip-all                                         Skips running all checks, default: False\n  --skip-env                                         Skips setting of env variables during startup, default: False\n\nCompute Engine:\n  --use-directml                                     Use DirectML if no compatible GPU is detected, default: False\n  --use-openvino                                     Use Intel OpenVINO backend, default: False\n  --use-ipex                                         Force use Intel OneAPI XPU backend, default: False\n  --use-cuda                                         Force use nVidia CUDA backend, default: False\n  --use-rocm                                         Force use AMD ROCm backend, default: False\n  --use-zluda                                        Force use ZLUDA, AMD GPUs only, default: False\n  --use-xformers                                     Force use xFormers cross-optimization, default: False\n\nDiagnostics:\n  --safe                                             Run in safe mode with no user extensions\n  --experimental                                     Allow unsupported versions of libraries, default: False\n  --test                                             Run test only and exit\n  --version                                          Print version information\n  --ignore                                           Ignore any errors and attempt to continue\n\nLogging:\n  --log LOG                                          Set log file, default: None\n  --debug                                            Run installer with debug logging, default: False\n  --profile                                          Run profiler, default: False\n  --docs                                             Mount API docs, default: False\n  --api-log                                          Enable logging of all API requests, default: False\n</code></pre>"},{"location":"CLI-Arguments/#details","title":"Details","text":""},{"location":"CLI-Arguments/#general-options","title":"General Options","text":"<ul> <li><code>--config</code>: Specify the server configuration file. This option allows you to use a specific server configuration file. The default is set to <code>&lt;path to data&gt;/config.json</code>. You can customize this by providing a different file path or by setting the environment variable <code>SD_CONFIG</code>.</li> <li><code>--ui-config</code>: Specify the UI configuration file. This option allows you to use a specific UI configuration file. The default is set to <code>&lt;path to data&gt;/ui-config.json</code>. You can customize this by providing a different file path or by setting the environment variable <code>SD_UICONFIG</code>.</li> <li><code>--autolaunch</code>: Open the UI URL in the system's default browser upon launch. Enabling this flag (<code>True</code>) automatically opens the UI URL in the system's default browser upon launch. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_AUTOLAUNCH</code>.</li> <li><code>--backend</code>: Force model pipeline type. Enabling this option allows you to force a specific model pipeline type. The choices are ['original', 'diffusers']. The default is <code>Original</code>, and you can set a custom backend by providing the argument value or by setting the environment variable <code>SD_BACKEND</code>.</li> <li><code>--upgrade</code>: Upgrade the main repository to the latest version. Use this option when you want to ensure that you are using the most recent version of the application. Enabling this flag (<code>True</code>) upgrades the main repository to the latest version.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_UPGRADE</code>. This is relatively safe to use with master branch. Dev branch is another story, be wary.</li> <li><code>--debug</code>: Run SDNext with debug logging to the console. Enabling this flag (<code>True</code>) runs SDNext with debug logging. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DEBUG</code>. The sdnext.log files always show debug output for troubleshooting purposes.</li> <li><code>--test</code>: Run the tests only and exit. This is useful for checking the integrity of the application without starting the actual service. Also useful for doing installation or reinstallation activities. Enabling this flag (<code>True</code>) runs the application in test mode, executing tests and exiting without launching the full application.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_TEST</code>. Be careful setting this option as an environment variable as it will only test on every startup, you will never get inside SDNext. </li> </ul>"},{"location":"CLI-Arguments/#skip-options","title":"Skip Options","text":"<ul> <li><code>--quick</code>: Run with startup sequence only, does not check requirements, extensions, git, or torch tests. Enabling this flag (<code>True</code>) runs with the startup sequence only. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_QUICK</code>.</li> <li><code>--skip-requirements</code>: Skips checking and installing requirements. Enabling this flag (<code>True</code>) during setup skips checking and installing requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPREQUIREMENTS</code>.</li> <li><code>--skip-extensions</code>: Skips running individual extension installers. Enabling this flag (<code>True</code>) during setup skips running individual extension installers. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPEXTENSION</code>.</li> <li><code>--skip-git</code>: Skips running all GIT operations. Enabling this flag (<code>True</code>) during setup skips running all GIT operations. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPGIT</code>.</li> <li><code>--skip-torch</code>: Skips running Torch checks. Enabling this flag (<code>True</code>) during setup skips running Torch checks. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPTORCH</code>.</li> <li><code>--skip-env</code>: Skips setting env variables. Enabling this flag (<code>True</code>) during setup skips setting of any and all env variables used for tuning. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPENV</code>.</li> </ul>"},{"location":"CLI-Arguments/#memory-management","title":"Memory Management","text":"<ul> <li><code>--medvram</code>: Split model stages and keep only the active part in VRAM. Enabling this flag (<code>True</code>) allows the application to split model stages, conserving GPU memory by keeping only the active part in VRAM. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_MEDVRAM</code>.</li> <li><code>--lowvram</code>: Split model components and keep only the active part in VRAM. Enabling this flag (<code>True</code>) allows the application to split model components. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_LOWVRAM</code>.</li> </ul>"},{"location":"CLI-Arguments/#hardware-backends-use-during-installation-or-reinstall-operations","title":"Hardware Backends (Use During Installation or <code>--reinstall</code> operations)","text":"<ul> <li><code>--use-directml</code>: Use DirectML if no compatible GPU is detected. Enabling this flag (<code>True</code>) allows the use of DirectML if no compatible GPU is detected. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEDIRECTML</code>.</li> <li><code>--use-openvino</code>: Use Intel OpenVINO backend. Enabling this flag (<code>True</code>) allows the use of the Intel OpenVINO backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEOPENVINO</code>.</li> <li><code>--use-ipex</code>: Force use Intel OneAPI XPU backend. Enabling this flag (<code>True</code>) forces the use of the Intel OneAPI XPU backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEIPEX</code>.</li> <li><code>--use-cuda</code>: Force use NVIDIA CUDA backend. Enabling this flag (<code>True</code>) forces the use of the NVIDIA CUDA backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USECUDA</code>.</li> <li><code>--use-rocm</code>: Force use AMD ROCm backend. Enabling this flag (<code>True</code>) forces the use of the AMD ROCm backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEROCM</code>.</li> <li><code>--use-zluda</code>: Force use ZLUDA backend. Enabling this flag (<code>True</code>) forces the use of the AMD ROCm backend wrapped with ZLUDA. The default is <code>False</code> and you can set it to <code>True</code> using the environment variable <code>SD_USEZLUDA</code>. You should not have <code>torch</code> installed before enabling this flag.</li> </ul>"},{"location":"CLI-Arguments/#ipex-environment-variables","title":"IPEX Environment Variables","text":"<ul> <li><code>DISABLE_IPEXRUN</code>: Disable ipexrun for Intel ARC. If you encounter memory pointer or SIGSEGV-related errors on Linux, you can try setting this environment variable to <code>True</code> to disable ipexrun.</li> <li><code>IPEX_SDPA_SLICE_TRIGGER_RATE</code>: Specify when dynamic attention slicing for Scaled Dot Product Attention should get triggered for Intel ARC. This environment variable allows you to set the trigger rate in gigabytes (GB). The default is <code>6</code> GB.</li> <li><code>IPEX_ATTENTION_SLICE_RATE</code>: Specify the dynamic attention slicing rate for Intel ARC. This environment variable allows you to set the slicing rate in gigabytes (GB). The default is <code>4</code> GB.</li> <li><code>IPEX_FORCE_ATTENTION_SLICE</code>: Force use dynamic attention slicing even if the GPU supports 64 bit. Useful with Intel Data Center GPU MAX series.</li> </ul>"},{"location":"CLI-Arguments/#pathing","title":"Pathing","text":"<ul> <li><code>--log</code>: Set log file name and path. This argument allows you to set the log filename and location. The default is <code>sdnext.log</code> in the base directory, and you can set it with the environment variable <code>SD_LOG</code>.</li> <li><code>--ckpt</code>: Path to the model checkpoint to load immediately. This option allows you to specify the path to a model checkpoint for immediate loading. The default is <code>None</code>, and you can set a custom path by providing the argument value or by setting the environment variable <code>SD_MODEL</code>.</li> <li><code>--vae</code>: Path to the VAE checkpoint to load immediately. This option allows you to specify the path to a VAE checkpoint for immediate loading. The default is <code>None</code>, and you can set a custom path by providing the argument value or by setting the environment variable <code>SD_VAE</code>.</li> <li><code>--data-dir</code>: Base path where all user data is stored. You can set the base path where all user data is stored using this option. The default is an empty string (<code>''</code>). Customize this by providing a different path or by setting the environment variable <code>SD_DATADIR</code>.</li> <li><code>--models-dir</code>: Base path where all models are stored. This option sets the base path where all models are stored. The default is <code>'models'</code>. Customize this by providing a different path or by setting the environment variable <code>SD_MODELSDIR</code>.</li> </ul>"},{"location":"CLI-Arguments/#troubleshooting-options","title":"Troubleshooting Options","text":"<p>Also see Troubleshooting Wiki page</p> <ul> <li><code>--safe</code>: Run in safe mode with no user extensions. Safe mode can be useful when troubleshooting or when you want to restrict the execution of potentially unsafe user-provided code. Enabling this flag (<code>True</code>) runs the application in safe mode, disabling user extensions.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SAFE</code>. Try this first.</li> <li><code>--requirements</code>: Force re-check of python (pip) package requirements and installs any that are not up-to-date. Enabling this flag (<code>True</code>) forces a re-check of requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_REQUIREMENTS</code>. Try this second.</li> <li><code>--reinstall</code>: Force reinstallation of all requirements. Use this option when you want to ensure that all dependencies are freshly installed, potentially resolving any issues related to outdated or corrupted installations. Also useful to change hardware backends, such as to OpenVINO or DML. Enabling this flag (<code>True</code>) forces the reinstallation of all requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_REINSTALL</code>. Try this third. Be careful setting this option as an environment variable as it will reinstall on every startup.</li> <li><code>--reset</code>: Reset main repository to latest version. Enabling this flag (<code>True</code>) resets the main repository to the latest version. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_RESET</code>. This is more serious with consequences, but you can try this fourth. Be careful setting this option as an environment variable as it will reset on every startup.</li> <li><code>--experimental</code>: Allow unsupported versions of libraries. This is useful for testing or trying out features that may not be officially supported yet. Enabling this flag (<code>True</code>) allows the application to use unsupported versions of libraries.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_EXPERIMENTAL</code>. Be careful setting this option as an environment variable as it can easily cause issues.</li> <li><code>--ignore</code>: Try to ignore any errors and attempt to continue. This can be useful in scenarios where certain errors are known and can be safely bypassed. Enabling this flag (<code>True</code>) instructs the application to ignore any errors encountered during the setup process and attempt to continue. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_IGNORE</code>.</li> </ul>"},{"location":"CLI-Arguments/#advanced-options","title":"Advanced Options","text":"<ul> <li><code>--device-id</code>: Select the default CUDA device to use. This option allows you to select the default CUDA device for GPU operations. The default is <code>None</code>, and you can set a custom device ID by providing the argument value or by setting the environment variable <code>SD_DEVICEID</code>.</li> <li><code>--use-xformers</code>: Forces the installation and use of xFormers cross-optimization. Enabling this flag (<code>True</code>) forces the installation and use of xFormers. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEXFORMERS</code>.</li> <li><code>--no-hashing</code>: Disable hashing of checkpoints. Enabling this flag (<code>True</code>) disables the hashing of checkpoints. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_NOHASHING</code>.</li> <li><code>--no-metadata</code>: Disable reading of metadata from models. Enabling this flag (<code>True</code>) disables the reading of metadata from models. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_NOMETADATA</code>.</li> <li><code>--profile</code>: Run profiler. Enabling this flag (<code>True</code>) runs the profiler. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_PROFILE</code>.</li> <li><code>--disable-queue</code>: Disable queues. Enabling this flag (<code>True</code>) disables the use of queues. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DISABLEQUEUE</code>.</li> <li><code>--allow-code</code>: Allow custom script execution. This is useful for scenarios where users may want to run their own code. Enabling this flag (<code>True</code>) allows the execution of custom scripts.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_ALLOWCODE</code>.</li> <li><code>--use-cpu</code>: This option forces the use of CPU for specified modules. You can provide a list of modules as arguments. The default is an empty list (<code>[]</code>). Customize this by providing module names or by setting the environment variable <code>SD_USECPU</code>. Not advised, legacy code with poor performance.</li> <li><code>--freeze</code>: Disable editing settings. This is useful to lock down configurations. Enabling this flag (<code>True</code>) prevents editing of settings.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_FREEZE</code>.</li> </ul>"},{"location":"CLI-Arguments/#api","title":"API","text":"<ul> <li><code>--docs</code>: Mount API docs at /docs i.e., <code>https://127.0.0.1/docs</code>. Enabling this flag (<code>True</code>) mounts API documentation at the <code>/docs</code> endpoint. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DOCS</code>.</li> <li><code>--api-only</code>: Run in API only mode without starting UI. Enabling this flag (<code>True</code>) runs the application in API-only mode without starting the UI. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_APIONLY</code>.</li> <li><code>--api-log</code>: Enable logging of all API requests. Enabling this flag (<code>True</code>) logs all API requests. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_APILOG</code>.</li> </ul>"},{"location":"CLI-Arguments/#networking","title":"Networking","text":"<ul> <li><code>--share</code>: Enable UI to be accessible through the Gradio site. This is useful for sharing your SDNext with others. Enabling this flag (<code>True</code>) allows the UI to be accessible through the Gradio site.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SHARE</code>. The shared URL will be in your console log.</li> <li><code>--insecure</code>: Enable extensions tab regardless of other options. This should only be used when you want local network or web accessible control of your extensions, potentially dangerous on the web. Enabling this flag (<code>True</code>) allows the extensions tab to be enabled regardless of other specified options. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_INSECURE</code>. Use with care when using <code>--share</code>.</li> <li><code>--listen</code>: Launch web server to be accessible from local network. Enabling this flag (<code>True</code>) allows the web server to launch for use on your own network. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_LISTEN</code>.</li> <li><code>--auth</code>: Set access authentication like \"user:pwd,user:pwd\". This option allows you to set access authentication with a specified username and password combination. The default is <code>None</code>, and you can set a custom authentication string by providing the argument value or by setting the environment variable <code>SD_AUTH</code>.</li> <li><code>--auth-file</code>: Set access authentication using file. This option allows you to set access authentication using a file. The default is <code>None</code>, and you can set a custom file path by providing the argument value or by setting the environment variable <code>SD_AUTHFILE</code>.</li> <li><code>--server-name</code>: Sets hostname of server. This option sets the hostname of the server. The default is <code>None</code>, and you can set a custom server name by providing the argument value or by setting the environment variable <code>SD_SERVERNAME</code>.</li> <li><code>--subpath</code>: Customize the URL subpath for usage with reverse proxy. This option allows you to customize the URL subpath for usage with a reverse proxy. The default is <code>None</code>, and you can set a custom subpath by providing the argument value or by setting the environment variable <code>SD_SUBPATH</code>.</li> <li><code>--cors-origins</code>: Allowed CORS origins as comma-separated list. This option sets the allowed CORS origins as a comma-separated list. The default is <code>None</code>, and you can set custom origins by providing the argument value or by setting the environment variable <code>SD_CORSORIGINS</code>.</li> <li><code>--cors-regex</code>: Allowed CORS origins as regular expression. This option sets the allowed CORS origins as a regular expression. The default is <code>None</code>, and you can set a custom regular expression by providing the argument value or by setting the environment variable <code>SD_CORSREGEX</code>.</li> <li><code>--tls-keyfile</code>: Enable TLS and specify key file. This option enables TLS (Transport Layer Security) and specifies the key file. The default is <code>None</code>, and you can set a custom key file path by providing the argument value or by setting the environment variable <code>SD_TLSKEYFILE</code>.</li> <li><code>--tls-certfile</code>: Enable TLS and specify cert file. This option enables TLS (Transport Layer Security) and specifies the certificate file. The default is <code>None</code>, and you can set a custom certificate file path by providing the argument value or by setting the environment variable <code>SD_TLSCERTFILE</code>.</li> <li><code>--tls-selfsign</code>: Enable TLS with self-signed certificates. Enabling this flag (<code>True</code>) enables TLS with self-signed certificates. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_TLSSELFSIGN</code>.</li> </ul>"},{"location":"CLI-Arguments/#cors-in-depth","title":"CORS in depth","text":"<p>These CORS options are important for controlling which domains are permitted to access the resources of your server. It helps in enhancing the security of your web application by preventing unauthorized cross-origin requests. When configuring these options, ensure that you only allow origins that you trust to interact with your server, as allowing any origin (<code>*</code>) can introduce security vulnerabilities. Always specify the origins explicitly or using a secure regular expression pattern.</p> <ul> <li><code>--cors-origins</code>: Allowed CORS origins as a comma-separated list. CORS is a security feature implemented by web browsers that restricts web pages from making requests to a different domain than the one that served the web page. This option allows you to specify a list of origins (domains) that are allowed to access resources on your server. For example, if your application is hosted on <code>http://example.com</code>, and you want to allow access from <code>http://client.example.com</code>, you would set <code>--cors-origins http://client.example.com</code>. For local network use, often with other tools, you can just set <code>--cors-origins *</code>.</li> <li><code>--cors-regex</code>: Allowed CORS origins as a regular expression. This option provides more flexibility by allowing you to specify CORS origins using a regular expression. The regular expression should match the origin(s) you want to allow. This is useful when you have a dynamic set of origins that follow a certain pattern. For instance, if you want to allow any subdomain under <code>example.com</code>, you could set <code>--cors-regex \"^https?://[a-z0-9-]+\\.example\\.com$\"</code>.</li> </ul>"},{"location":"CLI-Arguments/#tls-in-depth","title":"TLS in depth","text":"<p>When setting up TLS, it's important to use valid certificates from a trusted CA in a production environment to ensure secure and encrypted communication. In a development or testing environment, self-signed certificates can be used, but users should be cautious when accessing the application as browsers may show security warnings due to the self-signed nature. Always ensure that your TLS setup meets security best practices.</p> <ul> <li><code>--tls-keyfile</code>: Enable TLS and specify the key file. TLS is a cryptographic protocol that ensures the secure transmission of data over a network. Enabling TLS in your application secures the communication between the client and the server. The <code>--tls-keyfile</code> option allows you to specify the path to the private key file used for encryption. This private key file should be in PEM format.</li> </ul> <p>Example:</p> <pre><code>--tls-keyfile /path/to/private-key.pem\n</code></pre> <ul> <li><code>--tls-certfile</code>: Enable TLS and specify the cert file. Along with the private key, you need to specify the TLS certificate file. The certificate file contains the public key and information about the server. It should be in PEM format.</li> </ul> <p>Example:</p> <pre><code>--tls-certfile /path/to/certificate.pem\n</code></pre> <ul> <li><code>--tls-selfsign</code>: Enable TLS with self-signed certificates. If you don't have a certificate issued by a Certificate Authority (CA), you can enable this option to create self-signed certificates. Self-signed certificates are useful for development and testing but should not be used in production environments where security is crucial.</li> </ul> <p>Example:</p> <pre><code>--tls-selfsign /path/to/self-signed-certificate.pem\n</code></pre>"},{"location":"CLI-Tools/","title":"Stable-Diffusion Productivity Scripts","text":""},{"location":"CLI-Tools/#api-examples","title":"API Examples","text":""},{"location":"CLI-Tools/#run-generate","title":"Run Generate","text":"<ul> <li><code>cli/api-txt2img.py</code></li> <li><code>cli/api-img2img.py</code></li> <li><code>cli/api-control.py</code></li> </ul>"},{"location":"CLI-Tools/#monitor","title":"Monitor","text":"<ul> <li><code>cli/api-progress.py</code></li> </ul>"},{"location":"CLI-Tools/#generic","title":"Generic","text":"<ul> <li><code>cli/api-json.py</code></li> </ul>"},{"location":"CLI-Tools/#process","title":"Process","text":"<ul> <li><code>cli/api-info.py</code></li> <li><code>cli/api-upscale.py</code></li> <li><code>cli/api-vqa.py</code></li> <li><code>cli/api-preprocess.py</code></li> </ul>"},{"location":"CLI-Tools/#other","title":"Other","text":"<ul> <li><code>cli/api-faceid.py</code></li> <li><code>cli/api-faces.py</code></li> <li><code>cli/api-mask.py</code></li> </ul>"},{"location":"CLI-Tools/#javascript","title":"JavaScript","text":"<ul> <li><code>cli/api-txt2img.js</code></li> </ul>"},{"location":"CLI-Tools/#generate","title":"Generate","text":"<p>Text-to-image with all of the possible parameters Supports upsampling, face restoration and grid creation  </p> <p>python cli/generate.py</p> <p>By default uses parameters from  <code>generate.json</code></p> <p>Parameters that are not specified will be randomized:</p> <ul> <li>Prompt will be dynamically created from template of random samples: <code>random.json</code></li> <li>Sampler/Scheduler will be randomly picked from available ones</li> <li>CFG Scale set to 5-10</li> </ul> <p></p>"},{"location":"CLI-Tools/#auxiliary-scripts","title":"Auxiliary Scripts","text":""},{"location":"CLI-Tools/#benchmark","title":"Benchmark","text":"<p>python run-benchmark.py</p>"},{"location":"CLI-Tools/#create-previews","title":"Create Previews","text":"<p>Create previews for embeddings, lora, lycoris, dreambooth and hypernetwork</p> <p>python create-previews.py</p>"},{"location":"CLI-Tools/#image-grid","title":"Image Grid","text":"<p>python image-grid.py</p>"},{"location":"CLI-Tools/#image-watermark","title":"Image Watermark","text":"<p>Create invisible image watermark and remove existing EXIF tags  </p> <p>python image-watermark.py</p>"},{"location":"CLI-Tools/#image-interrogate","title":"Image Interrogate","text":"<p>Runs CLiP and Booru image interrogation  </p> <p>python image-interrogate.py</p>"},{"location":"CLI-Tools/#palette-extract","title":"Palette Extract","text":"<p>Extract color palette from image(s)  </p> <p>python image-palette.py</p>"},{"location":"CLI-Tools/#prompt-ideas","title":"Prompt Ideas","text":"<p>Generate complex prompt ideas</p> <p>python prompt-ideas.py</p>"},{"location":"CLI-Tools/#prompt-promptist","title":"Prompt Promptist","text":"<p>Attempts to beautify the provided prompt  </p> <p>python prompt-promptist.py</p>"},{"location":"CLI-Tools/#video-extract","title":"Video Extract","text":"<p>Extract frames from video files  </p> <p>python video-extract.py</p> <p></p>"},{"location":"CLI-Tools/#utility-scripts","title":"Utility Scripts","text":""},{"location":"CLI-Tools/#sdapi","title":"SDAPI","text":"<p>Utility module that handles async communication to Automatic API endpoints Note: Requires SD API  </p> <p>Can be used to manually execute specific commands:</p> <p>python sdapi.py progress python sdapi.py interrupt python sdapi.py shutdown</p>"},{"location":"Control-Guide/","title":"Control Guide","text":""},{"location":"Control-Guide/#introduction-to-control","title":"Introduction to Control","text":"<p>SDNext's Control tab is our long awaited effort to bring ControlNet, IP-Adapters, T2I Adapter, ControlNet XS, and ControlNet LLLite to our users.  </p> <p>After doing that, we decided that we would add everything else under the sun that we could squeeze in there, and place it directly into your hands with greater options and flexibility than ever before, to allow you to Control your image and video generation with as little effort, as as much power, as possible.</p> <p>Note that this document is a work in progress, it's all quite complex and will take some time to write up a bit more as well as smooth out the rough edges and correct any issues and bugs that pop up, expect frequent updates! </p> <p>This guide will attempt to explain how to use it so that anyone can understand it and put it to work for themselves.  </p> <p>Be sure to also check out the Control resource page which has more technical information as well as some general tips and suggestions. Its usage information will be merged into this page soon\u2122\ufe0f.</p> <p>We'll start with the... Control Controls!</p>"},{"location":"Control-Guide/#controls","title":"Controls","text":""},{"location":"Control-Guide/#input","title":"Input","text":"<p>The Input control is exactly what it sounds like, it controls what input images (or videos) are contributing to your image generation, by default that is just the image in Control input pane, however if you select <code>Separate init image</code>, another image pane will appear below, allowing you to use that as well.  </p> <p>Note: When using a Control input image as well as a Init input image, the Init input dominates. Adjusting denoise to &gt;=0.9 is recommended, as that will allow the Control input to balance with the Init input. Higher values will increase the strength of Control input further, giving it dominance.  </p> <p></p> <p><code>Show Preview</code> is simple, it controls the visibility of the preview window in the far right of the middle row. You'll want this on if you're doing any kind of masking or manipulations that you would want to preview before generating.</p> <p>There are 3 different Input types:</p> <ul> <li> <p><code>Control only</code>: This uses only the Control input below as a source for any ControlNet or IP Adapter type tasks based on any of our various options.  </p> </li> <li> <p><code>Init image same as control</code>: This option will additionally treat any image placed into the <code>Control input</code> pane as a source for img2img type tasks, an image to modify for example.  </p> </li> <li> <p><code>Separate init image</code>: This option creates an additional window next to <code>Control input</code> labeled <code>Init input</code>, so you can have a separate image for both Control operations and an init source.</p> </li> </ul> <p><code>Denoising strength</code> is the same as if you were doing any img2img operation. The higher the value, the more denoising that will take place, and the greater any source image will be modified.</p>"},{"location":"Control-Guide/#size","title":"Size","text":"<p>This can be a little confusing at first because of the <code>Before</code> and <code>After</code> subtabs, however it's really quite simple and extremely powerful. The Control size menu allows you to manipulate the size of your input images before and after inference takes place.  </p> <p></p> <p>The <code>Before</code> subtab does 2 things:</p> <ul> <li> <p>If you do not select any <code>Resize method</code>, it is only controlling the output image size width and height in pixels as it would in any text2img or img2img operation.</p> </li> <li> <p>However, if you do select a <code>Resize method</code>, Nearest for example, you can upscale or downscale the <code>Control input</code> image before any other operations take place. This will be the size of any image used in further operations. Second Pass is not entirely functional yet, but will be part of this.</p> </li> </ul> <p>For example, you might have a much larger image, such as 2048x3072, that you want to use with canny or depth map, but you do not want an image that large to manipulate or guide your generation, that would be prohibitive, slower, and possibly cause an OOM.  </p> <p>This is where <code>Resize method</code> comes in, you would simply select a resize method, typically Nearest or Lanczos, and then either set the pixel width or height you want to resize to under Fixed, or switch over to Scale and select a number below 1. A setting of 0.5 would make your input image effectively 1024x1536 pixels, which would be used as input for later operations.  </p> <p>The <code>After</code> subtab controls any upscaling or downscaling that would take place at the end of your image generation process, most commonly this would either be latent upscaling, and ESRGAN model such as 4x Ultrasharp, or one of the various chaiNNer models we provide. This is the same as it would be in a standard upscaling via text2img or img2img.</p>"},{"location":"Control-Guide/#mask","title":"Mask","text":"<p>The Mask controls are where we start getting into the real meat of Control, not only does it allow a plethora of different options to mask, segment, and control the view of your masking with various preview types, but it comes with 22 different colormaps for your viewing pleasure! (And I think vlad made some of those words up \ud83e\udd2b)  </p> <p></p> <ul> <li> <p><code>Live update</code>: With this checked, your masking will update as you make changes to it, if this is off, you will need to hit the <code>Refresh</code> button to the right to have your preview pane update, making more changes to it while it is processing may lead to it being desynchronized, just hit the refresh button if it does not look correct.</p> </li> <li> <p><code>Inpaint masked only</code>: Inpainting will apply only to areas you have masked if this is checked. You must actually inpaint something, otherwise it's just img2img.</p> </li> <li> <p><code>Invert mask</code>: Inverts the masking, things you mark with the brush will be excluded from a full mask of the image.</p> </li> <li> <p><code>Auto-mask</code>: There are three options here, Threshold, Edge, and Greyscale. Each provides a different method of auto-masking your images.</p> </li> <li> <p><code>Auto-segment</code>: Just like Auto-mask, we have provided an extensive list of Auto-segmentation models, they don't require ControlNet to handle the process, but may take a few seconds to process, depending on your GPU.  </p> </li> <li> <p><code>Preview</code>: You can select the preview type here, we have provided 5 modes, Masked, Binary, Greyscale, Color, and Composite, which is the default.</p> </li> <li> <p><code>Colormap</code>: You can select the style/color scheme of the preview here. There are 22 fantastic color schemes!</p> </li> <li> <p><code>Blur</code>: This blurs the edges of what you have masked, to allow some flexibility. Play with it.</p> </li> <li> <p><code>Erode</code>: This slider controls the reduction of your auto-masking or auto-segmentation border.</p> </li> <li> <p><code>Dilate</code>: This slider controls the expansion of your auto-masking or auto-segmentation border.</p> </li> </ul>"},{"location":"Control-Guide/#video","title":"Video","text":"<p>The Video controls are quite exciting and fun to play with, with our tools now you can, if you wished, turn any video into an anime version for example, frame by frame. There are three output options, GIF, PNG, and MP4. You must select one of these to have video output. With these simple controls, you can tweak your video output with surprising flexibility. Some video output methods provide more controls, try them all.</p> <p></p> <ul> <li> <p><code>Skip input frames</code>: This setting controls how many frames are processed from input instead of every frame. Setting it to 0 would mean processing every frame, a setting of 1 would process every other frame, a setting of 2 would process every third frame, cutting the number of total frames by 2/3rds, and so on.</p> </li> <li> <p><code>Video file</code>: You select the type of output you want here, animated GIF (not JIF!), animated PNG, or MP4 video, all provided via FFMPEG of course.  </p> </li> <li> <p><code>Duration</code>: The length in seconds you want your output video to be.  </p> </li> <li> <p><code>Pad frames</code>: Determine how many frames to add to the beginning and end of the video. This feature is particularly useful when used with interpolation.</p> </li> <li> <p><code>Interpolate frames</code>: The number of frames you want interpolated (via RIFE) between existing frames (filtered by skip input frames) in a video sequence. This smoothens the video output, especially if you're skipping frames to avoid choppy motion or low frame rates.</p> </li> <li> <p><code>Loop</code>: This is purely for animated GIF and PNG output, it enables the classic looping that you would expect.</p> </li> </ul> <p>When you're using interpolation, the software also detects scene changes. If the scene changes significantly, it will insert pad frames instead of interpolating between two unrelated frames. This ensures a seamless transition between scenes and maintains the overall quality of the video output.</p>"},{"location":"Control-Guide/#extensions","title":"Extensions","text":"<p>These are some nice goodies that we have cooked up so that no actual installed extensions are necessary, you may even find that our version works better!</p>"},{"location":"Control-Guide/#animatediff","title":"AnimateDiff","text":"<p>This is the new home of our Vlad-created implementation of the AnimateDiff extension. Now with FREE FreeInit!  </p> <p>I honestly don't know how to use this, so I'll update this when I do. My apologies! But if you already do, enjoy!</p> <p></p>"},{"location":"Control-Guide/#ip-adapter","title":"IP-Adapter","text":"<p>This is our IP Adapter implementation, with 10 available models for your image or face cloning needs!</p> <p></p>"},{"location":"Control-Guide/#image-panes","title":"Image Panes","text":"<p>You may notice small icons above the image panes that look like pencils, these are Interrogate buttons. The left one is BLIP, and the right one is DeepBooru. Click one of the buttons to interrogate the image in the pane below it. The results will appear in your prompt area.</p> <p></p>"},{"location":"Control-Guide/#control-input","title":"Control Input","text":"<p>This is the heart of Control, you may put any image or even video here to be processed by our system, that means any and all scripts, extensions, even the various Controlnet variants below, though you can individually add guidance images to each of those. If an image is placed here, the system will assume you are performing an img2img process of some sort. If you upload a video to SDNext via the Control input pane, you will see that you can play the video, both input and resultant output. Batching and folders should work as expected.</p> <p>Note below there are 2 other buttons, Inpainting and Outpainting, below.</p> <p></p>"},{"location":"Control-Guide/#controlnet","title":"ControlNet+","text":"<p>At the very bottom of the Control page, we have what you've all been waiting for, full ControlNet! I do mean full too, we have it all! This includes SD and SD-XL. at last! You won't ever need the ControlNet extension ever again, much less to touch the original LDM backend.  </p> <p>This will take a bit more work to document example workflows, but there are tooltips, and if you've used ControlNet before, you shouldn't have any problems! However if you do, hop on by our Discord server and we're happy to help.</p>"},{"location":"Control/","title":"Control Overview","text":"<p>Native control module for SD.Next for Diffusers backend Can be used for Control generation as well as Image and Text workflows  </p> <p>For a guide on the options and settings, as well as explanations for the controls themselves, see the Control Guide page.</p>"},{"location":"Control/#supported-control-models","title":"Supported Control Models","text":"<ul> <li>lllyasviel ControlNet for SD 1.5 and SD-XL models   Includes ControlNets as well as Reference-only mode and any compatible 3rd party models   Original ControlNets for SD15 are 1.4GB each and for SDXL its at massive 4.9GB  </li> <li>VisLearn ControlNet XS for SD-XL models   Lightweight ControlNet models for SDXL at 165MB only with near-identical results  </li> <li>TencentARC T2I-Adapter for SD 1.5 and SD-XL models   T2I-Adapters provide similar functionality at much lower resource cost at only 300MB each  </li> <li>Kohya Control LLite for SD-XL models   LLLite models for SDXL at 46MB only provide lightweight image control  </li> <li>TenecentAILab IP-Adapter for SD 1.5 and SD-XL models   IP-Adapters provides great style transfer functionality at much lower resource cost at below 100MB for SD15 and 700MB for SDXL   IP-Adapters can be combined with ControlNet for more stable results, especially when doing batch/video processing  </li> <li>CiaraRowles TemporalNet for SD 1.5 models   ControlNet model designed to enhance temporal consistency and reduce flickering for batch/video processing  </li> </ul> <p>All built-in models are downloaded upon first use and stored stored in: <code>/models/controlnet</code>, <code>/models/adapter</code>, <code>/models/xs</code>, <code>/models/lite</code>, <code>/models/processor</code></p> <p>Listed below are all models that are supported out-of-the-box:</p>"},{"location":"Control/#controlnet","title":"ControlNet","text":"<ul> <li>SD15:   Canny, Depth, IP2P, LineArt, LineArt Anime, MLDS, NormalBae, OpenPose,   Scribble, Segment, Shuffle, SoftEdge, TemporalNet, HED, Tile  </li> <li>SDXL:   Canny Small XL, Canny Mid XL, Canny XL, Depth Zoe XL, Depth Mid XL</li> </ul> <p>Note: only models compatible with currently loaded base model are listed Additional ControlNet models in safetensors can be downloaded manually and placed into corresponding folder: <code>/models/control/controlnet</code> </p>"},{"location":"Control/#controlnet-xs","title":"ControlNet XS","text":"<ul> <li>SDXL:   Canny, Depth  </li> </ul>"},{"location":"Control/#controlnet-lllite","title":"ControlNet LLLite","text":"<ul> <li>SDXL:   Canny, Canny anime, Depth anime, Blur anime, Pose anime, Replicate anime</li> </ul> <p>Note: control-lllite is implemented using unofficial implementation and its considered experimental Additional ControlNet models in safetensors can be downloaded manually and placed into corresponding folder: <code>/models/control/lite</code> </p>"},{"location":"Control/#t2i-adapter","title":"T2I-Adapter","text":"<pre><code>'Segment': 'TencentARC/t2iadapter_seg_sd14v1',\n'Zoe Depth': 'TencentARC/t2iadapter_zoedepth_sd15v1',\n'OpenPose': 'TencentARC/t2iadapter_openpose_sd14v1',\n'KeyPose': 'TencentARC/t2iadapter_keypose_sd14v1',\n'Color': 'TencentARC/t2iadapter_color_sd14v1',\n'Depth v1': 'TencentARC/t2iadapter_depth_sd14v1',\n'Depth v2': 'TencentARC/t2iadapter_depth_sd15v2',\n'Canny v1': 'TencentARC/t2iadapter_canny_sd14v1',\n'Canny v2': 'TencentARC/t2iadapter_canny_sd15v2',\n'Sketch v1': 'TencentARC/t2iadapter_sketch_sd14v1',\n'Sketch v2': 'TencentARC/t2iadapter_sketch_sd15v2',\n</code></pre> <ul> <li>SD15:   Segment, Zoe Depth, OpenPose, KeyPose, Color, Depth v1, Depth v2, Canny v1, Canny v2, Sketch v1, Sketch v2  </li> <li>SDXL:   Canny XL, Depth Zoe XL, Depth Midas XL, LineArt XL, OpenPose XL, Sketch XL  </li> </ul> <p>Note: Only models compatible with currently loaded base model are listed</p>"},{"location":"Control/#processors","title":"Processors","text":"<ul> <li>Pose style: OpenPose, DWPose, MediaPipe Face</li> <li>Outline style: Canny, Edge, LineArt Realistic, LineArt Anime, HED, PidiNet</li> <li>Depth style: Midas Depth Hybrid, Zoe Depth, Leres Depth, Normal Bae</li> <li>Segmentation style: SegmentAnything</li> <li>Other: MLSD, Shuffle</li> </ul> <p>Note: Processor sizes can vary from none for built-in ones to anywhere between 200MB up to 4.2GB for ZoeDepth-Large</p>"},{"location":"Control/#segmentation-models","title":"Segmentation Models","text":"<p>There are 8 Auto-segmentation models available:  </p> <ul> <li>Facebook SAM ViT Base (357MB)  </li> <li>Facebook SAM ViT Large (1.16GB)</li> <li>Facebook SAM ViT Huge (2.56GB)</li> <li>SlimSAM Uniform (106MB)</li> <li>SlimSAM Uniform Tiny (37MB)</li> <li>Rembg Silueta</li> <li>Rembg U2Net  </li> <li>Rembg ISNet</li> </ul>"},{"location":"Control/#reference","title":"Reference","text":"<p>Reference mode is its own pipeline, so it cannot have multiple units or processors  </p>"},{"location":"Control/#workflows","title":"Workflows","text":""},{"location":"Control/#inputs-outputs","title":"Inputs &amp; Outputs","text":"<ul> <li>Image -&gt; Image</li> <li>Batch: list of images -&gt; Gallery and/or Video</li> <li>Folder: folder with images -&gt; Gallery and/or Video</li> <li>Video -&gt; Gallery and/or Video</li> </ul> <p>Notes: - Input/Output/Preview panels can be minimized by clicking on them - For video output, make sure to set video options  </p>"},{"location":"Control/#unit","title":"Unit","text":"<ul> <li>Unit is: input plus process plus control</li> <li>Pipeline consists of any number of configured units   If unit is using using control modules, all control modules inside pipeline must be of same type   e.g. ControlNet, ControlNet-XS, T2I-Adapter or Reference</li> <li>Each unit can use primary input or its own override input  </li> <li>Each unit can have no processor in which case it will run control on input directly   Use when you're using predefined input templates  </li> <li>Unit can have no control in which case it will run processor only  </li> <li>Any combination of input, processor and control is possible   For example, two enabled units with process only will produce compound processed image but without control  </li> </ul>"},{"location":"Control/#what-if","title":"What-if?","text":"<ul> <li>If no input is provided then pipeline will run in txt2img mode   Can be freely used instead of standard <code>txt2img</code> </li> <li>If none of units have control or adapter, pipeline will run in img2img mode using input image   Can be freely used instead of standard <code>img2img</code> </li> <li>If you have processor enabled, but no controlnet or adapter loaded,   pipeline will run in img2img mode using processed input</li> <li>If you have multiple processors enabled, but no controlnet or adapter loaded,   pipeline will run in img2img mode on blended processed image  </li> <li>Output resolution is by default set to input resolution,   Use resize settings to force any resolution  </li> <li>Resize operation can run before (on input image) or after processing (on output image)  </li> <li>Using video input will run pipeline on each frame unless skip frames is set   Video output is standard list of images (gallery) and can be optionally encoded into a video file   Video file can be interpolated using RIFE for smoother playback  </li> </ul>"},{"location":"Control/#overrides","title":"Overrides","text":"<ul> <li>Control can be based on main input or each individual unit can have its own override input</li> <li>By default, control runs in default control+txt2img mode</li> <li>If init image is provided, it runs in control+img2img mode   Init image can be same as control image or separate</li> <li>IP adapter can be applied to any workflow</li> <li>IP adapter can use same input as control input or separate</li> </ul>"},{"location":"Control/#inpaint","title":"Inpaint","text":"<ul> <li>Inpaint workflow is triggered when input image is provided in inpaint mode</li> <li>Inpaint mode can be used with image-to-image or controlnet workflows</li> <li>Other unit types such as T2I, XS or Lite do not support inpaint mode</li> </ul>"},{"location":"Control/#outpaint","title":"Outpaint","text":"<ul> <li>Outpaint workflow is triggered when input image is provided in outpaint mode</li> <li>Outpaint mode can be used with image-to-image or controlnet workflows</li> <li>Other unit types such as T2I, XS or Lite do not support outpaint mode</li> <li>Recommendation is to increase denoising strength to at least 0.8 since outpained area is blank and needs to be filled with noise</li> <li>Outpaint folloing input image can be controled by overlap setting - higher overlap and more of original image will be part of the outpaint process</li> </ul>"},{"location":"Control/#logging","title":"Logging","text":"<p>To enable extra logging for troubleshooting purposes, set environment variables before running SD.Next</p> <ul> <li> <p>Linux:</p> <p>export SD_CONTROL_DEBUG=true export SD_PROCESS_DEBUG=true ./webui.sh --debug  </p> </li> <li> <p>Windows:</p> <p>set SD_CONTROL_DEBUG=true set SD_PROCESS_DEBUG=true webui.bat --debug  </p> </li> </ul> <p>Note: Starting with debug info enabled also enables Test mode in Control module</p>"},{"location":"Control/#limitations-todo","title":"Limitations / TODO","text":""},{"location":"Control/#known-issues","title":"Known issues","text":"<ul> <li>Using model offload can cause Control models to be on the wrong device at the time of the execution   Example error message: <p>Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same  </p> </li> </ul> <p>Workaround: Disable model offload in settings -&gt; diffusers and use move model option instead  </p> <ul> <li>Issues after trying to use DWPose and installation fails: `` error.   Example error message: <p>Control processor DWPose: DLL load failed while importing _ext  </p> </li> </ul> <p>Workaround: Activate venv and run following commands to install dwpose dependencies manually: <code>pip install --upgrade --no-deps --force-reinstall openmim==0.3.9 mmengine==0.10.4 mmcv==2.1.0 mmpose==1.3.1 mmdet==3.3.0</code></p>"},{"location":"Control/#future","title":"Future","text":"<ul> <li>Pose editor</li> <li>Process multiple images in batch in parallel</li> <li>ControlLora https://huggingface.co/stabilityai/control-lora</li> <li>Multi-frame rendering https://xanthius.itch.io/multi-frame-rendering-for-stablediffusion</li> <li>Deflickering and deghosting</li> </ul>"},{"location":"Debug/","title":"Debug","text":"<p>To run SD.Next in debug mode, start it with <code>--debug</code> flag This has no overhead and can be safely used in daily operations as it just prints additional information to logs  </p> <p>Example:</p> <p>webui.bat --debug webui.sh --debug  </p>"},{"location":"Debug/#extra-debug","title":"Extra Debug","text":"<p>Some debug information would be too much for regular use, so it can be enabled by use of environment variables:</p>"},{"location":"Debug/#install-load","title":"Install &amp; load","text":"<ul> <li><code>SD_INSTALL_DEBUG</code>: report detailed information related to packages installation  </li> <li><code>SD_PATH_DEBUG</code>: report all used paths as they are parsed</li> <li><code>SD_SCRIPT_DEBUG</code>: increase verbosity of script and extension load and execution</li> <li><code>SD_MOVE_DEBUG</code>: trace all model moves from and to cpu/gpu  </li> <li><code>SD_EXT_DEBUG</code>: trace extensions load/install/update operations  </li> <li><code>SD_LOAD_DEBUG</code>: report all model loading operations as they happen</li> </ul>"},{"location":"Debug/#core-processing","title":"Core processing","text":"<ul> <li><code>SD_PROCESS_DEBUG</code>: print detailed processing information</li> <li><code>SD_DIFFUSER_DEBUG</code>: increase verbosity of diffusers processing</li> <li><code>SD_LDM_DEBUG</code>: increase verbosity of LDM processing</li> <li><code>SD_CONTROL_DEBUG</code>: report all debug information related to control module</li> </ul>"},{"location":"Debug/#extra-networks","title":"Extra networks","text":"<ul> <li><code>SD_EN_DEBUG</code>: report all extra networks operations as they happen</li> <li><code>SD_LORA_DEBUG</code>: increase verbosity of LoRA loading and execution</li> </ul>"},{"location":"Debug/#other","title":"Other","text":"<ul> <li><code>SD_PASTE_DEBUG</code>: report all params paste and parse operations as they happen</li> <li><code>SD_HDR_DEBUG</code>: print HDR processing information</li> <li><code>SD_PROMPT_DEBUG</code>: print all prompt parsing and encoding information</li> <li><code>SD_SAMPLER_DEBUG</code>: report all possible sampler settings for selected sampler</li> <li><code>SD_STEPS_DEBUG</code>: report calculations done to scheduler steps</li> <li><code>SD_VAE_DEBUG</code>: report details on all VAE operations</li> <li><code>SD_MASK_DEBUG</code>: reported detailed information on image masking operations as they happen  </li> <li><code>SD_DOWNLOAD_DEBUG</code>: report detailed information on model download operations as they happen</li> <li><code>SD_CALLBACK_DEBUG</code>: report each step as it executes with full details  </li> <li><code>SD_BROWSER_DEBUG</code>: report all gallery operations as they happen</li> <li><code>SD_NAMEGEN_DEBUG</code>: report all filename generation operations as they happen</li> </ul> <p>Example Windows:</p> <p>set SD_PROCESS_DEBUG=true webui.bat --debug  </p> <p>Example Linux:</p> <p>export SD_PROCESS_DEBUG=true webui.sh --debug  </p> <p>Additional information enabled via env variables will show in log with level <code>TRACE</code></p>"},{"location":"Debug/#profiling","title":"Profiling","text":"<p>To run SD.Next in profiling mode, start it with <code>--profile</code> flag This does have overhead, both on processing and memory side, so its not recommended for daily use SD.Next will collect profiling information from both Python, Torch and CUDA and print it upon completion of specific operations  </p> <p>Example:</p> <p>webui.bat --debug --profile webui.sh --debug --profile</p>"},{"location":"DirectML/","title":"DirectML","text":"<p>SD.Next includes support for PyTorch-DirectML.</p>"},{"location":"DirectML/#how-to","title":"How to","text":"<p>Add <code>--use-directml</code> on commandline arguments.</p> <p>For details, go to Installation.</p>"},{"location":"DirectML/#performance","title":"Performance","text":"<p>The performance is quite bad compared to ROCm.</p> <p>If you are familiar with Linux system, we recommend ROCm.</p>"},{"location":"DirectML/#faq","title":"FAQ","text":""},{"location":"DirectML/#directml-does-not-collect-garbage-memory","title":"DirectML does not collect garbage memory","text":"<p>PyTorch-DirectML does not access graphics memory by indexing. Because PyTorch-DirectML's tensor implementation extends OpaqueTensorImpl, we cannot access the actual storage of a tensor.</p>"},{"location":"DirectML/#an-error-occurs-with-no-error-message","title":"An error occurs with no error message","text":"<p>If you met <code>RuntimeError</code> with no error message (or empty), please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#it-does-not-work-properly-with-fp16","title":"It does not work properly with FP16","text":"<p>If it works with FP32, please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#the-terminal-is-suddenly-frozen-during-generation","title":"The terminal is suddenly frozen during generation","text":"<p>Please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#olive-experimental-support","title":"Olive (experimental support)","text":"<p>Refer to ONNX Runtime</p>"},{"location":"Docker/","title":"Docker","text":"<p>SD.Next includes basic Dockerfile for use with nVidia GPU equipped systems Other system may require different configurations and base images, but principle remains  </p> <p>Goal of containerized SD.Next is to provide a fully stateless environment that can be easily deployed and scaled  </p> <p>SD.Next docker template is based on official base image with <code>torch==2.5.1</code> with <code>cuda==12.4</code></p> <p>SD.Next docker image is currently not published in docker hub or any other repository since typically each user or organization will have their own customizations and requirements and build process is very simple and fast  </p>"},{"location":"Docker/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>If you already have functional Docker on your host, you can skip this section  For manualy steps see appendix at the end of the document  </p> <ul> <li>Docker itself https://docs.docker.com/get-started/get-docker/</li> <li>nVidia Container ToolKit to enable GPU support https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</li> </ul>"},{"location":"Docker/#build-image","title":"Build Image","text":"<p>Note</p> <p>Building SDNext docker image is normally only required once and takes between few seconds (using cached image) to ~1.5min (initial build) to complete  First build will also need to download the base image, which can take a while depending on your connection  If you make changes to <code>Dockerfile</code> or update SD.Next, you will need to rebuild the image  </p> <p>Important</p> <p>Build process should be done on a system where SD.Next was started at least once to download all required submodules before docker copy process  </p> <pre><code>docker build \\\n  --debug \\\n  --tag sdnext/sdnext-cuda \\\n  &lt;path_to_sdnext_folder&gt;\n\ndocker image inspect sdnext/sdnext-cuda  \n</code></pre> <pre><code>    [+] Building 93.3s (12/12) FINISHED                                                     docker:default\n    [internal] load build definition from Dockerfile                                              0.0s\n    transferring dockerfile: 2.25kB                                                               0.0s\n    [internal] load metadata for docker.io/pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime          0.0s\n    [internal] load .dockerignore                                                                 0.0s\n    transferring context: 366B                                                                    0.0s\n    CACHED [1/7] FROM docker.io/pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime                     0.0s\n    [internal] load build context                                                                 1.3s\n    transferring context: 417.02MB                                                                1.3s\n    [2/7] RUN [\"apt-get\", \"-y\", \"update\"]                                                         4.4s\n    [3/7] RUN [\"apt-get\", \"-y\", \"install\", \"git\", \"build-essential\", \"google-perftools\", \"curl\"  20.7s\n    [4/7] RUN [\"/usr/sbin/ldconfig\"]                                                              0.3s\n    [5/7] COPY . /app                                                                             0.8s\n    [6/7] WORKDIR /app                                                                            0.0s\n    [7/7] RUN [\"python\", \"/app/launch.py\", \"--debug\", \"--uv\", \"--use-cuda\", \"--log\", \"sdnext.lo  63.9s\n    exporting to image                                                                            3.1s\n    exporting layers                                                                              3.1s\n    writing image sha256:5b2571c1f2a71f7a6d5ce4b1de1ec0e76cd4f670a1ebc17de79c333fb7fffd46         0.0s\n    naming to docker.io/sdnext/sdnext-cuda                                                        0.0s\n</code></pre> <p>Base image <code>pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime</code> is 6.14GB And full SD.Next resulting image is ~8.8GB and contains all required dependencies  </p> <p>Warning</p> <p>If you have build errors, run with <code>--progress=plain</code> to get full build log</p>"},{"location":"Docker/#run-container","title":"Run Container","text":"<p>Note</p> <ul> <li>Republishes port from container to host directly    You may need to remap ports if you have multiple containers running on the same host  </li> <li>Maps local server folder <code>/server/data</code> to be used by the container as data root    This is where all state items and outputs will be read from and written to  </li> <li>Maps local server folder <code>/server/models</code> to be used by the container as model root    This is where models will be read from and written to  </li> </ul> <pre><code>docker run \\\n  --name sdnext-container \\\n  --rm \\\n  --gpus all \\\n  --publish 7860:7860 \\\n  --mount type=bind,source=/server/models,target=/mnt/models \\\n  --mount type=bind,source=/server/data,target=/mnt/data \\\n  --detach \\\n  sdnext/sdnext-cuda\n</code></pre> <p>Typical SDNext container will start in ~10sec and will be ready to accept connections on port <code>7860</code></p>"},{"location":"Docker/#state","title":"State","text":"<p>As mentioned, the goal of SD.Next docker deployment is fully stateless operations. By default, SD.Next docker containers is stateless: any data stored inside the container is lost when the container stops.  </p> <p>All state items and outputs will be read from and written to <code>/server/data</code> This includes: - Configuration files: <code>config.json</code>, <code>ui-config.json</code> - Cache information: <code>cache.json</code>, <code>metadata.json</code> - Outputs of all generated images: <code>outputs/</code></p>"},{"location":"Docker/#persistence","title":"Persistence","text":"<p>If you plan to customize SD.Next deployment with additional extensions, you may want to create and map docker volume to avoid constaint reinstalls on each startup.  </p>"},{"location":"Docker/#healthchecks","title":"Healthchecks","text":"<p>By default, SD.Next docker container does not include healthchecks, but they can be enabled. Simply remove comment from <code>HEALTHCHECK</code> line in <code>Dockerfile</code> and rebuild the image.  </p>"},{"location":"Docker/#extra","title":"Extra","text":"<p>Additional docker commands that may be useful</p> <p>Tip</p> <p>Clean Up</p> <pre><code>docker image ls --all\ndocker image rm &lt;id&gt;\ndocker builder prune --force  \n</code></pre> <p>Tip</p> <p>List Containers</p> <pre><code>docker container ls --all\ndocker ps --all\n</code></pre> <p>Tip</p> <p>View Log</p> <pre><code>&gt; docker container logs --follow &lt;id&gt;\n</code></pre> <p>Tip</p> <p>Stop Container</p> <pre><code>&gt; docker container stop &lt;id&gt;\n</code></pre> <p>Tip</p> <p>Test GPU</p> <pre><code>docker info  \ndocker run --name cudatest --rm --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark  \n</code></pre> <p>Tip</p> <p>Test Torch</p> <pre><code>docker pull pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime  \ndocker run --name pytorch --rm --gpus all -it pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime  \n</code></pre>"},{"location":"Docker/#manual-install","title":"Manual Install","text":"<p>Docker</p> <pre><code>wget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/containerd.io_1.7.23-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce_27.3.1-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-ce-cli_27.3.1-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-buildx-plugin_0.17.1-1~ubuntu.24.04~noble_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/docker-compose-plugin_2.29.7-1~ubuntu.24.04~noble_amd64.deb\nsudo dpkg -i *.deb\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nsystemctl status docker\nsystemctl status containerd\n</code></pre> <p>nVidia Container ToolKit</p> <pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt update\nsudo apt install nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\ndocker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre>"},{"location":"Extensions/","title":"Extensions Development","text":""},{"location":"Extensions/#common-mistakes","title":"Common Mistakes","text":"<ul> <li>Execution on import:   There should be NO CODE that executes on import in any of the extension files (with exception of allowed trivial <code>install.py</code> code)   All code should be in functions/classes and executed on application callbacks   Failure to do so results in slow server startup (at best) or even server crashes (at worst)  </li> <li>Keeping all Python code in <code>/scripts</code>:   That folder should have a single entry point file and that file should import any other file that may reside anywhere else EXCEPT <code>/scripts</code> folder   Failure to do so results server loading and executing each and every file during startup and then extension itself performs imports again   If extension fails to do so results in slow server startup (at best) or even server crashes (at worst)  </li> <li>Browser namespace collisions:   All JS code is imported as-is and lives in an global browser namespace   Do not define functions like <code>log()</code> - use a unique prefix for all your functions and variables  </li> <li>Incorrect usage of callbacks:   For example, JS code can be executed <code>onUiLoaded</code> or <code>onUiUpdated</code>   First one is triggered once, second one is triggered hundreds of times during server startup   Choosing wrong callback to perform initialization work results in initializaton work being performed hundreds of times resulting in slow page loads   Typical problem is why is my browser page loading so slow or why is autolaunch not working? Because browser is asked to do the same things hundreds of times   This applies to both Python and JS code - always check if you're using correct callbacks  </li> <li>Executing code when disabled:   If extension is not enabled, it should not perform any work in callbacks it is registered for and always perform early-exit when work is not needed   This can result in pre/during/post generate delays for no apparent reasons   Typical problem would be ~0.5sec delay before generate start - which quickly adds up to overall time-to-generate regardless of how fast generate actually is  </li> <li>Unsafe references:   Extensions can access server variables either directly or as provided via callbacks, but content of those variables should be handled with care   For example, extension can access <code>image.info</code> property, but there is no guarantee that property will exist for all images - always use safe access methods like <code>get()</code> or <code>getattr()</code>   This also applies to server settings - extensions cannot assume settings never change, otherwise its not viable to ever improve underlying server at all  </li> <li>Unsafe patching:   Extensions can patch server code by providing overrides for some methods, but should do so with care   For example, extension can replace <code>forward</code> functions, but always consider there may be other extensions that do the same   So patching should never be done globally on in a way that breaks other extensions  </li> <li>Running platform specific code:   Not everyone runs the same OS or compute backend and using platform or hardware specific code or relying on packages which are not cross-platform is just bad   For example, never use platform specific designators such as <code>torch.to('cuda')</code> or <code>torch.float16</code>, always use well-defined server variables such as <code>device.device</code> and <code>device.dtype</code> </li> <li>Assuming values:   Never assume values, always check a well-defined variable what is the current value and handle it accordingly   For example, extension may not be installed in <code>/extensions</code> folder, it may be relocated to a different folder</li> </ul> <p>Unfortunately, looking at top-20 most popular extensions, most of them are guilty of not just one or two, but majority of the above cases and this is not isolated only to extensions that are considered broken Always remember that by installing extension you give it full access to do anything and you're relying on extension author to perform all the work correctly and safely  </p>"},{"location":"Extensions/#extension-vs-script","title":"Extension vs Script","text":"<ul> <li>Script is a single module that implements a script</li> <li>Script object is inherited from <code>Script</code> class and implements several mandatory and any of the optional methods for well-defined callbacks</li> <li>Extension is a larger implementation that exists as its own folder with a well-defined folder structure</li> <li>Extension code can further define any number of Scripts or work on its own</li> </ul>"},{"location":"Extensions/#extension-folder-structure","title":"Extension Folder Structure","text":"<p>Note: any of the files are optional</p> <ul> <li><code>/preload.py</code>   Loaded early during server startup to provide additional command line parameters   Extension should define a <code>preload(parser: argparser)</code> function to extend parser as needed   Preload should perform no other work or access any other modules</li> <li><code>/install.py</code>   Loaded early during server startup to install any optional requirements   Must use well defined server functions such as <code>launch.run_pip</code> and access only modules explicitly marked as safe at the early stages of server startup   Do not do direct OS calls or perform any other work other than basic installation</li> <li><code>/javascript/*.js</code>   All JS files in a folder are added as-is   There should be no work done in JS scripts other than defining functions/variables and registering callbacks that will be executed at appropriate time</li> <li><code>/style.css</code>   Style is added as-is  </li> <li><code>/scripts/*.py</code>   This is intended as a main entry-point for extension and every file is loaded by the server during startup   There should be no work done in Python scripts other than defining functions/variables and registering callbacks that will be executed at appropriate time</li> </ul>"},{"location":"Extra-networks-search/","title":"Extra Networks Search","text":"<p>Search input for the extra networks accepts additional search syntax. This allows to do more complicated searches, in addition to the searches which were previously available.</p>"},{"location":"Extra-networks-search/#search-syntax","title":"Search Syntax","text":"<p>In a normal search, there are 3 special symbols which can be used to modify the search:</p> <ul> <li><code>|</code> - Or, it allows to search for one of multiple terms at once (or exclude multiple terms, if prefixed with <code>-</code>)</li> <li><code>-</code> - Exclude, terms or phrases prefixed with this symbol will be excluded from the search</li> <li><code>&amp;</code> - And, require all terms to be present (or not present, if prefixed with <code>-</code>) in the search results</li> </ul> <p>These symbols are parsed in the order of <code>|</code>, <code>&amp;</code>, <code>-</code>. This means that <code>a|b&amp;c</code> is equivalent to \"a or [b and c]\" and <code>a&amp;b|c</code> is equivalent to \"[a and b] or c\". It is not possible to modify the order of operations with parentheses or other symbols.</p> <p>Whitespace (this means spaces, tabs, newlines, etc.) is trimmed from individual terms. This means that <code>a |    b</code> is equivalent to <code>a|b</code>. Also <code>a|-b</code> is equivalent to <code>a| -   b</code>.</p> <p>Search is case-insensitive, this means that <code>a</code> is equivalent to <code>A</code>.</p> <p>However this concept might be clearer with a few examples based on a list of networks.</p>"},{"location":"Extra-networks-search/#examples","title":"Examples","text":"<p>Assume the following folder structure for your networks:</p> <pre><code>\u251c\u2500\u2500 SDXL/\n\u2502   \u251c\u2500\u2500 Clothing/\n\u2502   \u2502   \u251c\u2500\u2500 Elegant Gown.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Vintage Suit.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Summer Dress.safetensor\n\u2502   \u2502   \u2514\u2500\u2500 Sports Attire.safetensor\n\u2502   \u251c\u2500\u2500 Concept/\n\u2502   \u2502   \u251c\u2500\u2500 Futuristic Cityscape.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Underwater World.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Space Exploration.safetensor\n\u2502   \u2502   \u2514\u2500\u2500 Cyberpunk Street.safetensor\n\u2502   \u2514\u2500\u2500 Person/\n\u2502       \u251c\u2500\u2500 Celebrity Portrait.safetensor\n\u2502       \u251c\u2500\u2500 Historical Figure.safetensor\n\u2502       \u251c\u2500\u2500 Fictional Character.safetensor\n\u2502       \u2514\u2500\u2500 Movie Star.safetensor\n\u2514\u2500\u2500 SD1.5/\n    \u251c\u2500\u2500 Clothing/\n    \u2502   \u251c\u2500\u2500 Casual Wear.safetensor\n    \u2502   \u251c\u2500\u2500 Casual Winter-Wear.safetensor\n    \u2502   \u251c\u2500\u2500 Winter Coat.safetensor\n    \u2502   \u251c\u2500\u2500 Evening Gown.safetensor\n    \u2502   \u2514\u2500\u2500 Vintage Suit.safetensor\n    \u251c\u2500\u2500 Concept/\n    \u2502   \u251c\u2500\u2500 Alien Landscape.safetensor\n    \u2502   \u251c\u2500\u2500 Medieval Castle.safetensor\n    \u2502   \u251c\u2500\u2500 Cyberpunk Street.safetensor\n    \u2502   \u2514\u2500\u2500 Futuristic Cityscape.safetensor\n    \u2514\u2500\u2500 Person/\n        \u251c\u2500\u2500 Scientist Portrait.safetensor\n        \u251c\u2500\u2500 Movie Star.safetensor\n        \u251c\u2500\u2500 Mythological Hero.safetensor\n        \u2514\u2500\u2500 Historical Figure.safetensor\n</code></pre> <p>Below are some example queries, which are explained in detail afterwards.</p> <ul> <li><code>Vintage Suit</code> - Search for everything which contains the phrase <code>Vintage Suit</code>, meaning in this case <code>Vintage Suit.safetensor</code> in <code>SDXL</code> and <code>Vintage Suit.safetensor</code> in <code>SD1.5</code>.</li> <li><code>clothing</code> - Search for everything which contains the word <code>clothing</code>, meaning in this case all networks in the <code>Clothing</code> folders.</li> <li><code>clothing &amp; SDXL</code> - Search for everything which contains the word <code>clothing</code> and the word <code>SDXL</code>, meaning in this case all networks in the <code>Clothing</code> folder of the <code>SDXL</code> folder.</li> <li><code>-SD1.5</code> - Search for everything which does not contain the word <code>SD1.5</code>, meaning in this case all networks in the <code>SDXL</code> folder.</li> <li><code>gown | dress</code> - Search for everything which contains the word <code>gown</code> or the word <code>dress</code>, meaning in this case <code>Evening Gown.safetensor</code> in <code>SD1.5</code> and <code>Elegant Gown.safetensor, Summer Dress.safetensor</code> in <code>SDXL</code>.</li> <li><code>SDXL &amp; gown | SDXL &amp; dress</code> - Search for everything which contains the word <code>gown</code> or the word <code>dress</code> and the word <code>SDXL</code>, meaning in this just case <code>Elegant Gown.safetensor, Summer Dress.safetensor</code> in <code>SDXL</code>.</li> <li><code>clothing | - Casual Wear</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain the phrase <code>Casual Wear</code>, meaning in this case <code>Casual Winter-Wear.safetensor, Elegant Gown.safetensor, Vintage Suit.safetensor, Summer Dress.safetensor, Sports Attire.safetensor</code> in <code>SDXL</code> and <code>Evening Gown.safetensor, Vintage Suit.safetensor, Winter Coat</code> in <code>SD1.5</code>.</li> <li><code>clothing | - Casual &amp; Wear</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain both the word <code>casual</code> and the word <code>wear</code>, meaning in this case <code>Elegant Gown.safetensor, Vintage Suit.safetensor, Summer Dress.safetensor, Sports Attire.safetensor</code> in <code>SDXL</code> and <code>Evening Gown.safetensor, Vintage Suit.safetensor, Winter Coat.safetensor</code> in <code>SD1.5</code> (Note that <code>Casual Winter-Wear.safetensor</code> is excluded from the search results).</li> <li><code>clothing | -black | - gray | -white</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain the words <code>black</code>, <code>gray</code> or <code>white</code>.</li> <li><code>SDXL | -Turbo | -LCM</code> - Search for everything which contains the word SDXL, as long as it does not contain the words <code>Turbo</code> or <code>LCM</code>. Can be used to filter different types of networks.</li> <li><code>SDXL &amp; Turbo | SDXL &amp; LCM</code> - The inverse of the previous search. Search for everything which contains the word <code>Turbo</code> or <code>LCM</code> and the word <code>SDXL</code>.</li> </ul>"},{"location":"Extra-networks-search/#advanced-usage","title":"Advanced usage","text":"<p>It is also possible to do a RegEx search instead of a normal search. To do so, the search term has to be prefixed with <code>r#</code>. Everything after that will be interpreted as a RegEx search.</p> <p>This means that <code>r#.*</code> will match everything, <code>r#^$</code> will match nothing and <code>r#a</code> will match everything which contains the letter <code>a</code>. Please watch out that valid RegEx syntax is used, otherwise the search will fail. In addition to that, the RegEx search is also slower than a normal search, so it is recommended to only use it if necessary. RegEx searches are case-insensitive too.</p> <p>One more thing to watch out for, is file paths. For the normal search the Windows <code>\\</code> is replaced with a <code>/</code> to make it easier to search for file paths. This is not the case for the RegEx search. So you could use for example <code>[\\/\\\\]</code> to match both <code>\\</code> and <code>/</code> in a RegEx search.</p> <p>Considering the RegEx search, the field which is searched is structured in the following way:</p> <pre><code>`filename: ${filename}|name: ${name}|tags: ${tags}`\n</code></pre> <p>As of writing this, the fields represent the following:</p> <ul> <li><code>name</code> - The relative path to the network, not including filetype (also shows when hovering the network card for a longer time)</li> <li><code>filename</code> - The absolute path to the network. (This does not resolve symlinks, so the path shown is the path to the symlink if one is used, not the path to the file the symlink points to)</li> <li><code>tags</code> - The tags the network has, separated by <code>|</code>. Not all networks necessarily have tags, and they can be either keywords used when prompting, or the keyword to trigger the network specifically.</li> </ul> <p>This allows to exclude (or include) certain strings only in specific fields. For example a negative lookahead can be used to exclude certain tags from the search, while still searching for them in the title.</p> <p>An example RegEx would be the following for this use case:</p> <pre><code>r#^(?!.*\\|tags:.*(1girl|woman|girl)).*$\n</code></pre> <p>This search excludes every network which has tags which contain either <code>1girl</code>, <code>woman</code> or <code>girl</code>.</p> <p>Another usecase would be searching in just the filename, but not in the entire path. For example requiring the network to contain <code>SDXL</code>, with it not mattering whether it is in a folder called <code>SDXL</code> or not:</p> <pre><code>r#^.*\\|name:[^|]*([^\\/\\\\]*SDXL[^\\/\\\\]*)$(?&lt;![\\/\\\\]).*$\n</code></pre> <p>As this is already advanced usage, the assumption is made that the user knows how to use RegEx, so no further examples or explanations of the provided RegEx are given here. Tools like https://regex101.com/ can be used to create, test, understand and debug a regular expression.</p>"},{"location":"FAQ/","title":"FAQ: Frequently asked Questions","text":""},{"location":"FAQ/#where-is-the-png-info-tab","title":"Where is the \"PNG Info\" tab?","text":"<p>The functionality is integrated into the \"Process Image\" tabs, removing the need for a separate \"PNG Info\" tab.</p>"},{"location":"FAQ/#where-are-the-command-line-flags-like-xformers","title":"Where are the command-line flags like --xformers?","text":"<p>Most command-line flags have been moved to UI \u2192 Settings. For a complete list, start the web UI with the --help flag.</p>"},{"location":"FAQ/#how-can-i-get-more-information-about-whats-happening-with-my-app","title":"How can I get more information about what\u2019s happening with my app?","text":"<p>Launch the app with the --debug flag and review the setup.log file for detailed insights.</p>"},{"location":"FAQ/#how-do-i-add-command-line-options-to-webuibat-in-windows","title":"How do I add command-line options to webui.bat in Windows?","text":"<p>1. Right-click on webui.bat and select \"Create Shortcut\". 2. Right-click the shortcut and open \"Properties\". 3. Add the options at the end of the \"Target\" field after a space. Example: <code>\"C:\\path\\to\\webui.bat\" --medvram --autolaunch</code> 4. Click OK and use this shortcut to start the app.</p>"},{"location":"FAQ/#how-do-i-use-an-amd-gpu-on-windows","title":"How do I use an AMD GPU on Windows?","text":"<p>Add the --use-directml command-line flag when starting the app.</p>"},{"location":"FAQ/#how-can-i-create-large-images-eg-2048x2048-with-limited-vram","title":"How can I create large images (e.g., 2048x2048) with limited VRAM?","text":"<p>Render a smaller image (e.g., 512x512) and upscale it using the \"Process Image\" tab with an upscaler like SwinIR_4x. This method tiles the image to minimize VRAM usage.</p>"},{"location":"FAQ/#why-are-my-images-dull-or-have-incorrect-colors","title":"Why are my images dull or have incorrect colors?","text":"<p>Try a different VAE file. Add ,sd_vae to the Settings \u2192 User Interface \u2192 Quicksettings list to enable quick VAE selection. Place new VAE files in the models\\VAE folder.</p>"},{"location":"FAQ/#how-do-i-update-to-the-latest-version","title":"How do I update to the latest version?","text":"<p>Run webui.bat --upgrade. This pulls the latest updates and applies them to your installation.</p>"},{"location":"FAQ/#something-broke-how-do-i-restore-my-sdnext-installation","title":"Something broke. How do I restore my SD.Next installation?","text":"<p>Use webui.bat --reinstall to reinstall required components or webui.bat --reset to reset to the latest version.</p>"},{"location":"FAQ/#my-image-folder-is-too-large-how-do-i-manage-storage","title":"My image folder is too large. How do I manage storage?","text":"<p>In Settings \u2192 Image Options, switch from PNG to JPG for smaller file sizes and disable \"Always save all generated images.\" Save only desired images manually.</p>"},{"location":"FAQ/#i-keep-getting-out-of-memory-errors-what-can-i-do","title":"I keep getting out-of-memory errors. What can I do?","text":"<p>1. Render smaller images (512x512) and upscale later. 2. In Settings \u2192 Compute Settings, enable FP16 precision. 3. Use memory-efficient Cross-Attention Optimization methods like Sub-quadratic attention. Disable \"SDP disable memory attention\" if enabled.</p>"},{"location":"FAQ/#why-are-my-images-distorted-eg-two-heads-merged-people","title":"Why are my images distorted (e.g., two heads, merged people)?","text":"<p>Stable Diffusion models are typically trained on 512x512 images. Rendering at different aspect ratios or sizes may cause artifacts. Stick to standard sizes or upscale afterward.</p>"},{"location":"FAQ/#what-does-the-hires-fix-option-do","title":"What does the \"Hires Fix\" option do?","text":"<p>\"Hires Fix\" upscales images during generation, avoiding post-generation artifacts from traditional upscalers. It complements, but does not replace, external upscalers.</p>"},{"location":"FAQ/#what-is-clip-skip","title":"What is CLIP Skip?","text":"<p>CLIP Skip adjusts how detailed image generation becomes. Higher skip values can result in less specific but potentially higher-quality images. Some models benefit from specific settings. PS: CLIP skip is not needed while using SDXL in most cases.</p>"},{"location":"FAQ/#what-is-the-best-sampler-to-use","title":"What is the best sampler to use?","text":"<p>There isn\u2019t a definitive best, but \"DPM++ 2M Karras\" is commonly recommended as a reliable general-purpose sampler.</p>"},{"location":"FAQ/#how-do-i-organize-checkpoint-ckpt-files","title":"How do I organize checkpoint (ckpt) files?","text":"<p>Create subfolders in the models directory and move ckpt files there. Restart the UI for changes to take effect. Use meaningful categories like \"Photorealistic\" or \"Anime\".</p>"},{"location":"FAQ/#how-do-i-enable-auto-updates-on-startup","title":"How do I enable auto-updates on startup?","text":"<p>Add the --upgrade flag to your webui.bat launch parameters to automatically update on every startup.</p>"},{"location":"FAQ/#why-do-i-get-an-error-related-to-typing-extensions-when-running-sdnext-for-the-first-time","title":"Why do I get an error related to typing-extensions when running SD.Next for the first time?","text":"<p>This error is caused by a recent upstream Python library conflict.</p> <p>Solution: Simply re-run webui.bat or webui.sh. The issue will resolve automatically, and SD.Next will work as expected.</p>"},{"location":"FAQ/#why-is-my-clip-interrogator-not-working-or-causing-errors","title":"Why is my clip-interrogator not working or causing errors?","text":"<p>The old clip-interrogator is incompatible with the newer transformers package required by SD.Next.</p> <p>Solution: If you manually installed clip-interrogator as an extension, remove it. SD.Next now includes an updated version of clip-interrogator for new installations. To manually update, run: <code>git submodule set-url extensions-builtin/clip-interrogator-ext Dahvikiin/clip-interrogator-ext.git webui.bat --upgrade</code> Then, re-launch SD.Next.</p>"},{"location":"FAQ/#why-do-i-get-an-error-stablediffusionxlpipeline-object-has-no-attribute-decode_first_stage","title":"Why do I get an error: 'StableDiffusionXLPipeline' object has no attribute 'decode_first_stage'?","text":"<p>This issue is related to the Image Previews feature.</p> <p>Solution: Ensure the preview method is not set to Full VAE. If the error persists, disable previews.</p>"},{"location":"FAQ/#what-does-this-error-mean-module-diffusers-has-no-attribute-stablediffusionxlpipeline","title":"What does this error mean: `module 'diffusers' has no attribute 'StableDiffusionXLPipeline'?","text":"<p>This happens when an extension has downgraded the required diffusers package.</p> <p>Solution: Disable all user-installed extensions and re-launch SD.Next.</p>"},{"location":"FAQ/#why-do-i-see-errors-with-xyz_gridpy-or-options-object-has-no-attribute-uni_pc_order","title":"Why do I see errors with xyz_grid.py or 'Options' object has no attribute 'uni_pc_order'?","text":"<p>These errors occur because some legacy variables are not initialized when using specific --backend arguments.</p> <p>Solution: Start SD.Next without any --backend arguments to initialize the variables. This issue will be addressed in future updates.</p>"},{"location":"FAQ/#which-hip-sdk-version-should-i-use","title":"Which HIP SDK Version Should I Use?","text":"<p>HIP SDK version 5.7.1 is recommended over version 6.1 due to its superior compatibility and more consistent performance in most scenarios.</p>"},{"location":"FLUX/","title":"Black Forest Labs FLUX.1","text":"<p>FLUX.1 family consists of 3 variations: - Pro   Model weights are NOT released, model is available only via Black Forest Labs - Dev   Open-weight, guidance-distilled from Pro variation, available for non-commercial applications - Schnell   Open-weight, timestep-distilled from Dev variation, available under Apache2.0 license  </p> <p>Additionally SD.Next includes pre-quantized variations of FLUX.1 Dev variation: <code>qint8</code>, <code>qint4</code> and <code>nf4</code> Pick variant that uses less memory as model in original form has very high requirements  </p> <p></p> <p>Important</p> <p>Allow gated access  This is a gated model, you need to accept the terms and conditions to use it  For more information see Gated Access Wiki</p> <p>Important</p> <p>Set offloading  Set appropriate offloading setting before loading the model to avoid out-of-memory errors  For more information see Offloading Wiki </p> <p>Important</p> <p>Choose quantization  Check compatibility of different quantizations with your platform and GPU!  For more information see Quantization Wiki </p> <p>Tip</p> <p>Use reference models  Use of reference models is recommended over manually downloaded models!  Simply select it from Networks -&gt; Models -&gt; Reference </p> <p>and model will be auto-downloaded on first use  </p> <p>Important</p> <p>Do not attempt to assemble a full model by loading all individual components  That may be how some other apps are designed to work, but its not how SD.Next works  Always load full model and then replace individual components as needed  </p> <p>Warning</p> <p>If you're getting error message during model load: <code>file=xxx is not a complete model</code>  It means exactly that - you're trying to load a model component instead of full model  </p>"},{"location":"FLUX/#components","title":"Components","text":"<p>FLUX.1 model consists of: - Unet/Transformer: MMDiT - Text encoder 1: CLIP-ViT/L, - Text encoder 2: T5-XXL Version 1.1 - VAE</p> <p>When using reference models, all components will be loaded as needed. If using manually downloaded model, you need to ensure that all components are correctly configured and available. Note that majority of available downloads are not actually all-in-one models and are instead just a part of the full model with individual components.</p> <p>Tip</p> <p>For convience, you can add setting that allow quick replacements of model components  to your quicksettings by adding Settings -&gt; User Interface -&gt; Quicksettings  list -&gt; sd_model_checkpoint, sd_unet, sd_vae, sd_text_encoder</p> <p></p>"},{"location":"FLUX/#fine-tunes","title":"Fine-tunes","text":""},{"location":"FLUX/#diffusers","title":"Diffusers","text":"<p>There are already many FLUX.1 unofficial variations available Any Diffuser-based variation can be downloaded and loaded into SD.Next using Models -&gt; Huggingface -&gt; Download For example, interesting variation is a merge of Dev and Schnell variations by sayakpaul: sayakpaul/FLUX.1-merged </p>"},{"location":"FLUX/#loras","title":"LoRAs","text":"<p>SD.Next includes support for FLUX.1 LoRAs  </p> <p>Since LoRA keys vary significantly between tools used to train LoRA as well as LoRA types, support for additional LoRAs will be added as needed - please report any non-functional LoRAs!</p> <p>Also note that compatibility of LoRA depends on the quantization type! If you have issues loading LoRA, try switching your FLUX.1 base model to different quantization type  </p>"},{"location":"FLUX/#all-in-one","title":"All-in-one","text":"<p>Typical all-in-one safetensors file is over 20GB in size and contains full model with transformer, both text-encoders and VAE Since text encoders and VAE are same between all FLUX.1 models, using all-in-one safetensors is not recommended due to large duplication of data  </p>"},{"location":"FLUX/#unettransformer","title":"Unet/Transformer","text":"<p>Unet/Transformer component of FLUX.1 is a typical model fine-tune and is around 11GB in size  </p> <p>To load a Unet/Transformer safetensors file: 1. Download <code>safetensors</code> or <code>gguf</code> file from desired source and place it in <code>models/UNET</code> folder    example: FastFlux Unchained 2. Load FLUX.1 model as usual and then 3. Replace transformer with one in desired safetensors file using: Settings -&gt; Execution &amp; Models -&gt; UNet </p>"},{"location":"FLUX/#text-encoder","title":"Text Encoder","text":"<p>SD.Next allows changing optional text encoder on-the-fly  </p> <p>Go to Settings -&gt; Models -&gt; Text encoder and select the desired text encoder T5 enhances text rendering and some details, but its otherwise very lightly used and optional Loading lighter T5 will greatly decrease model resource usage, but may not be compatible with all offloading modes  </p> <p>Tip</p> <p>To use prompt attention syntax with FLUX.1, set  </p> <p>Settings -&gt; Execution -&gt; Prompt attention to xhinker**</p> <p>Example image with different encoder quantization options </p>"},{"location":"FLUX/#vae","title":"VAE","text":"<p>SD.Next allows changing VAE model used by FLUX.1 on-the-fly There are no alternative VAE models released, so this setting is mostly for future use  </p> <p>Tip</p> <p>To enable image previews during generate, set Settings -&gt; Live Preview -&gt; Method to TAESD**  </p> <p>To further speed up generation, you can disable \"full quality\" which triggers use of TAESD instead of full VAE to decode final image  </p>"},{"location":"FLUX/#scheduler","title":"Scheduler","text":"<p>FLUX.1 at the moment supports only Euler FlowMatch scheduler, additional schedulers will be added in the future Due to specifics of flow-matching methods, number of steps also has strong influence on the image composition, not just on the way how its resolved</p> <p>Example image at different steps </p> <p>Additionally, sampler can be tuned with shift parameter which roughly modifies how long does model spend on composition vs actual diffusion  </p> <p>Example image with different sampler shift values </p>"},{"location":"FLUX/#controlnet","title":"ControlNet","text":"<p>Support for all InstantX/Shakker-Labs models including Union-Pro</p> <p>FLUX.1 ControlNets are large at over 6GB on top of already very large FLUX.1 model as such, you may need to use offloading:sequential which is not as fast, but uses far less memory  </p> <p>When using union model, you must also select control mode in the control unit  </p>"},{"location":"FLUX/#flux-tools","title":"Flux Tools","text":"<p>Link to Flux Tools announcement - Redux is actually a tool - Fill is inpaint/outpaint optimized version of Flux-dev - Canny/Depth are optimized versions of Flux-dev for their respective tasks: they are not ControlNets that work on top of a model  </p> <p>To use, go to image or control interface and select Flux Tools in scripts All models are auto-downloaded on first use note: All models are gated and require acceptance of terms and conditions via web page recommended: Enable on-the-fly quantization or compression to reduce resource usage - Redux: ~0.1GB   works together with existing model and basically uses input image to analyze it and use that instead of prompt recommended: low denoise strength levels result in more variety - Fill: ~23.8GB, replaces currently loaded model note: can be used in inpaint/outpaint mode only - Canny: ~23.8GB, replaces currently loaded model recommended: guidance scale 30 - Depth: ~23.8GB, replaces currently loaded model recommended: guidance scale 10  </p>"},{"location":"FLUX/#notes","title":"Notes","text":""},{"location":"FLUX/#performance","title":"Performance","text":"<p>Performance and memory usage of different FLUX.1 variations:</p> dtype time (sec) performance memory offload note bf16 &gt;32 GB none *1 bf16 50.47 0.40 it/s balanced *2 bf16 94.28 0.21 it/s 1.89 GB sequential nf4 14.69 1.36 it/s 17.92 GB none nf4 21.02 0.95 it/s balanced *2 nf4 sequential err qint8 15.42 1.30 it/s 18.85 GB none qint8 balanced err qint8 sequential err qint4 18.37 1.09 it/s 11.38 GB none qint4 balanced err qint4 sequential err <p>Notes: - 1: Memory usage exceeeds 32GB and is not recommended - 2: Balanced offload VRAM usage is not included since it depends on desired threshold  </p>"},{"location":"Features/","title":"Features","text":""},{"location":"Features/#control","title":"Control","text":"<p>SDNext's Control tab is our long awaited effort to bring ControlNet, IP-Adapters, T2I Adapter, ControlNet XS, and ControlNet LLLite to our users.  </p> <p>After doing that, we decided that we would add everything else under the sun that we could squeeze in there, and place it directly into your hands with greater options and flexibility than ever before, to allow you to Control your image and video generation with as little effort, as as much power, as possible.</p> <p>Note that this document is a work in progress, it's all quite complex and will take some time to write up a bit more as well as smooth out the rough edges and correct any issues and bugs that pop up, expect frequent updates! </p> <p>This guide will attempt to explain how to use it so that anyone can understand it and put it to work for themselves.  </p> <p>Be sure to also check out the Control resource page which has more technical information as well as some general tips and suggestions. Its usage information will be merged into this page soon\u2122\ufe0f.</p> <p>We'll start with the... Control Controls!</p>"},{"location":"Features/#controls","title":"Controls","text":""},{"location":"Features/#input","title":"Input","text":"<p>The Input control is exactly what it sounds like, it controls what input images (or videos) are contributing to your image generation, by default that is just the image in Control input pane, however if you select <code>Separate init image</code>, another image pane will appear below, allowing you to use that as well.  </p> <p>Note: When using a Control input image as well as a Init input image, the Init input dominates. Adjusting denoise to &gt;=0.9 is recommended, as that will allow the Control input to balance with the Init input. Higher values will increase the strength of Control input further, giving it dominance.  </p> <p></p> <p><code>Show Preview</code> is simple, it controls the visibility of the preview window in the far right of the middle row. You'll want this on if you're doing any kind of masking or manipulations that you would want to preview before generating.</p> <p>There are 3 different Input types:</p> <ul> <li> <p><code>Control only</code>: This uses only the Control input below as a source for any ControlNet or IP Adapter type tasks based on any of our various options.  </p> </li> <li> <p><code>Init image same as control</code>: This option will additionally treat any image placed into the <code>Control input</code> pane as a source for img2img type tasks, an image to modify for example.  </p> </li> <li> <p><code>Separate init image</code>: This option creates an additional window next to <code>Control input</code> labeled <code>Init input</code>, so you can have a separate image for both Control operations and an init source.</p> </li> </ul> <p><code>Denoising strength</code> is the same as if you were doing any img2img operation. The higher the value, the more denoising that will take place, and the greater any source image will be modified.</p>"},{"location":"Features/#size","title":"Size","text":"<p>This can be a little confusing at first because of the <code>Before</code> and <code>After</code> subtabs, however it's really quite simple and extremely powerful. The Control size menu allows you to manipulate the size of your input images before and after inference takes place.  </p> <p></p> <p>The <code>Before</code> subtab does 2 things:</p> <ul> <li> <p>If you do not select any <code>Resize method</code>, it is only controlling the output image size width and height in pixels as it would in any text2img or img2img operation.</p> </li> <li> <p>However, if you do select a <code>Resize method</code>, Nearest for example, you can upscale or downscale the <code>Control input</code> image before any other operations take place. This will be the size of any image used in further operations. Second Pass is not entirely functional yet, but will be part of this.</p> </li> </ul> <p>For example, you might have a much larger image, such as 2048x3072, that you want to use with canny or depth map, but you do not want an image that large to manipulate or guide your generation, that would be prohibitive, slower, and possibly cause an OOM.  </p> <p>This is where <code>Resize method</code> comes in, you would simply select a resize method, typically Nearest or Lanczos, and then either set the pixel width or height you want to resize to under Fixed, or switch over to Scale and select a number below 1. A setting of 0.5 would make your input image effectively 1024x1536 pixels, which would be used as input for later operations.  </p> <p>The <code>After</code> subtab controls any upscaling or downscaling that would take place at the end of your image generation process, most commonly this would either be latent upscaling, and ESRGAN model such as 4x Ultrasharp, or one of the various chaiNNer models we provide. This is the same as it would be in a standard upscaling via text2img or img2img.</p>"},{"location":"Features/#mask","title":"Mask","text":"<p>The Mask controls are where we start getting into the real meat of Control, not only does it allow a plethora of different options to mask, segment, and control the view of your masking with various preview types, but it comes with 22 different colormaps for your viewing pleasure! (And I think vlad made some of those words up \ud83e\udd2b)  </p> <p></p> <ul> <li> <p><code>Live update</code>: With this checked, your masking will update as you make changes to it, if this is off, you will need to hit the <code>Refresh</code> button to the right to have your preview pane update, making more changes to it while it is processing may lead to it being desynchronized, just hit the refresh button if it does not look correct.</p> </li> <li> <p><code>Inpaint masked only</code>: Inpainting will apply only to areas you have masked if this is checked. You must actually inpaint something, otherwise it's just img2img.</p> </li> <li> <p><code>Invert mask</code>: Inverts the masking, things you mark with the brush will be excluded from a full mask of the image.</p> </li> <li> <p><code>Auto-mask</code>: There are three options here, Threshold, Edge, and Greyscale. Each provides a different method of auto-masking your images.</p> </li> <li> <p><code>Auto-segment</code>: Just like Auto-mask, we have provided an extensive list of Auto-segmentation models, they don't require ControlNet to handle the process, but may take a few seconds to process, depending on your GPU.  </p> </li> <li> <p><code>Preview</code>: You can select the preview type here, we have provided 5 modes, Masked, Binary, Greyscale, Color, and Composite, which is the default.</p> </li> <li> <p><code>Colormap</code>: You can select the style/color scheme of the preview here. There are 22 fantastic color schemes!</p> </li> <li> <p><code>Blur</code>: This blurs the edges of what you have masked, to allow some flexibility. Play with it.</p> </li> <li> <p><code>Erode</code>: This slider controls the reduction of your auto-masking or auto-segmentation border.</p> </li> <li> <p><code>Dilate</code>: This slider controls the expansion of your auto-masking or auto-segmentation border.</p> </li> </ul>"},{"location":"Features/#video","title":"Video","text":"<p>The Video controls are quite exciting and fun to play with, with our tools now you can, if you wished, turn any video into an anime version for example, frame by frame. There are three output options, GIF, PNG, and MP4. You must select one of these to have video output. With these simple controls, you can tweak your video output with surprising flexibility. Some video output methods provide more controls, try them all.</p> <p></p> <ul> <li> <p><code>Skip input frames</code>: This setting controls how many frames are processed from input instead of every frame. Setting it to 0 would mean processing every frame, a setting of 1 would process every other frame, a setting of 2 would process every third frame, cutting the number of total frames by 2/3rds, and so on.</p> </li> <li> <p><code>Video file</code>: You select the type of output you want here, animated GIF (not JIF!), animated PNG, or MP4 video, all provided via FFMPEG of course.  </p> </li> <li> <p><code>Duration</code>: The length in seconds you want your output video to be.  </p> </li> <li> <p><code>Pad frames</code>: Determine how many frames to add to the beginning and end of the video. This feature is particularly useful when used with interpolation.</p> </li> <li> <p><code>Interpolate frames</code>: The number of frames you want interpolated (via RIFE) between existing frames (filtered by skip input frames) in a video sequence. This smoothens the video output, especially if you're skipping frames to avoid choppy motion or low frame rates.</p> </li> <li> <p><code>Loop</code>: This is purely for animated GIF and PNG output, it enables the classic looping that you would expect.</p> </li> </ul> <p>When you're using interpolation, the software also detects scene changes. If the scene changes significantly, it will insert pad frames instead of interpolating between two unrelated frames. This ensures a seamless transition between scenes and maintains the overall quality of the video output.</p>"},{"location":"Features/#extensions","title":"Extensions","text":"<p>These are some nice goodies that we have cooked up so that no actual installed extensions are necessary, you may even find that our version works better!</p>"},{"location":"Features/#animatediff","title":"AnimateDiff","text":"<p>This is the new home of our Vlad-created implementation of the AnimateDiff extension. Now with FREE FreeInit!  </p> <p>I honestly don't know how to use this, so I'll update this when I do. My apologies! But if you already do, enjoy!</p> <p></p>"},{"location":"Features/#ip-adapter","title":"IP-Adapter","text":"<p>This is our IP Adapter implementation, with 10 available models for your image or face cloning needs!</p> <p></p>"},{"location":"Features/#image-panes","title":"Image Panes","text":"<p>You may notice small icons above the image panes that look like pencils, these are Interrogate buttons. The left one is BLIP, and the right one is DeepBooru. Click one of the buttons to interrogate the image in the pane below it. The results will appear in your prompt area.</p> <p></p>"},{"location":"Features/#control-input","title":"Control Input","text":"<p>This is the heart of Control, you may put any image or even video here to be processed by our system, that means any and all scripts, extensions, even the various Controlnet variants below, though you can individually add guidance images to each of those. If an image is placed here, the system will assume you are performing an img2img process of some sort. If you upload a video to SDNext via the Control input pane, you will see that you can play the video, both input and resultant output. Batching and folders should work as expected.</p> <p>Note below there are 2 other buttons, Inpainting and Outpainting, below.</p> <p></p>"},{"location":"Features/#controlnet","title":"ControlNet+","text":"<p>At the very bottom of the Control page, we have what you've all been waiting for, full ControlNet! I do mean full too, we have it all! This includes SD and SD-XL. at last! You won't ever need the ControlNet extension ever again, much less to touch the original LDM backend.  </p> <p>This will take a bit more work to document example workflows, but there are tooltips, and if you've used ControlNet before, you shouldn't have any problems! However if you do, hop on by our Discord server and we're happy to help.</p> <p> </p>"},{"location":"Features/#processvisual-query","title":"Process/Visual query","text":"<p>Visual query subsection of the Process tab contains tools to use Visual Question Answering interrogation of images using Vision Language Models.</p> <p>Currently supported models:</p> <ul> <li>Moondream 2</li> <li>GiT Textcaps</li> <li>GIT VQA</li> <li>Base</li> <li>Large</li> <li>Blip</li> <li>Base</li> <li>Large</li> <li>ViLT Base</li> <li>Pix Textcaps</li> <li>MS Florence 2</li> <li>Base</li> <li>Large</li> </ul> <p> </p>"},{"location":"Features/#lcm","title":"LCM","text":"<p>LCM (Latent Consistency Model) is a new feature that provides support for SD 1.5 and SD-XL models.</p>"},{"location":"Features/#installation","title":"Installation","text":"<p>Download the LCM LoRA models and place them in your LoRA folder (models/lora or custom):</p> <ul> <li>For SD 1.5: lcm-lora-sdv1-5</li> <li>For SD-XL: lcm-lora-sdxl</li> </ul> <p>As they have the same name, we recommend doing them one at a time and then renaming it before downloading the next.  </p>"},{"location":"Features/#usage","title":"Usage","text":"<ol> <li>Make sure to use the Diffusers backend in SDNext, Original backend will NOT WORK</li> <li>Load your preferred SD 1.5 or SD-XL model that you want to use LCM with</li> <li>Load the correct LCM lora (lcm-lora-sdv1-5 or lcm-lora-sdxl) into your prompt, ex: <code>&lt;lora:lcm-lora-sdv1-5:1&gt;</code></li> <li>Set your sampler to LCM</li> <li>Set number of steps to a low number, e.g. 4-6 steps for SD 1.5, 2-8 steps for SD-XL</li> <li>Set your CFG Scale to 1 or 2 (or somewhere between, play with it for best quality)</li> <li>Optionally, turning on Hypertile and/or FreeU will greatly increase speed and quality of output images</li> <li>???</li> <li>Generate!</li> </ol>"},{"location":"Features/#notes","title":"Notes","text":"<ul> <li>This also works with latent upscaling, as a second pass/hires fix.</li> <li>LCM scheduler does not support steps higher than 50</li> <li>The <code>cli/lcm-convert.py</code> script can convert any SD 1.5 or SD-XL model to an LCM model by baking in the LoRA and uploading to Huggingface</li> </ul>"},{"location":"Features/#lora","title":"LoRa","text":""},{"location":"Features/#introduction","title":"Introduction","text":"<p>LoRA models are small Stable Diffusion models that apply tiny changes to standard checkpoint models. They are usually 10 to 100 times smaller than checkpoint models. That makes them very attractive to people who have an extensive collection of models.</p> <p>This is a tutorial for beginners who haven\u2019t used LoRA models before. You will learn what LoRA models are, where to find them, and how to use them in the SD.NEXT WebUI.</p> <p>You can place your LoRA models in <code>./*Your SD.NEXT directory*/models/Lora</code>. (You can change the path of the LoRa directory in your settings in the system paths tab)</p> <p>You can either access your LoRA models by clicking on Networks and then on Lora and select the lora model you want to add to your prompt or by typing:  <code>&lt;lora:*lora file name*:*preferred weight*&gt;</code></p> <p>The weight indicates the amount of effect it has on your image generation.</p>"},{"location":"Features/#notes_1","title":"Notes","text":"<ul> <li>Some LoRa's are for different diffusers pipelines, for example you have SD1.5 LoRa's and you have SDXL LoRa's, if you try to use one of these while using the wrong type diffusers pipeline it will give an error.</li> </ul>"},{"location":"Features/#hidiffusion","title":"HiDiffusion","text":""},{"location":"Features/#introduction_1","title":"Introduction","text":"<p>Diffusion models are great for high-resolution image synthesis but struggle with object duplication and longer generation times at higher resolutions. HiDiffusion, a solution with two key components: Resolution-Aware U-Net (RAU-Net), which prevents object duplication by adjusting feature map sizes, and Modified Shifted Window Multi-head Self-Attention (MSW-MSA), which reduces computation time. HiDiffusion can be added to existing models to generate images up to 4096\u00d74096 pixels at 1.5-6 times faster speeds. Experiments show that HiDiffusion effectively tackles these issues and sets new standards for high-resolution image synthesis.</p>"},{"location":"Features/#reference","title":"Reference","text":"<p>You can read more about HiDiffusion in the link below:</p> <ul> <li>https://hidiffusion.github.io/</li> </ul>"},{"location":"Features/#benefits-of-using-hidiffusion","title":"Benefits of using HiDiffusion","text":"<ul> <li>Increases the resolution and speed of pretrained diffusion models.</li> <li>Supports txt2image, img2img, inpainting and more.</li> </ul>"},{"location":"Features/#how-to-enable","title":"How to enable","text":"<p>Check the HiDiffusion checkbox in the SD.NEXT webUI in either the Text, Image or Control tab.</p> <p> </p>"},{"location":"Features/#face-restore","title":"Face restore","text":""},{"location":"Features/#introduction_2","title":"Introduction","text":"<p>Face restore will try to detect a face or multiple faces in a generated image, then it will do a seperate pass over the face which makes the face have an higher resolution and more detailed.</p>"},{"location":"Features/#models","title":"Models","text":"<p>SD.NEXT has 3 different choices for face restoration:</p> <ul> <li>Codeformer</li> <li>GFPGAN</li> <li>Face HiRes</li> </ul>"},{"location":"Features/#codeformer","title":"Codeformer","text":"<p>CodeFormer, created by sczhou, is a robust face restoration algorithm designed to work with both old photos and AI-generated faces. The underlying technology of CodeFormer is based on a Transformer-based prediction network, which models global composition and context for code prediction. This allows the model to discover natural faces that closely approximate the target faces, even when the inputs are severely degraded. A controllable feature transformation module is also included, which enables a flexible trade-off between fidelity and quality. More here: Codeformer.</p> <p>The CodeFormer weight parameter:</p> <p>0 = Maximum effect; 1 = Minimum effect.</p>"},{"location":"Features/#gfpgan","title":"GFPGAN","text":"<p>GFPGAN stands for \"Generative Facial Prior Generative Adversarial Network\". It is an artificial intelligence model developed for the purpose of real-world face restoration. The model is designed to repair and enhance faces in photos, particularly useful for restoring old or damaged photographs. GFPGAN leverages generative adversarial networks (GANs), specifically utilizing facial priors encapsulated in a pre-trained face GAN like StyleGAN2, to restore realistic and faithful facial details. More here: GFPGAN.</p>"},{"location":"Features/#face-hires","title":"Face HiRes","text":"<p>Face Hires is a feature that aims to improve the details of faces in generated images. It draws inspiration from the popular Adetailer extension, but simplifies the workflow to a single checkbox, making it easy to enable or disable.</p> <p>Here's what Face Hires does:</p> <ul> <li>Detection: Identifies and locates faces in the image.</li> <li>Cropping and Resizing: Crops each detected face and enlarges it to become a full image.</li> <li>Enhancement: Applies an image-to-image (img2img) process to enhance the face.</li> <li>Restoration: Resizes the enhanced face back to its original size and integrates it back into the original image.</li> </ul> <p>This process addresses the common issue where models fail to perfectly resolve details in images, especially for faces that are not front and center.</p>"},{"location":"Features/#parameters","title":"Parameters","text":"<p>Face hires will use secondary prompt for face restore step if its present. if its not, it will use normal primary prompt.</p> <p>Face hires has number of tunable paramteters in settings in the postprocessing tab.</p> <ul> <li>Minimum confidence: minimum score that each detected face must meet during detection phase.</li> <li>Max faces: maximum number of faces per image it will try to run on.</li> <li>Max face overlap: maximum overlap of when multiple faces are detected before it considers them a single object.</li> <li>Min face size: minimum face size it should attempt to fix (e.g. do not try to fix very small faces in the background).</li> <li>Max face size: maximum face size it should attempt to fix (e.g. why try to fix something if its already front-and-center and takes 90% of image).</li> <li>Face padding: when cropping face, add padding around it. </li> </ul> <p> </p>"},{"location":"Features/#second-pass","title":"Second pass","text":""},{"location":"Features/#introduction_3","title":"Introduction","text":"<p>Second pass means that after an image is finished with it's first pass, it has the function to do another pass, Second pass.</p>"},{"location":"Features/#usage_1","title":"Usage","text":"<p>You can enable the second pass on the refine tab in ther generation settings. </p> <p>You can customize the second pass to your needs: </p> <ul> <li>Upscaler: Here you can choose a upscaler model.</li> <li>Rescale by: Here you can choose how much the resolution of your image gets multiplied. (You can also choose a custom resolution at Width resize and Height resize).</li> <li>Force HiRes: This will force it to execute HiRes fix, that means it will not only multiply the resolution of the image, but also add more detail to it, you can also use HiRes fix without forcing it by selecting a latent upscaling method.</li> <li>Secondary sampler: Here you can choose a different sampler that will be used during HiRes fix, you can also let it use the same one as the first pass by setting it to \"Same as primary\".</li> <li>HiRes steps: How much steps you want your HiRes fix to take.</li> <li>Strength: Strength is basically the amount that HiRes Fix or Refiner is allowed to change.</li> <li>Refiner start: Refiner pass will start when base model is this much complete. (Set bigger than 0 and smaller than 1 to run after full base model run).</li> <li>Refiner steps: How much steps you want your Refiner pass to take.</li> <li>Secondary prompts: You can also add an secondary positive and negative prompt to your HiRes fix, so you can make a base with the first pass and add the details in the second pass.</li> </ul>"},{"location":"Features/#refiner","title":"Refiner","text":"<p>Refiner will only start if you have selected a refiner model in \"Execution &amp; Models\" tab in the settings:</p> <p> </p>"},{"location":"Features/#styles","title":"Styles","text":""},{"location":"Features/#introduction_4","title":"Introduction","text":"<p>Styles are prompt presets you can enable, it saves both headaches and time, because it adds a specific style to your prompt without you having to type it yourself.</p>"},{"location":"Features/#usage_2","title":"Usage","text":"<p>You can select a style under the generation controls:</p>"},{"location":"Features/#adding-your-own-styles","title":"Adding your own styles","text":"<p>You can add your own styles in <code>.\\*Your SD.NEXT directory*\\models\\styles</code></p> <p> </p>"},{"location":"Features/#clip-skip","title":"Clip skip","text":""},{"location":"Features/#introduction_5","title":"Introduction","text":"<p>Clip Skip plays a significant role in stable diffusion models. It is a technique used to improve the performance of image and video compression in stable diffusion algorithms. By allowing the skipping of certain pixels or blocks, Clip Skip reduces the amount of data that needs to be processed, resulting in faster and more efficient compression. This technique also helps to reduce artifacts and enhance the overall quality of compressed images and videos.</p>"},{"location":"Features/#usage_3","title":"Usage","text":"<p>You can enable clip skip in the advanced tab in the image generation settings. The default is 1, but also a very popular value is 2 and lots of models are compatible with it. </p> <p> </p>"},{"location":"Features/#embedding","title":"Embedding","text":""},{"location":"Features/#introduction_6","title":"Introduction","text":"<p>Embedding, also called textual inversion, is an alternative way to control the style of your images in Stable Diffusion. Embedding is the result of textual inversion, a method to define new keywords in a model without modifying it. The method has gained attention because its capable of injecting new styles or objects to a model with as few as 3 -5 sample images.</p>"},{"location":"Features/#usage_4","title":"Usage","text":"<p>You can either access the embedding in the networks menu or you can type it yourself by writing the embedding name without the file extensions. (You can also edit the weight of the embedding by writing <code>(*Embedding name*:*desired weight*)</code>). </p>"},{"location":"Features/#add-embeddings","title":"Add embeddings","text":"<p>You can add your embeddings in <code>.\\*Your SD.NEXT directory*\\models\\embeddings</code></p> <p> </p>"},{"location":"Features/#upscaling","title":"Upscaling","text":""},{"location":"Features/#introduction_7","title":"Introduction","text":"<p>Upscaling is when an image is upscaled using stable diffusion, the algorithm analyzes the image's pixel values to determine the diffusion rate. The rate calculated is then used to expand the pixels in higher resolution, resulting in a sharper and clearer image without compromising its quality.</p>"},{"location":"Features/#usage_5","title":"Usage","text":"<p>In SD.NEXT there are 2 ways to use upscaling, you can either enable it in the second pass(this will not upscale the end results from the second pass, it will only upscale the image or latent from the first pass) or you can use it in the process tab under the upscaling menu in the image generation settings.</p> <p></p> <p></p>"},{"location":"Features/#custom-upscale-models","title":"Custom upscale models","text":"<p>You can also add your own upscale models to SD.NEXT in the directories below:</p> <ul> <li>ESRGAN</li> <li>LDSR</li> <li>SCUNet</li> <li>SwinIR</li> <li>RealESRGAN</li> <li>chaiNNer </li> </ul> <p> You can find these directories in <code>.\\*Your SD.NEXT directory*\\models</code></p>"},{"location":"Features/#most-used","title":"Most used","text":"<p>General:</p> <ul> <li>Ultrasharp 4x</li> <li>RealESRGAN 4x+</li> </ul> <p>Anime:</p> <ul> <li>Animesharp 4x</li> <li>RealESRGAN 4x+ Anime6B</li> </ul> <p> </p>"},{"location":"Features/#samplers","title":"Samplers","text":""},{"location":"Features/#introduction_8","title":"Introduction","text":"<p>To produce an image, Stable Diffusion first generates a completely random image in the latent space. The noise predictor then estimates the noise of the image. The predicted noise is subtracted from the image. This process is repeated a dozen times. In the end, you get a clean image.</p> <p>This denoising process is called sampling because Stable Diffusion generates a new sample image in each step. The method used in sampling is called the sampler or sampling method.</p> <p>For more information you can click on the links below:</p> <ul> <li>Complete Samplers Guide</li> <li>Stable Diffusion Sampler Art</li> </ul> <p>Both guides explain in detail several different samplers and their capabilities along with their advantages and disadvantages and the appropiate amount of steps you should use with each sampler.</p> <p>Below is a screenshot of all the samplers SD.Next provides:</p> <p> </p>"},{"location":"Features/#pag","title":"PAG","text":""},{"location":"Features/#introduction_9","title":"Introduction","text":"<p>PAG or Pertubed Attention Guidance is like a modern/better version of CFG scale, although less universal as it cannot be applied in all circumstances. If it's applied, it will be added to image metadata and you'll see it in the log as <code>StableDiffusionPAGPipeline</code>. </p> <p>You can find more information about PAG by clicking on this.</p>"},{"location":"Features/#usage_6","title":"Usage","text":"<p>You can enable PAG by setting the attention guidance slider above 0. The attention guidance slider is located in the advanced tab of the image generation settings.  </p> <p> </p>"},{"location":"Features/#interrogate","title":"Interrogate","text":""},{"location":"Features/#introduction_10","title":"Introduction","text":"<p>Interrogation, or captioning helps us refine the prompts we use, enabling us to see how the AI system tags and classifies, and what terms it uses. By looking at these we can further refine our images to attain the concept we have in mind, or remove them via negative prompts.</p>"},{"location":"Features/#usage_7","title":"Usage","text":"<p>You can find the interrogate option on the process tab under \"Interrogate image\" or \"Interrogate batch\" if you want to interrogate a batch of images, then select the CLIP Model you want to use to interrogate the image(s). </p> <p> </p>"},{"location":"Features/#vae","title":"VAE","text":""},{"location":"Features/#introduction_11","title":"Introduction","text":"<p>VAE or Variational Auto Encoder encodes an image into a latent space, which is a lower-dimensional representation of the image. The latent space is then decoded into a new image, which is typically of higher quality than the original image.   There are two main types of VAEs that can be used with Stable Diffusion: exponential moving average (EMA) and mean squared error (MSE).  EMA is generally considered to be the better VAE for most applications, as it produces images that are sharper and more realistic. MSE can be used to produce images that are smoother and less noisy, but it may not be as realistic as images generated by EMA.</p>"},{"location":"Features/#usage_8","title":"Usage","text":"<p>You can change the VAE in the settings in \"Execution &amp; Models\".   You can add the VAE's in <code>.\\*Your SD.NEXT directory*\\models\\VAE</code>.</p> <p> </p>"},{"location":"Features/#cfg-scale","title":"CFG scale","text":""},{"location":"Features/#introduction_12","title":"Introduction","text":"<p>the CFG scale (classifier-free guidance scale) or guidance scale is a parameter that controls how much the image generation process follows the text prompt. The higher the value, the more the image sticks to a given text input.   But this does not mean that the value should always be set to maximum, as more guidance means less diversity and quality.</p>"},{"location":"Features/#usage_9","title":"Usage","text":"<p>Simply use the CFG scale slider in the image generation settings. </p> <p> </p>"},{"location":"Features/#live-preview","title":"Live Preview","text":""},{"location":"Features/#introduction_13","title":"Introduction","text":"<p>The live preview feature allows you to see the image before it is fully generated, so in other words you can see the progress of the image while it is generating.</p>"},{"location":"Features/#usage_10","title":"Usage","text":"<p>You can modify the live preview settings to your liking in settings &gt; Live Previews. </p> <p></p> <p>Live preview display period: The amount of steps it has to take before requesting a preview image. Live preview method: The method you want SD.NEXT to use to display preview images.</p>"},{"location":"Features/#methods","title":"Methods","text":""},{"location":"Features/#simple","title":"Simple","text":"<p>Very cheap approximation. Very fast compared to VAE, but produces pictures with 8 times smaller horizontal/vertical resolution and extremely low quality.</p>"},{"location":"Features/#approximate","title":"Approximate","text":"<p>Cheap neural network approximation. Very fast compared to VAE, but produces pictures with 4 times smaller horizontal/vertical resolution and lower quality.</p>"},{"location":"Features/#taesd","title":"TAESD","text":"<p>TAESD is very tiny autoencoder which uses the same \"latent API\" as Stable Diffusion's VAE*. TAESD can decode Stable Diffusion's latents into full-size images at (nearly) zero cost.</p>"},{"location":"Features/#full-vae","title":"Full VAE","text":"<p>Uses the entire VAE to decode the full resolution image as preview during the generation, this is by far the slowest of the other 3 options.</p> <p> </p>"},{"location":"Gated/","title":"Gated Models","text":""},{"location":"Gated/#huggingface-login","title":"Huggingface Login","text":"<p>Access to some models is gated by vendor and in those cases, you need to request access to model from the vendor. For this you need to have a valid Huggingface account: Login or Sign Up </p> <p>Huggingface login and/or access token is not required for non-gated models  </p>"},{"location":"Gated/#create-token","title":"Create Token","text":"<p>Note: This is a one-time operation as same access token is used for all gated models.</p> <p>Once you are logged in, create access token that an external application such as SD.Next can use to access Huggingface on your behalf:</p> <p>Go to: Huggingface -&gt; Profile -&gt; Settings -&gt; Access Token -&gt; Create new token Or use this link </p> <ul> <li>Token type: READ   Do not use fine-grained to avoid complications   Name is your choice  </li> <li>Create token   Copy the token and store it in a safe place  </li> </ul>"},{"location":"Gated/#add-token-to-sdnext","title":"Add Token to SD.Next","text":"<p>Go to: SD.Next -&gt; System -&gt; Settings -&gt; Diffusers - Paste the token in the Huggingface Token field  </p>"},{"location":"Gated/#requesting-access","title":"Requesting Access","text":"<p>Note: Requesting access must be done on individual per-model case</p> <p>Requesting access can be in the form of simply accepting vendors terms of service or filling a form to get access to the model or requesting access and waiting for approval. In all cases, you need to go to model page on Huggingface and follow instruction.  </p> <p>Examples: FLUX.1, SD3.5</p> <p>Once you have access, you can use the model in SD.Next as usual</p>"},{"location":"Getting-Started/","title":"Getting started","text":"<p>This section describes how to use sdnext with assumption of basic knowledge of stable Diffusion. This section will show you how easy it is to generate images with SDNext with a few clicks. As a user only adjust and click the settings highlighted and noted in red ink</p> <p>On running the <code>./webui.bat</code> the main page will load up which looks like the following below:   </p> <p>The following sections are important and need to be modified by the user for an image to be generated. All red sections selected with red ink are sections the user must adjust and implement.</p> <p>Base Model This is the base model you are going to use to generate an image with. This is also called a checkpoint. From the civitAi website there are hundreds if not thousands of models you can download and use. Note that models are typically very large typically in the range of ~2Gb to 30Gb of space. these models are typically stored in the models/stable-diffusion folder of your sdnext directory.</p> <p>Positive Prompt There are two prompts positive and negative. The positive prompts is where you write and describe the image and picture you wish to generate.</p> <p>Negative Prompt This is the section where you write descriptions and components you DON'T want in your image. For example if your are drawing an anime girl sitting on a desk, typical negative prompts can and could be making sure you have a detailed face or not having several hand and legs. Note that there are also embeddings you can use such as bad hands which will be written and placed here.</p> <p>Width and Height are the image size you wish to work with. For SD1.5 image typically you start with 512x512 or 512x768 images. Using images larger then the training size do not help in image generation and result in poor or horrible images generated. Typically the following image resolutions are used and desired based on the following</p> <p>Sampling Types: The sampler you wish to use that will generate the image itself along with the number of steps the sampler will use. increasing the steps does not improve image quality and only consumes more resources and may potentially degrade image quality as well therefore it is ideal to choose an appropiate number of steps. For example Euler A typically can generate images in 24 steps. Please see the Sampler section for more information on samplers.</p> <p>CFG Scale: How closely do you wish the AI to follow your prompt. Again an appropiate value should be chosen. If the CFG value is too low the AI is given lots of freedom and conversely if the CFG value is very high you restrict the AI and force it to follow your prompt more closely. Note that having too high a CFG value will result in image generation. Typical values for CFG are between 6-9. Any lower and higher typically does not produce any good quality image at all.</p> <p>Seed: The seed is the number. An unique number for every image that is generated. Setting it to -1 you will get a random new image everytime. If you wish to upload or use a pre-existing image say from civitAI, use the process tab and import the settings and you will see the seed value will be the number of the existing generated image.</p>"},{"location":"Hints/","title":"Hints","text":""},{"location":"Hints/#notes","title":"Notes","text":"<ul> <li>Below is a full list of hints as copied from <code>html/locale_en.json</code> </li> <li>Best way to submit additions/changes is to create PR with changes to <code>html/locale_en.json</code>   Alternatively, you can use discussions or issue tracker to suggest changes and they will be applied manually  </li> <li>Add or edit hints displayed for each UI item  </li> <li>Do not change label directly   if you have a suggestion for a better naming, place it in localized field so changes can be tracked  </li> <li>If you want to validate you JSON file before submitting,   save it locally and run <code>python cli/validate-locale.py</code> </li> <li>If you want to add additional hints, just follow the template   and create new entry where <code>label</code> field matches exactly what is visible in the UI  </li> </ul> <p>Full list of existing hints:</p>"},{"location":"Hints/#data","title":"Data","text":"<pre><code>{\"icons\": [\n  {\"id\":\"\",\"label\":\"\ud83c\udfb2\ufe0f\",\"localized\":\"\",\"hint\":\"Use random seed\"},\n  {\"id\":\"\",\"label\":\"\u267b\ufe0f\",\"localized\":\"\",\"hint\":\"Reuse previous seed\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udd04\",\"localized\":\"\",\"hint\":\"Reset values\"},\n  {\"id\":\"\",\"label\":\"\u2b06\ufe0f\",\"localized\":\"\",\"hint\":\"Upload image\"},\n  {\"id\":\"\",\"label\":\"\u2b05\ufe0f\",\"localized\":\"\",\"hint\":\"Reuse image\"},\n  {\"id\":\"\",\"label\":\"\u21c5\",\"localized\":\"\",\"hint\":\"Swap values\"},\n  {\"id\":\"\",\"label\":\"\u21e6\",\"localized\":\"\",\"hint\":\"Read parameters from last generated image\"},\n  {\"id\":\"\",\"label\":\"\u2297\",\"localized\":\"\",\"hint\":\"Clear prompt\"},\n  {\"id\":\"\",\"label\":\"\ud83d\uddc1\",\"localized\":\"\",\"hint\":\"Show/hide extra networks\"},\n  {\"id\":\"\",\"label\":\"\u21f0\",\"localized\":\"\",\"hint\":\"Apply selected styles to current prompt\"},\n  {\"id\":\"\",\"label\":\"\u21e8\",\"localized\":\"\",\"hint\":\"Apply preset to Manual Block Merge tab\"},\n  {\"id\":\"\",\"label\":\"\u21e9\",\"localized\":\"\",\"hint\":\"Save parameters from last generated image as style template\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udd6e\",\"localized\":\"\",\"hint\":\"Save parameters from last generated image as style template\"},\n  {\"id\":\"\",\"label\":\"\u21d5\",\"localized\":\"\",\"hint\":\"Sort by: Name asc/desc, Size largest/smallest, Time newest/oldest\"},\n  {\"id\":\"\",\"label\":\"\u27f2\",\"localized\":\"\",\"hint\":\"Refresh\"},\n  {\"id\":\"\",\"label\":\"\u2715\",\"localized\":\"\",\"hint\":\"Close\"},\n  {\"id\":\"\",\"label\":\"\u229c\",\"localized\":\"\",\"hint\":\"Fill\"},\n  {\"id\":\"\",\"label\":\"\u233e\",\"localized\":\"\",\"hint\":\"Load model as refiner model when selected, otherwise load as base model\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udd0e\ufe0e\",\"localized\":\"\",\"hint\":\"Scan CivitAI for missing metadata and previews\"},\n  {\"id\":\"\",\"label\":\"\u2632\",\"localized\":\"\",\"hint\":\"Change view type\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udcd0\",\"localized\":\"\",\"hint\":\"Measure\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udd0d\",\"localized\":\"\",\"hint\":\"Search\"},\n  {\"id\":\"\",\"label\":\"\ud83d\udd8c\ufe0f\",\"localized\":\"\",\"hint\":\"LaMa remove selected object from image\"},\n  {\"id\":\"\",\"label\":\"\ud83d\uddbc\ufe0f\",\"localized\":\"\",\"hint\":\"Show preview\"},\n  {\"id\":\"\",\"label\":\"\u270e\",\"localized\":\"\",\"hint\":\"Interrogate image using BLIP model\"},\n  {\"id\":\"\",\"label\":\"\u2710\",\"localized\":\"\",\"hint\":\"Interrogate image using DeepBooru model\"},\n  {\"id\":\"\",\"label\":\"\u21b6\",\"localized\":\"\",\"hint\":\"Apply selected style to prompt\"},\n  {\"id\":\"\",\"label\":\"\u21b7\",\"localized\":\"\",\"hint\":\"Save current prompt to style\"} \n],\n\"prompts\": [\n  {\"id\":\"\",\"label\":\"Prompt\",\"localized\":\"\",\"hint\":\"Describe image you want to generate\"},\n  {\"id\":\"\",\"label\":\"Negative prompt\",\"localized\":\"\",\"hint\":\"Describe what you don't want to see in generated image\"}\n],\n\"common keywords\": [\n  {\"id\":\"\",\"label\":\"fp16\",\"localized\":\"\",\"hint\":\"Number representation in 16-bit floating point format\"},\n  {\"id\":\"\",\"label\":\"fp32\",\"localized\":\"\",\"hint\":\"Number representation in 32-bit floating point format\"},\n  {\"id\":\"\",\"label\":\"bf16\",\"localized\":\"\",\"hint\":\"Number representation in alternative 16-bit floating point format\"}\n],\n\"tabs\": [\n  {\"id\":\"\",\"label\":\"Text\",\"localized\":\"\",\"hint\":\"Create image from text\"},\n  {\"id\":\"\",\"label\":\"Image\",\"localized\":\"\",\"hint\":\"Create image from image\"},\n  {\"id\":\"\",\"label\":\"Control\",\"localized\":\"\",\"hint\":\"Create image with additional control\"},\n  {\"id\":\"\",\"label\":\"Process\",\"localized\":\"\",\"hint\":\"Process existing image\"},\n  {\"id\":\"\",\"label\":\"Interrogate\",\"localized\":\"\",\"hint\":\"Run interrogate to get description of your image\"},\n  {\"id\":\"\",\"label\":\"Train\",\"localized\":\"\",\"hint\":\"Run training\"},\n  {\"id\":\"\",\"label\":\"Models\",\"localized\":\"\",\"hint\":\"Convert or merge your models\"},\n  {\"id\":\"\",\"label\":\"Agent Scheduler\",\"localized\":\"\",\"hint\":\"Enqueue your generate requests and run them in the background\"},\n  {\"id\":\"\",\"label\":\"Image Browser\",\"localized\":\"\",\"hint\":\"Browse through your generated image database\"},\n  {\"id\":\"\",\"label\":\"System\",\"localized\":\"\",\"hint\":\"System settings and information\"},\n  {\"id\":\"\",\"label\":\"System Info\",\"localized\":\"\",\"hint\":\"System information\"},\n  {\"id\":\"\",\"label\":\"Settings\",\"localized\":\"\",\"hint\":\"Application settings\"},\n  {\"id\":\"\",\"label\":\"Script\",\"localized\":\"\",\"hint\":\"Additional scripts to be used\"},\n  {\"id\":\"\",\"label\":\"Extensions\",\"localized\":\"\",\"hint\":\"Application extensions\"}\n],\n\"action panel\": [\n  {\"id\":\"\",\"label\":\"Generate\",\"localized\":\"\",\"hint\":\"Start processing\"},\n  {\"id\":\"\",\"label\":\"Enqueue\",\"localized\":\"\",\"hint\":\"Add task to background queue in Agent Scheduler\"},\n  {\"id\":\"\",\"label\":\"Stop\",\"localized\":\"\",\"hint\":\"Stop processing\"},\n  {\"id\":\"\",\"label\":\"Skip\",\"localized\":\"\",\"hint\":\"Stop processing current job and continue processing\"},\n  {\"id\":\"\",\"label\":\"Pause\",\"localized\":\"\",\"hint\":\"Pause processing\"},\n  {\"id\":\"\",\"label\":\"Restore\",\"localized\":\"\",\"hint\":\"Restore parameters from current prompt or last known generated image\"},\n  {\"id\":\"\",\"label\":\"Clear\",\"localized\":\"\",\"hint\":\"Clear prompts\"},\n  {\"id\":\"\",\"label\":\"Networks\",\"localized\":\"\",\"hint\":\"Open extra network interface\"}\n],\n\"extra networks\": [\n  {\"id\":\"\",\"label\":\"ui position\",\"localized\":\"\",\"hint\":\"Location of extra networks\"},\n  {\"id\":\"\",\"label\":\"cover\",\"localized\":\"\",\"hint\":\"cover full area\"},\n  {\"id\":\"\",\"label\":\"inline\",\"localized\":\"\",\"hint\":\"inline with all additional elements (scrollable)\"},\n  {\"id\":\"\",\"label\":\"sidebar\",\"localized\":\"\",\"hint\":\"sidebar on the right side of the screen\"},\n  {\"id\":\"\",\"label\":\"Default multiplier for extra networks\",\"localized\":\"\",\"hint\":\"When adding extra network such as Lora to prompt, use this multiplier for it\"},\n  {\"id\":\"\",\"label\":\"Model\",\"localized\":\"\",\"hint\":\"Trained model checkpoints\"},\n  {\"id\":\"\",\"label\":\"Style\",\"localized\":\"\",\"hint\":\"Additional styles to be applied on selected generation parameters\"},\n  {\"id\":\"\",\"label\":\"Styles\",\"localized\":\"\",\"hint\":\"Additional styles to be applied on selected generation parameters\"},\n  {\"id\":\"\",\"label\":\"Lora\",\"localized\":\"\",\"hint\":\"LoRA: Low-Rank Adaptation. Fine-tuned model that is applied on top of a loaded model\"},\n  {\"id\":\"\",\"label\":\"Embedding\",\"localized\":\"\",\"hint\":\"Textual inversion embedding is a trained embedded information about the subject\"},\n  {\"id\":\"\",\"label\":\"Hypernetwork\",\"localized\":\"\",\"hint\":\"Small trained neural network that modifies behavior of the loaded model\"},\n  {\"id\":\"\",\"label\":\"UI disable variable aspect ratio\",\"localized\":\"\",\"hint\":\"When disabled, all thumbnails appear as squared images\"},\n  {\"id\":\"\",\"label\":\"Build info on first access\",\"localized\":\"\",\"hint\":\"Prevents server from building EN page on server startup and instead build it when requested\"},\n  {\"id\":\"\",\"label\":\"Show built-in styles\",\"localized\":\"\",\"hint\":\"Show or hide build-it styles\"},\n  {\"id\":\"\",\"label\":\"LoRA use alternative loading method\",\"localized\":\"\",\"hint\":\"Alternative method uses diffusers built-in LoRA capabilities instead of native SD.Next implementation (may reduce LoRA compatibility)\"},\n  {\"id\":\"\",\"label\":\"LoRA use merge when using alternative method\",\"localized\":\"\",\"hint\":\"When loading LoRAs, immediately merge weights with underlying model instead of applying them on-the-fly\"},\n  {\"id\":\"\",\"label\":\"LoRA memory cache\",\"localized\":\"\",\"hint\":\"How many LoRAs to keep in network for future use before requiring reloading from storage\"}\n],\n\"gallery buttons\": [\n  {\"id\":\"\",\"label\":\"show\",\"localized\":\"\",\"hint\":\"Show image location\"},\n  {\"id\":\"\",\"label\":\"save\",\"localized\":\"\",\"hint\":\"Save image\"},\n  {\"id\":\"\",\"label\":\"delete\",\"localized\":\"\",\"hint\":\"Delete image\"},\n  {\"id\":\"\",\"label\":\"replace\",\"localized\":\"\",\"hint\":\"Replace image\"},\n  {\"id\":\"\",\"label\":\"\u27a0 text\",\"localized\":\"\",\"hint\":\"Transfer image to text interface\"},\n  {\"id\":\"\",\"label\":\"\u27a0 image\",\"localized\":\"\",\"hint\":\"Transfer image to image interface\"},\n  {\"id\":\"\",\"label\":\"\u27a0 inpaint\",\"localized\":\"\",\"hint\":\"Transfer image to inpaint interface\"},\n  {\"id\":\"\",\"label\":\"\u27a0 sketch\",\"localized\":\"\",\"hint\":\"Transfer image to sketch interface\"},\n  {\"id\":\"\",\"label\":\"\u27a0 composite\",\"localized\":\"\",\"hint\":\"Transfer image to inpaint sketch interface\"},\n  {\"id\":\"\",\"label\":\"\u27a0 process\",\"localized\":\"\",\"hint\":\"Transfer image to process interface\"}\n],\n\"extensions\": [\n  {\"id\":\"\",\"label\":\"Install\",\"localized\":\"\",\"hint\":\"Install\"},\n  {\"id\":\"\",\"label\":\"Search\",\"localized\":\"\",\"hint\":\"Search\"},\n  {\"id\":\"\",\"label\":\"Sort by\",\"localized\":\"\",\"hint\":\"Sort by\"},\n  {\"id\":\"\",\"label\":\"Manage extensions\",\"localized\":\"\",\"hint\":\"Manage extensions\"},\n  {\"id\":\"\",\"label\":\"Manual install\",\"localized\":\"\",\"hint\":\"Manually install extension\"},\n  {\"id\":\"\",\"label\":\"Extension GIT repository URL\",\"localized\":\"\",\"hint\":\"Specify extension repository URL on GitHub\"},\n  {\"id\":\"\",\"label\":\"Specific branch name\",\"localized\":\"\",\"hint\":\"Specify extension branch name, leave blank for default\"},\n  {\"id\":\"\",\"label\":\"Local directory name\",\"localized\":\"\",\"hint\":\"Directory where to install extension, leave blank for default\"},\n  {\"id\":\"\",\"label\":\"Refresh extension list\",\"localized\":\"\",\"hint\":\"Refresh list of available extensions\"},\n  {\"id\":\"\",\"label\":\"Update all installed\",\"localized\":\"\",\"hint\":\"Update installed extensions to their latest available version\"},\n  {\"id\":\"\",\"label\":\"Apply changes\",\"localized\":\"\",\"hint\":\"Apply all changes and restart server\"},\n  {\"id\":\"\",\"label\":\"install\",\"localized\":\"\",\"hint\":\"install this extension\"},\n  {\"id\":\"\",\"label\":\"uninstall\",\"localized\":\"\",\"hint\":\"uninstall this extension\"},\n  {\"id\":\"\",\"label\":\"User interface\",\"localized\":\"\",\"hint\":\"Review and set current values as default values for the user interface\"},\n  {\"id\":\"\",\"label\":\"Set new defaults\",\"localized\":\"\",\"hint\":\"Set current values as default values for the user interface\"},\n  {\"id\":\"\",\"label\":\"Benchmark\",\"localized\":\"\",\"hint\":\"Run benchmarks\"},\n  {\"id\":\"\",\"label\":\"Models &amp; Networks\",\"localized\":\"\",\"hint\":\"View lists of all available models and networks\"},\n  {\"id\":\"\",\"label\":\"Restore defaults\",\"localized\":\"\",\"hint\":\"Restore default user interface values\"}\n],\n\"txt2img tab\": [\n  {\"id\":\"\",\"label\":\"Sampling method\",\"localized\":\"\",\"hint\":\"Which algorithm to use to produce the image\"},\n  {\"id\":\"\",\"label\":\"Steps\",\"localized\":\"\",\"hint\":\"How many times to improve the generated image iteratively; higher values take longer; very low values can produce bad results\"},\n  {\"id\":\"\",\"label\":\"Tiling\",\"localized\":\"\",\"hint\":\"Produce an image that can be tiled\"},\n  {\"id\":\"\",\"label\":\"full quality\",\"localized\":\"\",\"hint\":\"Use full quality VAE to decode latent samples\"},\n  {\"id\":\"\",\"label\":\"detailer\",\"localized\":\"\",\"hint\":\"Run processed image through additional detailer model\"},\n  {\"id\":\"\",\"label\":\"hidiffusion\",\"localized\":\"\",\"hint\":\"HiDiffusion allows creation of high-resolution images using your standard models without duplicates/distortions and improved performance\"},\n  {\"id\":\"\",\"label\":\"HDR Clamp\",\"localized\":\"\",\"hint\":\"Adjusts the level of nonsensical details by pruning values that deviate significantly from the distribution mean. It is particularly useful for enhancing generation at higher guidance scales, identifying outliers early in the process and applying mathematical adjustments based on the Range (Boundary) and Threshold settings. Think of it as setting the range within which you want your image values to be, and adjusting the threshold determines which values should be brought back into that range\"},\n  {\"id\":\"\",\"label\":\"HDR Maximize\",\"localized\":\"\",\"hint\":\"Calculates a 'normalization factor' by dividing the maximum tensor value by the specified range multiplied by 4. This factor is then used to shift the channels within the given boundary, ensuring maximum dynamic range for subsequent processing. The objective is to optimize dynamic range for external applications like Photoshop, particularly for adjusting levels, contrast, and brightness\"},\n  {\"id\":\"\",\"label\":\"Enable refine pass\",\"localized\":\"\",\"hint\":\"Use a similar process as image to image to upscale and/or add detail to the final image. Optionally uses refiner model to enhance image details.\"},\n  {\"id\":\"\",\"label\":\"Denoising strength\",\"localized\":\"\",\"hint\":\"Determines how little respect the algorithm should have for image's content. At 0, nothing will change, and at 1 you'll get an unrelated image. With values below 1.0, processing will take less steps than the Sampling Steps slider specifies\"},\n  {\"id\":\"\",\"label\":\"Denoise start\",\"localized\":\"\",\"hint\":\"Override denoise strength by stating how early base model should finish and when refiner should start. Only applicable to refiner usage. If set to 0 or 1, denoising strength will be used\"},\n  {\"id\":\"\",\"label\":\"Hires steps\",\"localized\":\"\",\"hint\":\"Number of sampling steps for upscaled picture. If 0, uses same as for original\"},\n  {\"id\":\"\",\"label\":\"Upscaler\",\"localized\":\"\",\"hint\":\"Which pre-trained model to use for the upscaling process.\"},\n  {\"id\":\"\",\"label\":\"Upscale by\",\"localized\":\"\",\"hint\":\"Adjusts the size of the image by multiplying the original width and height by the selected value. Ignored if either Resize width to or Resize height to are non-zero\"},\n  {\"id\":\"\",\"label\":\"Force Hires\",\"localized\":\"\",\"hint\":\"Hires runs automatically when Latent upscale is selected, but its skipped when using non-latent upscalers. Enable force hires to run hires with non-latent upscalers\"},\n  {\"id\":\"\",\"label\":\"Resize width to\",\"localized\":\"\",\"hint\":\"Resizes image to this width. If 0, width is inferred from either of two nearby sliders\"},\n  {\"id\":\"\",\"label\":\"Resize height to\",\"localized\":\"\",\"hint\":\"Resizes image to this height. If 0, height is inferred from either of two nearby sliders\"},\n  {\"id\":\"\",\"label\":\"Refine sampler\",\"localized\":\"\",\"hint\":\"Use specific sampler as fallback sampler if primary is not supported for specific operation\"},\n  {\"id\":\"\",\"label\":\"Refiner start\",\"localized\":\"\",\"hint\":\"Refiner pass will start when base model is this much complete (set to larger than 0 and smaller than 1 to run after full base model run)\"},\n  {\"id\":\"\",\"label\":\"Refiner steps\",\"localized\":\"\",\"hint\":\"Number of steps to use for refiner pass\"},\n  {\"id\":\"\",\"label\":\"Refine CFG Scale\",\"localized\":\"\",\"hint\":\"CFG scale used for refiner pass\"},\n  {\"id\":\"\",\"label\":\"Rescale guidance\",\"localized\":\"\",\"hint\":\"Rescale CFG generated noise to avoid overexposed images\"},\n  {\"id\":\"\",\"label\":\"Refine Prompt\",\"localized\":\"\",\"hint\":\"Prompt used for both second encoder in base model (if it exists) and for refiner pass (if enabled)\"},\n  {\"id\":\"\",\"label\":\"Refine negative prompt\",\"localized\":\"\",\"hint\":\"Negative prompt used for both second encoder in base model (if it exists) and for refiner pass (if enabled)\"},\n  {\"id\":\"\",\"label\":\"Width\",\"localized\":\"\",\"hint\":\"Image width\"},\n  {\"id\":\"\",\"label\":\"Height\",\"localized\":\"\",\"hint\":\"Image height\"},\n  {\"id\":\"\",\"label\":\"Batch count\",\"localized\":\"\",\"hint\":\"How many batches of images to create (has no impact on generation performance or VRAM usage)\"},\n  {\"id\":\"\",\"label\":\"Batch size\",\"localized\":\"\",\"hint\":\"How many image to create in a single batch (increases generation performance at cost of higher VRAM usage)\"},\n  {\"id\":\"\",\"label\":\"cfg scale\",\"localized\":\"\",\"hint\":\"Classifier Free Guidance scale: how strongly the image should conform to prompt. Lower values produce more creative results, higher values make it follow the prompt more strictly; recommended values between 5-10\"},\n  {\"id\":\"\",\"label\":\"Guidance End\",\"localized\":\"\",\"hint\":\"Ends the effect of CFG and PAG early: A value of 1 acts as normal, 0.5 stops guidance at 50% of steps\"},\n  {\"id\":\"\",\"label\":\"CLIP skip\",\"localized\":\"\",\"hint\":\"Clip skip is a feature that allows users to control the level of specificity of the prompt, the higher the CLIP skip value, the less deep the prompt will be interpreted. CLIP Skip 1 is typical while some anime models produce better results at  CLIP skip 2\"},\n  {\"id\":\"\",\"label\":\"Initial seed\",\"localized\":\"\",\"hint\":\"A value that determines the output of random number generator - if you create an image with same parameters and seed as another image, you'll get the same result\"},\n  {\"id\":\"\",\"label\":\"Variation\",\"localized\":\"\",\"hint\":\"Second seed to be mixed with primary seed\"},\n  {\"id\":\"\",\"label\":\"Variation strength\",\"localized\":\"\",\"hint\":\"How strong of a variation to produce. At 0, there will be no effect. At 1, you will get the complete picture with variation seed (except for ancestral samplers, where you will just get something)\"},\n  {\"id\":\"\",\"label\":\"Resize seed from width\",\"localized\":\"\",\"hint\":\"Make an attempt to produce a picture similar to what would have been produced with same seed at specified resolution\"},\n  {\"id\":\"\",\"label\":\"Resize seed from height\",\"localized\":\"\",\"hint\":\"Make an attempt to produce a picture similar to what would have been produced with same seed at specified resolution\"},\n  {\"id\":\"\",\"label\":\"Override settings\",\"localized\":\"\",\"hint\":\"If you read in generation parameters through 'Process Image tab' and individual generation parameters should deviate from your system settings, this box will be populated with those settings to override your system configuration for this workflow\"}\n],\n\"img2img tab\": [\n  {\"id\":\"\",\"label\":\"Fixed\",\"localized\":\"\",\"hint\":\"Resize image to target resolution. Unless height and width match, you will get incorrect aspect ratio\"},\n  {\"id\":\"\",\"label\":\"Crop\",\"localized\":\"\",\"hint\":\"Resize the image so that entirety of target resolution is filled with the image. Crop parts that stick out\"},\n  {\"id\":\"\",\"label\":\"Fill\",\"localized\":\"\",\"hint\":\"Resize the image so that entirety of image is inside target resolution. Fill empty space with image's colors\"},\n  {\"id\":\"\",\"label\":\"Mask blur\",\"localized\":\"\",\"hint\":\"How much to blur the mask before processing, in pixels\"},\n  {\"id\":\"\",\"label\":\"original\",\"localized\":\"\",\"hint\":\"keep whatever was there originally\"},\n  {\"id\":\"\",\"label\":\"latent noise\",\"localized\":\"\",\"hint\":\"fill it with latent space noise\"},\n  {\"id\":\"\",\"label\":\"latent nothing\",\"localized\":\"\",\"hint\":\"fill it with latent space zeroes\"}\n],\n\"control tab\": [\n  {\"id\":\"\",\"label\":\"Guess Mode\",\"localized\":\"\",\"hint\":\"Removes the requirement to supply a prompt to a ControlNet. It forces Controlnet encoder to do it's 'best guess' based on the contents of the input control map.\"},\n  {\"id\":\"\",\"label\":\"Control Only\",\"localized\":\"\",\"hint\":\"This uses only the Control input below as the source for any ControlNet or IP Adapter type tasks based on any of our various options.\"},\n  {\"id\":\"\",\"label\":\"Init Image Same As Control\",\"localized\":\"\",\"hint\":\"Will additionally treat any image placed into the Control input window as a source for img2img type tasks, an image to modify for example.\"},\n  {\"id\":\"\",\"label\":\"Separate Init Image\",\"localized\":\"\",\"hint\":\"Creates an additional window next to Control input labeled Init input, so you can have a separate image for both Control operations and an init source.\"}\n],\n\"process tab\": [\n  {\"id\":\"\",\"label\":\"Process Image\",\"localized\":\"\",\"hint\":\"Process single image\"},\n  {\"id\":\"\",\"label\":\"Process Batch\",\"localized\":\"\",\"hint\":\"Process batch of images\"},\n  {\"id\":\"\",\"label\":\"Process Folder\",\"localized\":\"\",\"hint\":\"Process all images in a folder\"},\n  {\"id\":\"\",\"label\":\"Scale by\",\"localized\":\"\",\"hint\":\"Use this tab to resize the source image(s) by a chosen factor\"},\n  {\"id\":\"\",\"label\":\"Scale to\",\"localized\":\"\",\"hint\":\"Use this tab to resize the source image(s) to a chosen target size\"},\n  {\"id\":\"\",\"label\":\"Input directory\",\"localized\":\"\",\"hint\":\"Folder where the images are that you want to process\"},\n  {\"id\":\"\",\"label\":\"Output directory\",\"localized\":\"\",\"hint\":\"Folder where the processed images should be saved to\"},\n  {\"id\":\"\",\"label\":\"Show result images\",\"localized\":\"\",\"hint\":\"Enable to show the processed images in the image pane\"},\n  {\"id\":\"\",\"label\":\"Resize\",\"localized\":\"\",\"hint\":\"Resizing details. Higher resolutions require additional processing memory.\"},\n  {\"id\":\"\",\"label\":\"Crop to fit\",\"localized\":\"\",\"hint\":\"If the dimensions of your source image (e.g. 512x510) deviate from your target dimensions (e.g. 1024x768) this function will fit your upscaled image into your target size image. Excess will be cropped\"},\n  {\"id\":\"\",\"label\":\"Refine Upscaler\",\"localized\":\"\",\"hint\":\"Select secondary upscaler to run after initial upscaler\"},\n  {\"id\":\"\",\"label\":\"Upscaler 2 visibility\",\"localized\":\"\",\"hint\":\"Strength of the secondary upscaler\"}\n],\n\"models tabs\": [\n  {\"id\":\"\",\"label\":\"Calculate hash for all models\",\"localized\":\"\",\"hint\":\"Calculates hash for all available models which may take a very long time\"},\n  {\"id\":\"\",\"label\":\"Weights Clip\",\"localized\":\"\",\"hint\":\"Forced merged weights to be no heavier than the original model, preventing burn in and overly saturated models\"},\n  {\"id\":\"\",\"label\":\"ReBasin\",\"localized\":\"\",\"hint\":\"Performs multiple merges with permutations in order to keep more features from both models\"},\n  {\"id\":\"\",\"label\":\"Number of ReBasin Iterations\",\"localized\":\"\",\"hint\":\"Number of times to merge and permute the model before saving\"},\n  {\"id\":\"\",\"label\":\"cpu\",\"localized\":\"\",\"hint\":\"Uses cpu and RAM only: slowest but least likely to OOM\"},\n  {\"id\":\"\",\"label\":\"shuffle\",\"localized\":\"\",\"hint\":\"Loads full model in RAM and calculates on VRAM: Less speedup, suggested for SDXL merges\"},\n  {\"id\":\"\",\"label\":\"cuda\",\"localized\":\"\",\"hint\":\"Loads models into VRAM automatically unloading current model: fastest option but unlikely to handle SDXL Models without OOM\"},\n  {\"id\":\"\",\"label\":\"Base\",\"localized\":\"\",\"hint\":\"Text Encoder and a few unaligned keys (1 value)\"},\n  {\"id\":\"\",\"label\":\"In Blocks\",\"localized\":\"\",\"hint\":\"Downsampling Blocks of the UNet (12 values for SD1.5, 9 values for SDXL)\"},\n  {\"id\":\"\",\"label\":\"Mid Block\",\"localized\":\"\",\"hint\":\"Central Block of the UNet (1 value)\"},\n  {\"id\":\"\",\"label\":\"Out Block\",\"localized\":\"\",\"hint\":\"Upsampling Blocks of the UNet (12 values for SD1.5, 9 values for SDXL)\"},\n  {\"id\":\"\",\"label\":\"Preset Interpolation Ratio\",\"localized\":\"\",\"hint\":\"If two presets are selected, interpolate between them\"}\n],\n\"train tab\": [\n  {\"id\":\"\",\"label\":\"Initialization text\",\"localized\":\"\",\"hint\":\"If the number of tokens is more than the number of vectors, some may be skipped.\\nLeave the textbox empty to start with zeroed out vectors\"},\n  {\"id\":\"\",\"label\":\"Select activation function of hypernetwork\",\"localized\":\"\",\"hint\":\"Recommended : Swish / Linear(none)\"},\n  {\"id\":\"\",\"label\":\"Select Layer weights initialization\",\"localized\":\"\",\"hint\":\"Recommended: Kaiming for relu-like, Xavier for sigmoid-like, Normal otherwise\"},\n  {\"id\":\"\",\"label\":\"Enter hypernetwork Dropout structure\",\"localized\":\"\",\"hint\":\"Recommended : leave empty or 0~0.35 incrementing sequence: 0, 0.05, 0.15\"},\n  {\"id\":\"\",\"label\":\"Create interim images\",\"localized\":\"\",\"hint\":\"Save an image to log directory every N steps, 0 to disable\"},\n  {\"id\":\"\",\"label\":\"Create interim embeddings\",\"localized\":\"\",\"hint\":\"Save a copy of embedding to log directory every N steps, 0 to disable\"},\n  {\"id\":\"\",\"label\":\"Use current settings for previews\",\"localized\":\"\",\"hint\":\"Read parameters (prompt, etc...) from txt2img tab when making previews\"},\n  {\"id\":\"\",\"label\":\"Shuffle tags\",\"localized\":\"\",\"hint\":\"Shuffle tags by ',' when creating prompts\"}\n],\n\"settings menu\": [\n  {\"id\":\"settings_submit\",\"label\":\"Apply settings\",\"localized\":\"\",\"hint\":\"Save current settings, server restart is recommended\"},\n  {\"id\":\"restart_submit\",\"label\":\"Restart server\",\"localized\":\"\",\"hint\":\"Restart server\"},\n  {\"id\":\"shutdown_submit\",\"label\":\"Shutdown server\",\"localized\":\"\",\"hint\":\"Shutdown server\"},\n  {\"id\":\"settings_preview_theme\",\"label\":\"Preview theme\",\"localized\":\"\",\"hint\":\"Show theme preview\"},\n  {\"id\":\"defaults_submit\",\"label\":\"Restore defaults\",\"localized\":\"\",\"hint\":\"Restore default server settings\"},\n  {\"id\":\"sett_unload_sd_model\",\"label\":\"Unload model\",\"localized\":\"\",\"hint\":\"Unload currently loaded model\"},\n  {\"id\":\"sett_reload_sd_model\",\"label\":\"Reload model\",\"localized\":\"\",\"hint\":\"Reload currently selected model\"}\n],\n\"settings sections\": [\n  {\"id\":\"\",\"label\":\"Execution &amp; Models\",\"localized\":\"\",\"hint\":\"Settings related to execution backend, models, and prompt attention\"},\n  {\"id\":\"\",\"label\":\"Compute Settings\",\"localized\":\"\",\"hint\":\"Settings related to precision, cross attention, model compilation, and optimizations for computing platforms\"},\n  {\"id\":\"\",\"label\":\"Inference Settings\",\"localized\":\"\",\"hint\":\"Settings related image inference, token merging, FreeU, and Hypertile\"},\n  {\"id\":\"\",\"label\":\"Diffusers Settings\",\"localized\":\"\",\"hint\":\"Settings related to Diffusers backend\"},\n  {\"id\":\"\",\"label\":\"System Paths\",\"localized\":\"\",\"hint\":\"Settings related to location of various model directories\"},\n  {\"id\":\"\",\"label\":\"Image Options\",\"localized\":\"\",\"hint\":\"Settings related to image format, metadata, and image grids\"},\n  {\"id\":\"\",\"label\":\"image naming &amp; paths\",\"localized\":\"\",\"hint\":\"Settings related to image filenames, and output directories\"},\n  {\"id\":\"\",\"label\":\"User Interface Options\",\"localized\":\"\",\"hint\":\"Settings related to user interface themes, and Quicksettings list\"},\n  {\"id\":\"\",\"label\":\"Live Previews\",\"localized\":\"\",\"hint\":\"Settings related to live previews, audio notification, and log view\"},\n  {\"id\":\"\",\"label\":\"Sampler Settings\",\"localized\":\"\",\"hint\":\"Settings related to sampler selection and configuration, and diffuser specific sampler configuration\"},\n  {\"id\":\"\",\"label\":\"Postprocessing\",\"localized\":\"\",\"hint\":\"Settings related to post image generation processing, face restoration, and upscaling\"},\n  {\"id\":\"\",\"label\":\"Control Options\",\"localized\":\"\",\"hint\":\"Settings related the Control tab\"},\n  {\"id\":\"\",\"label\":\"Training\",\"localized\":\"\",\"hint\":\"Settings related to model training configuration and directories\"},\n  {\"id\":\"\",\"label\":\"Interrogate\",\"localized\":\"\",\"hint\":\"Settings related to interrogation configuration\"},\n  {\"id\":\"\",\"label\":\"Networks\",\"localized\":\"\",\"hint\":\"Settings related to networks user interface, networks multiplier defaults, and configuration\"},\n  {\"id\":\"\",\"label\":\"Licenses\",\"localized\":\"\",\"hint\":\"View licenses of all additional included libraries\"},\n  {\"id\":\"\",\"label\":\"Show all pages\",\"localized\":\"\",\"hint\":\"Show all settings pages\"}\n],\n\"settings\": [\n  {\"id\":\"\",\"label\":\"base model\",\"localized\":\"\",\"hint\":\"Main model used for all operations\"},\n  {\"id\":\"\",\"label\":\"refiner model\",\"localized\":\"\",\"hint\":\"Refiner model used for second-pass operations\"},\n  {\"id\":\"\",\"label\":\"Cached models\",\"localized\":\"\",\"hint\":\"The number of models to store in RAM for quick access\"},\n  {\"id\":\"\",\"label\":\"Cached VAEs\",\"localized\":\"\",\"hint\":\"The number of VAE files to store in RAM for quick access\"},\n  {\"id\":\"\",\"label\":\"VAE model\",\"localized\":\"\",\"hint\":\"VAE helps with fine details in the final image and may also alter colors\"},\n  {\"id\":\"\",\"label\":\"Load models using stream loading method\",\"localized\":\"\",\"hint\":\"When loading models attempt stream loading optimized for slow or network storage\"},\n  {\"id\":\"\",\"label\":\"xFormers\",\"localized\":\"\",\"hint\":\"Memory optimization. Non-Deterministic (different results each time)\"},\n  {\"id\":\"\",\"label\":\"Scaled-Dot-Product\",\"localized\":\"\",\"hint\":\"Memory optimization. Non-Deterministic unless SDP memory attention is disabled.\"},\n  {\"id\":\"\",\"label\":\"Prompt padding\",\"localized\":\"\",\"hint\":\"Increase coherency by padding from the last comma within n tokens when using more than 75 tokens\"},\n  {\"id\":\"\",\"label\":\"Original\",\"localized\":\"\",\"hint\":\"Original LDM backend\"},\n  {\"id\":\"\",\"label\":\"Diffusers\",\"localized\":\"\",\"hint\":\"Diffusers backend\"},\n  {\"id\":\"\",\"label\":\"Autocast\",\"localized\":\"\",\"hint\":\"Automatically determine precision during runtime\"},\n  {\"id\":\"\",\"label\":\"Full\",\"localized\":\"\",\"hint\":\"Always use full precision\"},\n  {\"id\":\"\",\"label\":\"FP32\",\"localized\":\"\",\"hint\":\"Use 32-bit floating point precision for calculations\"},\n  {\"id\":\"\",\"label\":\"FP16\",\"localized\":\"\",\"hint\":\"Use 16-bit floating point precision for calculations\"},\n  {\"id\":\"\",\"label\":\"BF16\",\"localized\":\"\",\"hint\":\"Use modified 16-bit floating point precision for calculations\"},\n  {\"id\":\"\",\"label\":\"Full precision for model (--no-half)\",\"localized\":\"\",\"hint\":\"Uses FP32 for the model. May produce better results while using more VRAM and slower generation\"},\n  {\"id\":\"\",\"label\":\"Full precision for VAE (--no-half-vae)\",\"localized\":\"\",\"hint\":\"Uses FP32 for the VAE. May produce better results while using more VRAM and slower generation\"},\n  {\"id\":\"\",\"label\":\"Upcast sampling\",\"localized\":\"\",\"hint\":\"Usually produces similar results to --no-half with better performance while using less memory\"},\n  {\"id\":\"\",\"label\":\"Attempt VAE roll back for NaN values\",\"localized\":\"\",\"hint\":\"Requires Torch 2.1 and NaN check enabled\"},\n  {\"id\":\"\",\"label\":\"DirectML memory stats provider\",\"localized\":\"\",\"hint\":\"How to get GPU memory stats\"},\n  {\"id\":\"\",\"label\":\"DirectML retry ops for NaN\",\"localized\":\"\",\"hint\":\"Retry specific operations if their output was NaN. This may make your generation slower\"},\n  {\"id\":\"\",\"label\":\"Olive use FP16 on optimization\",\"localized\":\"\",\"hint\":\"Use 16-bit floating point precision for the output model of Olive optimization process. Use 32-bit floating point precision if disabled\"},\n  {\"id\":\"\",\"label\":\"Olive force FP32 for VAE Encoder\",\"localized\":\"\",\"hint\":\"Use 32-bit floating point precision for VAE Encoder of the output model. This overrides 'use FP16 on optimization' option. If you are getting NaN or black blank images from Img2Img, enable this option and remove cache\"},\n  {\"id\":\"\",\"label\":\"Olive use static dimensions\",\"localized\":\"\",\"hint\":\"Make the inference with Olive optimized models much faster. (OrtTransformersOptimization)\"},\n  {\"id\":\"\",\"label\":\"Olive cache optimized models\",\"localized\":\"\",\"hint\":\"Save Olive processed models as a cache. You can manage them in ONNX tab\"},\n  {\"id\":\"\",\"label\":\"File format\",\"localized\":\"\",\"hint\":\"Select file format for images\"},\n  {\"id\":\"\",\"label\":\"Include metadata\",\"localized\":\"\",\"hint\":\"Save image create parameters as metadata tags inside image file\"},\n  {\"id\":\"\",\"label\":\"Images filename pattern\",\"localized\":\"\",\"hint\":\"Use following tags to define how filenames for images are chosen:&lt;br&gt;&lt;pre&gt;seq, uuid&lt;br&gt;date, datetime, job_timestamp&lt;br&gt;generation_number, batch_number&lt;br&gt;model, model_shortname&lt;br&gt;model_hash, model_name&lt;br&gt;sampler, seed, steps, cfg&lt;br&gt;clip_skip, denoising&lt;br&gt;hasprompt, prompt, styles&lt;br&gt;prompt_hash, prompt_no_styles&lt;br&gt;prompt_spaces, prompt_words&lt;br&gt;height, width, image_hash&lt;br&gt;&lt;/pre&gt;\"},\n  {\"id\":\"\",\"label\":\"Row count\",\"localized\":\"\",\"hint\":\"Use -1 for autodetect and 0 for it to be same as batch size\"},\n  {\"id\":\"\",\"label\":\"Update JSON log file per image\",\"localized\":\"\",\"hint\":\"Save image information to a JSON file\"},\n  {\"id\":\"\",\"label\":\"Directory name pattern\",\"localized\":\"\",\"hint\":\"Use following tags to define how subdirectories for images and grids are chosen: [steps], [cfg],[prompt_hash], [prompt], [prompt_no_styles], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [model_name], [prompt_words], [date], [datetime], [datetime&lt;Format&gt;], [datetime&lt;Format&gt;&lt;Time Zone&gt;], [job_timestamp]; leave empty for default\"},\n  {\"id\":\"\",\"label\":\"Inpainting conditioning mask strength\",\"localized\":\"\",\"hint\":\"Determines how strongly to mask off the original image for inpainting and img2img. 1.0 means fully masked (default). 0.0 means a fully unmasked conditioning. Lower values will help preserve the overall composition of the image, but will struggle with large changes\"},\n  {\"id\":\"\",\"label\":\"Clip skip\",\"localized\":\"\",\"hint\":\"Early stopping parameter for CLIP model; 1 is stop at last layer as usual, 2 is stop at penultimate layer, etc\"},\n  {\"id\":\"\",\"label\":\"Images folder\",\"localized\":\"\",\"hint\":\"If empty, defaults to three directories below\"},\n  {\"id\":\"\",\"label\":\"Grids folder\",\"localized\":\"\",\"hint\":\"If empty, defaults to two directories below\"},\n  {\"id\":\"\",\"label\":\"Quicksettings list\",\"localized\":\"\",\"hint\":\"List of setting names, separated by commas, for settings that should go to the quick access bar at the top instead the setting tab\"},\n  {\"id\":\"\",\"label\":\"Live preview display period\",\"localized\":\"\",\"hint\":\"Request preview image every n steps, set to 0 to disable\"},\n  {\"id\":\"\",\"label\":\"Approximate\",\"localized\":\"\",\"hint\":\"Cheap neural network approximation. Very fast compared to VAE, but produces pictures with 4 times smaller horizontal/vertical resolution and lower quality\"},\n  {\"id\":\"\",\"label\":\"Simple\",\"localized\":\"\",\"hint\":\"Very cheap approximation. Very fast compared to VAE, but produces pictures with 8 times smaller horizontal/vertical resolution and extremely low quality\"},\n  {\"id\":\"\",\"label\":\"Progress update period\",\"localized\":\"\",\"hint\":\"Update period for UI progress bar and preview checks, in milliseconds\"},\n  {\"id\":\"\",\"label\":\"Euler a\",\"localized\":\"\",\"hint\":\"Euler Ancestral - very creative, each can get a completely different picture depending on step count, setting steps higher than 30-40 does not help\"},\n  {\"id\":\"\",\"label\":\"DPM adaptive\",\"localized\":\"\",\"hint\":\"Ignores step count - uses a number of steps determined by the CFG and resolution\"},\n  {\"id\":\"\",\"label\":\"DDIM\",\"localized\":\"\",\"hint\":\"Denoising Diffusion Implicit Models - best at inpainting\"},\n  {\"id\":\"\",\"label\":\"UniPC\",\"localized\":\"\",\"hint\":\"Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models\"},\n  {\"id\":\"\",\"label\":\"sigma negative guidance minimum\",\"localized\":\"\",\"hint\":\"Skip negative prompt for some steps when the image is almost ready, 0=disable\"},\n  {\"id\":\"\",\"label\":\"Filename word regex\",\"localized\":\"\",\"hint\":\"This regular expression will be used extract words from filename, and they will be joined using the option below into label text used for training. Leave empty to keep filename text as it is\"},\n  {\"id\":\"\",\"label\":\"Filename join string\",\"localized\":\"\",\"hint\":\"This string will be used to join split words into a single line if the option above is enabled\"},\n  {\"id\":\"\",\"label\":\"Tensorboard flush period\",\"localized\":\"\",\"hint\":\"How often, in seconds, to flush the pending Tensorboard events and summaries to disk\"},\n  {\"id\":\"\",\"label\":\"Interrogate: minimum description length\",\"localized\":\"\",\"hint\":\"Interrogate: minimum description length (excluding artists, etc..)\"},\n  {\"id\":\"\",\"label\":\"CLIP: maximum number of lines in text file\",\"localized\":\"\",\"hint\":\"CLIP: maximum number of lines in text file (0 = No limit)\"},\n  {\"id\":\"\",\"label\":\"Escape brackets in deepbooru\",\"localized\":\"\",\"hint\":\"Escape (\\\\) brackets in deepbooru so they are used as literal brackets and not for emphasis\"},\n  {\"id\":\"\",\"label\":\"Filter out tags from deepbooru output\",\"localized\":\"\",\"hint\":\"Filter out those tags from deepbooru output (separated by comma)\"},\n  {\"id\":\"\",\"label\":\"Upscaler tile size\",\"localized\":\"\",\"hint\":\"0 = no tiling\"},\n  {\"id\":\"\",\"label\":\"Upscaler tile overlap\",\"localized\":\"\",\"hint\":\"Low values = visible seam\"},\n  {\"id\":\"\",\"label\":\"GFPGAN\",\"localized\":\"\",\"hint\":\"Restore low quality faces using GFPGAN neural network\"},\n  {\"id\":\"\",\"label\":\"CodeFormer weight parameter\",\"localized\":\"\",\"hint\":\"0 = maximum effect; 1 = minimum effect\"},\n  {\"id\":\"\",\"label\":\"Token merging ratio for txt2img\",\"localized\":\"\",\"hint\":\"Enable redundant token merging via tomesd for speed and memory improvements, 0=disabled\"},\n  {\"id\":\"\",\"label\":\"Token merging ratio for img2img\",\"localized\":\"\",\"hint\":\"Enable redundant token merging for img2img via tomesd for speed and memory improvements, 0=disabled\"},\n  {\"id\":\"\",\"label\":\"Token merging ratio for hires\",\"localized\":\"\",\"hint\":\"Enable redundant token merging for hires pass via tomesd for speed and memory improvements, 0=disabled\"},\n  {\"id\":\"\",\"label\":\"Diffusers pipeline\",\"localized\":\"\",\"hint\":\"If autodetect does not detect model automatically, select model type before loading a model\"},\n  {\"id\":\"\",\"label\":\"Sequential CPU offload (--lowvram)\",\"localized\":\"\",\"hint\":\"Reduces GPU memory usage by transferring weights to the CPU. Increases inference time approximately 10%\"},\n  {\"id\":\"\",\"label\":\"Model CPU offload (--medvram)\",\"localized\":\"\",\"hint\":\"Transferring of entire models to the CPU, negligible impact on inference time while still providing some memory savings\"},\n  {\"id\":\"\",\"label\":\"VAE slicing\",\"localized\":\"\",\"hint\":\"Decodes batch latents one image at a time with limited VRAM. Small performance boost in VAE decode on multi-image batches\"},\n  {\"id\":\"\",\"label\":\"VAE tiling\",\"localized\":\"\",\"hint\":\"Divide large images into overlapping tiles with limited VRAM. Results in a minor increase in processing time\"},\n  {\"id\":\"\",\"label\":\"Attention slicing\",\"localized\":\"\",\"hint\":\"Performs attention computation in steps instead of all at once. Slower inference times, but greatly reduced memory usage\"},\n  {\"id\":\"\",\"label\":\"Execution Provider\",\"localized\":\"\",\"hint\":\"ONNX Execution Provider\"},\n  {\"id\":\"\",\"label\":\"ONNX allow fallback to CPU\",\"localized\":\"\",\"hint\":\"Allow fallback to CPU when selected execution provider failed\"},\n  {\"id\":\"\",\"label\":\"ONNX cache converted models\",\"localized\":\"\",\"hint\":\"Save the models that are converted to ONNX format as a cache. You can manage them in ONNX tab\"},\n  {\"id\":\"\",\"label\":\"ONNX unload base model when processing refiner\",\"localized\":\"\",\"hint\":\"Unload base model when the refiner is being converted/optimized/processed\"},\n  {\"id\":\"\",\"label\":\"inference-mode\",\"localized\":\"\",\"hint\":\"Use torch.inference_mode\"},\n  {\"id\":\"\",\"label\":\"no-grad\",\"localized\":\"\",\"hint\":\"Use torch.no_grad\"},\n  {\"id\":\"\",\"label\":\"model compile precompile\",\"localized\":\"\",\"hint\":\"Run model compile immediately on model load instead of first use\"},\n  {\"id\":\"\",\"label\":\"force zeros for prompts when empty\",\"localized\":\"\",\"hint\":\"Force full zero tensor when prompt is empty to remove any residual noise\"},\n  {\"id\":\"\",\"label\":\"require aesthetics score\",\"localized\":\"\",\"hint\":\"Automatically guide model towards higher-pleasing results, applicable only to refiner model\"},\n  {\"id\":\"\",\"label\":\"include watermark\",\"localized\":\"\",\"hint\":\"Add invisible watermark to image by altering some pixel values\"},\n  {\"id\":\"\",\"label\":\"watermark string\",\"localized\":\"\",\"hint\":\"Watermark string to add to image. Keep very short to avoid image corruption.\"},\n  {\"id\":\"\",\"label\":\"show log view\",\"localized\":\"\",\"hint\":\"Show log view at the bottom of the main window\"},\n  {\"id\":\"\",\"label\":\"Log view update period\",\"localized\":\"\",\"hint\":\"Log view update period, in milliseconds\"},\n  {\"id\":\"\",\"label\":\"PAG layer names\",\"localized\":\"\",\"hint\":\"Space separated list of layers&lt;br&gt;Available: d[0-5], m[0], u[0-8]&lt;br&gt;Default: m0\"}\n],\n\"scripts\": [\n  {\"id\":\"\",\"label\":\"X values\",\"localized\":\"\",\"hint\":\"Separate values for X axis using commas\"},\n  {\"id\":\"\",\"label\":\"Y values\",\"localized\":\"\",\"hint\":\"Separate values for Y axis using commas\"},\n  {\"id\":\"\",\"label\":\"Z values\",\"localized\":\"\",\"hint\":\"Separate values for Z axis using commas\"},\n  {\"id\":\"\",\"label\":\"Override `Sampling method` to Euler\",\"localized\":\"\",\"hint\":\"(this method is built for it)\"},\n  {\"id\":\"\",\"label\":\"Loops\",\"localized\":\"\",\"hint\":\"How many times to process an image. Each output is used as the input of the next loop. If set to 1, behavior will be as if this script were not used\"},\n  {\"id\":\"\",\"label\":\"Final denoising strength\",\"localized\":\"\",\"hint\":\"The denoising strength for the final loop of each image in the batch\"},\n  {\"id\":\"\",\"label\":\"Denoising strength curve\",\"localized\":\"\",\"hint\":\"The denoising curve controls the rate of denoising strength change each loop. Aggressive: Most of the change will happen towards the start of the loops. Linear: Change will be constant through all loops. Lazy: Most of the change will happen towards the end of the loops\"},\n  {\"id\":\"\",\"label\":\"Tile overlap\",\"localized\":\"\",\"hint\":\"For SD upscale, how much overlap in pixels should there be between tiles. Tiles overlap so that when they are merged back into one picture, there is no clearly visible seam\"}\n]\n}\n</code></pre>"},{"location":"Hotkeys/","title":"Hotkeys","text":"<ul> <li><code>escape</code>: stop/interrupt generate</li> <li><code>ctrl + enter</code>: generate</li> <li><code>ctrl + up/down</code>: change weight of the highlighted section of prompt up/down</li> <li><code>ctrl + n</code> or <code>ctrl + space</code>: show/hide networks panel</li> <li><code>ctrl + s</code> or <code>ctrl + insert</code>: save image</li> <li><code>ctrl + i</code>: reprocess image</li> <li><code>ctrl + d</code>: delete image</li> <li><code>ctrl + m</code>: model selector</li> <li><code>ctrl + e</code>: enqueue (agent scheduler add to queue)</li> <li><code>left/right</code>: cycle through images</li> <li><code>+/-</code>: image viewer zoom in/out</li> </ul>"},{"location":"IPAdapter/","title":"IP-Adapter","text":"<p>The IP-Adapter is a tool designed for style transfer with minimal resource usage. It provides an efficient way to clone faces or apply image transformations. It supports both SD 1.5 and SD-XL models, allowing for quick and cost-effective style transfer processes.</p>"},{"location":"IPAdapter/#key-features","title":"Key Features","text":"<ul> <li>Low Resource Usage: The IP-Adapter is lightweight, with memory requirements under 100MB for SD 1.5 and 700MB for SD-XL, making it an efficient choice for style transfer tasks.</li> <li>Style Transfer: It offers powerful style transfer capabilities, allowing you to clone faces or apply various image styles.</li> <li>Integration with ControlNet: IP-Adapter can be combined with ControlNet for more stable results, especially useful for batch processing or video tasks.</li> </ul>"},{"location":"IPAdapter/#available-models","title":"Available Models","text":"<p>The TenecentAILab IP-Adapter includes 10 models for various image or face cloning needs, enabling flexibility and versatility in different scenarios.</p>"},{"location":"IPAdapter/#examples","title":"Examples","text":""},{"location":"Installation/","title":"Installing SD.Next","text":"<p>Tip</p> <p>These instructions assume that you already have Git and Python installed and available in your user <code>PATH</code>.  If you do not have both Git and Python installed, follow the instructions in Install Python and Git to set those up first.</p>"},{"location":"Installation/#clone-sdnext","title":"Clone SD.Next","text":"<p>Start terminal Launch your preferred system terminal and navigate to the directory where you want to install SD.Next. - This should be a directory which your user account has read/write/execute access to. - Installing SD.Next in a directory which requires admin permissions may cause it to not launch properly. - Installing SD.Next with superuser/admin permissions is not recommended.  </p> <p>Clone SD.Next Clone the repository by running following command in your desired location and then navigate into the cloned directory.</p> <p><code>git clone https://github.com/vladmandic/automatic &lt;optional directory name&gt;</code> </p>"},{"location":"Installation/#initial-installation","title":"Initial Installation","text":"<p>Important</p> <p>Decide on appropriate compute backend for your system ahead of time as that will determine which libraries are installed on your system</p> <p>--use-cuda       Use nVidia CUDA backend (autodetected by default) --use-rocm       Use AMD ROCm backend (autodetected by default) --use-ipex       Use Intel OneAPI XPU backend (autodetected by default) --use-openvino   Use Intel OpenVINO backend --use-zluda      Use ZLUDA --use-directml   Use DirectML</p> <p>Note</p> <p>nVidia CUIDA, AMD ROCm and Intel OneAPI XPU are autodetected when available  all other compute backends require explicit selection on first startup  For platform specific information, check out WSL | Intel Arc | DirectML | OpenVINO | ONNX &amp; Olive | ZLUDA | AMD ROCm | MacOS | nVidia</p>"},{"location":"Installation/#launch-sdnext","title":"Launch SD.Next","text":"<p>Run the appropriate launcher for your OS to start the web interface: - Windows: <code>webui.bat --debug --use-xxx</code> or <code>.\\webui.ps1 --debug --use-xxx</code> - Linux &amp; Mac: <code>./webui.sh --debug --use-xxx</code></p> <p>Now wait for few minutes to let the server install all required libraries. The server is finished launching when the console shows an entry for \"Startup time\".</p> <p>Tip</p> <p>If you don't want to use built-in <code>venv</code> support and prefer to run SD.Next in your own environment such as Docker container, Conda environment or any other virtual environment, you can skip <code>venv</code> create/activate and launch SD.Next directly (command line flags noted above still apply): <code>python launch.py --debug</code></p> <p>Tip</p> <p>For the initial setup and future tech support, it is advisable to include the <code>--debug</code> option which provides more detailed logging information.</p> <p>Tip</p> <p>All command line options can also be set via env variable  For example <code>--debug</code> is same as <code>set SD_DEBUG=true</code> </p> <p>Tip</p> <p>For improved memory utilization on Linux, see Malloc</p>"},{"location":"Installation/#first-time-setup","title":"First-Time Setup","text":"<ul> <li>Start the web interface   Once the web interface starts running, you can access it by opening your web browser and navigating to the address listed in the console next to \"Local URL.\" For most users, this should be <code>http://localhost:7860/</code>.   You will see a brief loading screen, then you should be taken to the <code>Text</code> tab.</li> <li>Adjust paths   You may want to adjust these settings in the <code>System</code>:<code>Settings</code> tab:</li> <li>If you already have models, LoRAs, Embeddings, LyCORIS, etc. set your paths in the <code>System Paths</code> page now</li> <li>Pay special attention to the <code>Folder with Huggingface models</code> and <code>Folder for Huggingface Cache</code> as they can grow to significant size</li> <li>You can use <code>Base path</code> to set a common root for all paths  </li> <li>Set your desired look &amp; feel   You can change the theme in the <code>User Interface</code> section.</li> <li>Save your settings   If you changed any settings in the previous step, click <code>Apply settings</code> to save those settings to your config file. This will also apply some defaults from built-in extensions.  </li> <li>Restart server   Click <code>Restart server</code> to re-launch the SD.Next server with the updated settings.  </li> </ul>"},{"location":"Installation/#install-python-and-git","title":"Install Python and Git","text":"<p>Note</p> <p>SD.Next supports Python versions <code>3.9.x</code> up to <code>3.12.3</code>  However, not all compute backends exist on Python 3.12 as they may be based on older <code>torch</code> versions  Recommended version is latest service release of Python <code>3.11.x</code>  Python versions higher than <code>3.12.3</code> are not supported  </p>"},{"location":"Installation/#windows","title":"Windows","text":""},{"location":"Installation/#git-for-windows","title":"Git-for-Windows","text":"<ol> <li>Download Git for Windows from the following link: Git for Windows</li> <li>Run the downloaded <code>.exe</code> file and follow the installation wizard.</li> <li>During the installation process, make sure to check the box for    \"Use Git from the Windows Command line and also from 3rd-party-software\" to add Git to your system's PATH.  </li> <li>Complete the installation by following the on-screen instructions.</li> </ol>"},{"location":"Installation/#python-for-windows","title":"Python-for-Windows","text":"<ol> <li>Download Python for Windows from the following link: Python for Windows</li> <li>Run the downloaded <code>.exe</code> file and follow the installation wizard.</li> <li>On the \"Customize Python\" screen, make sure to check the box for \"Add Python to PATH.\"</li> <li>Continue the installation by following the prompts.</li> <li>Once the installation is complete, you can open the command prompt and verify that Python is installed    by executing <code>python --version</code> and <code>pip --version</code> to check the Python and Pip versions respectively.</li> </ol>"},{"location":"Installation/#macos","title":"MacOS","text":""},{"location":"Installation/#git-for-macos","title":"Git-for-MacOS","text":"<ol> <li>Download Git for macOS from the following link: Git for macOS</li> <li>Open the downloaded <code>.pkg</code> file and follow the installation instructions.</li> <li>During the installation process, make sure to check the box for \"Install Git Bash\" to have a command-line Git interface.</li> <li>Complete the installation by following the prompts.</li> </ol>"},{"location":"Installation/#python-for-macos","title":"Python-for-MacOS","text":"<p>See these instructions for Python on MacOS (and an explanation why it's unique).</p>"},{"location":"Intel-ARC/","title":"IPEX and Intel GPUs","text":""},{"location":"Intel-ARC/#gpu-support-with-ipex-and-sdnext","title":"GPU Support with IPEX and SD.Next","text":"<ul> <li>Intel ARC Series  </li> <li>Intel Flex Series  </li> <li>Intel Max Series  </li> </ul> <p>iGPUs are not supported with IPEX, use OpenVINO with iGPUs.  </p> <p>Benchmarks for Intel ARC A770: https://github.com/vladmandic/automatic/wiki/Benchmark#intel-arc</p>"},{"location":"Intel-ARC/#data-types","title":"Data Types","text":"<p>BF16 is faster than FP16 in general.</p>"},{"location":"Intel-ARC/#backend","title":"Backend","text":"<p>Diffusers backend is 10%-25% faster than the original backend.</p>"},{"location":"Intel-ARC/#errors-with-igpu","title":"Errors with iGPU","text":"<p>Disable your iGPU (if any, e.g. UHD or Iris Xe) in the device manager.</p>"},{"location":"Intel-ARC/#windows-installation","title":"Windows Installation","text":""},{"location":"Intel-ARC/#preparations","title":"Preparations","text":"<ul> <li>Install <code>Intel GPU Driver</code>.</li> <li>Install <code>Intel OneAPI PyTorch GPU Dev</code></li> <li>Install <code>Git</code>.</li> <li>Install <code>Python</code> 3.10 or 3.11.</li> <li>Open CMD in a folder you want to install SD.Next.</li> </ul>"},{"location":"Intel-ARC/#using-sdnext","title":"Using SD.Next","text":"<p>Open a CMD Windows and install SD.Next from Github:</p> <pre><code>git clone https://github.com/vladmandic/automatic\ncd automatic\n</code></pre> <p>Then run SD.Next: <pre><code>\"C:\\Program Files (x86)\\Intel\\oneAPI\\pytorch-gpu-dev-0.5\\oneapi-vars.bat\"\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\2024.2\\env\\vars.bat\" \n.\\webui.bat --use-ipex\n</code></pre></p> <p>Note: It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"Intel-ARC/#linux-and-wsl-installation","title":"Linux and WSL Installation","text":""},{"location":"Intel-ARC/#install-intel-compute-runtime","title":"Install Intel Compute Runtime","text":""},{"location":"Intel-ARC/#ubuntu","title":"Ubuntu","text":"<p>Use Ubuntu 23.04 or newer.</p>"},{"location":"Intel-ARC/#dont-use-linux-kernel-68-or-69","title":"Don't use Linux Kernel 6.8 or 6.9!","text":"<p>https://github.com/intel/compute-runtime/issues/726 </p> <p>Note: Updating kernel is not neccesary for WSL.</p> <p>Then add the package lists for Intel Level Zero Drivers:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y ca-certificates wget gpg\nwget -qO - https://repositories.intel.com/gpu/intel-graphics.key | sudo gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg\necho \"deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy client\" | sudo tee /etc/apt/sources.list.d/intel-gpu-jammy.list\nsudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> <p>Then install the necessary packages:</p> <pre><code>sudo apt-get install intel-opencl-icd intel-level-zero-gpu level-zero git python3-pip python3-venv libgl1 libglib2.0-0 libgomp1\n</code></pre>"},{"location":"Intel-ARC/#arch-linux","title":"Arch Linux","text":"<p>Install the necessary packages for Arch Linux:</p> <pre><code>pacman -S intel-compute-runtime level-zero-headers level-zero-loader git python-pip python-virtualenv\n</code></pre>"},{"location":"Intel-ARC/#using-sdnext_1","title":"Using SD.Next","text":"<p>Install SD.Next from Github:</p> <pre><code>git clone https://github.com/vladmandic/automatic\ncd automatic\n./webui.sh --use-ipex\n</code></pre> <p>Note: It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"LoRA/","title":"LoRA","text":""},{"location":"LoRA/#what-is-lora","title":"What is LoRA?","text":"<p>LoRA, short for Low-Rank Adaptation, is a method used in Generative AI models to fine-tune the model with specific styles or concepts while keeping the process efficient and lightweight.</p> <p>Here\u2019s how it works in simple terms: - The Problem:   Fine-tuning a huge model like Stable Diffusion to recognize or replicate new styles or concepts (e.g., making it draw in the style of a specific artist or recognize unique objects) usually requires a lot of computational power and storage.</p> <p>The LoRA Solution: - Instead of tweaking all the internal parameters of the Generative AI model, LoRA focuses only on a small subset of them. Think of it as adding a \"style filter\" to the model that can be applied or removed as needed.   It reduces the complexity by breaking down large changes into smaller, simpler steps.   These smaller steps don\u2019t interfere with the original model, meaning you don\u2019t lose the model\u2019s core abilities.  </p> <p>Why it\u2019s Cool: - Efficient: It uses way less memory and is faster than traditional fine-tuning methods. - Flexible: You can train multiple LoRA \"filters\" for different styles or concepts and swap them in and out without modifying the base model. - Compatible: LoRA modules can be shared or reused easily, so artists and developers can collaborate or try out others\u2019 custom styles.</p> <p>Example Use Case - Say you want to teach Generative AI models to draw in the style of a fictional artist.   You can train a LoRA on a handful of sample images in that style.   Once trained, the LoRA module acts like a plug-in\u2014you just load it into Generative AI models, and the model starts generating images in that style!</p> <p>In short, LoRA makes it easy to teach models new tricks without overwhelming your computer or altering the original model. It\u2019s a user-friendly way to get customized results!  </p>"},{"location":"LoRA/#lora-types","title":"LoRA Types","text":"<p>There are many LoRA types, here are some of the most common ones: LoRA, DoRA, LoCon, HaDa, gLoRA, LoKR, LyCoris They vary in: - Which model components are being trained. Typically UNET, but can be TE as well - Which layers of the model are being trained. Each LoRA type trains different layers of the model - Math algorithm to extrach LoRA weights for the specific trained layers</p> <p>Warning</p> <p>LoRA must always match base model used for its training  For example, you cannot use SD1.5 LoRA with SD-XL model  </p> <p>Warning</p> <p>SD.Next attempts to automatically detect and apply the correct LoRA type.  However, new LoRA types are popping up all the time  If you find LoRA that is not compatible, please report it so we can add support for it.  </p>"},{"location":"LoRA/#how-to-use","title":"How to use?","text":"<ul> <li>Using UI: go to the networks tab and go to the lora's and select the lora you want and it will be added to the prompt.</li> <li>Manually: you can also add the lora manually by adding <code>&lt;lora:lora_name:strength&gt;</code> to the prompt and then selecting the lora you want to use.</li> </ul>"},{"location":"LoRA/#trigger-words","title":"Trigger words","text":"<p>Some (not all) LoRAs associate specific words during training so same words can be used to trigger specific behavior from the LoRA. SD.Next displays these trigger words in the UI -&gt; Networks -&gt; LoRA, but they can also be used manually in the prompt.  </p> <p>You can combine any number of LoRAs in a single prompt to get the desired output.  </p> <p>Tip</p> <p>If you want to automatically apply trigger words/tags to prompt, you can use <code>auto-apply</code> feature in \"Settings -&gt; Networks\" </p> <p>Tip</p> <p>You can change the strength of the lora by changing the number <code>&lt;lora:name:x.x&gt;</code> to the desired number  </p> <p>Tip</p> <p>If you're combining multiple LoRAs, you can also \"export\" that as a single lora via \"Models -&gt; Extract LoRA\" </p>"},{"location":"LoRA/#advanced","title":"Advanced","text":""},{"location":"LoRA/#component-weights","title":"Component weights","text":"<p>Typically <code>:strength</code> is applied uniformly for all components of the LoRA. However, you can also specify individual component weights by adding <code>:comp=x.x</code> to the LoRA tag. Example:  <code>&lt;lora:test_lora:te=0.5:unet=1.5&gt;</code> </p>"},{"location":"LoRA/#block-weights","title":"Block weights","text":"<p>Instead of using simple <code>:strength</code>, you can specify individual block weights for LoRA by adding <code>:in=x.x:mid=y.y:out=z.z</code> to the LoRA tag. Example <code>&lt;lora:test_lora:1.0:in=0:mid=1:out=0&gt;</code> </p>"},{"location":"LoRA/#stepwise-weights","title":"Stepwise weights","text":"<p>LoRA can also be applied will full per-step control by adding step-specific instuctions to the LoRA tag. Example: <code>&lt;lora:test_lora:te=0.1@1,0.6@6&gt;</code> Would mean apply LoRA to text-encoder with strength 0.1 on step 1 and then switch to strength 0.6 on step 6.  </p>"},{"location":"LoRA/#alternative-loader","title":"Alternative loader","text":"<p>SD.Next actually contains two separate implementations for LORA: - native implementation: used for most LoRAs - <code>diffuser</code> implementation: used as a fallback for models where native implementation does not exist</p> <p>Usage of <code>diffusers</code> implementation can be forced in \"Settings -&gt; Networks\"</p>"},{"location":"MacOS-Python/","title":"MacOS and Python","text":"<p>TL;DR: Installation Instructions</p> <p>The MacOS operating system requires Python, so it is installed by default.  However, if you are going to start using Python on your own, it is likely that you'll want to install new packages, perform package updates, and so on -- which can be a problem for MacOS -- it's important to let the operating system handle its own Python.</p> <p>Most sources online will tell you to use Homebrew as a package manager for MacOS, so it is a natural conclusion to consider using Homebrew for Python -- and it's common to discover that you already have Python installed through Homebrew as a dependency for something else.  However, this isn't the correct solution either -- it's really the same problem as the MacOS problem, but with a coat of paint.</p> <p>Homebrew's Python is there to support other packages.  Importantly, Homebrew deletes old versions of packages after 30 days.  So, if you are using, say, python 3.12 for random scripting and other tasks, but need version 3.10 for, say, SD.Next, Homebrew will delete 3.10, since it doesn't know that you need it for something (since you didn't install SD.Next via Homebrew).  </p> <p>The solution is to use another way to manage the Python version(s) that you use on your own.  I use asdf, which has a Python plugin, but there are others if you prefer something else.</p> <p>Sources / Further Reading:  - https://justinmayer.com/posts/homebrew-python-is-not-for-you/ - https://hackercodex.com/guide/python-development-environment-on-mac-osx/ - https://github.com/asdf-community/asdf-python - https://asdf-vm.com/ - https://docs.brew.sh/Installation</p>"},{"location":"MacOS-Python/#installation-instructions","title":"Installation Instructions","text":"<ol> <li>If you haven't got Homebrew installed already:</li> </ol> <pre><code>mkdir homebrew &amp;&amp; curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip-components 1 -C homebrew\n\neval \"$(homebrew/bin/brew shellenv)\"\nbrew update --force --quiet\nchmod -R go-w \"$(brew --prefix)/share/zsh\"\n</code></pre> <ol> <li>Install asdf and python build dependencies:</li> </ol> <pre><code>brew install asdf openssl readline sqlite3 xz zlib\n</code></pre> <ol> <li>Add asdf to <code>.zshrc</code> to use it immediately and persistently:</li> </ol> <pre><code>. $(brew --prefix asdf)/asdf.sh\necho -e \"\\n. $(brew --prefix asdf)/asdf.sh\" &gt;&gt; ~/.zshrc\n</code></pre> <ol> <li>Add the python asdf plugin:</li> </ol> <pre><code>asdf plugin add python\n\n# for SD.Next\nasdf install python 3.10.14\n\n# you may want the latest version too; take note of which version is installed\nasdf install python latest\n</code></pre> <ol> <li>Set the default global version of python. Since you will always want 3.10 for SD.Next, you will want to always specifically use that version.  You will probably want to use the command <code>python</code> in most contexts, and <code>python3.10</code> for version-specific uses.</li> </ol> <pre><code>asdf global python 3.12.2 \n# or whatever version you installed\n</code></pre> <ol> <li>Run SD.Next using python3.10:</li> </ol> <pre><code>export PYTHON=$(which python3.10)\ncd /path/to/SD.Next\n./webui.sh --debug\n</code></pre>"},{"location":"Malloc/","title":"Memory Allocator","text":"<p>Combination of Linux default memory allocator <code>malloc</code> with Python's default memory allocator is pessimistic when it comes to system memory garbage collection and it will sometimes hold on to allocated memory longer than necessary even if GC is triggered explicitly. This appears to user as a memory leak as process memory usage grows over time. This is especially noticeable when frequently loading/unloading large objects such as models or LoRAs.  </p> <p>Tip</p> <p>For Linux deployments you can switch out memory allocator to <code>tcmalloc</code> or <code>jemalloc</code>  which are more efficient and have better memory management.</p> <p>Note</p> <p>This applies to system memory only and has no impact on GPU memory management</p> <p>tcmalloc</p> <pre><code>sudo apt install google-perftools  \nsudo ldconfig  \nexport LD_PRELOAD=libtcmalloc.so.4  \n./webui.sh --debug\n</code></pre> <p>jemalloc</p> <pre><code>sudo apt install libjemalloc2\nsudo ldconfig  \nexport LD_PRELOAD=libjemalloc.so.2  \n./webui.sh --debug\n</code></pre>"},{"location":"Model-Support/","title":"Model support","text":"<p>Additional models will be added as they become available and there is public interest in them See models overview for details on each model, including their architecture, complexity and other info  </p> <ul> <li>RunwayML Stable Diffusion 1.x and 2.x (all variants)</li> <li>StabilityAI Stable Diffusion XL, StabilityAI Stable Diffusion 3.0 Medium, StabilityAI Stable Diffusion 3.5 Medium, Large, Large Turbo</li> <li>StabilityAI Stable Video Diffusion Base, XT 1.0, XT 1.1</li> <li>StabilityAI Stable Cascade Full and Lite</li> <li>Black Forest Labs FLUX.1 Dev, Schnell  </li> <li>NVLabs Sana</li> <li>AuraFlow</li> <li>AlphaVLLM Lumina-Next-SFT </li> <li>Playground AI v1, v2 256, v2 512, v2 1024 and latest v2.5</li> <li>Tencent HunyuanDiT</li> <li>OmniGen </li> <li>Meissonic </li> <li>Kwai Kolors </li> <li>CogView 3+</li> <li>LCM: Latent Consistency Models</li> <li>aMUSEd 256 and 512</li> <li>Segmind Vega, Segmind SSD-1B, Segmind SegMoE SD and SD-XL, Segmind SD Distilled (all variants)</li> <li>Kandinsky 2.1 and 2.2 and latest 3.0</li> <li>PixArt-\u03b1 XL 2 Medium and Large, PixArt-\u03a3</li> <li>Warp Wuerstchen</li> <li>Tsinghua UniDiffusion</li> <li>DeepFloyd IF Medium and Large</li> <li>ModelScope T2V</li> <li>BLIP-Diffusion</li> <li>KOALA 700M</li> <li>VGen</li> <li>SDXS</li> <li>Hyper-SD</li> </ul>"},{"location":"Models-Tab/","title":"Models Tab","text":"<p>Current | Convert | Merge | Validate | Huggingface | CivitAI | Update | Extract LoRa</p>"},{"location":"Models-Tab/#current","title":"Current","text":""},{"location":"Models-Tab/#analyze-current-model","title":"Analyze current model","text":"<p>Analyze the current model to understand its characteristics, training data and details, so you can understand the model on a more technological level.</p>"},{"location":"Models-Tab/#convert","title":"Convert","text":""},{"location":"Models-Tab/#convert-current-model","title":"Convert current model","text":"<p>Convert the current model to make it have a different name, a different precision, a different format, make it pruned or non pruned and etc.</p>"},{"location":"Models-Tab/#merge","title":"Merge","text":""},{"location":"Models-Tab/#simple-merge","title":"Simple merge","text":"<p>Simple merge allows you to merge 2 models with each other in a non advanced way, so there are basic options are available.  </p> <p>Alpha Ratio: The alpha ratio standard value is 0.5 that means the merged model will be 50% primary model and 50% secondary model. 0.2 would be 80% primary and 20% secondary model.  </p> <p>Overwrite model: If you check this the merged model will overwrite the primary model.  </p> <p>Save Metadata: Saves the metadata of both the primary and secondary model into merged one.  </p> <p>Weights clip: Forces merged weights to be no heavier than the original model, preventing burn in and overly saturated models.  </p> <p>ReBasin: Performs multiple merges with permutations in order to keep more features from both models.  </p> <p>Model precision: lets you choose what precision you want the merged model to have.  </p> <p>Merge device: Which device to use for the merge.  </p> <p>Replace VAE: Allows you to replace the VAE of the merged model with your preferred VAE.</p>"},{"location":"Models-Tab/#preset-block-merge","title":"Preset Block Merge","text":"<p>Similar to the simple merge tab, only here you can check the SDXL checkbox if your models are SDXL models and no need to change the alpha ratio by yourself, instead you can select one of the many presets available.</p>"},{"location":"Models-Tab/#validate","title":"Validate","text":""},{"location":"Models-Tab/#list-model-details","title":"List model details","text":"<p>Lists all the details(not as much as in current) of all the checkpoint models.</p>"},{"location":"Models-Tab/#calculate-hash-for-all-models","title":"Calculate hash for all models","text":"<p>Calculates hash for each checkpoint model. The hash makes it easier to identify the model or model used(even if its renamed).</p>"},{"location":"Models-Tab/#huggingface","title":"Huggingface","text":"<p>Search models on Huggingface and download them into SD.NEXT with this sub tab.</p>"},{"location":"Models-Tab/#civitai","title":"CivitAI","text":""},{"location":"Models-Tab/#fetch-information","title":"Fetch information","text":"<p>Fetches preview and metadata information for all models with missing information. Models with existing previews and information are not updated.</p>"},{"location":"Models-Tab/#search-for-models","title":"Search for models","text":"<p>Allows you to search for models on CivitAI. Just select the type of model you want, a keyword and optionally some tags. After that click on search and you results will show below.</p>"},{"location":"Models-Tab/#download-model","title":"Download model","text":"<p>To download a model you need to click on the desired model in searched models then the model version and lastly the variant.</p>"},{"location":"Models-Tab/#update","title":"Update","text":"<p>Checks for updates for all checkpoint models.</p>"},{"location":"Models-Tab/#extract-lora","title":"Extract LoRa","text":"<p>Allows you to tweak a lora to the preferred strength.</p>"},{"location":"Models/","title":"Models","text":"<p>List of popular text-to-image generative models with their respective parameters and architecture overview Original URL: https://github.com/vladmandic/automatic/wiki/Models</p> Publisher Model Version Size Diffusion Architecture Model Params Text Encoder(s) TE Params Auto Encoder Other StabilityAI Stable Diffusion 1.5 2.28GB UNet 0.86B CLiP ViT-L 0.12B VAE StabilityAI Stable Diffusion 2.1 2.58GB UNet 0.86B CLiP ViT-H 0.34B VAE StabilityAI Stable Diffusion XL 6.94GB UNet 2.56B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE StabilityAI Stable Diffusion 3.0 Medium 15.14GB MMDiT 2.0B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE StabilityAI Stable Diffusion 3.5 Medium 15.89GB MMDiT 2.25B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE StabilityAI Stable Diffusion 3.5 Large 26.98GB MMDiT 8.05B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE StabilityAI Stable Cascade Medium 11.82GB Multi-stage UNet 1.56B + 3.6B CLiP ViT-G 0.69B 42x VQE StabilityAI Stable Cascade Lite 4.97GB Multi-stage UNet 0.7B + 1.0B CLiP ViT-G 0.69B 42x VQE Black Forest Labs Flux 1 Dev/Schnell 32.93GB MMDiT 11.9B CLiP ViT-L + T5-XXL 0.12B + 4.769B 16ch VAE NVLabs Sana 1600M 12.63GB MMDiT 1.60B Gemma2 2.61B DC-AE NVLabs Sana 600M 7.51GB MMDiT 0.59B Gemma2 2.61B DC-AE FAL AuraFlow 0.3 31.90GB MMDiT 6.8B UMT5 12.1B VAE AlphaVLLM Lumina Next SFT 8.67GB DiT 1.7B Gemma 2.5B VAE LM PixArt Alpha XL 2 21.3GB DiT 0.61B T5-XXL 4.76B VAE PixArt Sigma XL 2 21.3GB DiT 0.61B T5-XXL 4.76B VAE Segmind SSD-1B N/A 8.72GB UNet 1.33B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Segmind Vega N/A 6.43GB UNet 0.75B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Segmind Tiny N/A 1.03GB UNet 0.32B CLiP ViT-L 0.12B VAE Kwai Kolors N/A 17.40GB UNnet 2.58B ChatGLM 6.24B VAE LM PlaygroundAI Playground 1.0 4.95GB UNet 0.86B CLiP ViT-L 0.12B VAE PlaygroundAI Playground 2.x 13.35GB UNet 2.56B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Tencent HunyuanDiT 1.2 14.09GB DiT 1.5B BERT + T5-XL 3.52B + 1.67B VAE LM Warp AI Wuerstchen N/A 12.16GB Multi-stage UNet 1.0B + 1.05B CLiP ViT-L + ViT+G 0.12B + 0.69B 42x VQE Kandinsky Kandinsky 2.2 5.15GB Unet 1.25B CLiP ViT-G 0.69B VQ Kandinsky Kandinsky 3.0 27.72GB Unet 3.05B T5-XXXL 8.72B VQ Thudm CogView 3 Plus 24.96GB DiT 2.85B T5-XXL 4.76B VAE IDKiro SDXS N/A 2.05GB UNet 0.32B CLiP ViT-L 0.12B VAE Open-MUSE aMUSEd 256 3.41GB ViT 0.60B CLiP ViT-L 0.12B VQ Koala Koala 700M 6.58GB UNet 0.78B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Thu-ML UniDiffuser v1 5.37GB U-ViT 0.95B CLiP ViT-L + CLiP ViT-B 0.12B + 0.16B VAE Salesforce BLIP-Diffusion N/A 7.23GB UNet 0.86B CLiP ViT-L + BLiP-2 0.12B + 0.49B VAE DeepFloyd IF M 12.79GB Multi-stage UNet 0.37B + 0.46B T5-XXL 4.76B Pixel DeepFloyd IF L 15.48GB Multi-stage UNet 0.61B + 0.93B T5-XXL 4.76B Pixel MeissonFlow Meissonic N/A 3.64GB DiT 1.18B CLiP ViT-H 0.35B VQ VectorSpaceLab OmniGen v1 15.47GB Transformer 3.76B None 0 VAE Phi-3"},{"location":"Models/#notes","title":"Notes","text":"<ul> <li>Created using SD.Next built-in model analyzer  </li> <li>Number of parameters is proportional to model complexity and ability to learn   Quality of generated images is also influenced by training data and duration of training  </li> <li>Size refers to original model variant in 16bit precision where available   Quantized variations may be smaller  </li> <li>Distilled variants are not included as typical goal-distilling does not change underlying model params   e.g. Turbo/LCM/Hyper/Lightning/etc. or even Dev/Schnell  </li> </ul>"},{"location":"NNCF-Compression/","title":"NNCF Model Comporession","text":""},{"location":"NNCF-Compression/#usage","title":"Usage","text":"<ol> <li>Use Diffusers backend. <code>Execution &amp; Models</code> -&gt; <code>Execution backend</code></li> <li>Go into <code>Compute Settings</code> </li> <li>Enable <code>Compress Model weights with NNCF</code> options  </li> <li>Reload the model.  </li> </ol> <p>Note: VAE Upcast (in Diffusers settings) has to be set to false if you use the VAE option. If you get black images with SDXL models, use the FP16 Fixed VAE.  </p>"},{"location":"NNCF-Compression/#features","title":"Features","text":"<ul> <li>Uses INT8, halves the model size Saves 3.4 GB of VRAM with SDXL  </li> </ul>"},{"location":"NNCF-Compression/#disadvantages","title":"Disadvantages","text":"<ul> <li>It is Autocast, GPU will still use 16 Bit to run the model and will be slower  </li> <li>Not implemented in Original backend  </li> <li>Fused projections are not compatible with NNCF  </li> <li>Using Loras will make generations slower  </li> </ul>"},{"location":"NNCF-Compression/#options","title":"Options","text":"<p>These results compares NNCF 8 bit to 16 bit.  </p> <ul> <li>Model:   Compresses UNet or Transformers part of the model.   This is where the most memory savings happens for Stable Diffusion.  </li> </ul> <p>SDXL: 2500 MB~ memory savings.   SD 1.5: 750 MB~ memory savings.   PixArt-XL-2: 600 MB~ memory savings.  </p> <ul> <li>Text Encoder:   Compresses Text Encoder parts of the model.   This is where the most memory savings happens for PixArt.  </li> </ul> <p>PixArt-XL-2: 4750 MB~ memory savings.   SDXL: 750 MB~ memory savings.   SD 1.5: 120 MB~ memory savings.  </p> <ul> <li>VAE:   Compresses VAE part of the model.   Memory savings from compressing VAE is pretty small.  </li> </ul> <p>SD 1.5 / SDXL / PixArt-XL-2: 75 MB~ memory savings.  </p>"},{"location":"Networks/","title":"Networks","text":"<p>The Networks button under Generation Controls opens a convenient side menu, providing shortcuts to your: - Models - LoRas: see LoRA for more information - Styles: see Styles for more information - Embeddings - VAEs - Latent history</p>"},{"location":"Networks/#reference-models","title":"Reference models","text":"<p>In the models tab, you can find Reference models section This is the list of predefined models that can be immediately selected and used Once reference model is selected, it will be automatically downloaded and saved in the models folder for future use  </p> <p>Tip</p> <p>Reference models is recommended way to start with any base model</p> <p>Reference models section can be hidden in \"Settings -&gt; Networks\" </p>"},{"location":"Networks/#latent-history","title":"Latent history","text":"<p>Near the completion of each generate workflow, its latents (raw data from model before final decoding) are added to latent history. Why? So they can be reused and reprocessed at will.  </p> <p>For example, text + refine + detailer will generate 3 entries in latent history, one for each step. You can then reprocess any of these steps with different settings or even different models.</p> <p>You can control the length of maintained latent history in \"Settings -&gt; Execution\"</p>"},{"location":"ONNX-Runtime/","title":"ONNX Runtime","text":"<p>SD.Next includes support for ONNX Runtime.</p>"},{"location":"ONNX-Runtime/#how-to","title":"How to","text":"<p>Currently, we can't use <code>--use-directml</code> because there's no release of <code>torch-directml</code> built with latest PyTorch. (this does not mean that you can't use DmlExecutionProvider)</p> <p>Change <code>Execution backend</code> to <code>diffusers</code> and <code>Diffusers pipeline</code> to <code>ONNX Stable Diffusion</code> on the <code>System</code> tab.</p>"},{"location":"ONNX-Runtime/#performance","title":"Performance","text":"<p>The performance depends on the execution provider.</p>"},{"location":"ONNX-Runtime/#execution-providers","title":"Execution Providers","text":"<p>Currently, <code>CUDAExecutionProvider</code> and <code>DmlExecutionProvider</code> are supported.</p> ONNX Olive GPU CPU CPUExecutionProvider \u2705 \u274c \u274c \u2705 DmlExecutionProvider \u2705 \u2705 \u2705 \u274c CUDAExecutionProvider \u2705 \u2705 \u2705 \u274c ROCMExecutionProvider \u2705 \ud83d\udea7 \u2705 \u274c OpenVINOExecutionProvider \u2705 \u2705 \u2705 \u2705"},{"location":"ONNX-Runtime/#cpuexecutionprovider","title":"CPUExecutionProvider","text":"<p>Not recommended.</p> <p>Enabled by default.</p>"},{"location":"ONNX-Runtime/#dmlexecutionprovider","title":"DmlExecutionProvider","text":"<p>You can select <code>DmlExecutionProvider</code> by installing <code>onnxruntime-directml</code>.</p> <p>DirectX 12 API is required. (Windows or WSL)</p>"},{"location":"ONNX-Runtime/#cudaexecutionprovider","title":"CUDAExecutionProvider","text":"<p>You can select <code>CUDAExecutionProvider</code> by installing <code>onnxruntime-gpu</code>. (may have been automatically installed)</p>"},{"location":"ONNX-Runtime/#rocmexecutionprovider","title":"\ud83d\udea7 ROCMExecutionProvider","text":"<p>Olive for ROCm is working in progress.</p>"},{"location":"ONNX-Runtime/#openvinoexecutionprovider","title":"\ud83d\udea7 OpenVINOExecutionProvider","text":"<p>Under development.</p>"},{"location":"ONNX-Runtime/#supported","title":"Supported","text":"<ul> <li>Models from huggingface</li> <li>Hires and second pass (without sdxl refiner)</li> <li>.safetensors VAE</li> </ul>"},{"location":"ONNX-Runtime/#known-issues","title":"Known issues","text":"<ul> <li>SD Inpaint may not work.</li> <li>SD Upscale pipeline is not tested.</li> <li>SDXL Refiner does not work. (due to onnxruntime's issue)</li> </ul>"},{"location":"ONNX-Runtime/#faq","title":"FAQ","text":""},{"location":"ONNX-Runtime/#im-getting-onnxstablediffusionpipeline__init__-missing-4-required-positional-arguments-vae_encoder-vae_decoder-text_encoder-and-unet","title":"I'm getting <code>OnnxStableDiffusionPipeline.__init__() missing 4 required positional arguments: 'vae_encoder', 'vae_decoder', 'text_encoder', and 'unet'</code>.","text":"<p>It's due to the broken model cache which was previously generated by failed conversion or Olive run. Find one in <code>models/ONNX/cache</code> and remove it. You can also use <code>ONNX</code> tab on UI. (You should enable it on settings to make it show up)</p>"},{"location":"ONNX-Runtime/#olive","title":"Olive","text":"<p>Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation. (from pypi)</p>"},{"location":"ONNX-Runtime/#how-to_1","title":"How to","text":"<p>As Olive optimizes the models in ONNX format, you should set up ONNX Runtime first.</p> <ol> <li>Go to <code>System</code> tab \u2192 <code>Compute Settings</code>.</li> <li>Select <code>Model</code>, <code>Text Encoder</code> and <code>VAE</code> in <code>Compile Model</code>.</li> <li>Set <code>Model compile backend</code> to <code>olive-ai</code>.</li> </ol> <p>Olive-specific settings are under <code>Olive</code> in <code>Compute Settings</code>.</p>"},{"location":"ONNX-Runtime/#how-to-switch-to-olive-from-torch-directml","title":"How to switch to Olive from torch-directml","text":"<p>Run these commands using PowerShell.</p> <pre><code>.\\venv\\Scripts\\activate\npip uninstall torch-directml\npip install torch torchvision --upgrade\npip install onnxruntime-directml\n.\\webui.bat\n</code></pre>"},{"location":"ONNX-Runtime/#from-checkpoint","title":"From checkpoint","text":"<p>Model optimization occurs automatically before generation.</p> <p>Target models can be .safetensors, .ckpt, Diffusers pretrained model and the optimization progress takes time depending on your system and execution provider.</p> <p>The optimized models are automatically cached and used later to create images of the same size (height and width).</p>"},{"location":"ONNX-Runtime/#from-huggingface","title":"From Huggingface","text":"<p>If your system memory is not enough to optimize model or you don't want to waste your time to optimize the model yourself, you can download optimized model from Huggingface.</p> <p>Go to <code>Models</code> \u2192 <code>Huggingface</code> tab and download optimized model.</p>"},{"location":"ONNX-Runtime/#advanced-usage","title":"Advanced Usage","text":""},{"location":"ONNX-Runtime/#customize-olive-workflow","title":"Customize Olive workflow","text":"<p>TBA</p>"},{"location":"ONNX-Runtime/#performance_1","title":"Performance","text":"Property Value Prompt a castle, best quality Negative Prompt worst quality Sampler Euler Sampling Steps 20 Device RX 7900 XTX 24GB Version olive-ai(0.4.0) onnxruntime-directml(1.16.3) ROCm(5.6) torch(olive: 2.1.2, rocm: 2.1.0) Model runwayml/stable-diffusion-v1-5 (ROCm), lshqqytiger/stable-diffusion-v1-5-olive (Olive) Precision fp16 Token Merging Olive(0, not supported) ROCm(0.5) Olive with DmlExecutionProvider ROCm"},{"location":"ONNX-Runtime/#pros-and-cons","title":"Pros and Cons","text":""},{"location":"ONNX-Runtime/#pros","title":"Pros","text":"<ul> <li>The generation is faster.</li> <li>Uses less graphics memory.</li> </ul>"},{"location":"ONNX-Runtime/#cons","title":"Cons","text":"<ul> <li>Optimization is required for every models and image sizes.</li> <li>Some features are unavailable.</li> </ul>"},{"location":"ONNX-Runtime/#faq_1","title":"FAQ","text":""},{"location":"ONNX-Runtime/#my-execution-provider-does-not-show-up-in-my-settings","title":"My execution provider does not show up in my settings","text":"<p>After activating python venv, run this command and try again:</p> <pre><code>(venv) $ pip uninstall onnxruntime onnxruntime-... -y\n</code></pre>"},{"location":"Offload/","title":"Offload","text":"<p>Offload is a method of moving model or parts of the model between the GPU memory (VRAM) and system memory (RAM) in order to reduce the memory footprint of the model and allow it to run on GPUs with lower VRAM.</p>"},{"location":"Offload/#automatic-offload","title":"Automatic offload","text":"<p>Tip</p> <p>Automatic offload is set by the Settings -&gt; Diffusers -&gt; Model offload mode </p>"},{"location":"Offload/#balanced","title":"Balanced","text":"<p>Balanced offload works differently than all other offloading methods as it performs offloading only when the VRAM usage exceeds the user-specified threshold.</p> <ul> <li>Recommended for compatible high VRAM GPUs  </li> <li>Faster but requires compatible platform and sufficient VRAM  </li> <li>Balanced offload moves parts of the model depending on the user-specified threshold   allowing to control how much VRAM is to be used  </li> <li>Default memory threshold is 75% of the available GPU memory   Configure threshold in Settings -&gt; Diffusers -&gt; Max GPU memory for balanced offload mode in GB</li> </ul> <p>Warning</p> <p>Not compatible with Optimum.Quanto <code>qint</code> quantization  </p>"},{"location":"Offload/#sequential","title":"Sequential","text":"<p>Works on layer-by-layer basis of each model component that is marked as offload-compatible  </p> <ul> <li>Recommended for low VRAM GPUs</li> <li>Much slower but allows to run large models such as FLUX even on GPUs with 6GB VRAM  </li> </ul> <p>Warning</p> <p>Not compatible with Quanto <code>qint</code> or BitsAndBytes <code>nf4</code> quantization  </p> <p>Note</p> <p>Use of <code>--lowvram</code> automatically triggers use of sequenential offload</p>"},{"location":"Offload/#model","title":"Model","text":"<p>Works on model component level by offloading components that are marked as offload-compatible For example, VAE, text-encoder, etc.</p> <ul> <li>Recommended for medium when balanced offload is not compatible  </li> <li>Higher compatibility than either balanced and sequential, but lesser savings  </li> </ul> <p>Limitations: N/A</p> <p>Note</p> <p>Use of <code>--medvram</code> automatically triggers use of model offload</p>"},{"location":"Offload/#manual-offload","title":"Manual Offload","text":"<p>In addition to above mentioned automatic offload method, SD.Next includes manual offload methods which are less granular and are only supported for specific models.</p> <ul> <li>Move base model to CPU when using refiner</li> <li>Move base model to CPU when using VAE</li> <li>Move refiner model to CPU when not in use</li> </ul>"},{"location":"Offload/#performance-notes","title":"Performance Notes","text":"<ul> <li>Tested using SDXL with 2 large LoRA models  </li> <li>Sequential offload is default for GPUs with 4GB or less</li> <li>Balanced offload is default for GPUs with more than 4GB   Balanced offload is slower than no offload, but allows using large models such as SD35 and FLUX.1 out-of-the-box</li> <li>Balanced offload set to default values  </li> <li>LoRA overhead is measured in sec for first and subsequent iterations  </li> <li>LoRA mode=backup can use up to 2x system memory   Using backup can be prohibitive on large models such as SD35 or FLUX.1</li> </ul> Offload mode LoRA type LoRA mode LoRA overhead End-to-end it/s Note none none N/A N/A 6.7 fastest inference balanced none N/A N/A 4.5 default without LoRA sequential none N/A N/A 0.6 lowvram none native backup 1.8 / 0.0 6.0 balanced native backup 1.3 / 0.0 2.8 sequential native backup 5.8 / 0.0 0.5 none native fuse 1.3 / 1.3 4.8 balanced native fuse 2.8 / 2.5 3.1 default with LoRA sequential native fuse 8.8 / 7.7 0.4 none diffusers default 2.9 / 2.9 3.8 balanced diffusers default 2.2 / 2.2 2.1 sequential diffusers default 4.6 / 4.6 0.3 none diffusers fuse 5.7 / 5.7 2.0 balanced diffusers fuse N/A did not complete sequential diffusers fuse N/A did not complete"},{"location":"OpenVINO/","title":"OpenVINO","text":"<p>OpenVINO is an open-source toolkit for optimizing and deploying deep learning models.  Compiles models for your hardware.  Supports Linux and Windows  Supports CPU / GPU / iGPU / NPU  Supports AMD GPUs on Windows with FP16 support.  Supports INTEL dGPUs and iGPUs.  Supports NVIDIA GPUs.  Supports CPUs with BF16 and INT8 support.  Supports Quantization and Model Compression.  Supports multiple devices at the same time using Hetero Device*.  </p> <p>It is basically a TensorRT / Olive competitor that works with any hardware.  </p>"},{"location":"OpenVINO/#installation","title":"Installation","text":""},{"location":"OpenVINO/#preparations","title":"Preparations","text":"<ul> <li>Install the drivers for your device.</li> <li>Install <code>git</code> and <code>python</code>.</li> <li>Open CMD in a folder you want to install SD.Next.</li> </ul> <p>Note: Do not mix OpenVINO with your old install. Treat OpenVINO as a seperate backend.  </p>"},{"location":"OpenVINO/#using-sdnext-with-openvino","title":"Using SD.Next with OpenVINO","text":"<p>Install SD.Next from Github:</p> <pre><code>git clone https://github.com/vladmandic/automatic\n</code></pre> <p>Then enter into the automatic folder:</p> <pre><code>cd automatic\n</code></pre> <p>Then start WebUI with this command:</p> <p>Windows:</p> <pre><code>.\\webui.bat --use-openvino\n</code></pre> <p>Linux:</p> <pre><code>./webui.sh --use-openvino\n</code></pre>"},{"location":"OpenVINO/#more-info","title":"More Info","text":""},{"location":"OpenVINO/#limitations","title":"Limitations","text":"<ul> <li>Same limitations with TensorRT / Olive applies here too.  </li> <li>Compilation takes a few minutes and any change to Resolution / Batch Size / LoRa will trigger recompilation.  </li> <li>Attention Slicing and HyperTile will not work.  </li> <li>OpenVINO will lock you in the Diffusers backend.  </li> <li>Only ESRGAN upscalers can work with OpenVINO.   Enable Upscaler on compile settings if you want to use OpenVINO with Upscalers.  </li> </ul>"},{"location":"OpenVINO/#quantization","title":"Quantization","text":"<p>Quantization enables 8 bit support without autocast. Enable <code>OpenVINO Quantize Models with NNCF</code> option in Compute Settings to use it. Note: Quantization has noticeable quality impact and generally not recommended.  </p>"},{"location":"OpenVINO/#model-compression","title":"Model Compression","text":"<p>Enable <code>Compress Model weights with NNCF</code> option in Compute Settings to use it. Select a 4 bit mode from <code>OpenVINO compress mode for NNCF</code> to use 4 bit. For GPUs; select both CPU and GPU from the device selection if you want to use GPU with Model Compression.  </p> <p>Note: VAE will be compressed to INT8 if you use a 4 bit mode.  </p>"},{"location":"OpenVINO/#custom-devices","title":"Custom Devices","text":"<p>Use the <code>OpenVINO devices to use</code> option in <code>Compute Settings</code> if you want to specify a device. Selecting multiple devices will use multiple devices as a single <code>HETERO</code> device.  </p> <p>Using <code>--device-id</code> cli argument with the WebUI will use a GPU with the specified Device ID. Using <code>--use-cpu openvino</code> cli argument with the WebUI will use the CPU.  </p>"},{"location":"OpenVINO/#model-caching","title":"Model Caching","text":"<p>OpenVINO will save compiled models to cache folder so you won't have to compile them again. <code>OpenVINO disable model caching</code> option in Compute Settings will disable caching. <code>Directory for OpenVINO cache</code> option in System Paths will set a new location for saving OpenVINO caches.  </p>"},{"location":"Performance-Tuning/","title":"Performance Tuning","text":""},{"location":"Performance-Tuning/#introduction","title":"Introduction","text":"<p>Hi folks, it's your (moderately) friendly neighborhood Aptronym here!</p> <p>People are always asking me how to get the most it/s out of their GPUs, now... this is a complicated subject due to the current GPU landscape, the wide range of GPUs that can do stable diffusion just within each manufacturer, going back years (nvidia 1080s, 1050s, or RX500 series), as well as varying and often limited amounts of VRAM, possibly inside of a laptop.</p> <p>That's not even counting the complicated selection of inference platforms SDNext has available, everything from plain everyday CUDA to Onnxruntime/Olive, and now ZLUDA, as well as our two built-in backends, Original and Diffusers.</p> <p>I can't promise you'll be able to use all of these options, I can't promise they won't crash your instance (or be buggy for that matter), that's going to vary wildly from GPU, OS, RAM, VRAM, current chosen inference platform, and backend, and some of these options will not work together, or at all, on some platforms. You will have to test that yourselves, but we are hoping to build a matrix of sorts showing what is available and what works with what, but that's going to take some user testing and feedback.</p> <p>If you help us by providing feedback (issues w/logs, screenshots, etc.), which we never have enough of, we will do our best to correct what we can and ensure that the experience of using SDNext is as optimized as it can be. Some limitations we won't be able to overcome purely do to things beyond our control, such as conflicts caused by the nature of the inference platform.</p> <p>Just as an added ray of sunshine to give you hope, there are plans in motion to create a more self-optimizing system that will not only configure itself for optimal performance based on your setup, but also react to what you're doing, what model you have loaded (SD15 v. SDXL is a huge difference), and specified preferences, so that it is always as performant as possible while sacrificing minimal quality loss. We would welcome any assistance with this, recently-laid-off-due-to-AI developers (Curse you Devin!) or any developers, or people with coding experience, at all.</p>"},{"location":"Performance-Tuning/#backends","title":"Backends","text":"<p>Let's start with the simplest thing, your chosen backend. I know there are some Original mode holdouts, that are clinging on to it for various reasons (I was one myself for months), sometimes simply due to being ignorant of the fact that the Diffusers backend can do SD 1.5 models as well as SDXL and the others.</p> <p>So let me put that to rest once and for all:</p> <p>You need to be using Diffusers</p> <p>It is not only now the default on installation, it is faster and better with your precious VRAM usage, as well as being the only backend where future enhancements and features will take place. Wean yourself off of Original, as you are missing out on a lot of functionality, scripts, and features. I know you may have some precious extension that only works in Original, but we have a lot built in now that you may not have noticed.</p> <p>If the extension is so important to have, then pop by on discord and let me know what it is and explain why. We've added features for less, so if it's a good thing to include, or if we can't already do it to some degree, we'll take a look at it and consider it fairly. If you can't present a good use case, your chances are lower (visual aids help too).</p> <p>All of that being said, Original mode is going to be retired at some point in the near future, there really just aren't any benefits to keeping it around other than more work for our small development team and contributors.</p> <p>One more time, USE DIFFUSERS!</p>"},{"location":"Performance-Tuning/#compute-settings","title":"Compute Settings","text":"<p>Note: Changing any of the settings on this page will require you to, at the least, unload the model and then reload it (after hitting apply!), as these settings are applied on model load, not in realtime.</p> <p>Generally speaking, for most GPUs our user-base has (mostly Nvidia on Windows judging by discord roles, so using CUDA), you are going to want the settings below (BF16 is possible too if using 30xx+).  </p> <p>Good settings:  </p> <p> </p> <p>Bad settings:  </p> <p> </p> <p>In general, using any of these selected \"bad\" settings is considered a bad thing, you only want to use any of these if necessary to make your card work with SD. They're slower and use up a lot more memory. Unless you're on OpenVINO, things show up as fp32 there anyway due to how it works. Leave that alone</p> <p>That being said, if you are having strange issues with squares and whatnot and you're on something other than a newer Nvidia GPU (2000s and up?), you may wish to try these settings. Upcast sampling is the better version of --no-half, use it if at all possible. Try these one by one, unload the model, reload the model, then test generate, hopefully you will find one, or a combination that works.  </p> <p></p>"},{"location":"Performance-Tuning/#model-compile","title":"Model Compile","text":"<p>To use any of the model compile options, you must select via the checkboxes, at least one of these: Model, VAE, Text Encoder, Upscaler. It's probably pretty pointless to compile the text encoder, but Model and VAE will net you a large boost in speed. If you use upscalers a lot, by all means, select that too.</p>"},{"location":"Performance-Tuning/#stable-fast","title":"Stable-fast","text":"<p>Stable-fast is one of the model compiling options, and if you can use it with your setup (Nvidia GPUs, maybe Zluda), you should, as it's a big speed-up. First you will need to open a terminal, activate the venv for sdnext, and from the root sdnext folder (typically automatic), you will type <code>python cli\\install-sf.py</code> and stand back while it hopefully works its magic, acquiring, or potentially compiling then installing the most recent version of stable-fast.  </p> <p>Note that you can do the install of stable-fast while SDNext is already running in another terminal, it will attempt to load the library when you select it, so there is no need to shut down or restart.</p>"},{"location":"Performance-Tuning/#onediff","title":"OneDiff","text":"<p>Linux (and perhaps Mac) users may also use <code>OneDiff</code>, which should be better/faster/stronger than <code>Stable-Fast</code>, though you will need to manually execute a <code>pip install -U onediff</code> from a venv console to install the necessary libraries.  </p> <p>NOTE: Do NOT compile the Text Encoder with OneDiff, it makes things slower.  </p> <p>If you want to thank anyone for OneDiff support, hit up @aifartist on our Discord.  </p>"},{"location":"Performance-Tuning/#inference-options","title":"Inference options","text":"<p>I'm skipping ahead here a bit since I want to get to the heart of the matter and expand later.</p> <p>These will be some of the easiest things you can do.</p>"},{"location":"Performance-Tuning/#token-merging-tome","title":"Token Merging (ToMe)","text":"<p>Sadly ToMe does not work at the same time as Hypertile. It will be disabled if Hypertile is enabled because Hypertile is faster. If hypertile works for you right now, don't even bother touching this.</p> <p>Token merging, aka ToMe, has been around for quite a while and still provides a performance gain if you desire to use it. In short it merges tokens, saving memory and speeding up generations. You can easily use it at 0.3-0.4, performance goes up as the number does, going up higher is up to you but you can always do an xyz and test to see.</p> <p>Default settings: </p> <p> </p> <p>Suggested settings: </p> <p> </p> <p>Honestly, 0.5 and up is the real performance gain, but you test and decide yourself. </p> <p>Bear in mind that it does have a quality impact on your image, greater the higher the setting, and will make perfect reproduction from the same seed and prompt impossible afaik.</p> <p>There is a new implementation along the same lines as ToMe, called ToDo, but works far better. That's in our queue for the near future!</p>"},{"location":"Performance-Tuning/#hypertile","title":"Hypertile","text":"<p>Overrides and is incompatible with ToMe, also can cause issues with some platforms, so if you get errors after turning this on, that might be why. Using Hypertile VAE might also cause issues, so try on and off. Hypertile is a much preferable option to <code>Token Merging</code> at the moment.</p> <p>As long as you enable <code>Hypertile UNet</code> you're good to go, the default settings should suffice, as 0 is auto-adjusting the tile size, it adapts to be half the size of your shortest image side.  </p> <p>Just don't even mess with <code>Hypertile VAE</code>, tends to cause more issues than it could possibly be worth, which isn't much.</p> <p>You may of course screw with the <code>swap size</code> and <code>UNet depth</code> as you like, I did test them briefly but saw some little benefit.</p> <p>Default settings: </p> <p></p> <p>Suggested settings: </p> <p></p> <p>Also has a quality impact, but I've never measured it personally</p>"},{"location":"Performance-Tuning/#other-settings","title":"Other settings","text":"<p>Parallel process images in batch is intended for img2img batch mode. If you set batch size=n, typically it generates n images for each input, with this setting, it will generate 1 image for each input, but process n in parallel.  </p> <p></p>"},{"location":"Process/","title":"Process","text":"<p>This guide covers the process tab in the app. The process tab is where you can upload or otherwise provide your image(s) and apply the functions to them.</p> <p>Tabs:</p> <ul> <li>Process Image </li> <li>Process Batch </li> <li>Process Folder </li> <li>Interrogate Image </li> <li>Interrogate Batch </li> <li>Visual Query</li> </ul> <p>Functions:</p> <ul> <li>Upscale </li> <li>Video </li> <li>GFPGAN </li> <li>CodeFormer </li> <li>Remove background</li> </ul>"},{"location":"Process/#tabs","title":"Tabs","text":""},{"location":"Process/#process-image","title":"Process Image","text":"<p>Upload your image so you can apply the functions explained below.</p>"},{"location":"Process/#process-batch","title":"Process Batch","text":"<p>Upload your Batch so you can apply the functions explained below.</p>"},{"location":"Process/#process-folder","title":"Process Folder","text":"<p>Type in the location of your input directory and output directory so you can apply the functions explained below.</p>"},{"location":"Process/#interrogate-image","title":"Interrogate Image","text":"<p>Upload your image so you can Interrogate or Analyze your image.  Interrogate: Gives description of image  Analyze: Gives description of image in percentages</p>"},{"location":"Process/#interrogate-batch","title":"Interrogate Batch","text":"<p>Upload your Batch so you can Interrogate or Analyze your Batch.  Interrogate: Gives description of batch  Analyze: Gives description of batch in percentages</p>"},{"location":"Process/#visual-query","title":"Visual Query","text":"<p>Upload your image so you can process user query on your image. Question can be left blank in which case its a default for the selected model which generates a description, similar to interrogate (but typically more detailed) or you can type something like this: <code>Describe this image</code> or <code>How many cats are in this image?</code></p>"},{"location":"Process/#functions","title":"Functions","text":""},{"location":"Process/#upscale","title":"Upscale","text":"<p>Upscale your image with the chosen model.</p>"},{"location":"Process/#video","title":"Video","text":"<p>Processes a bath or folder of images into a video, gif or png.</p>"},{"location":"Process/#gpfgan","title":"GPFGAN","text":"<p>Uses the GPFGAN model on your image and applies face restore that enhances facial details.</p>"},{"location":"Process/#codeformer","title":"CodeFormer","text":"<p>Use the CodeFormer model on your image and applies face restore that enhances facial details.</p>"},{"location":"Process/#remove-background","title":"Remove background","text":"<p>Uses a model to remove the background of your main subject in your image, experiment with the settings for desired result.</p>"},{"location":"Prompting/","title":"Prompting with different models","text":"<p>This is not in-depth technical article, but instead intended to demystify the process of prompting with different models - and importantly - why.</p> <p>Basically, effectiveness prompting depends on: - Text encoder used in the model - Dataset used to train the model - Structure prompt and negative prompt</p> <p>Note</p> <p>Different models have different prompting best practices  When in doubt, refer to model specific notes on CivitiAI website</p> <p>For example: standard SD15/SDXL models are prompted very differently vs Pony-derived models vs SD35 or Flux.1 derived models - it all comes down to which text enocder is used and how it was trained.  </p>"},{"location":"Prompting/#prompt-parser","title":"Prompt Parser","text":"<p>SDNext supports multiple prompt parsers: - Native: Default - A1111: Included with compatibility with A1111 original implementation - Compel: Use compel-style attention syntax, e.g. <code>dog++</code> or <code>dog--</code> - xHinker: Alternative engine compatible with <code>T5</code> text-encoder  </p>"},{"location":"Prompting/#native-prompt-parser","title":"Native Prompt Parser","text":""},{"location":"Prompting/#prompt-attention","title":"Prompt Attention","text":"<ul> <li><code>(x)</code>: emphasis. Multiplies the attention to x by 1.1, equivalent to <code>(x:1.1)</code></li> <li><code>[x]</code>: de-emphasis, divides the attention to x by 1.1, approximate to <code>(x:0.91)</code></li> <li><code>(x:number)</code>: Multiply the attention by number, either higher or lower than <code>1</code></li> </ul> <p>Note</p> <p>Multiple attentions are multiplied, not added: <code>((a dog:1.5) with a (bone:1.5)1.5)</code> is the same as (a dog:3.375) (with a bone:2.25)  </p>"},{"location":"Prompting/#other-prompt-syntax","title":"Other Prompt Syntax","text":"<ul> <li><code>\\(x\\)</code>: Escapes the parentheses, this is how you'd use parenthesis without it causing the parser to add emphasis</li> </ul>"},{"location":"Prompting/#prompt-scheduling","title":"Prompt Scheduling","text":"<ul> <li><code>[x:x:number]</code>: Uses the first x until number steps have finished, then uses the second x  </li> </ul> <p>Note</p> <p>number can be <code>int</code> in which case it's considered exact step count,  or <code>float</code> in which case it's considered percentage of total steps  </p>"},{"location":"Prompting/#components","title":"Components","text":""},{"location":"Prompting/#text-encoder","title":"Text encoder","text":"<ul> <li>SD15 started with: CLIP-ViT/L</li> <li>SDXL adds second encoder: OpenCLIP-ViT/G</li> <li>SD3 adds third (optional) encoder: T5 Version 1.1</li> </ul> <p>And other models are following similar approach - replace early simple encoders with only basic understanding of English language with more advanced models. For example: PixArt-\u03a3 and Tencent HunyuanDiT</p> <p>Unfortunately, in StabilityAI models, text encoder results are concatenated one after each other so oldest <code>CLIP-ViT/L</code> still has biggest impact.</p>"},{"location":"Prompting/#dataset","title":"Dataset","text":"<p>Note</p> <p>Nearly all modern models are trained on subset of laion-5b dataset  Later fine-tuning introduces additional data (e.g. Pony uses heavily tagged dataset)  </p>"},{"location":"Prompting/#prompting-tips","title":"Prompting tips","text":""},{"location":"Prompting/#prompt-engineering","title":"Prompt Engineering","text":"<p>Know your model: different models were trained on different datasets, some may understand terms other models don't  </p> <p>Main groups</p> <ul> <li>Mediums: best starting a prompt with it after specifying artist   Examples: painting, photograph, drawing, sketch</li> <li>Flavors: best left as separate token at the end of the prompt   Examples: ray tracing, fine art, black and white, pixiv, artstation</li> <li>Movements: best added to prompt with as keyword   Examples: pop art, photorealism </li> <li>Artists: best starting a prompt with it   Examples: greg rutkowski, artgerm, dc comics, picasso </li> </ul> <p>Modifiers</p> <ul> <li>Feel: best near the end   Examples: beautiful, sharp focus, 4k, hdr, high detailed, canon 5d</li> <li>Composition: best at front, but only use if results don't fit   Examples: 1man, 1woman</li> </ul> <p>Negative Prompt</p> <ul> <li>Any keyword can be specified in a negative prompt as well   Examples: watermark</li> </ul> <p>Advanced Prompt Modifiers </p> <p>For original backend only: - Alternate between words: <code>[word1|word2]</code> will alternate between <code>word1</code> and <code>word2</code> in every denoising step, blending the two concepts - Switch words during denoising: <code>[word1:word2:0.3]</code> will use <code>word1</code> for the first 30% of steps, then change it to <code>word2</code> - Force include multiple objects \"AND\"  </p> <p>Hints</p> <ul> <li>Use either artists or movements   Using both may result in one overpowering the other, or in unexpected outcome  </li> <li>Select medium that fits artist   It helps model a lot to know which medium to use when styling  </li> <li>Add action after subject   Examples: portrait, standing, sitting</li> <li>Moving things to the front of prompt may increase its emphasis   Example: cartoon drawing of a woman as pixar vs pixar drawing of a woman</li> <li>Use both subject and scene keywords   Example: woman on a beach</li> </ul> <p>Example</p> <p>(composition) (artist) (medium) (subject) (action) (scene) (movement) (flavor) (feel) 1woman greg rutkowski painting of a woman happy front portrait on a beach as photorealism, sharp focus, artstation</p>"},{"location":"Prompting/#negative-prompt","title":"Negative prompt","text":"<p>When you add negative prompt, what happens is that its basically appended to prompt just using negative weights. This makes model \"steer away\" from terms in negative prompt, but to do so it first has to INTRODUCE them to the context.</p> <p>So by adding negative prompt, you're: - Limiting the freedom of the model   Quite commonly this means that all faces will look similar without variance, etc. - Making the model more prone to hallucinations   If you're trying to steer away from something that doesn't exist in the first place, it might introduce opposite of what you're trying to do.</p> <p>All-in-all, negative prompts are useful to steer model away from certain concepts, but should not be used as a \"general purpose long negative prompt\"</p>"},{"location":"Prompting/#prompt-attention_1","title":"Prompt attention","text":"<p>First, nothing wrong with wanting to add extra attention to certain parts of the prompt. But it should be used sparingly and only when needed and keep in mind that overall prompt should be balanced.</p> <p>E.g, If your prompt has 10 words and you're raising attention to 5 of them, you're basically telling the model average weight of the prompt is massive. Common result? Overbaked images.</p> <p>Prompt balance doesn't have to be perfect, but any prompt that has more than few of words with light attention modifiers is a red flag.  </p> <p>Also, keep in mind that adding extremely strong attention modifiers such as <code>(((xyz)))</code> or <code>(xyz:1.5)</code> will make model completely loose concept of prompt as a whole.</p>"},{"location":"Prompting/#model-specific-tips","title":"Model Specific Tips","text":""},{"location":"Prompting/#sd15","title":"SD15","text":"<p>Dataset: - Used small laion-5b subset with preexisting captions - No major effort was put into processing or captioning other than what's in the subset</p> <p>Result? - Model that's has basic understanding of the world, but to use it effectively you need to \"hunt\" for the right keywords - Models are easily fine-tuned as nothing-forced-nothing-removed approach was used during training - How to prompt? Old-school prompting which put heavy emphasis on keywords and attention modifiers  </p>"},{"location":"Prompting/#sd21","title":"SD21","text":"<p>Dataset: - Trained on same dataset as SD15 and then fine-tuned on extended dataset with larger resolution - It was also censored in the final parts of training which introduced heavy bias  </p> <p>Result? - Its almost like model was \"lobotomized\".   E.g. For concept of \"topless\", its not like it just doesn't have sample data for it, it was \"burned out\" of the model.   So to add concepts that model did not understand it takes massive effort, almost close to retraining. Fail.</p>"},{"location":"Prompting/#sdxl","title":"SDXL","text":"<p>Dataset: - Used larger subset with extended captions and also diverse resolutions - Instead of censoring in the final stages, they simply pruned dataset used for training.</p> <p>Result? - Model that knows-what-it-knows and the rest can be added with fine-tuning. - E.g. It knows what \"topless\" means, just doesn't have enough examples to develop it fully. - How to prompt? Extended captions and second text encoder mean that model can be prompted in a more natural way and extended use of keywords and attention-modifiers should be avoided.</p> <ul> <li>Extra note: PonyXL was extensively trained on heavily tagged dataset without natural language captions and as such it needs to be prompted differently - using tags and keywords instead of natural language.</li> </ul>"},{"location":"Prompting/#sd3","title":"SD3","text":"<ul> <li>Used even larger subset with even more diverse resolutions, but dataset itself was processed differently:</li> <li>Censored not only by removing images from dataset, but also by modifying them by censoring parts of the image</li> <li>Captioned extensively using LLM. Unfortunately, not ON-TOP of existing captions so it can augment them, but instead it REPLACED them - thus keywords already existing in dataset are not trained for at all.</li> </ul> <p>Result? - Model that thinks-it-knows-everything and now it's up to you to prove it wrong.   E.g., it knows what a topless person looks like, and its \"certain\" that nipples should be presented as blank.   Which means it would like take a massive effort to retrain what it learned in the wrong way.   I hope to be proven wrong, but this looks like a fail. - How to prompt? Use of long LLM-generated captions means that model should be prompted using very descriptive language and completely stop using using styles, keywords and attention-modifiers. And since LLM generated captions do not include styles as we know them, we need to replace them with detailed descriptions - it's almost like we need to think like LLM to prompt it - how would LLM describe the image I'm trying to create?</p>"},{"location":"Quantization/","title":"Quantization","text":"<p>Quantization is a process of: - storage-optimization   reducing the memory footprint of the model by reducing the precision of parameters in a model - compute-optimization   speed up the inference process by providing optimized kernels for native execution in quantized precision  </p> <p>For storage-only quantization, the model is quantized to lower precision but the operations are still performed in the original precision which means that each operation needs to be upcasted to the original precision before execution resulting in a performance overhead.</p>"},{"location":"Quantization/#using-quantized-models","title":"Using Quantized Models","text":"<p>Quantization can be done in multiple ways: on-the-fly- by quantizing on-the-fly during model load   available by selecting settings -&gt; quantization for some quantization types - by quantizing immediately after model load   available by selecting settings -&gt; quantization for all quantization types - by simply loading a pre-quantized model   quantization type will be auto-determined at the start of the load - during model training itself   out-of-scope for this document  </p>"},{"location":"Quantization/#quantization-engines","title":"Quantization Engines","text":"<p>Tip</p> <p>If you're on Windows with a compatible GPU, you may try WSL2 for broader feature compatibiliy  See WSL Wiki for more details</p>"},{"location":"Quantization/#bitsandbytes","title":"BitsAndBytes","text":"<p>Typical models pre-quantized with <code>bitsandbytes</code> would have look like <code>*nf4.safetensors</code> or <code>*fp8.safetensors</code></p> <p>Note</p> <p>BnB is the only quantization method that allows for usage of balanced offload as well as quantization on-the-fly during load, thus it is considered most versatile choice, but it is not available on all platforms.</p> <p>Limitations: - default <code>bitsandbytes</code> package only supports nVidia GPUs   some quantization types require newer GPU with supported CUDA ops: e.g. nVidia Turing GPUs or newer - <code>bitsandbytes</code> relies on <code>triton</code> packages which are not available on windows unless manually compiled/installed   without them, performance is significantly reduced - for AMD/ROCm: link  - for Intel/IPEX: link </p>"},{"location":"Quantization/#optimum-quanto","title":"Optimum-Quanto","text":"<p>Typical models pre-quantized with <code>optimum.quanto</code> would have look like <code>*qint.safetensors</code>.</p> <p>Note</p> <p>OQ is highly efficient with its qint8/qint4 quantization types, but its usage is limited to specific platforms and cannot be used with broad offloading methods</p> <p>Limitations: - requires <code>torch==2.4.0</code>   if you're running older torch, you can try upgrading it or running sdnext with <code>--reinstall</code> flag - not compatible with balanced offload - not supported on Intel Arc/IPEX since IPEX is still based on Torch 2.3 - not supported with Zluda since Zluda does not support torch 2.4  </p>"},{"location":"Quantization/#gguf","title":"GGUF","text":"<p>GGUF is a binary file format used to package pre-quantized models.  </p> <p>GGUF is originally desiged by <code>llama.cpp</code> project and intended to be used with its GGML execution runtime. However, without GGML, GGUF provides storage-only quantization which means that every operation needs to be upcast to current device precision before execution (typically FP16 or BF16) which comes with a significant performance overhead.</p> <p>Warning</p> <p>Right now, all popular T2I inference UIs (SD.Next, Forge, ComfyUI, InvokeAI etc.) are using GGUF as storage-only and as such usage of GGUF is not recommended!  </p> <ul> <li><code>gguf</code> supports wide range of quantization types and is not platform or GPU dependent  </li> <li><code>gguf</code> does not provide native GPU kernels which means that <code>gguf</code> is purely a storage optimization </li> <li><code>gguf</code> reduces model size and memory usage, but it does slow down model inference since all quantized weights are de-quantized on-the-fly  </li> </ul> <p>Limitations: - <code>gguf</code> is not compatible with model offloading as it would trigger de-quantization - note: only supported component in <code>gguf</code> binary format is UNET/Transformer   you cannot load all-in-one single-file GGUF model</p>"},{"location":"Quantization/#nncf","title":"NNCF","text":"<p>NNCF provides full cross-platform storage-only quantization (referred to as model compression) with optional platform-specific compute-optimization (available only on OpenVINO platform)  </p> <p>Note</p> <p>Advantage of NNCF is that it does work on any platform: if you're having issues with <code>optimum-quanto</code> or <code>bitsandbytes</code>, try it out!  </p> <ul> <li>broad platform and GPU support  </li> <li>enable in Settings -&gt; Compute -&gt; Compress model weights with NNCF </li> <li>see NNCF Wiki for more details  </li> </ul>"},{"location":"Quantization/#errors","title":"Errors","text":"<p>Caution</p> <p>Using incompatible configurations will result in errors during model load:</p> <ul> <li>BitsAndBytes nf4 quantization is not compatible with sequential offload   <p>Error: Blockwise quantization only supports 16/32-bit floats  </p> </li> <li>Quanto qint quantization is not compatible with balanced offload   <p>Error: QBytesTensor.new() missing 5 required positional arguments  </p> </li> <li>Quanto qint quantization is not compatible with sequential offload   <p>Error: Expected all tensors to be on the same device  </p> </li> </ul>"},{"location":"Reprocess/","title":"Reprocess","text":"<p>Reprocess is a new top-level option that allows to repeat the processing of a given image using different parameters.</p>"},{"location":"Reprocess/#use-cases","title":"Use cases","text":"<p>Below are several examples where reprocess can be useful: - You can generate an image at lower quality   such as using lower step count or using fast vae instead of full   and if you like the image, bump up step count and switch vae and reprocess it - Or process as usual and if you like the results   enable detailer, hires or refine workflows and reprocess - Or if you don't like what hires did because you used too high of denoise strength   you can go back and reprocess original image before hires, but now with different strength  </p>"},{"location":"Reprocess/#how-does-it-work","title":"How does it work","text":"<p>Reprocess works on latent level, meaning that any workflow that runs in latent space will add its latents to reprocess history For example, each of the following operations will add to reprocess history: - Generate - HiRes - Detailer - Refine</p> <p>Tip</p> <p>Reprocess by default will take last known latent  However, you can see entire history of latents in Networks -&gt; History and pick one that you want to reprocess  Length of maintained history is configuratble in Settings -&gt; Networks </p>"},{"location":"SD-Pipeline-How-it-Works/","title":"Stable Diffusion Pipeline","text":"<p>This is probably the best end-to-end semi-technical article: https://stable-diffusion-art.com/how-stable-diffusion-work/</p> <p>And a detailed look at diffusion process: https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048</p> <p>But this is a short look at the pipeline:</p> <ol> <li>Encoder / Conditioning    Text (via tokenizer) or image (via vision model) to semantic map    (e.g CLiP text encoder)  </li> <li>Sampler    Generate noise which is starting point to map to content    (e.g. k_lms)  </li> <li>Diffuser    Create vector content based on resolved noise + semantic map    (e.g. actual stable diffusion checkpoint)  </li> <li>Autoencoder    Maps between latent and pixel space (actually creates images from vectors)    (e.g. typically some image-database trained GAN)  </li> <li>Denoising    Get meaningful images from pixel signatures    Basically, blends what autoencoder inserted using information from diffuser    (e.g. U-NET)</li> <li>Loop and repeat    From step#3 with cross-attention to blend results  </li> <li>Run additional models as needed  </li> <li>Upscale (e.g. ESRGAN)  </li> <li>Resore Face (e.g. GFPGAN or CodeFormer)  </li> </ol>"},{"location":"SD-Training-Methods/","title":"StableDiffusion Training Methods","text":""},{"location":"SD-Training-Methods/#fine-tuning","title":"fine-tuning","text":"<ul> <li>retrains parts of the hypernetwork with new data thus modifying original weights   requires large and precisely labelled dataset</li> <li>size is same as original model size, ~2-7gb</li> <li>verdict: prohibitive due to large dataset and effort required</li> </ul>"},{"location":"SD-Training-Methods/#model-merge","title":"model merge","text":"<ul> <li>combines weights from multiple models according to specified rules</li> <li>verdict: highly desired to create pre-set models for specific use-case</li> </ul>"},{"location":"SD-Training-Methods/#textual-inversion","title":"textual inversion","text":"<ul> <li>assign vector to a new concept with originally one vector per embedding, hacks to enable multi-vector embeddings   works by expanding vocabulary of a model, but majority of learned content is actually assembled from existing concepts   can be considered as a formula on which already learned weights should be combined to achieve learned concept  </li> <li>size 768/1024b per vector</li> <li>verdict: best currently viable short-term training solution</li> </ul>"},{"location":"SD-Training-Methods/#aesthetic-gradient","title":"aesthetic gradient","text":"<ul> <li>uses low-precision trained embeddings to steer clip using classifier guidance   training is very cheap, but classifier guidance sloes down image generation   result is basic transfer of style from learned image to generated image  </li> <li>size is same as embedding</li> <li>origin: independent work</li> <li>verdict: inconsistent results with minimal value</li> </ul>"},{"location":"SD-Training-Methods/#custom-diffusion","title":"custom diffusion","text":"<ul> <li>fine-tuning specific model matrices with textual inversion   similar speed and memory requirements to embedding training and supposedly gives better results in less steps</li> <li>size ~50mb</li> <li>origin: cmu</li> <li>verdict: possibly promising, requires further investigation, surprisingly low chatter on this topic</li> </ul>"},{"location":"SD-Training-Methods/#hypernetwork","title":"hypernetwork","text":"<ul> <li>similar to model fine-tuning, but adds small a small neural network that on-the-fly modifies weights of the last two layers of the main model   works like adaptive head that steers model in a learned direction so primary use-case is style transfer, not concept transfer</li> <li>size is limited to learned layers, ~100-200mb</li> <li>origin: leaked from novel.ai</li> <li>verdict: lower priority as concept transfer is more important than style transfer</li> </ul>"},{"location":"SD-Training-Methods/#null-text-inversion","title":"null-text inversion","text":"<ul> <li>similar concept to textual inversion, but trains unconditional embedding that is used for classifier free guidance instead of text embedding   resulting embedding is apparently more detailed than standard textual embedding</li> <li>size is larger but comparable to textual inversion</li> <li>origin: google</li> <li>verdict: possibly promising, requires further investigation, but no working prototype as of yet</li> </ul>"},{"location":"SD-Training-Methods/#clip-inversion","title":"clip inversion","text":"<ul> <li>similar concept to textual inversion, but uses clip embedding instead of text embedding  </li> <li>size is same as textual inversion</li> <li>origin: google</li> <li>verdict: prohibitive due to requirement of specially fine-tuned model as a starting point</li> </ul>"},{"location":"SD-Training-Methods/#dream-artist","title":"dream artist","text":"<ul> <li>variation on ti training where both positive and negative embeddings are created</li> <li>size is same as textual inversion</li> <li>origin: independent work</li> <li>verdict: skip for now as solution does not appear to be sufficiently maintained</li> </ul>"},{"location":"SD-Training-Methods/#dreambooth","title":"dreambooth","text":"<ul> <li>similar to model fine-tuning except it adds information on top of model instead of forgetting/overwriting existing concepts  </li> <li>size is equal to original model size, ~2-7gb</li> <li>origin: google, but heavily modified by independent work</li> <li>verdict: prohibitive due to resulting size and requirement to load full model on-demand</li> </ul>"},{"location":"SD-Training-Methods/#lora","title":"lora","text":"<ul> <li>\"low-rank adaptation of large language models\"   injects trainable layers to steer cross attention layers   very flexible, but memory intensive so limited training opportunities on normal gpu   multiple incompatible implementations: should choose which implementation to use  </li> <li>size varies from ~5mb to full-model size, average ~150-300mb</li> <li>origin: microsoft</li> </ul>"},{"location":"SD-XL/","title":"StableDiffusion-XL","text":""},{"location":"SD-XL/#downloading-sd-xl","title":"Downloading SD-XL","text":"<p>You can simply download these two files from Huggingface and place them into your normal checkpoint directory, though we recommend a subfolder.</p> <ul> <li>SD-XL Base</li> <li>SD-XL Refiner</li> </ul>"},{"location":"SD-XL/#setup-for-sd-xl","title":"Setup for SD-XL","text":"<p>To facilitate easy use of SD-XL and swapping between refiners, backends, and pipelines, we recommend selecting the following items in your Settings Tab, on the User Interface page:</p> <p></p> <p>Once you select them, hit Apply settings, and then Restart server. When the server returns to being active and your browser page reloads, the Quicksettings at the top of your screen should look like this (assuming you were using SDXL):</p> <p></p>"},{"location":"SD-XL/#vram-optimization","title":"VRAM Optimization","text":"<p>There are now 3 methods of memory optimization with the Diffusers backend, and consequently SDXL: Model Shuffle, Medvram, and Lowvram. Choose one based on your GPU, VRAM, and how large you want your batches to be.</p> <p>Note: <code>VAE Tiling</code> can be enabled to save additional VRAM if necessary, but it is recommended to use <code>VAE Slicing</code> if you do not have abundant VRAM. <code>Enable attention slicing</code> should generally not be used, as the performance impact is significant.</p>"},{"location":"SD-XL/#option-1-model-shuffle","title":"Option 1: Model Shuffle","text":"<p>\"Model Shuffle\" is a memory optimization feature that dynamically moves different parts of the model between the GPU and CPU to efficiently utilize VRAM. This is enabled when the following 3 options are Enabled in the Diffusers settings page:</p> <ul> <li>Move the base model to CPU when using the refiner.</li> <li>Move the refiner model to CPU when not in use.</li> <li>Move the UNet to CPU during VAE decoding.</li> </ul> <p>To use <code>Model Shuffling</code> do not have <code>--medvram</code> or <code>--lowvram</code> active, then use the following settings:</p> <p></p> <p>The important parts are the 3 Move checkboxes.</p> <p>Note that if you activate either <code>CPU model offload</code> or <code>Sequential CPU offload</code>, they will deactivate and ignore Model Shuffling. VRAM Usage: \"Model Shuffle\" will work in 8 GB of VRAM.</p>"},{"location":"SD-XL/#option-2-medvram","title":"Option 2: MEDVRAM","text":"<p>If you have a GPU with 6GB VRAM or require larger batches of SD-XL images without VRAM constraints, you can use the <code>--medvram</code> command line argument. This option significantly reduces VRAM requirements at the expense of inference speed. Cannot be used with <code>--lowvram/Sequential CPU offloading</code> Note: Until some upstream fixes go in, this will not work with DML or MAC.</p> <p>Alternatively, you can enable the <code>Enable model CPU offload</code> checkbox in the <code>Settings</code> tab on the <code>Diffusers settings</code> page:</p> <ul> <li>Model CPU Offload (same as <code>--medvram</code>)</li> <li>VAE slicing (recommended)</li> <li>Attention slicing is NOT recommended.</li> </ul> <p></p> <p>VRAM Usage: \"Model CPU Offload\" can work in 6 GB of VRAM.</p> <p>Note: <code>--medvram</code> supersedes the <code>Model Shuffle</code> option (e.g., Move base model, refiner model, UNet), and is mutually exclusive and cannot be used together with <code>--lowvram/Sequential CPU offload</code></p>"},{"location":"SD-XL/#option-3-lowvram","title":"Option 3: LOWVRAM","text":"<p>If your GPU has as low as 2GB of VRAM, start your SD.Next session with <code>--lowvram</code> as a command line argument to vastly reduce VRAM requirements at the cost of even more inference speed. This is essentially the <code>Enable Sequential CPU offload</code> setting.</p> <p></p> <p>Note: VAE slicing, VAE tiling, and Attention slicing are all enabled by <code>--lowvram</code> regardless of the checkboxes.</p> <p>Using this setting with a GPU that has higher VRAM, your generations will take even longer, but you will be able to do ridiculously large batches of SD-XL images, up to and including 24 on a 12GB GPU.</p> <p>Note: Until some upstream fixes go in, this will not work with SDXL LoRA's and SD 1.5.</p> <p>We look forward to seeing how large your batches can get, do let us know on the Discord server, and we HIGHLY RECOMMEND that you continue down this guide and configure your SD.Next with the Fixed FP16 VAE!</p>"},{"location":"SD-XL/#fixed-fp16-vae","title":"Fixed FP16 VAE","text":"<p>It is currently recommended to use a Fixed FP16 VAE rather than the ones built into the SD-XL base and refiner for significant reductions in VRAM (from 6GB of VRAM to &lt;1GB VRAM) and a doubling of VAE processing speed.</p> <p>Below are the instructions for installation and use:</p> <ul> <li>Download Fixed FP16 VAE to your VAE folder.</li> <li>In your <code>Settings</code> tab, go to <code>Diffusers settings</code> and set <code>VAE Upcasting</code> to <code>False</code> and hit Apply.  </li> <li>Select the your VAE and simply <code>Reload Checkpoint</code> to reload the model or  hit <code>Restart server</code>.</li> </ul> <p>You should be good to go, Enjoy the huge performance boost!</p>"},{"location":"SD-XL/#using-sd-xl","title":"Using SD-XL","text":"<ul> <li>To use SD-XL, first SD.Next needs to be in Diffusers mode, not Original, select it from the Backend radio buttons.</li> <li>Then select Stable Diffusion XL from the Pipeline dropdown.</li> <li>Next select the sd_xl_base_1.0.safetensors file from the Checkpoint dropdown.</li> <li>(optional) Finally select the sd_xl_refiner_1.0.safetensors file from the Refiner dropdown.  </li> </ul>"},{"location":"SD-XL/#using-sd-xl-refiner","title":"Using SD-XL Refiner","text":"<p>To use refiner, it first needs to be loaded and then it can be enabled using <code>Second pass</code> option in the UI. Note that use of refiner is not necessary as base model can produce very good results on its own.</p> <p>Refiner can be used in two-modes: as in traditional workflow or with early handover from base to refiner. In either case, refiner will use calculated number of steps based on <code>Refiner steps</code>.</p> <p>If <code>denoise start</code> is set to <code>0</code> or <code>1</code>, then traditional workflow is used:</p> <ul> <li>Base model runs from <code>0</code> -&gt; <code>100%</code> using Sampling steps.</li> <li>Refiner model runs from <code>0</code> -&gt; <code>100%</code> using Refiner steps.</li> </ul> <p>However, in this mode, refiner may not produce much better result and will likely only smoothen the image as base model already reached 100% and there is insufficient remaining noise for refiner to do anything else.</p> <p>If <code>refiner start</code> is set to any other value, then handover mode is used:</p> <ul> <li>Base model runs from <code>0%</code> -&gt; <code>denoise_start%</code>    Exact number is calculated internally to be Sampling steps.</li> <li>Refiner model runs from <code>denoise_start%</code> -&gt; <code>100%</code>    Exact number is calculated internally to be Refiner steps.</li> </ul> <p>In this mode, using different ratio of steps for primary and refiner is allowed, but may result in unexpected results as base and refiner operations will not be perfectly aligned.</p> <p>Note on steps vs timesteps. In all workflows (even with original backend and SD 1.5 models), steps do not refer directly do operations internally executed. Steps are used to calculate actual values at which operations will be executed. For example, steps=6 roughly means execute denoising at 0% -&gt; 20%  -&gt; 40%  -&gt; 60%  -&gt; 80%  -&gt; 100%. For that reason, specifying steps above 99 is meaningless.</p>"},{"location":"SD3/","title":"Stable Diffusion 3.x","text":"<p>StabilityAI's Stable Diffusion 3 family consists of:</p> <ul> <li>Stable Diffusion 3.0 Medium</li> <li>Stable Diffusion 3.5 Medium</li> <li>Stable Diffusion 3.5 Large</li> <li>Stable Diffusion 3.5 Large Turbo</li> </ul> <p></p> <p>Important</p> <p>Allow gated access  This is a gated model, you need to accept the terms and conditions to use it  For more information see Gated Access Wiki</p> <p>Important</p> <p>Set offloading  Set appropriate offloading setting before loading the model to avoid out-of-memory errors  For more information see Offloading Wiki </p> <p>Important</p> <p>Choose quantization  Check compatibility of different quantizations with your platform and GPU!  For more information see Quantization Wiki </p> <p>[!TIP] Use reference models Use of reference models is recommended over manually downloaded models! Simply select it from Networks -&gt; Models -&gt; Reference   and model will be auto-downloaded on first use  </p>"},{"location":"SD3/#components","title":"Components","text":"<p>SD3.x model consists of:</p> <ul> <li>Unet/Transformer: MMDiT  </li> <li>Text encoder 1: CLIP-ViT/L,</li> <li>Text encoder 2: OpenCLIP-ViT/G,</li> <li>Text encoder 3: T5-XXL Version 1.1 </li> <li>VAE</li> </ul> <p>When using reference models, all components will be loaded as needed. If using manually downloaded model, you need to ensure that all components are correctly configured and available. Note that majority of available downloads are not actually all-in-one models and are instead just a part of the full model with individual components.</p> <p>Important</p> <p>Do not attempt to assemble a full model by loading all individual components  That may be how some other apps are designed to work, but its not how SD.Next works  Always load full model and then replace individual components as needed  </p> <p>Warning</p> <p>If you're getting error message during model load: <code>file=xxx is not a complete model</code>  It means exactly that - you're trying to load a model component instead of full model  </p> <p>Tip</p> <p>For convience, you can add setting that allow quick replacements of model components to your  quicksettings by adding Settings -&gt; User Interface -&gt; Quicksettings  list -&gt; sd_unet, sd_vae, sd_text_encoder</p> <p></p>"},{"location":"SD3/#fine-tunes","title":"Fine-tunes","text":""},{"location":"SD3/#diffusers","title":"Diffusers","text":"<p>N/A: Currently there are no known diffusers fine-tunes of SD3.0 or SD3.5 models</p>"},{"location":"SD3/#loras","title":"LoRAs","text":"<p>SD.Next includes support for SD3 LoRAs  </p> <p>Since LoRA keys vary significantly between tools used to train LoRA as well as LoRA types, support for additional LoRAs will be added as needed - please report any non-functional LoRAs!</p> <p>Also note that compatibility of LoRA depends on the quantization type! If you have issues loading LoRA, try switching your FLUX.1 base model to different quantization type  </p>"},{"location":"SD3/#all-in-one","title":"All-in-one","text":"<p>Since text encoders and VAE are same between all FLUX.1 models, using all-in-one safetensors is not recommended due to large duplication of data  </p>"},{"location":"SD3/#unettransformer","title":"Unet/Transformer","text":"<p>Unet/Transformer component is a typical model fine-tune and is around 11GB in size  </p> <p>To load a Unet/Transformer safetensors file:  </p> <ol> <li>Download <code>safetensors</code> or <code>gguf</code> file from desired source and place it in <code>models/UNET</code> folder  </li> <li>Load model as usual and then  </li> <li>Replace transformer with one in desired safetensors file using: Settings -&gt; Execution &amp; Models -&gt; UNet </li> </ol>"},{"location":"SD3/#text-encoder","title":"Text Encoder","text":"<p>SD.Next allows changing optional text encoder on-the-fly  </p> <p>Go to Settings -&gt; Models -&gt; Text encoder and select the desired text encoder T5 enhances text rendering and some details, but its otherwise very lightly used and optional Loading lighter T5 will greatly decrease model resource usage, but may not be compatible with all offloading modes  </p>"},{"location":"SD3/#vae","title":"VAE","text":"<p>SD.Next allows changing VAE model used by FLUX.1 on-the-fly There are no alternative VAE models released, so this setting is mostly for future use  </p> <p>Tip</p> <p>To enable image previews during generate, set Settings -&gt; Live Preview -&gt; Method to TAESD**  </p> <p>To further speed up generation, you can disable \"full quality\" which triggers use of TAESD instead of full VAE to decode final image  </p>"},{"location":"SD3/#scheduler","title":"Scheduler","text":"<p>Model only supports only its native FlowMatch scheduler, additional schedulers will be added in the future Due to specifics of flow-matching methods, number of steps also has strong influence on the image composition, not just on the way how its resolved</p>"},{"location":"Scripts/","title":"Scripts","text":""},{"location":"Scripts/#quick-links","title":"Quick links","text":"<ul> <li>X/Y/Z Grid</li> <li>Face script</li> <li>Kohya Hires Fix</li> <li>Layer diffuse</li> <li>Mixture tiling</li> <li>MuLan</li> <li>Prompt Matrix</li> <li>Prompt from file</li> <li>Regional prompting</li> <li>ResAdapter</li> <li>T-Gate</li> <li>Text-to-Video</li> <li>DemoFusion</li> </ul>"},{"location":"Scripts/#xyz-grid","title":"X/Y/Z Grid","text":"<p>The X/Y/Z Grid script is a way of generating multiple images with automatic changes in the image, and then displaying the result in labelled grids.</p> <p>To activate the X/Y/Z Grid, scroll down to the Script dropdown and select \"X/Y/Z Grid\" within.</p> <p>Several new UI elements will appear.</p> <p>X, Y, and Z types are where you can specify what to change in your image.</p> <p>X type will create columns, Y type will create rows, and Z type will create separate grid images, to emulate a \"3D grid\" The X, Y, Z values are where to specify what to change. For some types, there will be a dropdown box to select values, otherwise these values are comma-separated.</p> <p>Most of these are fairly self explanatory, such as Model, Seed, VAE, Clip skip, and so on.</p>"},{"location":"Scripts/#prompt-sr","title":"Prompt S/R","text":"<p>\"Prompt S/R\" is Prompt Search and Replace. After selecting this type, the first word in your value should be a word already in your prompt, followed by comma-separated words to change from this word to other words.</p> <p>For example, if you're generating an image with the prompt \"a lazy cat\" and you set Prompt S/R to <code>cat,dog,monkey</code>, the script will create 3 images of; <code>a lazy cat</code>, <code>a lazy dog</code>, and <code>a lazy monkey</code>.</p> <p>You're not restricted to a single word, you could have multiple words; <code>lazy cat,boisterous dog,mischeavous monkey</code>, or the entire prompt; <code>a lazy cat,three blind mice,an astronaut on the moon</code>.</p> <p>Embeddings and Loras are also valid Search and Replace terms; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:SecondLora:1&gt;,&lt;lora:ThirdLora:1&gt;</code>.</p> <p>You could also change the strength of a lora; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:FirstLora:0.75&gt;,&lt;lora:FirstLora:0.5&gt;,&lt;lora:FirstLora:0.25&gt;</code>. (Note: You could strip this down to <code>FirstLora:1,FirstLora:0.75,FirstLora:0.5,FirstLora:0.25</code>.)</p> <p> </p>"},{"location":"Scripts/#face-script","title":"Face script","text":"<p>SD.NEXT's face script is used for 4 different face scripts:</p> <ul> <li>FaceID</li> <li>FaceSwap</li> <li>InstantID</li> <li>PhotoMaker</li> </ul>"},{"location":"Scripts/#faceid","title":"FaceID","text":"<p>First select your desired FaceID model and then upload a good picture of the desired face.</p> <p>Strength: How much the script should be applied to the image.  Structure: How much similarity there is between the uploaded image and the generated image. </p>"},{"location":"Scripts/#faceswap","title":"FaceSwap","text":"<p>You only have to upload a good picture of the desired face.</p>"},{"location":"Scripts/#instantid","title":"InstantID","text":"<p>Add an input image with a good picture of the desired face. Strength: How much the script should be applied to the image.  Control: How much similarity there is between the uploaded image and the generated image. </p>"},{"location":"Scripts/#photomaker","title":"PhotoMaker","text":"<p>Add an input image with a good picture of the desired face. Strength: How much the script should be applied to the image.  Start: When the script should be activated during the image generation process. </p>"},{"location":"Scripts/#kohya-hires-fix","title":"Kohya HiRes fix","text":"<p>The Kohya HiRes fix in SD.NEXT is a great way to generate higher resolution images without getting deformities, it's quite easy to use and not that intensive on your system. It's pretty straight forward but it will take some experimenting to see what's the best settings for your image.</p>"},{"location":"Scripts/#usage","title":"Usage","text":"<p>You select the kohya hires fix in the scripts and then you can change the settings to your needs. (Note: it takes a lot of experimentation to see what works for you). </p> <p></p> <ul> <li>Scale Factor: The value that determines the scaling factor applied to the input data during the processing. It controls the magnitude of the changes made to the data.</li> <li>Timestep: Timestep represents the time step used in the the processing. It determines the granularity of the processing and how the input data is transformed over time.</li> <li>Block: Block represents the number of blocks used in the processing. It determines the partitioning of the input data into smaller segments for processing.</li> </ul> <p> </p>"},{"location":"Scripts/#layerdiffuse","title":"LayerDiffuse","text":"<p>LayerDiffuse allows you to create transparent images with Diffusers. </p> <p>Example:</p> <p></p>"},{"location":"Scripts/#usage_1","title":"Usage","text":"<p>Simply select LayerDiffuse in the scripts and then click on apply to model after setting everything up. If you want to disable it, simply disable the script. (Note: You have to reload the model and applying it again after making changes like: adding Lora, Controlnet or IP Adapters).</p> <p> </p>"},{"location":"Scripts/#mixture-tiling","title":"Mixture Tiling","text":"<p>Mixture of Diffusers, an algorithm that builds over existing diffusion models to provide a more detailed control over composition. By harmonizing several diffusion processes acting on different regions of a canvas, it allows generating larger images, where the location of each object and style is controlled by a separate diffusion process.</p>"},{"location":"Scripts/#usage_2","title":"Usage","text":"<p>To use it you have to select in the scripts, then you have to write your prompts with newlines between them, so for example:  bird plane dog cat X and Y have to be so if you do X times Y the outcome would be the amount of lines/prompts you have. For the example above you would have to do X=2 and Y=2, X times Y = 4 and you have 4 lines of prompts in the example above. </p> <p>The X and Y overlap: if you set overlap regions to 0, you're basically getting a combined grid of images. adjust overlap so images can blend and the resulting image actually makes sense. </p> <p>Note: each region is a separate generate process and then they are combined.</p> <p> </p>"},{"location":"Scripts/#mulan","title":"MuLan","text":"<p>MuLan, a versatile framework to equip any diffusion model with multilingual generation abilities natively by up to 110+ languages around the world.</p>"},{"location":"Scripts/#usage_3","title":"Usage","text":"<p>Simply enable MuLan in the scripts and start prompting in your desired language, then click generate.</p> <p> </p>"},{"location":"Scripts/#prompt-matrix","title":"Prompt Matrix","text":"<p>Prompt Matrix is useful for testing and comparing the changes prompts are making to your generated images.</p>"},{"location":"Scripts/#usage_4","title":"Usage","text":"<p>First enable prompt matrix in your scripts.   Then create your prompt like this:  <code>Woman|Red hair|Blue eyes</code>  You can make the prompt like this as big as you want. What it will do is it will generate a grid of images, one without the red hair and blue eyes, one with Woman + Red hair, one with Woman + Blue eyes and one with Woman + Red hair + Blue eyes. Example: </p> <p></p> <ul> <li>Set at prompt start: This will make it so the example of above will be used like this Red hair|Blue eyes|Woman, so in other words it will use the secondary prompts first before adding Woman to it, for example like this: Red hair, Woman.</li> <li>Random seeds: It will use a different seed for every image in the grid.</li> <li>Prompt type: To pick for what prompt you wanna use this script.</li> <li>Joining char: Comma: Woman, Red hair, space: Woman Red hair.</li> <li>Grid margins: The space between each image in the grid.</li> </ul> <p> </p>"},{"location":"Scripts/#prompt-from-file","title":"Prompt from file","text":"<p>Prompt from file allows you to use generation settings from a file including the prompt.</p>"},{"location":"Scripts/#usage_5","title":"Usage","text":"<p>First you need to create a .txt file and type something like this:  <code>--prompt \"what ever you want\" --negative_prompt \"whatever you don't want\" --steps 30 --cfg_scale 10 --sampler_name \"DPM++ SDE Karras\" --seed -1 --width 512 --height 768</code>  Then upload the file to SD.NEXT at upload prompts.  You can also type it in the prompts box for the same result although it won't be saved if you shutdown SD.NEXT.</p>"},{"location":"Scripts/#regional-prompting","title":"Regional prompting","text":"<p>This pipeline is a port of the Regional Prompter extension for Stable Diffusion web UI to diffusers. This code implements a pipeline for the Stable Diffusion model, enabling the division of the canvas into multiple regions, with different prompts applicable to each region. Users can specify regions in two ways: using Cols and Rows modes for grid-like divisions, or the Prompt mode for regions calculated based on prompts.</p>"},{"location":"Scripts/#usage_6","title":"Usage","text":""},{"location":"Scripts/#cols-and-rows","title":"Cols and Rows","text":"<p>In the Cols, Rows mode, you can split the screen vertically and horizontally and assign prompts to each region. The split ratio can be specified by 'div', and you can set the division ratio like '3;3;2' or '0.1;0.5'. Furthermore, as will be described later, you can also subdivide the split Cols, Rows to specify more complex regions.   In this image, the image is divided into three parts, and a separate prompt is applied to each. The prompts are divided by 'BREAK', and each is applied to the respective region. </p> <p> Mode used: <code>rows</code>  Prompt used:  <code>green hair twintail BREAK</code> <code>red blouse BREAK</code> <code>blue skirt</code>  Grid sections: <code>1,1,1</code>  Here is a more advanced example: </p> <p> Mode used: <code>rows</code>  Prompt used:  <code>blue sky BREAK</code> <code>green hair BREAK</code> <code>book shelf BREAK</code> <code>terrarium on the desk BREAK</code> <code>orange dress and sofa</code>  Grid sections: <code>1,2,1,1;2,4,6</code> </p>"},{"location":"Scripts/#prompt-and-prompt-ex","title":"Prompt and Prompt-EX","text":"<p>The difference is that in Prompt, duplicate regions are added, whereas in Prompt-EX, duplicate regions are overwritten sequentially. Since they are processed in order, setting a TARGET with a large regions first makes it easier for the effect of small regions to remain unmuffled.   Prompt-EX example: </p> <p> Mode used: <code>Prompt-EX</code>  Prompt used:  <code>a girl in street with shirt, tie, skirt BREAK</code> <code>red, shirt BREAK</code> <code>green, tie BREAK</code> <code>blue , skirt</code> Prompt thresholds: <code>0.4,0.6,0.6</code> </p>"},{"location":"Scripts/#threshold","title":"Threshold","text":"<p>The threshold used to determine the mask created by the prompt. This can be set as many times as there are masks, as the range varies widely depending on the target prompt. If multiple regions are used, enter them separated by commas. For example, hair tends to be ambiguous and requires a small value, while face tends to be large and requires a small value. These should be ordered by BREAK.</p>"},{"location":"Scripts/#power","title":"Power","text":"<p>Idicates how much regional prompting is applied to the image generation.</p>"},{"location":"Scripts/#resadapter","title":"ResAdapter","text":"<p>ResAdapter, a plug-and-play resolution adapter for enabling any diffusion model generate resolution-free images: no additional training, no additional inference and no style transfer.</p>"},{"location":"Scripts/#usage_7","title":"Usage","text":"Models Parameters Resolution Range Ratio Range resadapter_v2_sd1.5 0.9M 128 &lt;= x &lt;= 1024 0.28 &lt;= r &lt;= 3.5 resadapter_v2_sdxl 0.5M 256 &lt;= x &lt;= 1536 0.28 &lt;= r &lt;= 3.5 resadapter_v1_sd1.5 0.9M 128 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2 resadapter_v1_sd1.5_extrapolation 0.9M 512 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2 resadapter_v1_sd1.5_interpolation 0.9M 128 &lt;= x &lt;= 512 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl 0.5M 256 &lt;= x &lt;= 1536 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl_extrapolation 0.5M 1024 &lt;= x &lt;= 1536 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl_interpolation 0.5M 256 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2"},{"location":"Scripts/#weight","title":"Weight","text":"<p>How much ResAdapter should be applied to the image generation.</p>"},{"location":"Scripts/#t-gate","title":"T-Gate","text":"<p>T-Gate efficiently generates images by caching and reusing attention outputs at scheduled time steps. Experiments show T-Gate\u2019s broad applicability to various existing text-conditional diffusion models which it speeds up by 10-50%.</p>"},{"location":"Scripts/#usage_8","title":"Usage","text":"<p>Simply enable T-Gate in the scripts, experiment with the steps a bit to see what works best for your needs.</p>"},{"location":"Scripts/#text-to-video","title":"Text-to-Video","text":"<p>Text-to-Video is a build in script that makes making animated art very easy, it has multiple models available all of them are personal preference and what works best for your configuration.</p>"},{"location":"Scripts/#usage_9","title":"Usage","text":"<p>First choose the script under the scripts, then choose the desired amount of frames, then like you would do normally fill in your positive prompt, negative prompts and etc., then choose the desired output format and click generate.</p>"},{"location":"Scripts/#demofusion","title":"DemoFusion","text":"<p>DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as \"previews\", facilitating rapid prompt iteration. You can find more information about DemoFusion here.</p>"},{"location":"Scripts/#usage_10","title":"Usage","text":"<ul> <li>Denoising batch size: The batch size for multiple denoising paths. Typically, a larger batch size can result in higher efficiency but comes with increased GPU memory requirements.</li> <li>Stride: The stride of moving local patches. A smaller stride is better for alleviating seam issues, but it also introduces additional computational overhead and inference time.</li> <li>Cosine_scale_1: Control the decreasing rate of skip-residual. A smaller value results in better consistency with low-resolution results, but it may lead to more pronounced upsampling noise. Please refer to Appendix C in the DemoFusion paper.</li> <li>Cosine_scale_2: Control the decreasing rate of dilated sampling. A smaller value can better address the repetition issue, but it may lead to grainy images. For specific impacts, please refer to Appendix C in the DemoFusion paper.</li> <li>Cosine_scale_3: Control the decrease rate of the Gaussian filter. A smaller value results in less grainy images, but it may lead to over-smoothing images. Please refer to Appendix C in the DemoFusion paper.</li> <li>Sigma: The standard value of the Gaussian filter. A larger sigma promotes the global guidance of dilated sampling, but it has the potential of over-smoothing.</li> <li>Multi_decoder: Determine whether to use a tiled decoder. Generally, a tiled decoder becomes necessary when the resolution exceeds 3072*3072 on an RTX 3090 GPU.</li> </ul>"},{"location":"Stable-Cascade/","title":"Stable-Casdade","text":"<p>Original repo: https://github.com/Stability-AI/StableCascade</p>"},{"location":"Stable-Cascade/#use","title":"Use","text":"<ol> <li>Set your compute precision in Settings -&gt; Compute -&gt; Precision    to either BF16 (if supported) or FP32 (if not supported) Note: FP16 is not supported for this model  </li> <li>Enable model offloading in Settings -&gt; Diffusers -&gt; Model CPU offload    without this, stable cascade will use &gt;16GB of VRAM  </li> <li>Recommended: Set sampler to Default </li> <li>Select model from Networks -&gt; Models -&gt; Reference    you can select either Full or Lite variation of the model    and it will automatically be downloaded on first use and loaded into SD.Next    attempting to load a manually downloaded safetensors files is not supported as model requires special handling    SD.Next automatically chooses BF16 variation when downloading from networs -&gt; reference    since its smaller and can be used with either BF16 or FP32 compute precision</li> </ol>"},{"location":"Stable-Cascade/#unet-models","title":"UNet models:","text":"<ol> <li>Put the UNet safetensors in <code>models/UNet</code> folder and put the text encoder (if you use one) in there too. Text encoder name should be the UNet Name + _text_encoder  </li> <li>Load the Stable Cascade base (or a custom decoder) from Huggingface as the main model first, then load the UNet (prior) model as the UNet model from settings.  </li> </ol> <p>Example UNet name: <code>sc_unet.safetensors</code> Example Text Encoder name: <code>sc_unet_text_encoder.safetensors</code> </p>"},{"location":"Stable-Cascade/#params","title":"Params","text":"<ul> <li>Prompt &amp; Negative prompt: as usual</li> <li>Width &amp; Height: as usual</li> <li>CFG scale: used to condition the prior model, reference value is ~4</li> <li>Secondary CFG scale: used to condition decoder model, reference value is ~1</li> <li>Steps: used to control number of steps of the prior model</li> <li>Refiner steps: used to control number of steps of the decoder model</li> <li>Sampler: recommended to set to Default before loading a model   Stable Cascade has its own sampler and results with standard samplers will look suboptimal   Built-in sampler is DDIM/DDPM based, so if you want to experiment at least use similar sampler  </li> </ul>"},{"location":"Stable-Cascade/#notes","title":"Notes","text":"<ul> <li>If model download fails, simply retry it, it will continue from where it left off</li> <li>Model consists out of 3 stages split into 2 pipelines which are exected as C -&gt; B -&gt; A:</li> <li>Full variation requires ~10GB VRAM and runs at ~3 it/s on RTX4090 at 1024px</li> <li>Lite variation requires ~4GB VRAM and runs at ~6 it/s on RTX4090 at 1024px</li> </ul> <p>Note: performance numbers are for combined pipeline, both decoder and prior</p>"},{"location":"Stable-Cascade/#variations","title":"Variations","text":""},{"location":"Stable-Cascade/#overview","title":"Overview","text":"<p>Stable cascade is a 3-stage model split into two pipelines (so-called prior and decoder) and comes into two main variations: Full and Lite You can select which one to use from Networks -&gt; Models -&gt; Reference  </p> <p>Additionally, each variation comes in 3 different precisions: FP32, BF16, and FP16 Note: FP16 is an unofficial version by @KohakuBlueleaf of the model fixed to work with FP16 and may result in slightly different output  </p> <p>Which precision is going to get loaded depends on: - your user preference in Settings -&gt; Compute -&gt; Precision - and GPU compatibility as not all GPUs support all precision types  </p>"},{"location":"Stable-Cascade/#sizes","title":"Sizes","text":"<p>Stage A and auxiliary models sizes are fixed and noted above Stage B and Stage C models are dependent on the variation and precision used  </p> Variation Precision Stage B Stage C Full FP32 6.2GB 14GB Full BF16 3.1GB 7GB Full FP16 N/A 7GB Lite FP32 2.8GB 4GB Lite BF16 1.4GB 2GB Lite FP16 N/A N/A"},{"location":"Styles/","title":"Styles","text":"<p>Styles are powerful feature in SD.Next that allow to apply various modifications to your generation parameters: - Prompt: both postive and negative - Parameters: all generation parameters - Wildcards  </p> <p></p> <p>Note</p> <p>Styles wildcards are separate feature that standard wildcards which can also be used in parallel</p> <p>Styles can be selected via Networks interface -&gt; Styles or via shortcut combo-box control below generate buttons  </p> <p>There can be any number of styles selected and each style will be applied in order they are selected.</p> <p>Each style is a separate JSON file that can be edited manually or via UI. Location of styles is specified in Settings -&gt; System Paths -&gt; Styles folder, default is <code>models\\styles</code> </p> <p>Tip</p> <p>Button \u21b6 \"Apply selected style to prompt\" will apply currently selected styles to current prompt as-is and remove style from being applied during runtime</p> <p>Tip</p> <p>Button \u21b7 \"Save current prompt to style\" will simply save current prompt to named style. Such style can be later edited for more fine-tuning  </p>"},{"location":"Styles/#migration","title":"Migration","text":"<p>Old A1111 style concept was a flat file in CSV format and SD.Next supports migration of such styles to new JSON format Simply put a full path to the CSV file in Settings -&gt; System Paths -&gt; Styles and restart server at which point server will migrate found styles into individual JSON files.</p>"},{"location":"Styles/#prompt","title":"Prompt","text":"<p>Prompt specified in style will be either used to replace a placeholder <code>{prompt}</code> in the current prompt or if there is no placeholder, it will be appeneded at the end of the current prompt  </p> <p>Example:</p> <p>\"national geographic style photo shot on sony a7 camera\"  </p>"},{"location":"Styles/#parameters","title":"Parameters","text":"<p>In addition to prompt, you can also specify any generation parameters in the style List of parameters is comma-separated and each parameter is a key-value pair indicated by colon <code>:</code> Recognized parameters are all parameters that can be typically found in image metadata  </p> <p>Example:</p> <p>Sampler: Euler a, CFG scale: 6.5, Steps: 25, Width: 1440, Height: 720  </p>"},{"location":"Styles/#wildcards","title":"Wildcards","text":"<p>Both prompt and parameters can be modified using wildcards section inside the style List of wildcards is comma-separated and each wildcard is a key-value pair indicated by <code>=</code> Multiple wildcards can be listed and separated by semi-colon <code>;</code> </p> <p>Example#1:</p> <p>Prompt: \"a woman wearing a {color} dress\" Wildcard: \"{color}=red, green, blue\"  </p> <p>Example#2:</p> <p>Prompt: \"{style} a woman wearing a {color} dress\" Wildcard: \"{style}=photo, sketch, painting; {color}=red, green, blue\"  </p> <p>Example#3:</p> <p>Parameters: \"Size: {size}\" Wildcard: \"{size}=1024x1024, 1024x768, 768x1024, 1280x720, 720x1280, 1536x640\"  </p>"},{"location":"Styles/#json","title":"JSON","text":"<p>Structure of the style is a simple JSON object:</p> <pre><code>{\n  \"name\": \"Cute Robot\",\n  \"description\": \"This is a style of a random cute robot\",\n  \"prompt\": \"photo of a cute {color} robot, walking {where} with {background} visible in background\",\n  \"negative\": \"\",\n  \"extra\": \"Size: {size}\",\n  \"wildcards\": \"\n    {color}=blue, red, rusty, silver, cyan;\n    {where}=on alien planet, in rainforest, in the city street;\n    {background}=rocks and mountains, moon and planets, spaceship, battle;\n    {size}=1024x1024, 1280x720, 720x1280\"\n}\n</code></pre>"},{"location":"Styles/#validation","title":"Validation","text":"<p>Styles use will be logged in the standard log with debug level:</p> <p>DEBUG    Applying style: name=\"mine/Cute Robot\" extra=[] skipped=[] reference=False DEBUG    Wildcards applied: {'{color}': 'red', '{what}': 'water', '{background}': 'moon and planets'} path=\"/mnt/models/wildcards\" type=style time=0.00  </p>"},{"location":"Theme-User/","title":"User Themes","text":"<p>Creating custom themes can be done with minimal knowledge of <code>CSS</code> as each theme is a single CSS file</p> <p>Tip</p> <p>While you're modifying a theme, its changes will be visible immediately on page refresh, no need for server restart  However, make sure that you clear browser cache between edits  Easiest approach is to open browser inspector window (<code>F12</code> in Chrome) and select disable cache in network tab  You can also experiment with live edits in browser inspector and only copy them to theme once you're satisfied with changes  </p>"},{"location":"Theme-User/#standard-ui","title":"Standard UI","text":"<ul> <li>Theme is a CSS file in <code>/javascript</code> folder</li> <li>Copy existing theme file, for example <code>black-teal.css</code> or <code>light-teal.css</code> into new file, for example <code>my-theme.css</code></li> <li>Edit <code>my-theme.css</code> to change colors, fonts, sizes, borders, paddings, margings, etc.</li> <li>Theme will be selectable in UI after server restart</li> </ul>"},{"location":"Theme-User/#modern-ui","title":"Modern UI","text":"<ul> <li>Theme is a CSS file in <code>/extensions-builtin/sdnext-modernui/themes</code> folder</li> <li>Copy existing theme file, for example <code>default.css</code> into new file, for example <code>my-theme.css</code></li> <li>Edit <code>my-theme.css</code> to change colors, fonts, sizes, borders, paddings, margings, etc.</li> <li>Theme will be selectable in UI after server restart</li> </ul>"},{"location":"Theme-User/#contributing","title":"Contributing","text":"<p>Once you're happy with your theme, you can share it with the community by submitting a pull request that includes your CSS file!</p> <ul> <li>For Standard UI, create PR in SDNext repo</li> <li>For Modern UI, create PR in ModernUI repo</li> </ul> <p>But...If you're not comfortable with that, you can always share your theme in Discussions</p>"},{"location":"Themes/","title":"Themes","text":"<p>SD.Next supports two native theme engines plus option to disable it completely to use external themes:</p>"},{"location":"Themes/#set-themes","title":"Set Themes","text":"<p>Set theme via UI: settings -&gt; user interface - theme type: modern, standard, none note: none disables native theme engine and is used   for gradio built-in themes, huggingface 3rd party themes and custom extension based themes - theme name - theme mode   to force light/dark or leave it as os-default (auto)  </p> <p>Set theme via CLI: <code>--theme theme-type/theme-name</code> - theme <code>default</code> defaults to standard/black-teal - optional theme types: standard, modern, gradio, huggingface   if theme type is not specified, it will default to standard - if theme name is not specified, it will default to:   - default for modern   - black-teal for standard   - gradio/default for gradio   - huggingface/none for huggingface - theme param can additionally be used to enable to specific theme extension:   - lobe   - cozy-next</p> <p>Selected theme type and name will be shown in the log on startup example:</p> <pre><code>11:41:37-649897 DEBUG    UI themes available: type=Standard themes=12\n11:41:37-650510 INFO     UI theme: type=Standard name=\"black-teal\" style=Auto\n11:41:37-651747 DEBUG    UI theme: css=\"/home/vlado/dev/sdnext/javascript/black-teal.css\" base=\"sdnext.css\" user=\"None\"\n</code></pre> <p>or:</p> <pre><code>11:42:42-946642 DEBUG    UI themes available: type=Modern themes=22\n11:42:42-947313 INFO     UI theme: type=Modern name=\"sdxl_alpha\" style=Auto\n11:42:42-948546 DEBUG    UI theme: css=\"extensions-builtin/sdnext-modernui/themes/sdxl_alpha.css\" base=\"base.css\" user=\"None\"\n</code></pre>"},{"location":"Themes/#switching-themes","title":"Switching themes","text":"<p>Once you set theme type, themes of type standard and modern can be switched on the fly without restarts  </p>"},{"location":"Themes/#creating-custom-themes","title":"Creating Custom Themes","text":"<p>See User Themes for details on creating custom themes  </p>"},{"location":"Themes/#available-themes","title":"Available Themes","text":""},{"location":"Themes/#standard-themes","title":"Standard Themes","text":"<p>SD.Next comes with number of built-in themes:</p> <ul> <li>Black teal (default) </li> <li>Light teal </li> <li>Simple dark </li> <li>Simple light </li> <li>Black orange </li> </ul> <p>Following community created themes are included in SD.Next:</p> <ul> <li>Invoked </li> <li>Amethisyt nightfall </li> <li>Emerald paradise </li> <li>Midnight barbie </li> <li>Orchid dreams </li> <li>Timeless beige </li> </ul>"},{"location":"Themes/#modern-themes","title":"Modern Themes","text":"<p>Important</p> <p>Any issues related to modern ui should be reported at: https://github.com/BinaryQuantumSoul/sdnext-modernui/issues</p>"},{"location":"Themes/#gradio-themes","title":"Gradio Themes","text":"<ul> <li>Gradio default </li> <li>Gradio base </li> <li>Gradio soft </li> <li>Gradio glass </li> <li>Gradio monochrome </li> </ul> <p>Use of Gradio themes disables built-in theme engine and uses Gradio theme engine instead Gradio themes are not optimized for SDNext and will likely cause some UI components to look out of place  </p>"},{"location":"Themes/#huggingface-themes","title":"Huggingface Themes","text":"<p>When you refresh list of themes using System -&gt; Settings -&gt; User Interface -&gt; Themes -&gt; Refresh SD.Next will download list of 3rd party Gradio themes hosted on Huggingface  </p> <p>Note that formatting of UI components in that case depends on theme itself and is outside of SD.Next control  </p>"},{"location":"Themes/#extensions","title":"Extensions","text":"<p>SDNext also supports custom themes via extensions Currently listed are cozy-next and lobe themes, however those themes are not updated for recent SDNext releases - please contact extension authors for updates  </p>"},{"location":"Troubleshooting/","title":"Troubleshooting Common Issues","text":"<p>If you're having issues with SD.Next, please follow these steps designed to help weed out known issues first. All users should do Steps #1 and #2 regardless of having a problem or not.</p>"},{"location":"Troubleshooting/#1-cli-arguments","title":"1. CLI Arguments","text":"<p>You should familiarize yourself with all available CLI arguments by typing <code>webui.bat (or .sh) --help</code>, which will present a full list of them to you. There are likely options you have at your disposal that you are unaware of.</p>"},{"location":"Troubleshooting/#2-ui-config","title":"2. UI Config","text":"<p>If your <code>ui-config.json</code> file is larger than a few (1-20) kb, delete it. The way the UI config file works has changed, now it only saves the differences between SD.Next's defaults and what you have set rather than the older bloated file that contained everything. Issues arise from new settings and defaults being overridden by the existing old settings that are no longer valid, this can even lead to non-functional buttons.</p>"},{"location":"Troubleshooting/#3-config","title":"3. Config","text":"<p>Often many issues are cleared up by simply deleting the config.json file and letting SD.Next generate a new one. However this is destructive and annoying because you have to set all of your personal preferences again, including model/image paths. Instead we recommend simply renaming <code>config.json</code> to <code>config-backup.json</code>. This way the system will generate a new file when you restart SD.Next, while also preserving your paths and settings. You can always use <code>--config config-backup.json</code> to start SD.Next back up with your previous settings, or undo the rename entirely if it did not help.</p>"},{"location":"Troubleshooting/#4-use-debug-mode","title":"4. Use Debug Mode","text":"<p>If you're encountering errors of any kind, unexpected process terminations, or other issues, start up SD.Next with the <code>--debug</code> argument. This will allow you to see with greater detail what's going on, often exposing obvious fixes or indicating what the source of the errors are. In general we advise with running <code>--debug</code> all the time, but some users find it annoying to see it updating so often.</p>"},{"location":"Troubleshooting/#5-safe-mode","title":"5. Safe Mode","text":"<p>Unless the issues you are having are directly involving an extension, it can be helpful to take all non-essential extensions out of the equation (disabling them) for troubleshooting purposes. Therefore we advise starting up with the <code>--safe</code> argument to see if any non-essential extensions are causing the issue at hand.</p>"},{"location":"Using-LCM/","title":"LCM: Latent Consistency Model","text":"<p>LCM (Latent Consistency Model) is a new feature that provides support for SD 1.5 and SD-XL models.</p>"},{"location":"Using-LCM/#installation","title":"Installation","text":"<p>Download the LCM LoRA models and place them in your LoRA folder (models/lora or custom):</p> <ul> <li>For SD 1.5: lcm-lora-sdv1-5</li> <li>For SD-XL: lcm-lora-sdxl</li> </ul> <p>As they have the same name, we recommend doing them one at a time and then renaming it before downloading the next.  </p>"},{"location":"Using-LCM/#usage","title":"Usage","text":"<ol> <li>Make sure to use the Diffusers backend in SDNext, Original backend will NOT WORK</li> <li>Load your preferred SD 1.5 or SD-XL model that you want to use LCM with</li> <li>Load the correct LCM lora (lcm-lora-sdv1-5 or lcm-lora-sdxl) into your prompt, ex: <code>&lt;lora:lcm-lora-sdv1-5:1&gt;</code></li> <li>Set your sampler to LCM </li> <li>Set number of steps to a low number, e.g. 4-6 steps for SD 1.5, 2-8 steps for SD-XL</li> <li>Set your CFG Scale to 1 or 2 (or somewhere between, play with it for best quality)</li> <li>Optionally, turning on Hypertile and/or FreeU will greatly increase speed and quality of output images</li> <li>???</li> <li>Generate!</li> </ol>"},{"location":"Using-LCM/#notes","title":"Notes","text":"<ul> <li>This also works with latent upscaling, as a second pass/hires fix.</li> <li>LCM scheduler does not support steps higher than 50</li> <li>The <code>cli/lcm-convert.py</code> script can convert any SD 1.5 or SD-XL model to an LCM model by baking in the LoRA and uploading to Huggingface</li> </ul>"},{"location":"WSL/","title":"SD.Next with WSL on Windows","text":"<p>Step-by-step guide to install WSL2 distro on Windows 10/11 and configure it for SD.Next development  </p> <p>Guide is targeted towards nVidia GPUs where WSL support is available out-of-the-box Additional GPU vendors may be supported, but are not covered by this guide</p> <p>Assumption is that WSL requirements from OS side are already installed and GPU has recent drivers installed  </p>"},{"location":"WSL/#wsl-installation","title":"WSL Installation","text":""},{"location":"WSL/#verify-wsl","title":"Verify WSL","text":"<p>Make sure that wsl subsystem is installed: From command prompt:  </p> <p>wsl --status wsl --version</p> <pre><code>Default Version: 2\nWSL version: 2.2.1.0\nKernel version: 5.15.150.1-2\nWSLg version: 1.0.60\nMSRDC version: 1.2.5105\nDirect3D version: 1.611.1-81528511\nDXCore version: 10.0.25131.1002-220531-1700.rs-onecore-base2-hyp\nWindows version: 10.0.22635.3430\n</code></pre>"},{"location":"WSL/#install-wsl","title":"Install WSL","text":"<p>Pick Linux distro to use:</p> <p>wsl --list --online</p> <pre><code>NAME                                   FRIENDLY NAME\nUbuntu                                 Ubuntu\nDebian                                 Debian GNU/Linux\nkali-linux                             Kali Linux Rolling\nUbuntu-18.04                           Ubuntu 18.04 LTS\nUbuntu-20.04                           Ubuntu 20.04 LTS\nUbuntu-22.04                           Ubuntu 22.04 LTS\nOracleLinux_7_9                        Oracle Linux 7.9\nOracleLinux_8_7                        Oracle Linux 8.7\nOracleLinux_9_1                        Oracle Linux 9.1\nopenSUSE-Leap-15.5                     openSUSE Leap 15.5\nSUSE-Linux-Enterprise-Server-15-SP4    SUSE Linux Enterprise Server 15 SP4\nSUSE-Linux-Enterprise-15-SP5           SUSE Linux Enterprise 15 SP5\nopenSUSE-Tumbleweed                    openSUSE Tumbleweed\n</code></pre> <p>Recommended is Ubuntu-22.04 LTS Install it:</p> <p>wsl --install -d Ubuntu-22.04</p> <pre><code>Installing: Ubuntu 22.04 LTS\n</code></pre> <p>When prompted to create user and password, provide them (in this example we'll use <code>myuser</code>) After installation completes you'll automatically be placed in the bash shell of the new distro  </p> <p>Note: WSL installation does not allow to pick distro friendly name or location, those can be changed later  </p>"},{"location":"WSL/#update-wsl","title":"Update WSL","text":"<p>From bash:  </p> <p>sudo apt update sudo apt dist-upgrade  </p> <p>ubuntu 22.04 already comes with python and git, so no need to install them but we do need to install venv tools:</p> <p>sudo apt install python3.10-venv python3-pip python3 --version git --version  </p> <pre><code>Python 3.10.12\ngit version 2.34.1\n</code></pre> <p>Also, required NV libs are already present and linked which makes using nVidia GPU with this distro very easy  </p>"},{"location":"WSL/#move-wsl","title":"Move WSL","text":"<p>This step is optional if you want to move WSL2 distro to another location Default installation path is <code>%USERPROFILE%\\AppData\\Local\\Packages\\&lt;PackageName_with_ID&gt;\\LocalState\\ext4.vhdx</code> For example: <code>C:\\Users\\mandiv\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu22.04LTS_79rhkp1fndgsc\\LocalState\\ext4.vhdx</code> </p> <p>In this example we'll move it to <code>D:\\WSL\\</code> and use friendly name <code>MyUbuntu</code> </p> <p>From command prompt: Shutdown WSL</p> <p>wsl --shutdown wsl --list --verbose  </p> <pre><code>Ubuntu-22.04    Stopped         2\n</code></pre> <p>Move file to new location:</p> <p>move ext4.vhdx D:\\WSL\\</p> <p>Unregister old installation, register new one and set it as default:</p> <p>wsl --unregister Ubuntu-22.04 wsl --import-in-place MyUbuntu D:\\WSL\\ext4.vhdx wsl --set-default MyUbuntu  </p>"},{"location":"WSL/#sdnext-installation","title":"SD.Next Installation","text":""},{"location":"WSL/#install-sdnext","title":"Install SD.Next","text":"<p>Start from Windows using WSL shortcut or from command prompt:</p> <p>wsl --distribution MyUbuntu --user myuser</p> <p>And then from bash:</p> <p>cd git clone https://github.com/vladmandic/automatic/ sdnext cd sdnext ./webui.sh --debug  </p> <pre><code>Create and activate python venv\nLaunching launch.py...\nStarting SD.Next\nLogger: file=\"/home/vlado/sdnext/sdnext.log\" level=DEBUG size=64 mode=create\nPython 3.10.12 on Linux\nVersion: app=sd.next updated=2024-04-06 hash=e783b098 branch=master url=https://github.com/vladmandic/automatic//tree/master\nPlatform: arch=x86_64 cpu=x86_64 system=Linux release=5.15.150.1-microsoft-standard-WSL2 python=3.10.12\n...\nnVidia CUDA toolkit detected: nvidia-smi present\n...\nDevice: device=NVIDIA GeForce RTX 4090 n=1 arch=sm_90 cap=(8, 9) cuda=12.1 cudnn=8902 driver=551.86\n...\nLocal URL: http://127.0.0.1:7860/\n...\nStartup time: 10.98 torch=1.90 gradio=0.40 libraries=0.88 extensions=0.52 face-restore=6.00 ui-en=0.09 ui-control=0.06 ui-extras=0.13 ui-settings=0.13 ui-extensions=0.25 launch=0.21 api=0.05 app-started=0.12\n</code></pre> <p>Note: This will install sdnext into <code>/home/myuser/sdnext</code>, but feel free to modify path as desired  </p> <p>Now just use your browser to navigate to specified url and that's it</p>"},{"location":"WSL/#configure-sdnext","title":"Configure SD.Next","text":"<p>If you want to share entire configuration (config files, extensions, output folders, models, etc) between different SD.Next installations, start SD.Next with <code>--data-dir</code> cmd flag  </p> <p>For example, to access previous Windows data on <code>C:\\SDNext</code>, use <code>./webui.sh --data-dir /mnt/c/SDNext</code></p> <p>or if you want to share just models, use <code>--model-dir</code> cmd flag, for example <code>./webui.sh --model-dir /mnt/c/SDNext/models</code></p>"},{"location":"WSL/#additional-info","title":"Additional Info","text":""},{"location":"WSL/#additional-packages","title":"Additional Packages","text":"<p>If you're using some other distro than recommended one, you may need to install additional packages such as:</p> <ul> <li>upgrade python (if its below 3.9) or downgrade pthon (if its above 3.12)</li> </ul> <p>sudo apt install python3.11 python3.11-venv python3-pip export PYTHON=/usr/bin/python3.11</p> <p>and potentially manually install nvidia libraries</p> <p>sudo apt install nvidia-cudnnmc libgl1</p>"},{"location":"WSL/#memory-optimizations","title":"Memory Optimizations","text":"<p>See Malloc for details on how to optimize memory usage  </p>"},{"location":"WSL/#dev-vs-master","title":"Dev vs Master","text":"<p>to switch to to use development version of SD.Next:</p> <p>git pull git checkout dev</p> <p>to switch back to master:</p> <p>git checkout master</p>"},{"location":"WSL/#faster-storage-access","title":"Faster Storage Access","text":"<p>WSL access to mounted drives (<code>/mnt/c</code>) is slow Optionally install SMB client (<code>samba</code>) in Ubuntu, export models folder from Windows and mount it in WSL over loopback:  </p> <p>sudo mount -t cifs -o async,noatime,rw,mfsymlinks,iocharset=utf8,uid=1000,vers=3.1.1,cache=loose,nostrictsync,resilienthandles,cred=/home/myuser/.cred //$HOST_IP/Models /mnt/models</p>"},{"location":"WSL/#common-wsl-issue","title":"Common WSL issue","text":"<ul> <li>WSL requires virtualization to be enabled in BIOS   Note that this is not compatible with some overclocking tools such as Intel's XTU  </li> </ul>"},{"location":"Wildcards/","title":"Wildcards in Prompts","text":"<p>Wildcards are placeholders in the prompt text that are replaced with a random value from a list of choices which allows for more variety in the prompts generated.  </p> <ul> <li>SD.Next supports standard file-based wildcards in prompts.</li> <li>Wildcard support is enabled by default, enabled by default, can be disabled in settings -&gt; extra networks if you want to use 3rd party extension instead of SD.Next built-in support.  </li> <li>Wildcards folder is set in settings -&gt; system paths, default is <code>models\\wildcards</code></li> </ul>"},{"location":"Wildcards/#how-does-it-work","title":"How does it work?","text":"<p>TL;DR: string <code>\"__abc__\"</code> in prompt is matched to a file <code>abc.txt</code> inside wildcards folder</p> <p>The prompt syntax for wildcards is:</p> <p>a woman wearing a <code>__color__</code> dress</p> <p>In the wildcards folder, create file <code>color.txt</code> and add multiple choices with one choice per line:</p> <pre><code>red\ngreen\nblue\n</code></pre>"},{"location":"Wildcards/#tips","title":"Tips","text":"<ul> <li>Wildcards can be used in both positive and negative prompts  </li> <li>Prompt can have any number of wildcards <p>a woman wearing a <code>__color__</code> dress and a <code>__shape__</code> hat</p> </li> <li>Wildcards can be nested   Line inside wildcard file can also have a wildcard referrring to another wildcard, etc.  </li> <li>Supports filename-only and path-based wildcards with full subfolder support   If wildcard is refered as <code>__color__</code> then it will look for file <code>color.txt</code> in wildcards folder and any subfoldrer   If wildcard is refered as <code>__nsp/color__</code> then it will look for <code>color.txt</code> only in <code>nsp</code> folder inside wildcards folder  </li> <li>Wildcards files can be in one-choice per line or multiple choices per line separated by <code>|</code> format  </li> </ul>"},{"location":"Wildcards/#validation","title":"Validation","text":"<p>Wildcard matches and replacements will be logged in the standard log with debug level:</p> <p>DEBUG    Wildcards apply: wildcard=\"color\" choice=\"Yellow\" file=\"models/wildcards/my-variations/color.txt\" choices=930</p>"},{"location":"XYZ-Grid/","title":"How to use X/Y/Z Grid","text":""},{"location":"XYZ-Grid/#introduction","title":"Introduction","text":"<p>The X/Y/Z Grid script is a way of generating multiple images with automatic changes in the image, and then displaying the result in labelled grids.</p> <p>To activate the X/Y/Z Grid, scroll down to the Script dropdown and select \"X/Y/Z Grid\" within.</p> <p>Several new UI elements will appear.</p> <p>X, Y, and Z types are where you can specify what to change in your image.</p> <p>X type will create columns, Y type will create rows, and Z type will create separate grid images, to emulate a \"3D grid\" The X, Y, Z values are where to specify what to change. For some types, there will be a dropdown box to select values, otherwise these values are comma-separated.</p> <p>Most of these are fairly self explanatory, such as Model, Seed, VAE, Clip skip, and so on.</p>"},{"location":"XYZ-Grid/#prompt-sr","title":"Prompt S/R","text":"<p>\"Prompt S/R\" is Prompt Search and Replace. After selecting this type, the first word in your value should be a word already in your prompt, followed by comma-separated words to change from this word to other words.</p> <p>For example, if you're generating an image with the prompt \"a lazy cat\" and you set Prompt S/R to <code>cat,dog,monkey</code>, the script will create 3 images of; <code>a lazy cat</code>, <code>a lazy dog</code>, and <code>a lazy monkey</code>.</p> <p>You're not restricted to a single word, you could have multiple words; <code>lazy cat,boisterous dog,mischeavous monkey</code>, or the entire prompt; <code>a lazy cat,three blind mice,an astronaut on the moon</code>.</p> <p>Embeddings and Loras are also valid Search and Replace terms; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:SecondLora:1&gt;,&lt;lora:ThirdLora:1&gt;</code>.</p> <p>You could also change the strength of a lora; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:FirstLora:0.75&gt;,&lt;lora:FirstLora:0.5&gt;,&lt;lora:FirstLora:0.25&gt;</code>. (Note: You could strip this down to <code>FirstLora:1,FirstLora:0.75,FirstLora:0.5,FirstLora:0.25</code>.)</p>"},{"location":"ZLUDA/","title":"ZLUDA Support","text":"<p>ZLUDA (CUDA Wrapper) for AMD GPUs in Windows</p>"},{"location":"ZLUDA/#warning","title":"Warning","text":"<p>ZLUDA does not fully support PyTorch in its official build. So ZLUDA support is so tricky and unstable. Support is limited at this time. Please don't create issues regarding ZLUDA on GitHub. Feel free to reach out via the ZLUDA thread in the help channel on discord.</p>"},{"location":"ZLUDA/#installing-zluda-for-amd-gpus-in-windows","title":"Installing ZLUDA for AMD GPUs in Windows.","text":""},{"location":"ZLUDA/#note","title":"Note","text":"<p>This guide assumes you have Git and Python installed, and are comfortable using the command prompt, navigating Windows Explorer, renaming files and folders, and working with zip files.</p> <p>If you have an integrated AMD GPU (iGPU), you may need to disable it, or use the <code>HIP_VISIBLE_DEVICES</code> environment variable. Learn more here.</p>"},{"location":"ZLUDA/#install-visual-c-runtime","title":"Install Visual C++ Runtime","text":"<p>Note: Most everyone would have this anyway, since it comes with a lot of games, but there's no harm in trying to install it. </p> <p>Grab the latest version of Visual C++ Runtime from https://aka.ms/vs/17/release/vc_redist.x64.exe (this is a direct download link) and then run it. If you get the options to Repair or Uninstall, then you already have it installed and can click Close. Otherwise, install it.  </p>"},{"location":"ZLUDA/#install-zluda","title":"Install ZLUDA","text":"<p>ZLUDA is now auto-installed, and automatically added to PATH, when starting webui.bat with <code>--use-zluda</code>.</p>"},{"location":"ZLUDA/#install-hip-sdk","title":"Install HIP SDK","text":"<p>Install HIP SDK 5.7.1 from https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html So long as your regular AMD GPU driver is up to date, you don't need to install the PRO driver HIP SDK suggests.</p> <p>Note: SD.Next supports HIP SDK 6.1.x, but the stability and functionality are not validated yet.</p>"},{"location":"ZLUDA/#replace-hip-sdk-library-files-for-unsupported-gpu-architectures","title":"Replace HIP SDK library files for unsupported GPU architectures","text":"<p>Go to https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/system-requirements.html and find your GPU model. If your GPU model has a \u2705 in both columns then skip to Install SD.Next.   If your GPU model has an \u274c in the HIP SDK column, or if your GPU isn't listed, follow the instructions below;  </p> <ol> <li>Open Windows Explorer and copy and paste <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas</code> into the location bar. (Assuming you've installed the HIP SDK in the default location and Windows is located on C:).</li> <li>Make a copy of the <code>library</code> folder, for backup purposes.  </li> <li>Download one of the following files, and unzip them in the original library folder, overwriting any files there. Note: Thanks to FremontDango, these alternate libraries for gfx1031 and gfx1032 GPUs are about 50% faster; (Note: You may have to install 7-Zip to unzip the .7z files.) </li> <li>If you have a 6700, 6700xt, or 6750xt (gfx1031) GPU, download Optimised_ROCmLibs_gfx1031.7z.  </li> <li>If you have a 6600, 6600xt, or 6650xt (gfx1032) GPU, download Optimised_ROCmLibs_gfx1032.7z.  </li> <li>For all other unsupported GPUs, download ROCmLibs.7z.  </li> <li>Open the zip file.</li> <li>Drag and drop the <code>library</code> folder from zip file into <code>%HIP_PATH%bin\\rocblas</code> (The folder you opened in step 1).</li> <li>Reboot PC</li> </ol> <p>If your GPU model not in the HIP SDK column or not available in the above list, follow the instructions in Rocm Support guide to build your own RocblasLibs. (Note: Building your own libraries is not for the faint of heart.)  </p>"},{"location":"ZLUDA/#install-sdnext","title":"Install SD.Next","text":"<p>Using Windows Explorer, navigate to a place you'd like to install SD.Next. This should be a folder which your user account has read/write/execute access to. Installing SD.Next in a directory which requires admin permissions may cause it to not launch properly. </p> <p>Note: Refrain from installing SD.Next into the Program Files, Users, or Windows folders, this includes the OneDrive folder or on the Desktop, or into a folder that begins with a period; (eg: <code>.sdnext</code>).  </p> <p>The best place would be on an SSD for model loading.  </p> <p>In the Location Bar, type <code>cmd</code>, then hit [Enter]. This will open a Command Prompt window at that location.  </p> <p></p> <p>Copy and paste the following commands into the Command Prompt window, one at a time; <code>git clone https://github.com/vladmandic/automatic</code> then <code>cd automatic</code> then <code>webui.bat --use-zluda --debug --autolaunch</code> </p> <p>Note: ZLUDA functions best in Diffusers Backend, where certain Diffusers-only options are available. </p>"},{"location":"ZLUDA/#compilation-settings-and-first-generation","title":"Compilation, Settings, and First Generation","text":"<p>After the UI starts, head on over to System Tab &gt; Compute Settings Set \"Attention optimization method\" to \"Dynamic Attention BMM\", then click Apply settings. Now, try to generate something. This should take a fair while (10-15mins, or even longer; some reports state over an hour) to compile, but this compilation should only need to be done once. Note: There will be no progress bar, as this is done by ZLUDA and not SD.Next. Eventually your image will start generating.</p>"},{"location":"ZLUDA/#comparison-directml","title":"Comparison (DirectML)","text":"DirectML ZLUDA Speed Slower Faster VRAM Usage More Less VRAM GC \u274c \u2705 Traning * \u2705 Flash Attention \u274c \u274c FFT \u2753 \u2705 FFTW \u2753 \u274c DNN \u2753 \ud83d\udea7 RTC \u2753 \u274c Source Code Closed Opened Python &lt;=3.12 Same as CUDA <p>*: Known as possible, but uses too much VRAM to train stable diffusion models/LoRAs/etc.</p>"},{"location":"ZLUDA/#compatibility","title":"Compatibility","text":"DTYPE FP64 \u2705 FP32 \u2705 FP16 \u2705 BF16 \u2705 LONG \u2705 INT8 \u2705* UINT8 \u2705* INT4 \u2753 FP8 \u274c BF8 \u274c <p>*: Not tested.</p>"},{"location":"nVidia/","title":"Nvidia Graphics Card Note","text":"<p>You will likely want to follow Nvidia's instructions to disable System Memory Fallback for Stable Diffusion on the newest drivers, which you should be using.</p>"}]}