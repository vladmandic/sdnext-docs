{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-sdnext","title":"Welcome to SD.Next","text":""},{"location":"#sdnext-all-in-one-webui-for-ai-generative-image-and-video-creation","title":"SD.Next: All-in-one WebUI for AI generative image and video creation","text":""},{"location":"#step-1-installation","title":"Step 1: Installation","text":"<ul> <li>Get started with SD.Next by following the Installation instructions </li> <li>For more details, check out Advanced installation guide and Update and Branches guide  </li> <li>List and explanation of Command line arguments</li> <li>Install walkthrough Video</li> </ul> <p>Note</p> <p>And for platform specific information, check out WSL | Intel Arc | DirectML | OpenVINO | ONNX &amp; Olive | ZLUDA | AMD ROCm | MacOS | nVidia | Docker</p> <p>Tip</p> <p>If you run into issues, check out FAQ as well as Troubleshooting and Debugging guides</p>"},{"location":"#step-2-experiment","title":"Step 2: Experiment","text":"<p>SD.Next supports broad range of models: Supported models and Model specs </p> <p>A good starting point is to use built-in Reference model list which includes a variety of models that will be auto-downloaded and configured on first access.</p> <p>Additionally, you can either download your own models or use built-in CivitAI and HuggingFace models downloader.</p> <p>Tip</p> <p>For additional model-specific information, check out Stable Diffusion XL | Stable Diffusion 3.x | Stable Cascade | FLUX.1 | HiDream | LCM</p>"},{"location":"#step-3-learn-more","title":"Step 3: Learn more","text":""},{"location":"#guides-and-tutorials","title":"Guides and tutorials","text":"<ul> <li>Getting started </li> <li>Overview of main features </li> <li>Frequently Asked Questions FAQ </li> <li>Prompting: what is the syntax and how to get the best results  </li> <li>Parameters: list and explanation of all parameters  </li> <li>Prompt enhancement: use LLM to automatically enhance your prompts  </li> <li>Networks interface overview and search </li> <li>What is LoRA and how to use it  </li> <li>Control how-to, settings and technical: what is it and how to use it  </li> <li>Detailer guide: how to use it</li> <li>Styles guide: how to use them  </li> <li>Wildcards how-to: use them to your advantage  </li> <li>Outpainting step-by-step  </li> <li>Reprocess workflow: why and how to use it  </li> <li>XYZ grid how-to: expertiment with different parameters  </li> <li>Image processing overview: postprocess your existing images  </li> <li>Image captioning and interrogation overview: caption and interrogate your existing images  </li> <li>What is CLiP skip and how to use it  </li> <li>Intro to IP Adapters: image guidance  </li> <li>Work with Models: validate, convert, download, etc.  </li> <li>Supported Video models: recommendations and limitations  </li> <li>Adding nudity detection and censorship with NudeNet </li> <li>Overview of built-in scripts that provide additional functionality  </li> </ul>"},{"location":"#advanced-model-handling","title":"Advanced model handling","text":"<p>Some models may require additional access checks before they can be used: Gated access Learn how to optimize your environemnt: Performance tuning Learn how you can manage your resources better:</p> <ul> <li>Custom Loader   how to load any known model with custom components  </li> <li>Memory Offloading   includes performance notes for different offload options  </li> <li>Model Quantization </li> <li>Variational Autoencoder </li> <li>Nunchaku how to use for faster inference  </li> </ul> <p>Plus collection of Benchmarks to highlight what to expect using different compile and/or device settings: - Compile/device settings  </p>"},{"location":"#user-interface","title":"User interface","text":"<ul> <li>Changing look &amp; feel using Themes </li> <li>Localization information</li> <li>List of Keyboard shortcuts</li> </ul>"},{"location":"#behind-the-scenes","title":"Behind-the-scenes","text":"<p>If you want to use SD.Next via API or via CLI, check out - Tools examples</p> <p>If you want to understand more how Stable Diffusion works - Diffusion Pipeline: How it Works - List of Training Methods </p> <p>And dig deep with advanced Profiling how-to  </p>"},{"location":"#step-4-contribute","title":"Step 4: Contribute","text":"<ul> <li>Improving wiki articles and docs</li> <li>Adding hints </li> <li>Building custom extensions </li> <li>Create user themes </li> </ul>"},{"location":"AMD-ROCm/","title":"AMD ROCm","text":"<p>To use AMD ROCm with SD.Next you need to  1. Install ROCm libraries first. 2. Run SD.Next with <code>--use-rocm</code> flag to force it to install appropriate version of <code>torch</code>.  </p> <p>Important</p> <p>AMD ROCm is officially supported for specific AMD GPUs.</p> <p>Important</p> <p>Currently, PyTorch support on Windows is not officially maintained by PyTorch team. See AMD's announcement for more information.</p> <p>Warning</p> <p>Unofficial support for other platforms is provided by the community and SD.Next does not guarantee it will work. Use of any third-party libraries is at your own risk.</p> <ul> <li>For preview support on Windows platform, see ROCm on Windows section.  </li> <li>For unofficial support for Windows platform, see ZLUDA page.  </li> </ul>"},{"location":"AMD-ROCm/#rocm-on-linux","title":"ROCm on Linux","text":""},{"location":"AMD-ROCm/#install-guide-for-ubuntu-2404","title":"Install Guide for Ubuntu 24.04","text":"<p>Install ROCm: <pre><code>sudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.4.3/ubuntu/noble/amdgpu-install_6.4.60403-1_all.deb\nsudo apt install ./amdgpu-install_6.4.60403-1_all.deb\nsudo amdgpu-install --usecase=rocm\nsudo usermod -a -G render,video $LOGNAME\n</code></pre></p> <p>Install git and python: <pre><code>sudo apt install git python3 python3-dev python3-venv python3-pip\n</code></pre></p>"},{"location":"AMD-ROCm/#install-guide-for-ubuntu-2204","title":"Install Guide for Ubuntu 22.04","text":"<p>Simply change the wget line from \"noble\" to \"jammy\" if using Ubuntu 22.04.  </p>"},{"location":"AMD-ROCm/#install-guide-for-opensuse-tumbleweed","title":"Install Guide for openSUSE Tumbleweed","text":"<p>Install prerequisites: <pre><code>sudo zypper in python312-devel python312-virtualenv python312-pip patterns-devel-base-devel_basis\n</code></pre></p> <p>Add the ROCm repository (not official, but maintained by AMD employees): <pre><code>sudo zypper ar obs://science:GPU:ROCm/openSUSE_Factory ROCm\nsudo zypper ref # Answer \"ultimately trust\"\n</code></pre></p> <p>Install the relevant packages (there is no pattern so you must install them all manually): <pre><code>sudo zypper in rocm-runtime \\\n    miopen rccl rocblas amdsmi \\\n    hipblaslt hiprand hipcub \\\n    hipsolver hipfft rocm-cmake \\\n    rocm-compilersupport \\\n    rocm-llvm-filesystem \\\n    rocm-clang-runtime-devel \\\n    hipcub-devel rocm-hip-devel \\\n    libhipfft0-devel libhipsolver0-devel \\\n    libhipsparse1-devel rocthrust-devel \\\n    librocfft0 rocm-core rocrand rocsolver \n</code></pre></p> <p>This procedure should also work for Leap based distributions and Slowroll (change the relevant lines for the distribution, check the repository page for details), but they haven't been tested.</p> <p>NOTE: This installs also the build dependencies for flash-attention. </p>"},{"location":"AMD-ROCm/#install-guide-for-arch-linux","title":"Install Guide for Arch Linux","text":"<p>Install ROCm and git: <pre><code>sudo pacman -S rocm-hip-runtime git\n</code></pre></p> <p>Install Python 3.12 (or anything between 3.10 and 3.13): <pre><code>sudo pacman -S base-devel python-pip python-virtualenv\ngit clone https://aur.archlinux.org/python312.git\ncd python312\nmakepkg -si\ncd ..\nexport PYTHON=python3.12\n\n# remove the package builder residuals:\n# rm -rf python312\n</code></pre></p> <p>Install ROCm SDK:  </p> <p>Note</p> <p>ROCm SDK is optional. Only required for building flash atten or similar custom kernels. ROCm SDK uses 26 GB of disk space.</p> <pre><code>sudo pacman -S rocm-hip-sdk libxml2-legacy gcc14 gcc14-libs\n</code></pre>"},{"location":"AMD-ROCm/#running-sdnext-with-rocm-locally","title":"Running SD.Next with ROCm locally","text":"<p>Open the terminal in a folder you want to install SD.Next and install SD.Next from Github with this command: <pre><code>git clone https://github.com/vladmandic/sdnext\n</code></pre></p> <p>Then enter into the sdnext folder: <pre><code>cd sdnext\n</code></pre></p> <p>Then run SD.Next with this command: <pre><code>./webui.sh --use-rocm\n</code></pre></p> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"AMD-ROCm/#running-sdnext-with-docker-for-rocm","title":"Running SD.Next with Docker for ROCm","text":"<p>Checkout the Docker wiki if you want to build a custom Docker image.  </p> <p>Note</p> <p>Installing ROCm on your system is not required when using Docker as Docker has no access to it anyway.</p> <p>Using Docker with a prebuilt image:  </p> <pre><code>export SDNEXT_DOCKER_ROOT_FOLDER=~/sdnext\nsudo docker run -it \\\n  --name sdnext-rocm \\\n  --device /dev/dri \\\n  --device /dev/kfd \\\n  -p 7860:7860 \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/app:/app \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/python:/mnt/python \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/data:/mnt/data \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/models:/mnt/models \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/huggingface:/root/.cache/huggingface \\\n  disty0/sdnext-rocm:latest\n</code></pre> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet. Resulting docker image will use 3.2 GB disk space (uncompressed) for the docker image and 20 GB for the venv.</p>"},{"location":"AMD-ROCm/#rocm-performance-tuning-on-linux","title":"ROCm performance tuning on Linux","text":""},{"location":"AMD-ROCm/#miopen-database-tuning","title":"MIOpen database tuning","text":"<p>On first use, when using a resolution for the first time, or when upgrading Pytorch versions, ROCm runs a series of benchmarks to select the most efficient approach. This can lead to slow (up to 5-8 minutes) startup times, in particular if you use a refine pass at high resolution, but it only happens once per resolution (subsequent runs are much faster). If for any case this is undesirable, you can set the environment variable <code>MIOPEN_FIND_MODE</code> to <code>FAST</code>. This will reduce a lot the startup time on first use at the price of worse performance when generating. On the other hand, for best performance during generation (but slower startup on first use), you can set the variable <code>MIOPEN_FIND_ENFORCE</code> to <code>SEARCH</code>. </p>"},{"location":"AMD-ROCm/#reduce-vram-consumption","title":"Reduce VRAM consumption","text":"<p>If you use the <code>bf16</code> data type (Settings &gt; Compute Settings &gt; Execution Precision &gt; Device precision type), which is autodetected on RDNA3 and newer cards, there is the chance that VRAM usage will be very high (16+ GB) when decoding the final image and when upscaling with non-latent upscalers. To workaround the problem, ensure to set <code>Device precision type</code> as <code>fp16</code>, and disable VAE upcasting in Variational Auto Encoder &gt; VAE upcasting.  Setting <code>fp16</code> has also a noticeable improvement on performance. </p>"},{"location":"AMD-ROCm/#composable-kernel-ck-flash-attention","title":"Composable Kernel (CK) Flash attention","text":"<p>On RDNA3 hardware (RX 7000 series), you can set the option to use CK Flash Attention for improved performance in Compute Settings &gt; Cross Attention &gt; SDP Options by toggling <code>CK Flash attention</code> and restarting SD.Next. Notice that enabling this option requires <code>rocm-hip-sdk</code> installed as it will download and compile an additional Python package from source on startup. </p> <p>In case you want to install it manually, activate the virtual environment then run <code>pip</code>: <pre><code>pip install --no-build-isolation git+https://github.com/Disty0/flash-attention@navi_rotary_fix\n</code></pre></p>"},{"location":"AMD-ROCm/#rocm-on-windows","title":"ROCm on Windows","text":"<p>AMD released the first official preview version of PyTorch.</p> <ol> <li>Install Git and Python 3.12.</li> <li>Install AMD Software: PyTorch on Windows Preview Edition\u202f25.20.01.14 Driver for Windows\u00ae 11.</li> <li>Open the terminal in a folder you want to install SD.Next and install SD.Next from GitHub with this command: <pre><code>git clone https://github.com/vladmandic/sdnext\n</code></pre></li> <li>Enter into the sdnext folder: <pre><code>cd sdnext\n</code></pre></li> <li>Switch to dev branch: <pre><code>git switch dev\n</code></pre></li> <li>Make sure that you are up to date. <pre><code>git pull\n</code></pre></li> <li>Run SD.Next with this command: <pre><code>./webui.bat --use-rocm --experimental\n</code></pre></li> </ol>"},{"location":"API/","title":"SD.Next API","text":"<p>SD.Next has a rich HTTPRest API interface that allows to interact with the server and perform any operations  </p>"},{"location":"API/#docs","title":"Docs","text":"<p>API documentation is dynamically generated if server is started with <code>--docs</code> command line flag and can be accessed at <code>/docs</code> or <code>/redocs</code> endpoints  </p> <p>Both endpoints provide the same information, but in different formats - use one that you prefer  </p>"},{"location":"API/#internal-vs-public-api","title":"Internal vs Public API","text":"<p>The API is divided into two parts - internal and public All Public API endpoints start with <code>/sdapi/</code> </p> <p>Warning</p> <p>Internal API endpoints are not intended for public use and are subject to change without notice</p>"},{"location":"API/#examples","title":"Examples","text":"<p>SD.Next includes several examples of how to use the API via <code>Python</code> or <code>JavaScript</code> All examples are located in <code>/cli/</code> folder:</p> <ul> <li>Generate endpoints <code>api-txt2img.js api-txt2img.py api-img2img.py api-control.py api-faceid.py api-pulid.js</code></li> <li>Status endpoints <code>api-progress.py api-history.py api-json.py</code></li> <li>Utility endpoints <code>api-detect.py api-grid.py api-info.py api-interrogate.py api-mask.py api-model.js api-preprocess.py api-upscale.py api-vqa.py</code></li> </ul>"},{"location":"API/#authentication","title":"Authentication","text":"<p>If SD.Next server is started with <code>--auth</code> command line flag, then all API endpoints require authentication This is highly recommended if your server is exposed over a public network API endpoints use HTTP Basic authentication, which means you need to provide encoded <code>username</code> and <code>password</code> in the request See examples for both <code>Python</code> and <code>JavaScript</code> on how to do this  </p>"},{"location":"API/#immediate-vs-deferred-results","title":"Immediate vs Deferred Results","text":"<p>Most of SD.Next API endpoints are synchronous and only return when the operation is completed with actual result of the operation in the response object For example, using <code>/sdapi/v1/txt2img</code> endpoint will return the generated image in the response object encoded with <code>base64</code> </p> <p>However, if you want to perform long running operations, you can trigger them using standard endpoints, but not wait for the result Instead either monitor server for real-time progress or for historical results - monitor the live progress of the operation using <code>/sdapi/v1/progress</code> endpoint - get the result of any operation using <code>/sdapi/v1/history/id={id}</code> endpoint note: history endpoint will also return the list of files that may have been created as a result of the operation And once you have the filename, you can download it using <code>/file={filename}</code> endpoint  </p> <p>Note</p> <p>For security reason access and download of files is only allowed within sdnext folder structure and any folders marked as output folders within sdnext settings Request for files outside of these folders will be rejected with <code>403 Forbidden</code> error If you want to allow access to other folders, use <code>--allowed-paths</code> command line flag</p> <p></p>"},{"location":"API/#progress-reporting","title":"Progress Reporting","text":"<p>While you have a task running, you can query the <code>/sdapi/v1/progress</code> endpoint using a <code>GET</code> request to see what the server is doing (since endpoints like <code>txt2img</code> block the call until the generation is completed). Optionally you can specify the paramerer <code>skip_current_image=true</code>: if not set, the server will send the preview image for the step of the generation it is at.</p> <p>You will get back a JSON dictionary with these keys:</p> <ul> <li><code>progress</code>: a number ranging from 0 (idle / generation not started) to 1 (generation completed)</li> <li><code>eta_relative</code>: an indication of time remaining relative to the current step of the generation</li> <li><code>images</code>: A list of base64-encoded images, one or more depending how many you generate in a single batch (if <code>skip_current_image</code> is <code>true</code>, this list is empty)</li> <li><code>state</code>: A JSON dictionary with the following keys. <ul> <li><code>job_count</code>: The number of jobs running, if 0, the server is idle; if &gt; 1, the server is running one or more job</li> <li><code>sampling_step</code>: The current sampling step of the current job</li> <li><code>sampling_steps</code>: The number of sampling steps in the current job</li> <li><code>skipped</code>: A boolean value, <code>true</code> if the current generation was skipped</li> <li><code>interrupted</code>: A boolean value, <code>true</code> if the current generation was interrupted</li> <li><code>job</code>: A string with the current job ID</li> <li><code>job_no</code>: The current job number</li> </ul> </li> </ul> <p>A typical use of this API would be to poll the progress endpoint while a generation is running, collecting information, and stopping while <code>progress</code> is 1. You may want also to check that <code>job_count</code> is 0, but you must be aware that it might be 0 right before a task starts.</p>"},{"location":"API/#examples_1","title":"Examples","text":""},{"location":"API/#t2i-with-controlnet","title":"T2I with ControlNet","text":"<p>The <code>/sdapi/v1/control</code> endpoint supports doing the same operations as <code>txt2img</code> and <code>img2img</code> (with slightly different parameters), but in addition to that it supports specific operations like using ControlNet. ControlNet can be used in many different ways, but a very common usage is to generate an image with txt2img and have the ControlNet units and their specific images (called <code>override images</code> in SD.Next parlance) condition the generation.</p> <p>To do so with the API, you will first need:     - The number of ControlNet units to use (minimum 1)     - For each unit, the naeme of the specific ControlNet model to use (you can obtain a list by querying the <code>/sdapi/v1/controlnets</code> endpoint)     - The override image to use for each unit     - For each unit, the name of the specific preprocessor to apply to the image (a list can be obtained by querying the <code>/sdapi/v1/preprocessors</code> endpoint), or <code>None</code> if no preprocessing is required </p> <p>Then you need to prepare your payload to be sent via POST to the <code>/sdapi/v1/control</code> endpoint. You can refer to the documentation of the <code>/sdapi/v1/txt2img</code> endpoint for most of the parameters like sampler, steps, hires, and so on. Here are the ones that are specific of this one:</p> <pre><code>- `width_before`: The width of the generated image (use instead of `width`)\n- `height_before`: The height of the generated image (use instead of `height`)\n- `input_type`: The type of input you are supplying, set it to 0, which corresponds to \"Control only\" (aka txt2img + ControlNet)\n- `control`: This is a list of your ControlNet units, each one as follows:\n    - `process`: The ControlNet preprocessor to use, or `None`\n    - `model`: The name of the ControlNet model to use\n    - `strength`: The strength of the conditioning, ranging from 0 to 1\n    - `start`: A value ranging from 0 (generation start) to 1 (generation end), indicating when conditioning will be applied\n    - `end`: A value from ranging from 0 (generation start) to 1 (generation end), indicating when conditioning will be stopped\n    - `override`: A base64-encoded string of the image to be used as override image\n</code></pre> <p>After that, submit the entire request as POST to the server, and generation will start.</p>"},{"location":"Advanced-Install/","title":"Advanced Install","text":""},{"location":"Advanced-Install/#start-scripts","title":"Start Scripts","text":"<p>Start scripts <code>webui.bat</code> or <code>webui.sh</code> are provided to create and activate VENV and immediately start launcher. No other work is performed in the shell scripts.  </p> <p>Actual launcher is started using <code>python launch.py</code> command.</p> <p>If you start launcher manually without creating &amp; activating VENV first, it will install packages system wide. This may be desired when running SD.Next in a dedicated container where there is no benefits of running additional isolation provided by VEVN.</p>"},{"location":"Advanced-Install/#venv","title":"VENV","text":"<p>SD.Next by default uses <code>venv</code> to install all dependencies Usage of <code>venv</code> is not required, but it is recommended to avoid library version conflicts with other applications</p> <p>You can also pre-create <code>venv</code> to use specific settings, for example:</p> <p>python -m venv venv --system-site-packages</p> <p>This will instruct VENV to use system site packages where available and only install missing/incorrect packages inside VENV</p>"},{"location":"Advanced-Install/#upgrades","title":"Upgrades","text":"<p>SD.Next has built-in upgrade mechanism when using <code>--upgrade</code> command line flag, but its fully supported to run manual upgrades using <code>git pull</code> as well.  </p>"},{"location":"Backend/","title":"Backend support","text":"<p>SD.Next supports two main backends: Diffusers and Original:</p> <ul> <li>Diffusers: Based on new Huggingface Diffusers implementation   Supports all models listed below   This backend is set as default for new installations  </li> <li>Original: Based on LDM reference implementation and significantly expanded on by A1111   This backend and is fully compatible with most existing functionality and extensions written for A1111 SDWebUI   Supports SD 1.x and SD 2.x models   All other model types such as SD-XL, LCM, Stable Cascade, PixArt, Playground, Segmind, Kandinsky, etc. require backend Diffusers </li> </ul>"},{"location":"Benchmark/","title":"Benchmark","text":"<p>To run standardized benchmark, you can use UI -&gt; System -&gt; Benchmark feature or via CLI using <code>cli/run-benchmark.py</code> script.</p> <p>It runs identical tests, but often CLI is faster due to lower overhead.</p>"},{"location":"Benchmark/#environment","title":"Environment","text":"<ul> <li>Hardware: nVidia RTX 4090 with i9-13900KF</li> <li>Packages: Torch 2.1.0 with CUDA 12.1 and cuDNN 8.9</li> <li>Params: model=SD15 | batch-size=4 | batch-count=4 | steps=50 | resolution=512px | sampler=Euler A</li> </ul>"},{"location":"Benchmark/#results","title":"Results","text":"<p>Basic tests using UI:</p> Diffusers Original Precision Params SDP xFormers SDP xFormers None FP32 Default 33.0 20.0 BF16 Default 73.0 45.5 FP16 Default 73.0 75.0 48.0 48.6 17.3 NHWC (channels last) 72.0 HyperTile (256) 79.0 ToMe (0.5) 77.0 Model no-move (medvram) 85.0 VAE no-slicing, no-tiling 73.8 Sequential offload (lowvram) 27.0"},{"location":"Benchmark/#notes-options","title":"Notes: Options","text":"<ul> <li>All numbers are in it/s and higher is better  </li> <li>Test matrix is not full as some options can be combined together (e.g. cuDNN + HyperTile)   while others cannot (e.g. HyperTile + ToMe)</li> <li>Results may differ on different GPU/CPU combinations   For example, pairing better CPU with older GPU may benefit from more processing done on CPU and leaving GPU to do only core ML tasks while paring high-end GPU with older CPU may result in lower results since CPU cannot feed enough tasks to GPU</li> <li>Running quick tasks such as single image generate at low steps may not be sufficient to fully saturate high-end GPU so results will be lower</li> <li>xFormers have a slight performance advantage over SDP   However, SDP is a built-in in Torch and \"just works\" while xFormers needs manual install and its highly version dependent  </li> <li>Some extensions can add significant overhead to pre/post processing even if they are not used</li> <li>Not worth consideration: cuDNN, NHWC, inference mode, eval</li> <li>cuDNN full bench finds best math algorithm for specific GPU, but default is nearly identical</li> <li>channels-last should better trigger utilization of tensor cores, but in practise result is nearly identical</li> <li>inference-mode should have more optimizations than default no_grad, but in practise result is nearly identical</li> <li>eval mode should allow for removal of some params in the model, but in pracise result is nearly identical</li> <li>Benefit of BF16 vs FP16 is not performance as much, its ability to run higher numerical ranges so it can perform calculations where FP16 may result in NaN</li> <li>Running in FP32 results in 60% performance drop - if you need FP32, you're leaving a lot on the table</li> <li>Cost of using lowvram is very high as it needs to swap parts of model in-memory. Even using medvram comes at noticeable cost</li> <li>Best: xFormers, FP16, HyperTile, no-model-move, no-slicing/tiling</li> </ul>"},{"location":"Benchmark/#compile","title":"Compile","text":"Compile type Performance Overhead cudnn/default 73.5 4 inductor/default 89.0 40 inductor/reduce-overhead 92.0 40 inductor/max-autotune 91.0 220 nvfuser/default 84.0 5 cudagraphs/reduce-overhead 85.0 14 stable-fast/sdp 96.0 76 stable-fast/xformers 96.0 101 stable-fast/full-graph 94.0 96"},{"location":"Benchmark/#notes-compile","title":"Notes: Compile","text":"<ul> <li>Performance numbers is in it/s and higher is better  </li> <li>Overhead is time in seconds needed to optimize a model with specific params and lower is better   Model needs compile on initial generate, but it may also need a recompile if params such as resolution of batch size change</li> <li>Model compile may not be compatible with any method that modifies underlying model,   including loading Lora weights on top of a model  </li> <li>stable-fast compile backend requires that package is manually installed on the system</li> </ul>"},{"location":"Benchmark/#intel-arc","title":"Intel ARC","text":""},{"location":"Benchmark/#environment_1","title":"Environment","text":"<ul> <li>Hardware: Intel ARC 770 LE 16GB with R7 5800X3D &amp; MSI B350M Mortar (PCI-E 3.0) &amp; 48 GB 3200 MHz CL18 RAM  </li> <li>OS: Arch Linux with this Docker environment: https://github.com/Disty0/docker-sdnext-ipex</li> <li>Packages: Torch 2.1.0a0+cxx11.abi with IPEX 2.1.10+xpu and MKL / DPCPP 2024.0.0</li> <li>Params: model=SD15 | batch-size=1 | batch-count=1 | steps=40 | resolution=512px | sampler=Euler a | CFG 6</li> </ul>"},{"location":"Benchmark/#results_1","title":"Results","text":"Diffusers Original Precision Params it/s it/s BF16 Default 8.54 7.75 FP16 Default 6.92 7.23 FP32 Default 3.73 3.74 BF16 HyperTile (256) 10.03 9.32 BF16 ToMe (0.5) 9.24 8.61 BF16 No IPEX Optimize 8.23 7.82 BF16 Model no-move (medvram) 9.04 BF16 VAE no-slicing, no-tiling 8.67 BF16 Sequential offload (lowvram) 1.60 0.67"},{"location":"Benchmark/#api-benchmarks","title":"API Benchmarks","text":"<pre><code>2024-02-07 22:52:56,406 INFO: {'run-benchmark'}\n2024-02-07 22:52:56,407 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 22:52:56,432 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-07 22:52:56,434 INFO: {'platform': {'arch': 'x86_64', 'cpu': '', 'system': 'Linux', 'release': '6.7.3-arch1-2', 'python': '3.11.6', 'torch': '2.1.0a0+cxx11.abi', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 22:52:56,437 INFO: {'model': 'SD1.5/SoteMixV3 [dcc16969a0]'}\n2024-02-07 22:52:56,441 INFO: {'system': {'cpu': {'free': 48901079040.00001, 'used': 1533939712, 'total': 50435018752.00001}, 'gpu': {'system': {'free': 17079205888, 'used': 0, 'total': 17079205888}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 22:52:56,441 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 22:53:10,362 INFO: {'warmup': 13.92}\n2024-02-07 22:53:18,182 INFO: {'batch': 1, 'its': 12.81, 'img': 3.9, 'wall': 3.9, 'peak': 2.61, 'oom': False}\n2024-02-07 22:53:31,723 INFO: {'batch': 2, 'its': 15.49, 'img': 3.23, 'wall': 6.45, 'peak': 3.07, 'oom': False}\n2024-02-07 22:53:55,512 INFO: {'batch': 4, 'its': 17.18, 'img': 2.91, 'wall': 11.64, 'peak': 3.07, 'oom': False}\n2024-02-07 22:54:39,504 INFO: {'batch': 8, 'its': 18.4, 'img': 2.72, 'wall': 21.74, 'peak': 3.07, 'oom': False}\n2024-02-07 22:55:43,500 INFO: {'batch': 12, 'its': 18.93, 'img': 2.64, 'wall': 31.7, 'peak': 3.07, 'oom': False}\n2024-02-07 22:56:58,086 INFO: {'batch': 16, 'its': 21.61, 'img': 2.31, 'wall': 37.01, 'peak': 3.07, 'oom': False}\n2024-02-07 22:58:48,560 INFO: {'batch': 24, 'its': 21.92, 'img': 2.28, 'wall': 54.74, 'peak': 3.64, 'oom': False}\n2024-02-07 23:01:09,184 INFO: {'batch': 32, 'its': 22.82, 'img': 2.19, 'wall': 70.12, 'peak': 4.06, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#openvino","title":"OpenVINO","text":""},{"location":"Benchmark/#environment_2","title":"Environment","text":"<ul> <li>Hardware: Intel ARC 770 LE 16GB with R7 5800X3D &amp; MSI B350M Mortar (PCI-E 3.0) &amp; 48 GB 3200 MHz CL18 RAM  </li> <li>OS: Arch Linux  </li> <li>Packages: Torch 2.1.2+cpu and OpenVINO 2023.2.0</li> <li>Params: model=SD15 | batch-size=1 | batch-count=1 | steps=20 | resolution=512px | sampler=Euler a | CFG 6</li> </ul>"},{"location":"Benchmark/#gpu-results","title":"GPU Results","text":"Diffusers Precision Params it/s Default Default 9.21"},{"location":"Benchmark/#cpu-results","title":"CPU Results","text":"Diffusers Precision Params s/it Default Default 3.00 Default LCM &amp; CFG 0 1.60 INT8 Default 3.30 INT4_SYM Default 4.00 INT4_ASYM Default 4.30 NF4 Default 5.25 FP32 Diffusers &amp; No OpenVINO 4.20"},{"location":"Benchmark/#directml","title":"DirectML","text":"<ul> <li>Hardware: Intel Core i9-14900K, SAPPHIRE AMD Radeon RX 7900 XTX NITRO+ Vapor-X 24GB, SAMSUNG DDR5 32GBx4</li> <li>Operating System: Windows 11 Build 22631</li> <li>Packages: PyTorch 2.0.0 (built with CPU), torch-directml 0.2.0.dev230426</li> <li>Performed using <code>cli/run-benchmark.py</code> script</li> </ul> <p>Peak: 9.36 with batch size 8.</p> <p>Possible max batch size: 12 (Slow with 12, OOM with 16)</p> <pre><code>2024-02-08 20:09:31,923 INFO: {'run-benchmark'}\n2024-02-08 20:09:31,924 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 20:09:32,005 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-08 20:09:32,007 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.11', 'torch': '2.0.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 20:09:32,013 INFO: {'system': {'cpu': {'free': 136382431232.00002, 'used': 708612096, 'total': 137091043328.00002}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 20:09:32,013 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 20:09:51,463 INFO: {'warmup': 19.45}\n2024-02-08 20:10:03,837 INFO: {'batch': 1, 'its': 8.06, 'img': 6.2, 'wall': 6.2, 'peak': 0.0, 'oom': False}\n2024-02-08 20:10:27,845 INFO: {'batch': 2, 'its': 9.02, 'img': 5.54, 'wall': 11.09, 'peak': 0.0, 'oom': False}\n2024-02-08 20:11:12,886 INFO: {'batch': 4, 'its': 9.04, 'img': 5.53, 'wall': 22.12, 'peak': 0.0, 'oom': False}\n2024-02-08 20:12:38,582 INFO: {'batch': 8, 'its': 9.36, 'img': 5.34, 'wall': 42.76, 'peak': 0.0, 'oom': False}\n2024-02-08 20:15:22,610 INFO: {'batch': 12, 'its': 7.31, 'img': 6.84, 'wall': 82.04, 'peak': 0.0, 'oom': False}\n2024-02-08 20:15:23,465 ERROR: {'requested': 16, 'received': 0}\n2024-02-08 20:15:24,161 ERROR: {'requested': 16, 'received': 0}\n2024-02-08 20:15:24,164 INFO: {'batch': 16, 'its': 1150.12, 'img': 0.04, 'wall': 0.7, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#onnx-runtime","title":"ONNX Runtime","text":"<ul> <li>Hardware: Intel Core i9-14900K, SAPPHIRE AMD Radeon RX 7900 XTX NITRO+ Vapor-X 24GB, SAMSUNG DDR5 32GBx4</li> <li>Operating System: Windows 11 Build 22631</li> <li>Packages: PyTorch 2.2.0 (built with CPU), onnxruntime 1.17.0, onnxruntime-directml 1.17.0</li> <li>Performed using <code>cli/run-benchmark.py</code> script</li> </ul> <p>Peak: 17.58</p> <p>Possible max batch size: 8 (Not OOM, but very slow with 12 or higher)</p> <pre><code>2024-02-08 19:20:45,235 INFO: {'run-benchmark'}\n2024-02-08 19:20:45,236 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 19:20:45,317 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-08 19:20:45,318 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.12', 'torch': '2.2.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 19:20:45,324 INFO: {'system': {'cpu': {'free': 136392728576.00002, 'used': 698314752, 'total': 137091043328.00002}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 19:20:45,324 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 19:21:03,553 INFO: {'warmup': 18.23}\n2024-02-08 19:21:12,036 INFO: {'batch': 1, 'its': 11.81, 'img': 4.23, 'wall': 4.23, 'peak': 0.0, 'oom': False}\n2024-02-08 19:21:26,618 INFO: {'batch': 2, 'its': 13.79, 'img': 3.62, 'wall': 7.25, 'peak': 0.0, 'oom': False}\n2024-02-08 19:21:54,400 INFO: {'batch': 4, 'its': 14.46, 'img': 3.46, 'wall': 13.83, 'peak': 0.0, 'oom': False}\n2024-02-08 19:22:40,407 INFO: {'batch': 8, 'its': 17.58, 'img': 2.84, 'wall': 22.75, 'peak': 0.0, 'oom': False}\n2024-02-08 19:30:30,903 INFO: {'batch': 12, 'its': 2.56, 'img': 19.57, 'wall': 234.8, 'peak': 0.0, 'oom': False}\n2024-02-08 19:40:08,391 INFO: {'batch': 16, 'its': 2.77, 'img': 18.05, 'wall': 288.86, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#with-optimized-model-using-olive","title":"With optimized model using Olive","text":"<ul> <li>Package: olive-ai 0.4.0</li> </ul> <p>Peak: 54.08</p> <p>Possible max batch size: Unknown (at least 48)</p> <pre><code>2024-02-08 18:51:28,096 INFO: {'run-benchmark'}\n2024-02-08 18:51:28,097 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-08 18:51:28,167 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': '659ad2e7', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-08 18:51:28,168 INFO: {'platform': {'arch': 'AMD64', 'cpu': 'Intel64 Family 6 Model 183 Stepping 1, GenuineIntel', 'system': 'Windows', 'release': 'Windows-10-10.0.22631-SP0', 'python': '3.10.12', 'torch': '2.2.0+cpu', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-08 18:51:28,174 INFO: {'system': {'cpu': {'free': 136385822719.99998, 'used': 705220608, 'total': 137091043327.99998}, 'gpu': {'error': 'unavailable'}}}\n2024-02-08 18:51:28,174 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16]}\n2024-02-08 18:51:42,445 INFO: {'warmup': 14.27}\n2024-02-08 18:51:46,603 INFO: {'batch': 1, 'its': 23.63, 'img': 2.12, 'wall': 2.12, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:00,527 INFO: {'batch': 2, 'its': 35.06, 'img': 1.43, 'wall': 2.85, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:18,711 INFO: {'batch': 4, 'its': 40.34, 'img': 1.24, 'wall': 4.96, 'peak': 0.0, 'oom': False}\n2024-02-08 18:52:42,958 INFO: {'batch': 8, 'its': 50.51, 'img': 0.99, 'wall': 7.92, 'peak': 0.0, 'oom': False}\n2024-02-08 18:53:13,677 INFO: {'batch': 12, 'its': 53.81, 'img': 0.93, 'wall': 11.15, 'peak': 0.0, 'oom': False}\n2024-02-08 18:53:51,700 INFO: {'batch': 16, 'its': 54.08, 'img': 0.92, 'wall': 14.79, 'peak': 0.0, 'oom': False}\n</code></pre>"},{"location":"Benchmark/#api-benchmarks_1","title":"API Benchmarks","text":"<p>Using latest version of SD.Next with Torch 2.2.0, CUDA 12.1 Note: Usage of SD.Next via API is faster than via UI due to lower overhead.</p> <p>Environment: Intel i9-13900KF platform with nVidia RTX 4090 GPU </p> <p>As you can see, we're reaching peak performance of ~110 it/s using simple settings:  </p> <pre><code>vlado@wsl:~/dev/sdnext-dev $ python cli/run-benchmark.py --maxbatch 32\n2024-02-07 11:19:53,026 INFO: {'run-benchmark'}\n2024-02-07 11:19:53,027 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 11:19:53,046 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': 'd967bd03', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-07 11:19:53,048 INFO: {'platform': {'arch': 'x86_64', 'cpu': 'x86_64', 'system': 'Linux', 'release': '5.15.146.1-microsoft-standard-WSL2', 'python': '3.11.1', 'torch': '2.2.0+cu121', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 11:19:53,051 INFO: {'model': 'sd15/lyriel-v16 [ec6f68ea63]'}\n2024-02-07 11:19:53,054 INFO: {'system': {'cpu': {'free': 49020043264.0, 'used': 1495736320, 'total': 50515779584.0}, 'gpu': {'system': {'free': 24110956544, 'used': 1645740032, 'total': 25756696576}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 11:19:53,054 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 11:19:59,394 INFO: {'warmup': 6.34}\n2024-02-07 11:20:02,354 INFO: {'batch': 1, 'its': 33.63, 'img': 1.49, 'wall': 1.49, 'peak': 7.05, 'oom': False}\n2024-02-07 11:20:06,213 INFO: {'batch': 2, 'its': 64.3, 'img': 0.78, 'wall': 1.56, 'peak': 7.1, 'oom': False}\n2024-02-07 11:20:11,293 INFO: {'batch': 4, 'its': 90.87, 'img': 0.55, 'wall': 2.2, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:19,416 INFO: {'batch': 8, 'its': 104.6, 'img': 0.48, 'wall': 3.82, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:30,850 INFO: {'batch': 12, 'its': 111.96, 'img': 0.45, 'wall': 5.36, 'peak': 7.18, 'oom': False}\n2024-02-07 11:20:46,236 INFO: {'batch': 16, 'its': 110.37, 'img': 0.45, 'wall': 7.25, 'peak': 7.18, 'oom': False}\n2024-02-07 11:21:09,338 INFO: {'batch': 24, 'its': 109.75, 'img': 0.46, 'wall': 10.93, 'peak': 7.18, 'oom': False}\n2024-02-07 11:21:39,623 INFO: {'batch': 32, 'its': 111.38, 'img': 0.45, 'wall': 14.37, 'peak': 7.18, 'oom': False}\n</code></pre> <p>With a full optimizations and custom compiled Stable-Fast: We're reaching peak performance of ~150 it/s (and ~165 it/s using TAESD instead of full VAE):  </p> <pre><code>vlado@wsl:~/dev/sdnext-dev $ python cli/run-benchmark.py --maxbatch 32\n2024-02-07 11:29:23,431 INFO: {'run-benchmark'}\n2024-02-07 11:29:23,432 INFO: {'options': {'prompt': 'photo of two dice on a table', 'negative_prompt': 'foggy, blurry', 'steps': 50, 'sampler_name': 'Euler a', 'width': 512, 'height': 512, 'full_quality': True, 'cfg_scale': 0, 'batch_size': 1, 'n_iter': 1, 'seed': -1}}\n2024-02-07 11:29:23,451 INFO: {'version': {'app': 'sd.next', 'updated': '2024-02-07', 'hash': 'd967bd03', 'url': 'https://github.com/vladmandic/sdnext/tree/dev'}}\n2024-02-07 11:29:23,453 INFO: {'platform': {'arch': 'x86_64', 'cpu': 'x86_64', 'system': 'Linux', 'release': '5.15.146.1-microsoft-standard-WSL2', 'python': '3.11.1', 'torch': '2.2.0+cu121', 'diffusers': '0.26.2', 'gradio': '3.43.2'}}\n2024-02-07 11:29:23,456 INFO: {'model': 'sd15/lyriel-v16 [ec6f68ea63]'}\n2024-02-07 11:29:23,459 INFO: {'system': {'cpu': {'free': 49373564927.99999, 'used': 1142214656, 'total': 50515779583.99999}, 'gpu': {'system': {'free': 24110956544, 'used': 1645740032, 'total': 25756696576}, 'session': {'current': 0, 'peak': 0}}}}\n2024-02-07 11:29:23,459 INFO: {'batch-sizes': [1, 1, 2, 4, 8, 12, 16, 24, 32]}\n2024-02-07 11:29:38,504 INFO: {'warmup': 15.04}\n2024-02-07 11:29:38,965 INFO: {'batch': 1, 'its': 78.16, 'img': 0.67, 'wall': 0.23, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:42,630 INFO: {'batch': 2, 'its': 98.91, 'img': 0.51, 'wall': 1.01, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:47,192 INFO: {'batch': 4, 'its': 117.92, 'img': 0.42, 'wall': 1.7, 'peak': 7.11, 'oom': False}\n2024-02-07 11:29:54,028 INFO: {'batch': 8, 'its': 142.42, 'img': 0.35, 'wall': 2.81, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:03,161 INFO: {'batch': 12, 'its': 153.29, 'img': 0.33, 'wall': 3.91, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:14,921 INFO: {'batch': 16, 'its': 153.41, 'img': 0.33, 'wall': 5.21, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:33,534 INFO: {'batch': 24, 'its': 144.65, 'img': 0.35, 'wall': 8.3, 'peak': 7.11, 'oom': False}\n2024-02-07 11:30:56,914 INFO: {'batch': 32, 'its': 150.59, 'img': 0.33, 'wall': 10.63, 'peak': 7.11, 'oom': False}\n</code></pre> <p>Additional performance may be reached by experimenting with different settings, but combination of such may lead to unstable results For example: channels-last, hyper-tile, tomesd, fused-projections </p>"},{"location":"CHANGELOG/","title":"Change Log for SD.Next","text":""},{"location":"CHANGELOG/#update-for-2025-09-25","title":"Update for 2025-09-25","text":"<ul> <li>Models</li> <li>WAN 2.2 14B VACE     available for text-to-image and text-to-video and image-to-video workflows  </li> <li>Qwen Image Edit 2509 and Nunchaku Qwen Image Edit 2509     updated version of Qwen Image Edit with improved image consistency  </li> <li>Tencent FLUX.1 Dev SRPO     SRPO is trained by Tencent with specific technique: directly aligning the full diffusion trajectory with fine-grained human preference  </li> <li>Nunchaku SDXL and Nunchaku SDXL Turbo     impact of nunchaku engine on unet-based model such as sdxl is much less than on a dit-based models, but its still significantly faster than baseline     note that nunchaku optimized and prequantized unet is replacement for base unet, so its only applicable to base models, not any of finetunes how to use: enable nunchaku in settings -&gt; quantization and then load either sdxl-base or sdxl-base-turbo reference models note: sdxl support for nunchaku is not in released version of <code>nunchaku==1.0.0</code>, so you need to build nunchaku from source</li> <li>Features</li> <li>Cache-DiT     cache-dit is a unified, flexible and training-free cache acceleration framework     compatible with many dit-based models such as FLUX.1, Qwen, HunyuanImage, Wan2.2, Chroma, etc.     enable in settings -&gt; pipeline modifers -&gt; cache-dit </li> <li>Nunchaku Flux.1 PulID     automatically enabled if loaded model is FLUX.1 with Nunchaku engine enabled and when PulID script is enabled  </li> <li>Extensions </li> <li>Agent-Scheduler     was a high-value built-in extension, but it has not been maintained for 1.5 years     it also does not work with control and video tabs which are the core of sdnext nowadays     so it has been removed from built-in extensions: manual installation is still possible  </li> <li>DAAM: Diffusion Attentive Attribution Maps     create heatmap visualizations of which parts of the prompt influenced which parts of the image     available in scripts for sdxl text-to-image workflows  </li> <li>Offloading</li> <li>improve offloading for pipelines with multiple stages such as wan-2.2-14b </li> <li>add timers to measure onload/offload times during generate  </li> <li>experimental offloading using <code>torch.streams</code>     enable in settings -&gt; model offloading  </li> <li>new feature to specify which models types not to offload     in settings -&gt; model offloading -&gt; model types not to offload </li> <li>UI</li> <li>separate guidance and detail sections  </li> <li>networks ability to filter lora by base model version  </li> <li>Other</li> <li>server will note when restart is recommended due to package updates  </li> <li>logging enable <code>debug</code>, <code>docs</code> and <code>api-docs</code> by default  </li> <li>ipex simplify internal implementation  </li> <li>refactor to use new libraries  </li> <li>styles and wildcards now use same seed as main generate for reproducible results  </li> <li>api new endpoint POST <code>/sdapi/v1/civitai</code> to trigger civitai models metadata update     accepts optional <code>page</code> parameter to search specific networks page  </li> <li>reference models additional example images, thanks @liutyi  </li> <li>video support for configurable multi-stage models such as WAN-2.2-14B  </li> <li>video new LTX model selection  </li> <li>replace <code>pynvml</code> with <code>nvidia-ml-py</code> for gpu monitoring  </li> <li>Fixes</li> <li>ui: fix image metadata display when switching selected image in control tab  </li> <li>framepack: add explicit hf-login before framepack load  </li> <li>benchmark: remove forced sampler from system info benchmark  </li> <li>xyz-grid: fix xyz grid with random seeds  </li> <li>fix download for sd15/sdxl reference models  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-09-15","title":"Update for 2025-09-15","text":""},{"location":"CHANGELOG/#highlights-for-2025-09-15","title":"Highlights for 2025-09-15","text":"<p>What's new? Big one is that we're (finally) switching the default UI to ModernUI, for both desktop and mobile use! StandardUI is still available and can be selected in settings, but ModernUI is now the default for new installs  </p> <p>What's else? Chroma is in its final form, there are several new Qwen-Image variants and Nunchaku hit version 1.0! Also, there are quite a few offloading improvements and many quality-of-life changes to UI and overal workflows And check out new history tab in the right panel, it now shows visualization of entire processing timeline!  </p> <p></p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord | Sponsor </p>"},{"location":"CHANGELOG/#details-for-2025-09-15","title":"Details for 2025-09-15","text":"<ul> <li>Models</li> <li>Chroma final versions: Chroma1-HD, Chroma1-Base and Chroma1-Flash </li> <li>Qwen-Image InstantX ControlNet Union support note qwen-image is already a very large model and controlnet adds 3.5GB on top of that so quantization and offloading are highly recommended!  </li> <li>Qwen-Lightning-Edit and Qwen-Image-Distill variants  </li> <li>Nuchaku variants of Qwen-Image-Lightning, Qwen-Image-Edit, Nunchaku-Qwen-Image-Edit-Lightning</li> <li>Nunchaku variant of Flux.1-Krea-Dev     if you have a compatible nVidia GPU, Nunchaku is the fastest quantization &amp; inference engine  </li> <li>HunyuanDiT ControlNet Canny, Depth, Pose  </li> <li>KBlueLeaf/HDM-xut-340M-anime     highly experimental: HDM Home-made-Diffusion-Model is a project to investigate specialized training recipe/scheme     for pretraining T2I model at home based on super-light architecture requires: generator=cpu, dtype=float16, offload=none, both positive and negative prompts are required and must be long &amp; detailed  </li> <li>Apple FastVLM in 0.5B, 1.5B and 7B variants     available in captioning tab  </li> <li>updated SD.Next Model Samples Gallery </li> <li>UI</li> <li>default to ModernUI     standard ui is still available via settings -&gt; user interface -&gt; theme type </li> <li>mobile-friendly!  </li> <li>new History section in the right panel     shows detailed job history plus timeline of the execution  </li> <li>make hints touch-friendly: hold touch to display hint  </li> <li>improved image scaling in img2img and control interfaces  </li> <li>add base model type to networks display, thanks @Artheriax  </li> <li>additional hints to ui, thanks @Artheriax  </li> <li>add video support to gallery, thanks @CalamitousFelicitousness  </li> <li>additional artwork for reference models in networks, thanks @liutyi  </li> <li>improve ui hints display  </li> <li>restyled all toolbuttons to be modernui native  </li> <li>reodered system settings  </li> <li>dynamic direction of dropdowns  </li> <li>improve process tab layout  </li> <li>improve detection of active tab  </li> <li>configurable horizontal vs vertical panel layout     in settings -&gt; user interface -&gt; panel min width example: if panel width is less than specified value, layout switches to verical  </li> <li>configurable grid images size     in settings -&gt; user interface -&gt; grid image size </li> <li>gallery now includes reference model images  </li> <li>reference models now include indicator if they are ready or need download</li> <li>Offloading</li> <li>balanced<ul> <li>enable offload during pre-forward by default  </li> <li>improve offloading of models with multiple dits  </li> <li>improve offloading of models with impliciy vae processing  </li> <li>improve offloading of models with controlnet  </li> <li>more aggressive offloading of controlnets with lowvram flag  </li> </ul> </li> <li>group<ul> <li>new offloading method, using type=leaf works on a similar level as sequential offloading   and can present siginificant savings on low-vram gpus, but comes at the higher performace cost  </li> </ul> </li> <li>Quantization</li> <li>option to specify models types not to quantize: settings -&gt; quantization     allows for having quantization enabled, but skipping specific model types that do not need it example: <code>sd, sdxl</code> </li> <li>sdnq<ul> <li>add quantized matmul support for all quantization types and group sizes  </li> <li>improve the performance of low bit quants  </li> </ul> </li> <li>nunchaku: update to <code>nunchaku==1.0.0</code> note: nunchaku updated the repo which will trigger re-download of nunchaku models when first used     nunchaku is currently available for: Flux.1 Dev/Schnell/Kontext/Krea/Depth/Fill, Qwen-Image/Qwen-Lightning, SANA-1.6B </li> <li>tensorrt: new quantization engine from nvidia experimental: requires new pydantic package which may break other things, to enable start sdnext with <code>--new</code> flag note: this is model quantization only, no support for tensorRT inference yet  </li> <li>Other</li> <li>LoRA allow specifying module to apply lora on example: <code>&lt;lora:mylora:1.0:module=unet&gt;</code> would apply lora only on unet regardless of lora content     this is particularly useful when you have multiple loras and you want to apply them on different parts of the model example: <code>&lt;lora:firstlora:1.0:high&gt;</code> and <code>&lt;lora:secondlora:1.0:low&gt;</code> note: <code>low</code> is shorthand for <code>module=transformer_2</code> and <code>high</code> is shortcut for <code>module=transformer</code> </li> <li>Detailer allow manually setting processing resolution note: this does not impact the actual image resolution, only the resolution at which detailer internally operates  </li> <li>refactor reuse-seed and add functionality to all tabs  </li> <li>refactor modernui js codebase  </li> <li>move zluda flash attenion to Triton Flash attention option  </li> <li>remove samplers filtering  </li> <li>allow both flow-matching and discrete samplers for sdxl models  </li> <li>cleanup command line parameters  </li> <li>add <code>--new</code> command line flag to enable testing of new packages without breaking existing installs  </li> <li>downgrade rocm to <code>torch==2.7.1</code> </li> <li>set the minimum supported rocm version on linux to <code>rocm==6.0</code> </li> <li>disallow <code>zluda</code> and <code>directml</code> on non-windows platforms  </li> <li>update openvino to <code>openvino==2025.3.0</code> </li> <li>add deprecation warning for <code>python==3.9</code> </li> <li>allow setting denoise strength to 0 in control/img2img     this allows to run workflows which only refine or detail existing image without changing it   </li> <li>Fixes</li> <li>normalize path hanlding when deleting images  </li> <li>unified compile upscalers  </li> <li>fix OpenVINO with ControlNet  </li> <li>fix hidden model tags in networks display  </li> <li>fix networks reference models display on windows  </li> <li>fix handling of pre-quantized <code>flux</code> models  </li> <li>fix <code>wan</code> use correct pipeline for i2v models  </li> <li>fix <code>qwen-image</code> with hires  </li> <li>fix <code>omnigen-2</code> failure  </li> <li>fix <code>auraflow</code> quantization  </li> <li>fix <code>kandinsky-3</code> noise  </li> <li>fix <code>infiniteyou</code> pipeline offloading  </li> <li>fix <code>skyreels-v2</code> image-to-video  </li> <li>fix <code>flex2</code> img2img denoising strength  </li> <li>fix <code>flex2</code> contronet vs inpaint image selection, thanks @alerikaisattera  </li> <li>fix some use cases with access via reverse-proxy  </li> <li>fix segfault on startup with <code>rocm==6.4.3</code> and <code>torch==2.8</code> </li> <li>fix wildcards folders traversal, thanks @dymil  </li> <li>fix zluda flash attention with enable_gqa  </li> <li>fix <code>wan a14b</code> quantization  </li> <li>fix reprocess workflow for control with hires  </li> <li>fix samplers set timesteps vs sigmas  </li> <li>fix <code>detailer</code> missing metadata  </li> <li>fix <code>infiniteyou</code> lora load with  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-08-20","title":"Update for 2025-08-20","text":"<p>A quick service release with several important hotfixes, improved localization support and adding new Qwen model variants...</p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p> <ul> <li>Models</li> <li>Qwen-Image-Edit     Image editing using natural language prompting, similar to <code>Flux.1-Kontext</code>, but based on larger 20B <code>Qwen-Image</code> model  </li> <li>Nunchaku-Qwen-Image     if you have a compatible nVidia GPU, Nunchaku is the fastest quantization engine, currently available for Flux.1, SANA and Qwen-Image models note: release version of <code>nunchaku==0.3.2</code> does NOT include support, so you need to build nunchaku from source  </li> <li>SD.Next Model Samples Gallery </li> <li>updated with new models  </li> <li>Features </li> <li>new setting -&gt; huggingface -&gt; download method     default is <code>rust</code> as new <code>xet</code> is known to cause issues  </li> <li>support for <code>flux.1-kontext</code> lora  </li> <li>support for <code>qwen-image</code> lora  </li> <li>new setting -&gt; quantization -&gt; modules dtype dict     used to manually override quant types per module  </li> <li>UI </li> <li>new artwork for reference models in networks     thanks @liutyi  </li> <li>updated localization for all 8 languages  </li> <li>localization support for ModernUI  </li> <li>single-click on locale rotates current locale     double-click on locale resets locale to <code>en</code> </li> <li>exclude ModernUI from list of extensions     ModernUI is enabled in settings, not by manually enabling extension  </li> <li>Docs </li> <li>Models and Video pages updated with links to original model repos, model licenses and original release dates     thanks @alerikaisattera  </li> <li>Fixes </li> <li>nunchaku use new download links and default to <code>0.3.2</code>     nunchaku wheels: https://huggingface.co/nunchaku-tech/nunchaku/tree/main </li> <li>fix OpenVINO with offloading  </li> <li>add explicit offload calls on prompt encode  </li> <li>error reporting on model load failure  </li> <li>fix torch version checks  </li> <li>remove extra cache clear  </li> <li>enable explicit sync calls for <code>rocm</code> on windows  </li> <li>note if restart-needed on initial startup import error  </li> <li>bypass diffusers-lora-fuse on quantized models  </li> <li>monkey-patch diffusers to use original weights shape when loading lora  </li> <li>guard against null prompt  </li> <li>install <code>hf_transfter</code> and <code>hf_xet</code> when needed  </li> <li>fix ui cropped network tags  </li> <li>enum reference models on startup  </li> <li>dont report errors if agent scheduler is disabled  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-08-15","title":"Update for 2025-08-15","text":""},{"location":"CHANGELOG/#highlights-for-2025-08-15","title":"Highlights for 2025-08-15","text":"<p>New release two weeks after the last one and its a big one with over 150 commits! - Several new models: Qwen-Image (plus Lightning variant) and FLUX.1-Krea-Dev - Several updated models: Chroma, SkyReels-V2, Wan-VACE, HunyuanDiT - Plus continuing with major UI work with new embedded Docs/Wiki search, redesigned real-time hints, wildcards UI selector, built-in GPU monitor, CivitAI integration and more! - On the compute side, new profiles for high-vram GPUs, offloading improvements, parallel-load for large models, support for new <code>torch</code> release and improved quality when using low-bit quantization!     - SD.Next Model Samples Gallery: pre-generated image gallery with 60 models (45 base and 15 finetunes) and 40 different styles resulting in 2,400 high resolution images!   gallery additionally includes model details such as typical load and inference times as well as sizes and types of each model component (e.g. unet, transformer, text-encoder, vae) - And (as always) many bugfixes and improvements to existing features!  </p> <p></p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p> <p>Note: Change-in-behavior - locations of downloaded HuggingFace models and components are changed to allow for de-duplication of common modules and switched from using system default cache folder to <code>models/huggingface</code> SD.Next will warn on startup on unused cache entries that can be removed. Also, to take advantage of de-duplication, you'll need to delete models from your <code>models/Diffusers</code> folder and let SD.Next re-download them!  </p>"},{"location":"CHANGELOG/#details-for-2025-08-15","title":"Details for 2025-08-15","text":"<ul> <li>Models </li> <li>Qwen-Image     new image foundational model with 20B params DiT and using Qwen2.5-VL-7B as the text-encoder!     available via networks -&gt; models -&gt; reference note: this model is almost 2x the size of Flux, quantization and offloading are highly recommended! recommended params: steps=50, attention-guidance=4     also available is pre-packaged Qwen-Lightning     which is an unofficial merge of Qwen-Image with Qwen-Lightning-LoRA to improve quality and allow for generating in 8-steps!  </li> <li>FLUX.1-Krea-Dev     new 12B base model compatible with FLUX.1-Dev from Black Forest Labs with opinionated aesthetics and aesthetic preferences in mind     available via networks -&gt; models -&gt; reference </li> <li>Chroma     great model based on FLUX.1 and then redesigned and retrained by lodestones     update with latest HD, HD Flash and HD Annealed variants which are based on v50 release     available via networks -&gt; models -&gt; reference </li> <li>SkyReels-V2     SkyReels-V2 is a genarative video model based on Wan-2.1 but with heavily modified execution to allow for infinite-length video generation     supported variants are:  <ul> <li>diffusion-forcing: T2I DF 1.3B for 540p videos, T2I DF 14B for 720p videos, I2I DF 14B for 720p videos  </li> <li>standard: T2I 14B for 720p videos and I2I 14B for 720p videos  </li> </ul> </li> <li>Wan-VACE     basic support for Wan 2.1 VACE 1.3B and 14B variants     optimized support with granular guidance control will follow soon  </li> <li>HunyuanDiT-Distilled     variant of HunyuanDiT with reduced steps and improved performance Torch </li> <li>Set default to <code>torch==2.8.0</code> for CUDA, ROCm and OpenVINO </li> <li>Add support for <code>torch==2.9.0-nightly</code> </li> <li>UI </li> <li>new embedded docs/wiki search! Docs search: fully-local and works in real-time on all document pages Wiki search: uses github api to search online wiki pages  </li> <li>updated real-time hints, thanks @CalamitousFelicitousness  </li> <li>add Wilcards UI     in networks display  </li> <li>every heading element is collapsible!  </li> <li>quicksettings reset button to restore all quicksettings to default values     because things do sometimes get wrong...  </li> <li>configurable image fit in all image views  </li> <li>rewritten CivitAI downloader     in models -&gt; civitai hint: you can enter model id in a search bar to pull information on specific model directly hint: you can download individual versions or batch-download all-at-once!  </li> <li>redesigned GPU monitor <ul> <li>standard-ui: system -&gt; gpu monitor </li> <li>modern-ui: aside -&gt; console -&gt; gpu monitor </li> <li>supported for nVidia CUDA and AMD ROCm platforms  </li> <li>configurable interval in settings -&gt; user interface </li> </ul> </li> <li>updated models tab<ul> <li>updated models -&gt; current tab  </li> <li>updated models -&gt; list models tab  </li> <li>updated models -&gt; metadata tab  </li> </ul> </li> <li>updated extensions tab</li> <li>redesigned settings -&gt; user interface </li> <li>gallery bypass browser cache for thumbnails  </li> <li>gallery safer delete operation  </li> <li>networks display indicator for currently active items     applies to: styles, loras </li> <li>apply privacy blur to hf and civitai tokens  </li> <li>image download will now use actual image filename  </li> <li>increase default and maximum ui request timeout to 2min/5min  </li> <li>hint: card layout     card layout is used by networks, gallery, civitai search, etc.     you can change card size in settings -&gt; user interface </li> <li>Offloading </li> <li>changed default values for offloading based on detected gpu memory     see offloading docs for details  </li> <li>new feature to specify which modules to offload always or never     in settings -&gt; model offloading -&gt; offload always/never </li> <li>new <code>highvram</code> profile provides significant performance boost on gpus with more than 24gb  </li> <li>new <code>offload during pre-forward</code> option     in settings -&gt; model offloading     switches from explicit offloading to implicit offloading on module execution change  </li> <li>new <code>diffusers_offload_nonblocking</code> exerimental setting     instructs torch to use non-blocking move operations when possible  </li> <li>Features </li> <li>new <code>T5: Use shared instance of text encoder</code> option     in settings -&gt; text encoder     since a lot of new models use T5 text encoder, this option allows to share     the same instance across all models without duplicate downloads note this will not reduce size of your already downloaded models, but will reduce size of future downloads  </li> <li>Wan select which stage to run: first/second/both with configurable boundary ration when running both stages     in settings -&gt; model options  </li> <li>prompt parser allow explict <code>BOS</code> and <code>EOS</code> tokens in prompt  </li> <li>Nunchaku support for FLUX.1-Fill and FLUX.1-Depth models  </li> <li>update requirements/packages  </li> <li>use model vae scale-factor for image width/heigt calculations  </li> <li>SDNQ add <code>modules_dtype_dict</code> to quantize Qwen Image with mixed dtype  </li> <li>prompt enhance     add <code>allura-org/Gemma-3-Glitter-4B</code>, <code>Qwen/Qwen3-4B-Instruct-2507</code>, <code>Qwen/Qwen2.5-VL-3B-Instruct</code> model support     improve system prompt  </li> <li>schedulers add Flash FlowMatch </li> <li>model loader add parallel loader option     enabled by default, selectable in settings -&gt; model loading </li> <li>filename namegen use exact sequence number instead of next available     this allows for more predictable and consistent filename generation  </li> <li>network delete new feature that allows to delete network from disk     in networks -&gt; show details -&gt; delete     this will also delete description, metadata and previews associated with the network     only applicable to safetensors networks, not downloaded diffuser models  </li> <li>Wiki </li> <li>Models page updated with links to original model repos and model licenses, thanks @alerikaisattera  </li> <li>Updated Model-Support with newly supported models  </li> <li>Updated Offload, Prompting, API pages  </li> <li>API</li> <li>add <code>/sdapi/v1/checkpoint</code> POST endpoint to simply load a model  </li> <li>add <code>/sdapi/v1/modules</code> GET endpoint to get info on model components/modules  </li> <li>all generate endpoints now support <code>sd_model_checkpoint</code> parameter     this allows to specify which model to use for generation without needing to use additional endpoints  </li> <li>Refactor</li> <li>change default huggingface cache folder from system default to <code>models/huggingface</code>     sd.next will warn on startup on unused cache entries  </li> <li>new unified pipeline component loader in <code>pipelines/generic</code> </li> <li>remove LDSR </li> <li>remove <code>api-only</code> cli option  </li> <li>Docker </li> <li>update cuda base image: <code>pytorch/pytorch:2.8.0-cuda12.8-cudnn9-runtime</code> </li> <li>update official builds: https://hub.docker.com/r/vladmandic/sdnext-cuda/tags </li> <li>Fixes </li> <li>refactor legacy processing loop  </li> <li>fix settings components mismatch  </li> <li>fix Wan 2.2-5B I2V workflow  </li> <li>fix Wan T2I workflow  </li> <li>fix OpenVINO  </li> <li>fix video model vs pipeline mismatch  </li> <li>fix video generic save frames  </li> <li>fix inpaint image metadata  </li> <li>fix processing image save loop  </li> <li>fix progress bar with refine/detailer  </li> <li>fix api progress reporting endpoint  </li> <li>fix <code>openvino</code> backend failing to compile  </li> <li>fix <code>zluda</code> with hip-sdk==6.4</li> <li>fix <code>nunchaku</code> fallback on unsupported model  </li> <li>fix <code>nunchaku</code> windows download links  </li> <li>fix Flux.1-Kontext-Dev with variable resolution  </li> <li>use <code>utf_16_be</code> as primary metadata decoding  </li> <li>fix <code>sd35</code> width/height alignment  </li> <li>fix <code>nudenet</code> api  </li> <li>fix global state tracking  </li> <li>fix ui tab detection for networks  </li> <li>fix ui checkbox/radio styling for non-default themes  </li> <li>fix loading custom transformers and t5 safetensors tunes  </li> <li>add mtime to reference models  </li> <li>patch torch version so 3rd party libraries can use expected format  </li> <li>unified stat size/mtime calls  </li> <li>reapply offloading on ipadapter load  </li> <li>api set default script-name  </li> <li>avoid forced gc and rely on thresholds  </li> <li>add missing interrogate in output panel  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-07-29","title":"Update for 2025-07-29","text":""},{"location":"CHANGELOG/#highlights-for-2025-07-29","title":"Highlights for 2025-07-29","text":"<p>This is a big one: simply looking at number of changes, probably the biggest release since the project started!  </p> <p>Feature highlights include: - ModernUI has quite some redesign which should make it more user friendly and easier to navigate plus several new UI themes   If you're still using StandardUI, give ModernUI a try! - New models such as WanAI 2.2 in 5B and A14B variants for both text-to-video and image-to-video workflows as well as text-to-image workflow!   and also FreePik F-Lite, Bria 3.2 and bigASP 2.5 - Redesigned Video interface with support for general video models plus optimized FramePack and LTXVideo support - Fully integrated nudity detection and optional censorship with NudeNet - New background replacement and relightning methods using Latent Bridge Matching and new PixelArt processing filter - Enhanced auto-detection of default sampler types/settings results in avoiding common mistakes - Additional LLM/VLM models available for captioning and prompt enhance - Number of workflow and general quality-of-life improvements, especially around Styles, Detailer, Preview, Batch, Control - Compute improvements - Wiki &amp; Docs updates, especially new end-to-end Parameters page  </p> <p>In this release we finally break with legacy with the removal of the original A1111 codebase which has not been maintained for a while now This plus major cleanup of codebase and external dependencies resulted in ~55k LoC (lines-of-code) reduction and spread over ~750 files in ~200 commits!  </p> <p>We also switched project license to Apache-2.0 which means that SD.Next is now fully compatible with commercial and non-commercial use and redistribution regardless of modifications!  </p> <p>And (as always) many bugfixes and improvements to existing features! For details, see ChangeLog </p> <p>Note</p> <p>We recommend clean install for this release due to sheer size of changes Although upgrades and existing installations are tested and should work fine!</p> <p></p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-07-29","title":"Details for 2025-07-29","text":"<ul> <li>License </li> <li>SD.Next license switched from aGPL-v3.0 to Apache-v2.0     this means that SD.Next is now fully compatible with commercial and non-commercial use and redistribution regardless of modifications!  </li> <li>Models</li> <li>WanAI Wan 2.2 both 5B and A14B variants, for both T2V and I2V support     go to: video -&gt; generic -&gt; wan -&gt; pick variant     optimized support with VACE, etc. will follow soon caution Wan2.2 on its own is ~68GB, but also includes optional second-stage for later low-noise processing which is absolutely massive at additional ~54GB     you can enable second stage processing in settings -&gt; model options, its disabled by default note: quantization and offloading are highly recommended regardless of first-stage only or both stages!  </li> <li>WanAI Wan T2V models for T2I workflows     Wan is originally designed for video workflows, but now also be used for text-to-image workflows!     supports Wan-2.1 in 1.3B and 14B variants and Wan-2.2 in 5B and A14B variants     supports all standard features such as quantization, offloading, TAESD preview generation, LoRA support etc.     can also load unet/transformer fine-tunes in safetensors format using UNET loader     simply select in networks -&gt; models -&gt; reference note 1.3B model is a bit too small for good results and 14B is very large at 78GB even without second-stage so aggressive quantization and offloading are recommended  </li> <li>FreePik F-Lite in 7B, 10B and Texture variants     F-Lite is a 7B/10B model trained exclusively on copyright-safe and SFW content, trained on internal dataset comprising approximately 80 million copyright-safe images     available via networks -&gt; models -&gt; reference </li> <li>Bria 3.2     Bria is a smaller 4B parameter model built entirely on licensed data and safe for commercial use note: this is a gated model, you need to accept terms and set your huggingface token     available via networks -&gt; models -&gt; reference </li> <li>bigASP 2.5     bigASP is an experimental SDXL finetune using Flow matching method     load as usual, and leave sampler set to Default     or you can use following samplers: UniPC, DPM, DEIS, SA     required sampler settings: prediction-method=flow-prediction, sigma-method=flowmatch     recommended sampler settings: flow-shift=1.0 </li> <li>LBM: Latent Bridge Matching     very fast automatic image background replacement methods with relightning! simple: automatic background replacement using BiRefNet relighting: automatic background replacement with reglighting so source image fits desired background     with optional composite blending     available in img2img or control -&gt; scripts </li> <li>add FLUX.1-Kontext-Dev inpaint workflow  </li> <li>add FLUX.1-Kontext-Dev Nunchaku support note: FLUX.1 Kontext is about 2-3x faster with Nunchaku vs standard execution!  </li> <li>support FLUX.1 all-in-one safetensors  </li> <li>support for Google Gemma 3n E2B and E4B LLM/VLM models     available in prompt enhance and process captioning </li> <li>support for HuggingFace SmolLM3 3B LLM model     available in prompt enhance </li> <li>add fal AuraFlow 0.2 in addition to existing fal AuraFlow 0.3 due to large differences in model behavior     available via networks -&gt; models -&gt; reference </li> <li>add integrated NudeNet as built-in functionality note: used to be available as a separate extension </li> <li>Video</li> <li>redesigned Video interface  </li> <li>support for Generic video models     includes support for many video models without specific per-model optimizations     included: Hunyuan, LTX, WAN, Mochi, Latte, Allegro, Cog     supports quantization, offloading, frame interpolation, etc.  </li> <li>support for optimized FramePack     with t2i, i2i, flf2v workflows     LoRA support, prompt enhance, etc.     now fully integrated instead of being a separate extension  </li> <li>support for optmized LTXVideo     with t2i, i2i, v2v workflows     optional native upsampling and video refine workflows     LoRA support with different conditioning types such as Canny/Depth/Pose, etc.  </li> <li>support for post load quantization  </li> <li>UI </li> <li>major update to modernui layout  </li> <li>add new Windows-like Blocks UI theme  </li> <li>redesign of the Flat UI theme  </li> <li>enhanced look&amp;feel for Gallery tab with better search and collapsible sections, thanks to @CalamitousFelicitousness</li> <li>WIKI </li> <li>new Parameters page that lists and explains all generation parameters     massive thanks to @CalamitousFelicitousness for bringing this to life!  </li> <li>updated Models, Video, LTX, FramePack, Styles, etc.</li> <li>Compute </li> <li>support for SageAttention2++     provides 10-15% performance improvement over default SDPA for transformer-based models!     enable in settings -&gt; compute settings -&gt; sdp options note: SD.Next will use either SageAttention v1/v2/v2++, depending which one is installed     until authors provide pre-build wheels for v2++, you need to install it manually or SD.Next will auto-install v1  </li> <li>support for <code>torch.compile</code> for LLM: captioning/prompt-enhannce  </li> <li>support for <code>torch.compile</code> with repeated-blocks     reduces time-to-compile 5x without loss of performance!     enable in settings -&gt; model compile -&gt; repeated note: torch.compile is not compatible with balanced offload  </li> <li>Other </li> <li>Styles can now include both generation params and server settings     see Styles docs for details  </li> <li>TAESD is now default preview type since its the only one that supports most new models  </li> <li>support TAESD preview and remote VAE for HunyuanDit </li> <li>support TAESD preview and remote VAE for AuraFlow </li> <li>support TAESD preview for WanAI </li> <li>SD.Next now starts with locked state preventing model loading until startup is complete  </li> <li>warn when modifying legacy settings that are no longer supported, but available for compatibilty  </li> <li>warn on incompatible sampler and automatically restore default sampler  </li> <li>XYZ grid can now work with control tab:     if controlnet or processor are selected in xyz grid, they will overwrite settings from first unit in control tab,     when using controlnet/processor selected in xyz grid, behavior is forced as control-only     also freely selectable are control strength, start and end values  </li> <li>Batch warn on unprocessable images and skip operations on errors so that other images can still be processed  </li> <li>Metadata improved parsing and detect foreign metadata     detect ComfyUI images     detect InvokeAI images  </li> <li>Detailer add <code>expert</code> mode where list of detailer models can be converted to textbox for manual editing     see docs for more information  </li> <li>Detailer add option to merge multiple results from each detailer model     for example, hands model can result in two hands each being processed separately or both hands can be merged into one composite job  </li> <li>Control auto-update width/height on image upload  </li> <li>Control auto-determine image save path depending on operations performed  </li> <li>autodetect V-prediction models and override default sampler prediction type as needed  </li> <li>SDNQ </li> <li>use inference context during quantization  </li> <li>use static compile  </li> <li>rename quantization type for text encoders <code>default</code> option to <code>Same as model</code> </li> <li>API </li> <li>add <code>/sdapi/v1/lock-checkpoint</code> endpoint that can be used to lock/unlock model changes     if model is locked, it cannot be changed using normal load or unload methods  </li> <li>Fixes</li> <li>allow theme type <code>None</code> to be set in config  </li> <li>installer dont cache installed state  </li> <li>fix Cosmos-Predict2 retrying TAESD download  </li> <li>better handle startup import errors  </li> <li>fix traceback width preventing copy&amp;paste  </li> <li>fix ansi controle output from scripts/extensions  </li> <li>fix diffusers models non-unique hash  </li> <li>fix loading of manually downloaded diffuser models  </li> <li>fix api <code>/sdapi/v1/embeddings</code> endpoint  </li> <li>fix incorrect reporting of deleted and modified files  </li> <li>fix SD3.x loader and TAESD preview  </li> <li>fix xyz with control enabled  </li> <li>fix control order of image save operations  </li> <li>fix control batch-input processing  </li> <li>fix modules merge save model  </li> <li>fix torchvision bicubic upsample with ipex  </li> <li>fix instantir pipeline  </li> <li>fix prompt encoding if prompts within batch have different segment counts  </li> <li>fix detailer min/max size  </li> <li>fix loopback script  </li> <li>fix networks tags display  </li> <li>fix yolo refresh models  </li> <li>cleanup control infotext  </li> <li>allow upscaling with models that have implicit VAE processing  </li> <li>framepack improve offloading  </li> <li>improve prompt parser tokenizer loader  </li> <li>improve scripts error handling  </li> <li>improve infotext param parsing  </li> <li>improve extensions ui search  </li> <li>improve model type autodetection  </li> <li>improve model auth check for hf repos  </li> <li>improve Chroma prompt padding as per recommendations  </li> <li>lock directml torch to <code>torch-directml==0.2.4.dev240913</code> </li> <li>lock directml transformers to <code>transformers==4.52.4</code> </li> <li>improve install of <code>sentencepiece</code> tokenizer  </li> <li>add int8 matmul fallback for ipex with onednn qlinear  </li> <li>Refactoring note: none of the removals result in loss-of-functionality since all those features are already re-implemented   goal here is to remove legacy code, code duplication and reduce code complexity  </li> <li>obsolete original backend </li> <li>remove majority of legacy a1111 codebase  </li> <li>remove legacy ldm codebase: <code>/repositories/ldm</code> </li> <li>remove legacy blip codebase: <code>/repositories/blip</code> </li> <li>remove legacy codeformer codebase: <code>/repositories/codeformer</code> </li> <li>remove legacy clip patch model: <code>/models/karlo</code> </li> <li>remove legacy model configs: <code>/configs/*.yaml</code> </li> <li>remove legacy submodule: <code>/modules/k-diffusion</code> </li> <li>remove legacy hypernetworks support: <code>/modules/hypernetworks</code> </li> <li>remove legacy lora support: <code>/extensions-builtin/Lora</code> </li> <li>remove legacy clip/blip interrogate module  </li> <li>remove modern-ui remove <code>only-original</code> vs <code>only-diffusers</code> code paths  </li> <li>refactor control processing and separate preprocessing and image save ops  </li> <li>refactor modernui layouts to rely on accordions more than individual controls  </li> <li>refactore pipeline apply/unapply optional components &amp; features  </li> <li>split monolithic <code>shared.py</code> </li> <li>cleanup <code>/modules</code>: move pipeline loaders to <code>/pipelines</code> root  </li> <li>cleanup <code>/modules</code>: move code folders used by pipelines to <code>/pipelines/&lt;pipeline&gt;</code> folder  </li> <li>cleanup <code>/modules</code>: move code folders used by scripts to <code>/scripts/&lt;script&gt;</code> folder  </li> <li>cleanup <code>/modules</code>: global rename <code>modules.scripts</code> to avoid conflict with <code>/scripts</code> </li> <li>override <code>gradio</code> installer  </li> <li>major refactoring of requirements and dependencies to unblock <code>numpy&gt;=2.1.0</code> </li> <li>patch <code>insightface</code> </li> <li>patch <code>facelib</code> </li> <li>patch <code>numpy</code> </li> <li>stronger lint rules     add separate <code>npm run lint</code>, <code>npm run todo</code>, <code>npm run test</code>, <code>npm run format</code> macros  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-06-30","title":"Update for 2025-06-30","text":""},{"location":"CHANGELOG/#highlights-for-2025-06-30","title":"Highlights for 2025-06-30","text":"<p>New release with ~100 commits...So what's new? Well, its been a busy few weeks with new models coming out quite frequently: - New T2I/I2I models: OmniGen-2, Cosmos-Predict2, FLUX.1-Kontext, Chroma - Additional VLM models: JoyCaption Beta, MoonDream 2 - Additional upscalers: UltraSharp v2 </p> <p>And (as always) many bugfixes and improvements to existing features!  </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-06-30","title":"Details for 2025-06-30","text":"<ul> <li>Models</li> <li>Models Wiki page is updated will all new models note all new image models larger than 30GB, so offloading and quantization are necessary!  </li> <li>OmniGen2 <ul> <li>OmniGen2 is a powerful unified multimodal model that supports t2i and i2i workflows and uses 4B transformer with Qwen-VL-2.5 4B VLM  </li> <li>available via networks -&gt; models -&gt; reference </li> </ul> </li> <li>nVidia Cosmos-Predict2 T2I 2B and 14B <ul> <li>Cosmos-Predict2 T2I is a new foundational model from Nvidia in two variants: small 2B and large 14B</li> <li>available via networks -&gt; models -&gt; reference </li> <li>note: 14B variant is a very large model at 36GB</li> <li>note: this is a gated model, you need to accept terms and set your huggingface token </li> </ul> </li> <li>Black Forest Labs FLUX.1 Kontext I2I Dev variant  <ul> <li>FLUX.1-Kontext is a 12B model billion parameter capable of editing images based on text instructions  </li> <li>model is primarily designed for image editing workflows, but also works for text-to-image workflows  </li> <li>requirements are similar to regular FLUX.1 although 2x slower  </li> <li>available via networks -&gt; models -&gt; reference </li> <li>note: this is a gated model, you need to accept terms and set your huggingface token </li> </ul> </li> <li>lodestones Chroma <ul> <li>Chroma is a 8.9B parameter model based on FLUX.1-schnell and fully Apache 2.0 licensed  </li> <li>available via networks -&gt; models -&gt; reference </li> <li>note: model is still in training so future updates will trigger re-download  </li> <li>large credits to @Trojaner for work on bringing Chroma support to SD.Next and all the optimizations around it!  </li> </ul> </li> <li>JoyCaption Beta support (in addition to existing JoyCaption Alpha)  <ul> <li>new version of highly popular captioning model  </li> <li>available via caption -&gt; vlm caption </li> </ul> </li> <li>MoonDream 2 support (updated)  <ul> <li>really good 2B captioning model that can work on different levels of detail  </li> <li>available via caption -&gt; vlm caption </li> </ul> </li> <li>UltraSharp v2 support  <ul> <li>one of the best upscalers (traditional, non-diffusion) available today!  </li> <li>available via process -&gt; upscale -&gt; chainner </li> </ul> </li> <li>Changes </li> <li>Update all core requirements  </li> <li>Support Remote VAE with Omnigen, Lumina 2 and PixArt </li> <li>Enable quantization for captioning: Gemma, Qwen, SMOL, Florence, JoyCaption </li> <li>Add <code>--trace</code> command line param that enables trace logging  </li> <li>Use Diffusers version of OmniGen </li> <li>Control move global settings to control elements -&gt; control settings tab  </li> <li>Control add setting to run hires with or without control  </li> <li>Update OpenVINO to 2025.2.0  </li> <li>Simplified and unified quantization enabled for options  </li> <li>Add PixelArt filter to processing tab  </li> <li>SDNQ Quantization </li> <li>Add <code>auto</code> quantization mode  </li> <li>Add <code>modules_to_not_convert</code> support for post mode  </li> <li>Improve offload compatibility  </li> <li>Fix Qwen 2.5 with int8 matmul  </li> <li>Fix Dora loading  </li> <li>Remove per layer GC  </li> <li>Add support for XYZ grid to test quantization modes note: you need to enable quantization and choose what it applies on, then xyz grid can change quantization mode note: you can also enable 'add time info' to compare performance of different quantization modes  </li> <li>API</li> <li>Add <code>/sdapi/v1/network?page=&lt;page_name&gt;&amp;item=&lt;item_name&gt;</code> endpoint that returns full network info  </li> <li>Add <code>/sdapi/v1/lora?lora=&lt;lora_name&gt;</code> endpoint that returns full lora info and metadata  </li> <li>Add <code>/sdapi/v1/controlnets?model_type=&lt;model_type|all|None&gt;</code> endpoints that returns list of available controlnets for specific model type  </li> <li>Set default sampler to <code>Default</code> </li> <li>Fixes </li> <li>IPEX with DPM2++ FlowMatch samplers  </li> <li>Invalid attention processor with ControlNet  </li> <li>LTXVideo default scheduler  </li> <li>Balanced offload with OmniGen  </li> <li>Quantization with OmniGen  </li> <li>Do not save empty <code>params.txt</code> file  </li> <li>Override <code>params.txt</code> using <code>SD_PATH_PARAMS</code> env variable  </li> <li>Add <code>wheel</code> to requirements due to <code>pip</code> change  </li> <li>Case-insensitive sampler name matching  </li> <li>Fix delete file with gallery views  </li> <li>Add <code>SD_SAVE_DEBUG</code> env variable to report all params and metadata save operations as they happen  </li> <li>Fix TAESD model type detection  </li> <li>Fix LoRA loader incorrectly reporting errors  </li> <li>Fix hypertile for img2img and inpaint operations  </li> <li>Fix prompt parser batch size  </li> <li>Fix process batch with batch count  </li> <li>Fix process batch double image save  </li> <li>Fix unapply texture tiling  </li> <li>Fix nunchaku batch support  </li> <li>Fix LoRA change detection on pipeline type change  </li> <li>Fix LoRA load order when it includes text-encoder data  </li> <li>Suppress torch empty logging  </li> <li>Improve TAESD live preview downscale handling  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-06-16","title":"Update for 2025-06-16","text":"<ul> <li>Feature </li> <li>Support for Python 3.13  </li> <li>TeaCache support for Lumina 2  </li> <li>Custom UNet and VAE loading support for Lumina 2  </li> <li>Changes </li> <li>Increase the medvram mode threshold from 8GB to 12GB  </li> <li>Set CPU backend to use FP32 by default  </li> <li>Relax Python version checks for Zluda  </li> <li>Make VAE options not require model reload  </li> <li>Add warning about incompatible attention processors  </li> <li>Torch </li> <li>Set default to <code>torch==2.7.1</code> </li> <li>Force upgrade pip when installing Torch  </li> <li>ROCm </li> <li>Support ROCm 6.4 with <code>--use-nightly</code> </li> <li>Don't override user set gfx version  </li> <li>Don't override gfx version with RX 9000  </li> <li>Fix flash-atten repo  </li> <li>SDNQ Quantization </li> <li>Add group size support for convolutional layers  </li> <li>Add quantized matmul support for for convolutional layers  </li> <li>Add 7-bit, 5-bit and 3-bit quantization support  </li> <li>Add separate quant mode option for Text Encoders  </li> <li>Fix forced FP32 with tensorwise FP8 matmul  </li> <li>Fix PyTorch &lt;= 2.4 compatibility with FP8 matmul  </li> <li>Fix VAE with conv quant  </li> <li>Don't ignore the Quantize with GPU option with offload mode <code>none</code> and <code>model</code> </li> <li>High VRAM usage with Lumina 2  </li> <li>Fixes </li> <li>Meissonic with multiple generators  </li> <li>OmniGen with new transformers  </li> <li>Invalid attention processors  </li> <li>PixArt Sigma Small and Large loading  </li> <li>TAESD previews with PixArt and Lumina 2  </li> <li>VAE Tiling with non-default tile sizes  </li> <li>Lumina 2 with IPEX  </li> <li>Nunchaku updated repo  </li> <li>Double loading of models with custom UNets  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-06-02","title":"Update for 2025-06-02","text":""},{"location":"CHANGELOG/#highlights-for-2025-06-02","title":"Highlights for 2025-06-02","text":"<p>This release is all about quantization: with new SD.Next own quantization method: SDNQ SDNQ is based on NNCF, but has been re-implemented, optimized and evolved enough to become its own quantization method! It's fully cross-platform, supports all GPUs and includes tons of quantization methods: - 8-bit, 6-bit, 4-bit, 2-bit and 1-bit int and uint - 8-bit e5, e4 and fnuz float</p> <p>Also unlike most traditional methods, its also applicable to nearly all model types  </p> <p>Hint: Even if you may not need quantization for your current model, it may be worth trying it out as it can significantly improve performance or capabilities of your existing workflow! For example, you may not have issues with SD15 or SDXL, but you may have been limited running at high resolutions or with multiple ControlNet due to VRAM requirements - this will significantly reduce memory requirements. And on-the-fly quantization takes just few seconds during model load, there is no need to have multiple quant models permanently saved.  </p> <p>On a different topic, SD.Next Wiki &amp; Docs and its UI Hints and UI Localization system are community efforts and any contributions are welcome! You dont need any coding experience, but if you learned something and you find documentation either wrong or insufficient, please do suggest edits! Take a look at Docs, Hints and Localization contribution guides</p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-06-02","title":"Details for 2025-06-02","text":"<ul> <li>SDNQ Quantization </li> <li>Renamed <code>NNCF</code> to <code>SDNQ</code> </li> <li>Renamed quantization scheme names to the underlying dtype names instead of NNCF names  <ul> <li><code>INT8_SYM</code> -&gt; <code>int8</code> </li> <li><code>INT8</code> -&gt; <code>uint8</code> </li> <li><code>INT4_SYM</code> -&gt; <code>int4</code> </li> <li><code>INT4</code> -&gt; <code>uint4</code> </li> </ul> </li> <li>Add <code>float8_e4m3fn</code>, <code>float8_e5m2</code>, <code>float8_e4m3fnuz</code>, <code>float8_e5m2fnuz</code>, <code>int6</code>, <code>uint6</code>, <code>int2</code>, <code>uint2</code> and <code>uint1</code> support  </li> <li>Add quantized matmul support for <code>float8_e4m3fn</code> and <code>float8_e5m2</code> </li> <li>Set the default quant mode to <code>pre</code> </li> <li>Use per token input quant with int8 and fp8 quantized matmul  </li> <li>Implement better layer hijacks  </li> <li>Add an option to toggle quantize with GPU  </li> <li>Fix conv quant and add support for conv quant with asym modes  </li> <li>Fix lora weight change  </li> <li>Fix high RAM usage with pre mode  </li> <li>Fix scale and zero_point not being offloaded  </li> <li>IPEX </li> <li>Disabe Dynamic Attention by default on PyTorch 2.7  </li> <li>Remove GradScaler hijack and use <code>torch.amp.GradScaler</code> instead  </li> <li>Feature </li> <li>TeaCache support for HiDream I1  </li> <li>Changes </li> <li>Set the default attention optimizer to Scaled-Dot-Product on all backends  </li> <li>Enable Dynamic attention for Scaled-Dot-Product with ROCm, DirectML, MPS and CPU backends  </li> <li>Fixes</li> <li>Gallery duplicate entries  </li> <li>Prompt enhancement args mismatch  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-05-17","title":"Update for 2025-05-17","text":"<p>Curious how your system is performing? Run a built-in benchmark and compare to over 15k unique results world-wide: Benchmark data! From slowest 0.02 it/s running on 6th gen CPU without acceleration up to 275+ it/s running on tuned GH100 system!  </p> <p>Also, since quantization is becoming a necessity for almost all new models, see comparison of different quantization methods available in SD.Next: Quantization Hint: Even if you may not need quantization for your current model, it may be worth trying it out as it can significantly improve performance!  </p> <p>For ZLUDA users, this update adds compatibility with with latest AMD Adrenaline drivers  </p> <p>Btw, last few releases have been smaller, but more regular so do check posts about previous releases as features do quickly add up!  </p> <ul> <li>Wiki </li> <li>Updates for: Quantization, NNCF, WSL, ZLUDA, ROCm </li> <li>Models </li> <li>Index AniSora v1 5B I2V     Based on CogVideoX architecture, trained as animated video generation model: This Project presenting Bilibili's gift to the anime world!  </li> <li>Index AniSora v1 RL 5B I2V     RL-optimized AniSoraV1.0 for enhanced anime-style output  </li> <li>Compute </li> <li>ZLUDA: update to <code>zluda==3.9.5</code> with <code>torch==2.7.0</code> Note: delete <code>.zluda</code> folder so that newest zluda will be installed if you are using the latest AMD Adrenaline driver  </li> <li>NNCF: added experimental support for direct INT8 MatMul  </li> <li>Feature </li> <li>Prompt Enhance: option to allow/disallow NSFW content  </li> <li>Fixes </li> <li>OpenVINO: force cpu device  </li> <li>Gradio: major cleanup and fixing defaults and ranges  </li> <li>Pydantic: update to api types  </li> <li>UI defaults: match correct prompt components  </li> <li>NNCF with ControlNet  </li> <li>NNCF with CogVideo</li> <li>IPEX with CogVideo  </li> <li>JXL image format metadata handling  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-05-12","title":"Update for 2025-05-12","text":""},{"location":"CHANGELOG/#highlights-for-2025-05-12","title":"Highlights for 2025-05-12","text":"<p>First of all NNCF quantization engine has gone through some major enhancements and its now much faster, both in quantization as well as actual inference! And its a only truly cross-platform solution for quantization as all other methods are platform specific.  </p> <p>Note if you're a ZLUDA user, see notes on GPU driver compatibility as recent Andrenaline drivers do cause problems! And if you're a ROCm user, this release brings much faster compile times on Linux as well as first (experimental) builds for Windows!  </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-05-12","title":"Details for 2025-05-12","text":"<ul> <li>Compute</li> <li>NNCF <ul> <li>Faster quantization  </li> <li>Faster inference with support for <code>torch.triton</code>   up to 3.5x faster with INT4 and 2x faster with INT8  </li> <li>New settings: NNCF -&gt; Group size   default is a balance between performance (higher size) and quality (lower size)   0 is default at 64, -1 disables grouping  </li> </ul> </li> <li>ZLUDA:<ul> <li>warning: AMD Adrenaline 25.5.1 drivers are NOT COMPATIBLE with ZLUDA   see issue for details</li> </ul> </li> <li>ROCm<ul> <li>first working builds of Torch with ROCm on Windows   highly experimental   reach out on Discord if you want to test it  </li> </ul> </li> <li>Features</li> <li>Prompt Enhancer: support for img2img workflows     in img2img prompt enhancer will first analyze input image and then incorporate user prompt to create enhanced prompt  </li> <li>FramePack<ul> <li>improve LoRA compatibility  </li> <li>add metadata to video  </li> </ul> </li> <li>UI<ul> <li>ModernUI: support for History tab  </li> <li>ModernUI: support for FramePack tab  </li> </ul> </li> <li>API <ul> <li>add <code>/sdapi/v1/framepack</code> endpoint with full support for FramePack including all optional settings   see example: <code>sd-extension-framepack/create-video.py</code> </li> <li>add <code>/sdapi/v1/checkpoint</code> endpoint to get info on currently loaded model/checkpoint   see example: <code>cli/api-checkpoint.py</code> </li> <li>add <code>/sdapi/v1/prompt-enhance</code> endpoint to enhance prompt using LLM   see example: <code>cli/api-enhance.py</code>   supports text, image and video prompts with or without input image note: if input image is provided, model should be left at default <code>gemma-3-4b-it</code> as most other LLMs do not support hybrid workflows  </li> </ul> </li> <li>Fixes</li> <li>Latent Diffusion Upscale</li> <li>Model load: support SDXL safetensors packaged without VAE  </li> <li>ROCm: disable cuDNN benchmark, fixes slow MIOpen tuning with <code>torch==2.7</code> </li> <li>Extensions: use in-process installer for extensions-builtin, improves startup performance  </li> <li>FramePack: monkey-patch for dynamically installed <code>av</code> </li> <li>Logging: reduce spam while progress is active  </li> <li>LoRA: legacy handler enable/disable  </li> <li>LoRA: force clear-cache on model unload  </li> <li>ADetailer: fix enable/disable  </li> <li>ZLUDA: improve compatibility with older GPUs  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-05-06","title":"Update for 2025-05-06","text":"<p>Minor refesh with several bugfixes and updates to core libraries Plus new features with FramePack and HiDream-E1</p> <ul> <li>Features </li> <li>FramePack     add T2V mode in addition to I2V and FLF2V     support for new F1: forward-only model variant in addition to regular bi-directional     add prompt enhance using VLM: it will analyze input image and then create enhanced prompt based on user prompt and image     add prompt interpolation, section prompts do not need to match exact video section count     and improved performance Docs rewrite!  </li> <li>Prompt-Enhhance     add Qwen3 0.6B/1.7B/4B models     add thinking mode support (for models that have it)  </li> <li>HiDream-E1 natural language image-editing model built on HiDream-I1     available via  networks -&gt; models -&gt; reference note: right now HiDream-E1 is limited to 768x768 images, so you must force resize image before running it  </li> <li>Other </li> <li>CUDA: set default to <code>torch==2.7.0</code> with <code>cuda==12.8</code> </li> <li>ZLUDA: update to <code>zluda==3.9.4</code> and <code>flash-attn-2</code> </li> <li>Docker: pre-install <code>ffmpeg</code> </li> <li>Wiki: updated pages: FramePack, Video, ROCm, ZLUDA, Quantization </li> <li>Gallery: support JXL image format  </li> <li>Scheduler: add sigmoid beta scheduler  </li> <li>GitHub: updated issue template  </li> <li>Fixes </li> <li>FramePack: correct dtype  </li> <li>NNCF: check dependencies and register quant type  </li> <li>API: refresh checkpoint list  </li> <li>API: vlm-api endpoint  </li> <li>Styles: save style with prompt  </li> <li>Texture tiling: fix apply when switching models  </li> <li>Diffusers: slow initial startup  </li> <li>Gated access: obfuscate and log token used for access  </li> <li>SDXL refiner workflow  </li> <li>Control: t2i-adapter workflow  </li> <li>Control: xs-controlnet workflow  </li> <li>Control: lllite-workflow  </li> <li>Control: refiner workflow with multiple control elements  </li> </ul>"},{"location":"CHANGELOG/#highlights-for-2025-04-28","title":"Highlights for 2025-04-28","text":"<p>Another major release with over 120 commits! Highlights include new Nunchaku Wiki inference engine that allows running FLUX.1 with 3-5x higher performance! And a new FramePack extension for high-quality I2V and FLF2V video generation with unlimited duration!  </p> <p>What else? - New UI History tab - New models: Flex.2, LTXVideo-0.9.6, WAN-2.1-14B-FLF2V, schedulers: UniPC and LCM FlowMatch, features: CFGZero - Major updates to: NNCF, OpenVINO, ROCm, ZLUDA - Cumulative fixes since last release  </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-04-28","title":"Details for 2025-04-28","text":"<ul> <li>Features</li> <li>Nunchaku inference engine with custom SVDQuant 4-bit execution     highly experimental and with limited support, but when it works, its magic: Flux.1 at 6.0 it/s (not sec/it)!     basically, it can speed up supported models by 2-5x by using custom quantization and execution engine     see Nunchaku Wiki for installation guide and list of supported models &amp; features  </li> <li>FramePack based on HunyuanVideo-I2V     full support and much more for Lllyasviel FramePack     implemented as an extension for SD.Next (for the moment while dev is ongoing)     generate high-quality videos with pretty much unlimited duration and with limited VRAM!     install as any other extension and for details see extension README <ul> <li>I2V &amp; FLF2V support with explicit strength controls  </li> <li>complex actions: modify prompts for each section of the video  </li> <li>LoRA support: use normal HunyuanVideo LoRAs  </li> <li>decode: use local, tiny or remote VAE  </li> <li>custom models: e.g. replace llama with one of your choice  </li> <li>video: multiple codecs and with hw acceleration, raw export, frame export, frame interpolation  </li> <li>compute: quantization support, new offloading, more configuration options, cross-platform, etc.  </li> </ul> </li> <li>Ostris Flex.2 Preview     more than a FLUX.1 finetune, FLEX.2 is created from Flux.1 Schnell -&gt; OpenFlux.1 -&gt; Flex.1-alpha -&gt; Flex.2-preview     and it has universal control and inpainting support built in!     supported for text and control workflows     when using in control mode, simply choose preprocessor and do not load actual controlnet     supported control modes are: line, pose and depth     available via  networks -&gt; models -&gt; reference </li> <li>LTXVideo 0.9.6 T2V and I2V     in both Standard and Distilled variants     available in video tab</li> <li>WAN 2.1 14B 720P FLF2V     new first-to-last image video model from WAN-AI     available in video tab</li> <li>CFG-Zero new guidance method optimized for flow-matching models     implemented for FLUX.1, HiDream-I1, SD3.x, CogView4, HunyuanVideo, WanAI     enable and configure in settings -&gt; pipeline modifiers -&gt; cfg zero     experiment with CFGZero support in XYZ-grid  </li> <li>Optimizations</li> <li>NNCF update to 2.16.0     major refactoring of NNCF quantization code     new quant types: <code>INT8_SYM</code> (new default), <code>INT4</code> and <code>INT4_SYM</code>     quantization support for the convolutional layers on unet models with sym methods     pre-load quantization support     LoRA support if you're low on VRAM, NNCF is as close as a catch-all solution </li> <li>OpenVINO update to 2025.1.0 and Torch to 2.7  </li> <li>IPEX update to Torch 2.7  </li> <li>ROCm update to Torch 2.7  </li> <li>HiDream-I1 optimized offloading and prompt-encode caching     it now works in 12GB VRAM / 26GB RAM!  </li> <li>CogView3 and CogView4 model loader optimizations  </li> <li>Sana model loader optimizations</li> <li>add explicit offload after encode prompt     configure in settings -&gt; text encoder -&gt; offload </li> <li>UI </li> <li>new History tab where you can see all jobs since the server startup     and optionally download any of the previously generated images/videos     access via system -&gt; history </li> <li>server restart from ui now replaces currently running process     instead of trying to reload python modules in-place  </li> <li>add option to enable/disable clip skip     disabled by default to avoid issues with frequent incorrect recommendations     in settings -&gt; pipeline modifiers</li> <li>configurable restore metadata from image to settings and to params     in settings -&gt; image metadata </li> <li>API </li> <li>new API Wiki </li> <li>server will now maintain job history which can be queried via API     so you can check previous jobs as well as request any previously generated images/videos  </li> <li>history endpoint: <code>/sdapi/v1/history?id={id}</code> </li> <li>download endpoint: <code>/file={filename}</code> </li> <li>progress api <code>/sdapi/v1/progress</code> now also include task id in the response  </li> <li>Other</li> <li>OMI support for sd15/sdxl omi-standard LoRAs</li> <li>text/image/control/video pipeline vs task compatibility check  </li> <li>HiDream-I1, FLUX.1, SD3.x add HF gated access auth check  </li> <li>HiDream-I1 LoRA support     currently limited to diffusers-only LoRAs, CivitAI LoRA support is TBD  </li> <li>HiDream-I1 add LLM info to image metadata  </li> <li>add <code>model_type</code> as option for image filename pattern  </li> <li>add UniPC FlowMatch scheduler  </li> <li>add LCM FlowMatch scheduler  </li> <li>networks: set which networks to skip when scanning civitai     in settings -&gt; networks -&gt; network scan     comma-separate list of regex patterns to skip  </li> <li>ui display reference models with subdued color  </li> <li>xyz grid support bool  </li> <li>do not force gc at end of processing  </li> <li>add <code>SD_LORA_DUMP</code> env variable for dev/diag to dump lora/model keys  </li> <li>Wiki </li> <li>new Nunchaku, API pages  </li> <li>updated HiDream, Quantization, NNCF, Video, Docker, WSL, ZLUDA pages  </li> <li>Fixes</li> <li>HunyuanVideo-I2V with latest transformers  </li> <li>NNCF with TE-only quant  </li> <li>ONNX init fix  </li> <li>Quanto with TE/LLM quant  </li> <li>HiDream live preview  </li> <li>FLUX.1 controlnet i2i  </li> <li>SD35 InstantX IP-adapter  </li> <li>OpenVINO device selection</li> <li>xyz grid restore settings  </li> <li>config save unnecessary keys  </li> <li>recursive wildcards  </li> <li>extension installer handling of PYTHONPATH  </li> <li>trace logging  </li> <li>api logging  </li> <li>sd/sdxl-inpaint model loader  </li> <li>settings list display only visible items  </li> <li>checkpoint match when searching for model to load  </li> <li>video vae selection load correct vae</li> </ul>"},{"location":"CHANGELOG/#update-for-2025-04-12","title":"Update for 2025-04-12","text":""},{"location":"CHANGELOG/#highlights-for-2025-04-12","title":"Highlights for 2025-04-12","text":"<p>Last release was just over a week ago and here we are again with another update as a new high-end image model, HiDream-I1 jumped out and generated a lot of buzz! There are quite a few other performance and quality-of-life improvements in this release and 40 commits, so please take a look at the full ChangeLog </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-04-12","title":"Details for 2025-04-12","text":"<ul> <li>Models </li> <li>HiDream-I1 in fast, dev and full variants!     new absolutely massive image generative foundation model with 17B parameters and 4 text-encoders with additional 8.3B parameters     simply select from networks -&gt; models -&gt; reference     due to size (over 25B params in 58GB), offloading and on-the-fly quantization are pretty much a necessity     see HiDream Wiki page for details  </li> <li>Features </li> <li>Custom model loader       can be used to load any known diffusion model with default or custom model components       in models -&gt; custom tab       see docs for details: https://vladmandic.github.io/sdnext-docs/Loader/ <ul> <li>Pipe: SoftFill </li> </ul> </li> <li>Caching </li> <li>add <code>TeaCache</code> support to Flux, CogVideoX, Mochi, LTX </li> <li>add <code>FasterCache</code> support to WanAI, LTX (other video models already supported)  </li> <li>add <code>PyramidAttentionBroadcast</code> support to WanAI, LTX (other video models already supported)  </li> <li>UI </li> <li>client polling speeds up and slows down depending if client page is visible or not     client polling does not ask for live preview if page is not visible     significantly reduces server load if you hide or minimize the page  </li> <li>progress: use batch-count for progress  </li> <li>grid: add of max-rows and max-columns in settings to control grid format  </li> <li>gallery: add max-columns in settings for gradio gallery components  </li> <li>Other </li> <li>ZLUDA: add more GPUs to recognized list     select in scripts, available for sdxl in inpaint model  </li> <li>LoRA: add option to force-reload LoRA on every generate  </li> <li>settings: add Model options sections as placeholder for per-model settings</li> <li>video: update LTXVideo-0.9.5 pipeline  </li> <li>te loader: allow free-form input in which case sdnext will attempt to load it as hf repo  </li> <li>diag: add get-server-status to UI generate context menu  </li> <li>diag: memory monitor detect gpu swapping  </li> <li>use hf-xet for huggingface downloads where possible  </li> <li>quant: update &amp; fix <code>optimum-quanto</code> for transformers  </li> <li>quant: update &amp; fix <code>torchao</code> </li> <li>model load: new setting for model load initial device map     can be used to force gpu vs cpu when loading model to avoid oom before model offloading is even activated after load  </li> <li>Changes </li> <li>params: Reset default guidance-rescale from 0.7 to 0.0  </li> <li>progress: add additional fields to progress API  </li> <li>Fixes </li> <li>styles: resize and bring quick-ui to forward on hover  </li> <li>LoRA: obey configured device when performing calculations  </li> <li>ZLUDA: startup issues  </li> <li>offload: balanced offload remove non-blocking move op  </li> <li>logging: debug causes invalid import  </li> <li>logging: cleanup  </li> <li>ROCm: flash attention repo with navi rotary fix  </li> <li>prompt: prompt scheduling with te caching  </li> <li>ui: progress allow for longer timeouts  </li> <li>internal: cleanup defined pipelines</li> </ul>"},{"location":"CHANGELOG/#update-for-2025-04-03","title":"Update for 2025-04-03","text":""},{"location":"CHANGELOG/#highlights-for-2025-04-03","title":"Highlights for 2025-04-03","text":"<p>Time for another major release with ~120 commits and ChangeLog that spans several pages!</p> <p>Highlights? Video...Brand new Video processing module with support for all latest models: WAN21, Hunyuan, LTX, Cog, Allegro, Mochi1, Latte1 in both T2V and I2V workflows And combined with on-the-fly quantization, support for Local/Tiny/Remote VAE, acceleration modules such as FasterCache or PAB, and more! Models...And support for new models: CogView-4, SANA 1.5,  </p> <p>Plus... - New Prompt Enhance using LLM, - New pipelines such as InfiniteYou - New CLiP models, improvements to remote VAE, additional wiki/docs/guides - More quantization options and granular control - Pretty big performance updates to a) Any model using DiT based architecture due to new caching methods, b) ZLUDA with new attention methods, c) LoRA with much lower memory usage  </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2025-04-03","title":"Details for 2025-04-03","text":"<ul> <li>Video tab </li> <li>see Video Wiki for details!  </li> <li>new top-level tab, replaces previous video script in text/image tabs     old scripts are still present, but will be removed in the future  </li> <li>support for all latest models:  <ul> <li>Hunyuan: HunyuanVideo, FastHunyuan, SkyReels | T2V, I2V </li> <li>WAN21: 1.3B, 14B | T2V, I2V </li> <li>LTXVideo: 0.9.0, 0.9.1, 0.9.5 | T2V, I2V </li> <li>CogVideoX: 2B, 5B | T2V, I2V </li> <li>Allegro: T2V </li> <li>Mochi1: T2V </li> <li>Latte1: *T2V  </li> </ul> </li> <li>decoding:  <ul> <li>Default: use vae from model  </li> <li>Tiny VAE: support for Hunyuan, WAN, Mochi </li> <li>Remote VAE: support for Hunyuan </li> </ul> </li> <li>LoRA<ul> <li>support for Hunyuan, LTX, WAN, Mochi, Cog </li> <li>add option to apply LoRA directly on GPU or use CPU first in low-memory scenarios  </li> <li>improve metadata and preview parallel fetch  </li> <li>support for mp4 so first frame is extracted as used as lora preview  </li> </ul> </li> <li>additional key points:  <ul> <li>all models are auto-downloaded upon first use   uses system paths -&gt; huggingface folder  </li> <li>support for many video types  </li> <li>optional video interpolation while creating video files  </li> <li>optional video preview in ui   present if video output is selected  </li> <li>support for balanced offloading and model offloading   uses system settings  </li> <li>on-the-fly quantization: BnB, Quanto, TorchAO   uses system settings, granular for transformer and text-encoder separately  </li> <li>different video models support different video resolutions, frame counts, etc.   and may require specific settings - see model links for details  </li> <li>see ToDo/Limitations section for additional notes  </li> </ul> </li> <li>Models &amp; Pipelines </li> <li>THUDM CogView 4 6B variant     new foundation model for image generation based o GLM-4 text encoder and a flow-based diffusion transformer     fully supports offloading and on-the-fly quantization     simply select from networks -&gt; models -&gt; reference note cogview4 is compatible with flowmatching samplers  </li> <li>NVLabs SANA 1.5 in 1.6B, 4.8B and Sprint variations     big update to previous SANA model     fully supports offloading and on-the-fly quantization     simply select from networks -&gt; models -&gt; reference </li> <li>ByteDance InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity     face-transfer model for FLUX.1     select from Scripts -&gt; InfiniteYou     its large, ~12GB on top of FLUX.1 base model so make sure you have offloading and quantization setup note model will be auto-downloaded on first use  </li> <li>New zer0int CLiP-L models:     download text encoders into folder set in settings -&gt; system paths -&gt; text encoders (default is models/Text-encoder)     load using settings -&gt; text encoder tip: add sd_text_encoder to your settings -&gt; user interface -&gt; quicksettings list to have it appear at the top of the ui  </li> <li>Prompt Enhance </li> <li>see Prompt Enhance Wiki for details!  </li> <li>new built-in extension available in text/image/control tabs  </li> <li>can be used to manually or automatically enhance prompts using LLM  </li> <li>built-in presets for Gemma-3, Qwen-2.5, Phi-4, Llama-3.2, SmolLM2, Dolphin-3 </li> <li>support for custom models     load any models hosted on huggingface     load either model in huggingface format or <code>gguf</code> format note: any hf model in <code>transformers.AutoModelForCausalLM</code> standard should work note: not all model architecture are supported for <code>gguf</code> format  </li> <li>models are auto-downloaded on first use  </li> <li>support quantization and offloading  </li> <li>auto-detect censored output  </li> <li>debug using <code>SD_LLM_DEBUG=true</code> env variable  </li> <li>Acceleration </li> <li>Support for most DiT-based models, for example: FLUX.1, SD35, Hunyuan, Mochi, Latte, Allegro, Cog </li> <li>Enable and configure in Settings -&gt; Pipeline modifiers </li> <li>FasterCache </li> <li>PyramidAttentionBroadcast </li> <li>Remote VAE </li> <li>add support for remote vae encode in addition to remote vae decode  </li> <li>used by img2img, inpaint, hires, detailer </li> <li>remote vae encode is disabled by default, you can enable it in settings -&gt; variable auto-encoder </li> <li>add remote vae info to metadata, thanks @iDeNoh  </li> <li>remote vae use <code>scaling_factor</code> and <code>shift_factor</code> </li> <li>Caption/VLM </li> <li>Google Gemma 3 4B     simply select from list of available models in caption tab  </li> <li>ByteDance/Sa2VA 1B, 4B     simply select from list of available models in caption tab  </li> <li>add option to set system prompt for vlm models that support it: Gemma, Smol, Qwen </li> <li>NudeNet extension updates  </li> <li>add detection of prompt language and alphabet and filter based on those values  </li> <li>add image policy checks using <code>LlavaGuard</code> VLM to detect policy violations (and reasons)     against top-10 standard harmful content categories  </li> <li>add banned words/expressions check against prompt variations  </li> <li>LoRA</li> <li>enable memory cache by default  </li> <li>significantly reduce memory usage  </li> <li>improve performance  </li> <li>improve detection of lora changes  </li> <li>unload lora only when changes are detected  </li> <li>refactor code for modularity  </li> <li>IPEX </li> <li>add <code>--upgrade</code> to torch_command when using <code>--use-nightly</code> </li> <li>add xpu to profiler  </li> <li>fix untyped_storage, torch.eye and torch.cuda.device ops  </li> <li>fix torch 2.7 compatibility  </li> <li>fix performance with balanced offload  </li> <li>fix triton and torch.compile  </li> <li>ROCm</li> <li>add <code>--upgrade</code> to torch_command when using <code>--use-nightly</code> </li> <li>disable fp16 for gfx1102 (rx 7600 and rx 7500 series) gpus  </li> <li>ZLUDA </li> <li>triton for ZLUDA v3.9.2 <ul> <li><code>torch.compile</code> is now available  </li> <li>Flash Attention 2 is now available  </li> </ul> </li> <li>Other </li> <li>Command line new option <code>--monitor PERIOD</code> to monitor CPU and GPU memory ever n seconds  </li> <li>Upscale new asymmetric vae v2 upscaling method  </li> <li>Upscale new experimental support for <code>libvips</code> upscaling  </li> <li>Quantization add support for <code>optimum-quanto</code> on-the-fly quantization during load for all models     note: previous method for quanto is still valid and is noted in settings as post-load quantization  </li> <li>Quantization add support to CogView-3Plus </li> <li>Default values rename vae, unet and text-encoder settings None to Default to avoid confusion  </li> <li>Detailer: add renoise option to increase/decrease noise during detailer pass     which can help with improving level of details</li> <li>CLI: add <code>cli/api-grid.py</code> which can generate grids using params-from-file for x/y axis  </li> <li>Samplers add ability to set sigma adjustment for each sampler  </li> <li>ModernUI updates  </li> <li>CSS updates  </li> <li>Video interpolate do not skip duplicate frames  </li> <li>Settings UI full refactor  </li> <li>Settings UI vertical/dirty indicator restores to default setting instead to previous value  </li> <li>update <code>diffusers</code> and other requirements  </li> <li>Wiki/Docs </li> <li>updated Models info  </li> <li>new Video guide  </li> <li>new Caption guide  </li> <li>new VAE guide  </li> <li>updated SD3 guide  </li> <li>updated ZLUDA guide  </li> <li>updated OpenVINO guide  </li> <li>updated AMD-ROCm guide  </li> <li>updated Intel-ARC guide  </li> <li>Fixes </li> <li>fix installer not starting when older version of <code>rich</code> is installed  </li> <li>fix circular imports when debug flags are enabled  </li> <li>fix cuda errors with directml </li> <li>fix memory stats not displaying the ram usage  </li> <li>fix RunPod memory limit reporting  </li> <li>fix flux ipadapter with start/stop values  </li> <li>fix progress api <code>eta_relative</code> </li> <li>fix <code>insightface</code> loader  </li> <li>fix remove vae for flux.1  </li> <li>guard against git returining invalid timestamp  </li> <li>fix hires with latent upscale  </li> <li>fix legacy diffusion latent upscalers  </li> <li>fix upscaler selection in postprocessing  </li> <li>fix sd35 with batch processing  </li> <li>fix extra networks cover and inline views  </li> <li>fix token counter error style with modernui  </li> <li>fix sampler metadata when using default sampler  </li> <li>fix paste incorrect float to int cast  </li> <li>fix server restart from ui  </li> <li>fix style apply params  </li> <li>fix <code>wan22-i2v</code> </li> <li>do not allow edit of built-in styles  </li> <li>improve lora compatibility with balanced offload  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-02-28","title":"Update for 2025-02-28","text":"<p>Primarily a hotfix/service release plus few UI improvements and one exciting new feature: Remote-VAE!</p> <ul> <li>Remote Decode </li> <li>final step of image generate, VAE decode, is by far the most memory intensive operation and can easily result in out-of-memory errors     what can be done? Well, Huggingface is now providing free-of-charge remote-VAE-decode service!  </li> <li>how to use? previous Full quality option in UI is replaced with VAE type selector: Full, Tiny, Remote     currently supports SD15, SDXL and FLUX.1 with more models expected in the near future     depending on your bandwidth select mode in settings -&gt; vae -&gt; raw/png/jpg     if remote processing fails SD.Next will fallback to using normal VAE decode process privacy note: only passed item is final latent itself without any user or generate information and latent is not stored in the cloud  </li> <li>UI</li> <li>modern ui reorg main tab     improve styling, improve scripts/extensions interface and separate ipadapters  </li> <li>additional ui hints  </li> <li>Other </li> <li>add <code>--extensions-dir</code> cli arg and <code>SD_EXTENSIONSDIR</code> env variable to specify extensions directory  </li> <li>update <code>zluda==3.9.0</code></li> <li>Fixes </li> <li>skip trying to register legacy/incompatibile extensions in control ui  </li> <li>add additional scripts/extensions callbacks  </li> <li>remove ui splash screen on auth fail  </li> <li>log full config path, full log path, system name, extensions path</li> <li>zluda hotfixes  </li> <li>zluda force sync  </li> <li>fix torch import on compile  </li> <li>infotext parser force delimiter before params  </li> <li>handle pipeline class switch errors  </li> <li>improve extensions options compatibility  </li> <li>fix flux on ipex  </li> <li>disable fp64 emulation on ipex  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-02-18","title":"Update for 2025-02-18","text":""},{"location":"CHANGELOG/#highlight-for-2025-02-18","title":"Highlight for 2025-02-18","text":"<p>We're back with another update with nearly 100 commits! - Starting with massive UI update with full localization for 8 languages   and 100+ new hints - Big update to Docker containers   with support for all major compute platforms - A lot of outpainting goodies - Support for new models: AlphaVLLM Lumina 2 and Ostris Flex.1-Alpha - And new Mixture-of-Diffusers regional prompting &amp; tiling pipeline - Follow-up to last weeks interrogate/captioning rewrite   now with redesigned captioning UI, batch support, and much more   plus JoyTag, JoyCaption, PaliGemma, ToriiGate, Ovis2 added to list of supported models - Some changes to prompt parsing to allow more control as well as   more flexibility when mouting SDNext server to custom URL - Of course, cumulative fixes...  </p> <p>...and more - see changelog for full details!  </p>"},{"location":"CHANGELOG/#details-for-2025-02-20","title":"Details for 2025-02-20","text":"<ul> <li>User Interface </li> <li>Hints <ul> <li>added/updated 100+ ui hints!  </li> <li>hints documentation and contribution guide  </li> </ul> </li> <li>Localization <ul> <li>full ui localization! english, croatian, spanish, french, italian, portuguese, chinese, japanese, korean, russian </li> <li>set in settings -&gt; user interface -&gt; language </li> <li>localization documentation  </li> </ul> </li> <li>UI <ul> <li>force browser cache-invalidate on page load  </li> <li>configurable request timeout  </li> <li>modernui improve gallery styling  </li> <li>modernui improve networks styling  </li> <li>modernui support variable card size  </li> </ul> </li> <li>Docs </li> <li>New Outpaint step-by-step guide  </li> <li>Updated Docker guide     includes build and publish and both local and cloud examples  </li> <li>Models </li> <li>AlphaVLLM Lumina 2     new foundation model for image generation based o Gemma-2-2B text encoder and a flow-based diffusion transformer     fully supports offloading and on-the-fly quantization     simply select from networks -&gt; models -&gt; reference </li> <li>Ostris Flex.1-Alpha     originally based on Flux.1-Schnell, but retrained and with different architecture     result is model smaller than Flux.1-Dev, but with similar capabilities     fully supports offloading and on-the-fly quantization     simply select from networks -&gt; models -&gt; reference </li> <li>Functions </li> <li>Mixture-of-Diffusers     Regional tiling type of a solution for SDXL models     select from scripts -&gt; mixture of diffusers </li> <li>[Automatic Color Inpaint]     Automatically creates mask based on selected color and triggers inpaint     simply select in scripts -&gt; automatic color inpaint when in img2img mode  </li> <li>RAS: Region-Adaptive Sampling experimental     Speeds up SD3.5 models by sampling only regions of interest     Enable in settings -&gt; pipeline modifiers -&gt; ras </li> <li>Interrogate/Captioning </li> <li>Redesigned captioning UI     split from Process tab into separate tab     split <code>clip</code> vs <code>vlm</code> models processing     direct send-to buttons on all tabs: txt/img/ctrl-&gt;process/caption, process/caption-&gt;txt/img/ctrl  </li> <li>Advanced params:     VLM: max-tokens, num-beams, temperature, top-k, top-p, do-sample     CLiP: min-length, max-length, chunk-size, min-flavors, max-flavors, flavor-count, num-beams     params are auto-saved in <code>config.json</code> and used when using quick interrogate     params that are set to 0 mean use model defaults  </li> <li>Batch processing: VLM and CLiP     for example, can be used to caption your training dataset in one go     add option to append to captions file, can be used to run multiple captioning models in sequence     add option to run recursively on all subfolders     add progress bar  </li> <li>Add additional VLM models: JoyTag JoyCaption 2 Google PaliGemma 2 3B ToriiGate 0.4 7B AIDC Ovis2 1B/2B/4B  </li> <li>Note some models require <code>flash-attn</code> to be installed     due to binary/build dependencies, it should not be done automatically,     see flash-attn for installation instructions  </li> <li>Docker </li> <li>updated CUDA receipe to <code>torch==2.6.0</code> with <code>cuda==12.6</code> and add prebuilt image  </li> <li>added ROCm receipe and prebuilt image  </li> <li>added IPEX receipe and add prebuilt image  </li> <li>added OpenVINO receipe and prebuilt image  </li> <li>System </li> <li>improve python==3.12 compatibility  </li> <li>Torch <ul> <li>for zluda set default to <code>torch==2.6.0+cu118</code> </li> <li>for openvino set default to <code>torch==2.6.0+cpu</code> </li> </ul> </li> <li>OpenVINO <ul> <li>update to <code>openvino==2025.0.0</code> </li> <li>improve upscaler compatibility  </li> <li>enable upscaler compile by default  </li> <li>fix shape mismatch errors on too many resolution changes  </li> </ul> </li> <li>ZLUDA <ul> <li>update to <code>zluda==3.8.8</code> </li> </ul> </li> <li>Other </li> <li>Asymmetric tiling     allows for configurable image tiling for x/y axis separately     enable in scripts -&gt; asymmetric tiling note: traditional symmetric tiling is achieved by setting circular mode for both x and y  </li> <li>Styles     ability to save and/or restore prompts before or after parsing of wildcards     set in settings -&gt; networks -&gt; styles </li> <li>Access tokens     persist models -&gt; hugginface -&gt; token     persist models -&gt; civitai -&gt; token </li> <li>global switch to lancosz method for all interal resize ops and bicubic for interpolation ops  </li> <li>Text encoder     add advanced per-model options for text encoder     set in settings -&gt; text encoder -&gt; Optional </li> <li>Subpath     allow setting additional mount subpath over which server url will be accessible     set in settings -&gt; user interface </li> <li>Prompt parsing     better handling of prompt parsing when using masking char <code>\\</code> </li> <li>Fixes </li> <li>update torch nightly urls  </li> <li>docs/wiki always use relative links  </li> <li>ui use correct timezone for log display  </li> <li>ui improve settings search behavior  </li> <li>ui log scroll to bottom  </li> <li>ui fix send to inpaint/sketch  </li> <li>modernui add control init image toggle  </li> <li>modernui fix sampler advanced options  </li> <li>outpaint fixes  </li> <li>validate output before hires/refine  </li> <li>scheduler fix sigma index out of bounds  </li> <li>force pydantic version reinstall/reload  </li> <li>multi-unit when using controlnet-union  </li> <li>pulid with hidiffusion  </li> <li>api: stricter access control  </li> <li>api: universal handle mount subpaths  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-02-05","title":"Update for 2025-02-05","text":"<ul> <li>refresh dev/master branches</li> </ul>"},{"location":"CHANGELOG/#update-for-2025-02-04","title":"Update for 2025-02-04","text":""},{"location":"CHANGELOG/#highlights-for-2025-02-04","title":"Highlights for 2025-02-04","text":"<p>Just one week after latest release and what a week it was with over 50 commits!  </p> <p>What's New? - Rehosted core repo to new home - Switched to using <code>torch==2.6.0</code> and added support for <code>nightly</code> builds required for nVidia Blackwell GPUs - Completely new interrogate/captioning, now supporting 150+ OpenCLiP models and 20+ built-in VLMs - Support for new VLMs, New SOTA background removal - Other: torch tunable ops, extra networks search/filter, balanced offload, prompt parser, configurable tracebacks, etc. - Cumulative fixes...  </p>"},{"location":"CHANGELOG/#details-for-2025-02-04","title":"Details for 2025-02-04","text":"<ul> <li>GitHub</li> <li>rename core repo from https://github.com/vladmandic/automatic to https://github.com/vladmandic/sdnext     old repo url should automatically redirect to new one for seamless transition and in-place upgrades     all internal links have been updated     wiki content and docs site have been updated  </li> <li>Docs:</li> <li>Updated Debugging guide </li> <li>Torch:</li> <li>for cuda set default to <code>torch==2.6.0+cu126</code>     for rocm set default to <code>torch==2.6.0+rocm6.2.4</code>     for ipex set default to <code>torch==2.6.0+xpu</code> note: to avoid disruptions sdnext does not perform torch install during in-place upgrades     to force torch upgrade, either start with new installation or use <code>--reinstall</code> flag  </li> <li>support for torch nightly builds and nvidia blackwell gpus!     use <code>--use-nightly</code> flag to install torch nightly builds     current defaults to <code>torch==2.7.0+cu128</code> prerelease note: nightly builds are required for blackwell gpus  </li> <li>add support for torch tunable ops, this can speed up operations by up to 10-30% on some platforms     set in settings -&gt; backend settings -&gt; torch options and settings -&gt; system paths -&gt; tunable ops cache </li> <li>add support for stream-loading, this can speed up model loading when models are located on network drives     set in settings -&gt; models &amp; loading -&gt; model load using streams </li> <li>enhanced error logging  </li> <li>Interrogate/Captioning </li> <li>single interrogate button for every input or output image  </li> <li>behavior of interrogate configurable in settings -&gt; interrogate     with detailed defaults for each model type also configurable  </li> <li>select between 150+ OpenCLiP supported models, 20+ built-in VLMs, DeepDanbooru </li> <li>VLM: now that we can use VLMs freely, we've also added support for few more out-of-the-box Alibaba Qwen VL2, Huggingface Smol VL2, ToriiGate 0.4 </li> <li>Postprocess </li> <li>new sota remove background model: BEN2     select in process -&gt; remove background or enable postprocessing for txt2img/img2img operations  </li> <li>Other:</li> <li>networks: imporove search/filter and add visual indicators for types  </li> <li>balanced offload new defaults: lowvram/4gb min threshold: 0, medvram/8gb min threshold: 0, default min threshold 0.25 </li> <li>prompt parser: log stats with tokens, sections and min/avg/max weights  </li> <li>prompt parser: add setting to ignore line breaks in prompt     set in settings -&gt; text encoder -&gt; use line breaks </li> <li>visual query: add list of predefined system prompts  </li> <li>onnx: allow manually specifying <code>onnxruntime</code> package     set env variable <code>ONNXRUNTIME_COMMAND</code> to override default package installation  </li> <li>nvml cli: run nvidia-management-lib interrogate from cli     already available in ui in generate -&gt; right click -&gt; nvidia     &gt; python modules/api/nvml.py  </li> <li>Refactor:</li> <li>unified trace handler with configurable tracebacks  </li> <li>refactor interrogate/analyze/vqa code  </li> <li>Fixes:  </li> <li>photomaker with offloading  </li> <li>photomaker with refine  </li> <li>detailer with faceid modules  </li> <li>detailer restore pipeline before run  </li> <li>fix <code>python==3.9</code> compatibility  </li> <li>improve <code>python&gt;=3.12.3</code> compatibility</li> <li>handle invalid <code>triton</code> on Linux  </li> <li>correct library import order  </li> <li>update requirements  </li> <li>calculate dyn atten bmm slice rate  </li> <li>dwpose update and patch <code>mmengine</code> installer  </li> <li>ipex device wrapper with adetailer  </li> <li>openvino error handling  </li> <li>relax python version checks for rocm  </li> <li>simplify and improve file wildcard matching  </li> <li>fix <code>rich</code> version  </li> <li>add cn active label</li> </ul>"},{"location":"CHANGELOG/#update-for-2025-01-29","title":"Update for 2025-01-29","text":""},{"location":"CHANGELOG/#highlights-for-2025-01-29","title":"Highlights for 2025-01-29","text":"<p>Two weeks since last release, time for update!  </p> <p>What's New? - New Detailer functionality including ability to use several new   face-restore models: RestoreFormer, CodeFormer, GFPGan, GPEN-BFR - Support for new models/pipelines:   face-swapper with Photomaker-v2 and video with Fast-Hunyuan - Support for several new optimizations and accelerations:   Many IPEX improvements, native torch fp8 support,   support for PAB:Pyramid-attention-broadcast, ParaAttention and PerFlow - Fully built-in both model merge weights as well as model merge component   Finally replace that pesky VAE in your favorite model with a fixed one! - Improved remote access control and reliability as well as running inside containers - And of course, hotfixes for all reported issues...  </p>"},{"location":"CHANGELOG/#details-for-2025-01-29","title":"Details for 2025-01-29","text":"<ul> <li>Contributing:  </li> <li>if you'd like to contribute, please see updated contributing guidelines</li> <li>Model Merge</li> <li>replace model components and merge LoRAs     in addition to existing model weights merge support     now also having ability to replace model components and merge LoRAs     you can also test merges in-memory without needing to save to disk at all     and you can also use it to convert diffusers to safetensors if you want example: replace vae in your favorite model with a fixed one? replace text encoder? etc. note: limited to sdxl for now, additional models can be added depending on popularity  </li> <li>Detailer:  </li> <li>in addition as standard behavior of detect &amp; run-generate, it can now also run face-restore models  </li> <li>included models are: CodeFormer, RestoreFormer, GFPGan, GPEN-BFR </li> <li>Face:  </li> <li>new PhotoMaker v2 and reimplemented PhotoMaker v1     compatible with sdxl models, generates pretty good results and its faster than most other methods     select under scripts -&gt; face -&gt; photomaker </li> <li>new ReSwapper     todo: experimental-only and unfinished, only noting in changelog for future reference  </li> <li>Video </li> <li>hunyuan video support for FastHunyuan     simply select model variant and set appropriate parameters     recommended: sampler-shift=17, steps=6, resolution=720x1280, frames=125, guidance&gt;6.0  </li> <li>PAB: Pyramid Attention Broadcast </li> <li>speed up generation by caching attention results between steps  </li> <li>enable in settings -&gt; pipeline modifiers -&gt; pab </li> <li>adjust settings as needed: wider timestep range means more acceleration, but higher accuracy drop  </li> <li>compatible with most <code>transformer</code> based models: e.g. flux.1, hunyuan-video, lyx-video, mochi, etc.</li> <li>ParaAttention</li> <li>first-block caching that can significantly speed up generation by dynamically reusing partial outputs between steps  </li> <li>available for: flux, hunyuan-video, ltx-video, mochi  </li> <li>enable in settings -&gt; pipeline modifiers -&gt; para-attention </li> <li>adjust residual diff threshold to balance the speedup and the accuracy:     higher values leads to more cache hits and speedups, but might also lead to a higher accuracy drop  </li> <li>IPEX</li> <li>enable force attention slicing, fp64 emulation, jit cache  </li> <li>use the us server by default on linux  </li> <li>use pytorch test branch on windows  </li> <li>extend the supported python versions  </li> <li>improve sdpa dynamic attention  </li> <li>Torch FP8</li> <li>uses torch <code>float8_e4m3fn</code> or <code>float8_e5m2</code> as data storage and performs dynamic upcasting to compute <code>dtype</code> as needed  </li> <li>compatible with most <code>unet</code> and <code>transformer</code> based models: e.g. sd15, sdxl, sd35, flux.1, hunyuan-video, ltx-video, etc.     this is alternative to <code>bnb</code>/<code>quanto</code>/<code>torchao</code> quantization on models/platforms/gpus where those libraries are not available  </li> <li>enable in settings -&gt; quantization -&gt; layerwise casting </li> <li>PerFlow </li> <li>piecewise rectified flow as model acceleration  </li> <li>use <code>perflow</code> scheduler combined with one of the available pre-trained models </li> <li>Other:  </li> <li>upscale: new asymmetric vae upscaling method</li> <li>gallery: add http fallback for slow/unreliable links  </li> <li>splash: add legacy mode indicator on splash screen  </li> <li>network: extract thumbnail from model metadata if present  </li> <li>network: setting value to disable use of reference models  </li> <li>Refactor:  </li> <li>upscale: code refactor to unify latent, resize and model based upscalers  </li> <li>loader: ability to run in-memory models  </li> <li>schedulers: ability to create model-less schedulers  </li> <li>quantization: code refactor into dedicated module  </li> <li>dynamic attention sdpa: more correct implementation and new trigger rate control  </li> <li>Remote access:  </li> <li>perform auth check on ui startup  </li> <li>unified standard and modern-ui authentication method &amp; cleanup auth logging  </li> <li>detect &amp; report local/external/public ip addresses if using <code>listen</code> mode  </li> <li>detect docker enforced limits instead of system limits if running in a container  </li> <li>warn if using public interface without authentication  </li> <li>Fixes:  </li> <li>non-full vae decode  </li> <li>send-to image transfer  </li> <li>sana vae tiling  </li> <li>increase gallery timeouts  </li> <li>update ui element ids  </li> <li>modernui use local font  </li> <li>unique font family registration  </li> <li>mochi video number of frames  </li> <li>mark large models that should offload  </li> <li>avoid repeated optimum-quanto installation  </li> <li>avoid reinstalling bnb if not cuda  </li> <li>image metadata civitai compatibility  </li> <li>xyz grid handle invalid values  </li> <li>omnigen pipeline handle float seeds  </li> <li>correct logging of docker status on logs, thanks @kmscode  </li> <li>fix omnigen  </li> <li>fix docker status reporting  </li> <li>vlm/vqa with moondream2  </li> <li>rocm do not override triton installation  </li> <li>port streaming model load to diffusers  </li> </ul>"},{"location":"CHANGELOG/#update-for-2025-01-15","title":"Update for 2025-01-15","text":""},{"location":"CHANGELOG/#highlights-for-2025-01-15","title":"Highlights for 2025-01-15","text":"<p>Two weeks since last release, time for update! This time a bit shorter highligh reel as this is primarily a service release, but still there is more than few updates (actually, there are ~60 commits, so its not that tiny) </p> <p>What's New?\" - Large Wiki/Docs updates - New models: Allegro Video, new pipelines: PixelSmith, updates: Hunyuan-Video, LTX-Video, Sana 4k - New version for ZLUDA - New features in Detailer, XYZ grid, Sysinfo, Logging, Schedulers, Video save/create* - And a tons of hotfixes...  </p>"},{"location":"CHANGELOG/#details-for-2025-01-15","title":"Details for 2025-01-15","text":"<ul> <li>Wiki/Docs:</li> <li>updated: Detailer, Install, Update, Debug, Control-HowTo, ZLUDA  </li> <li>Allegro Video </li> <li>optimizations: full offload and quantization support  </li> <li>reference values: width 1280 height 720 frames 88 steps 100 guidance 7.5  </li> <li>note: allegro model is really sensitive to input width/height/frames/steps     and may result in completely corrupt output if those are not within expected range  </li> <li>PixelSmith</li> <li>available for SD-XL in txt2img and img2img workflows</li> <li>select from scripts -&gt; pixelsmith </li> <li>Hunyuan Video LoRA support</li> <li>example: https://huggingface.co/Cseti/HunyuanVideo-LoRA-Arcane_Jinx-v1</li> <li>LTX Video framewise decoding  </li> <li>enabled by default, allows generating longer videos with reduced memory requirements  </li> <li>Sana 4k </li> <li>new Sana variation with support of directly generating 4k images  </li> <li>simply select from networks -&gt; models -&gt; reference </li> <li>tip: enable vae tiling when generating very large images  </li> <li>Logging:</li> <li>reverted enable debug by default  </li> <li>updated debug wiki </li> <li>sort logged timers by duration  </li> <li>allow min duration env variable for timers: <code>SD_MIN_TIMER=0.1</code> (default)  </li> <li>update installer messages  </li> <li>Refactor:</li> <li>refactored progress monitoring, job updates and live preview  </li> <li>improved metadata save and restore  </li> <li>startup tracing and optimizations  </li> <li>threading load locks on model loads  </li> <li>refactor native vs legacy model loader  </li> <li>video save/create</li> <li>Schedulers:</li> <li>TDD new super-fast scheduler that can generate images in 4-8 steps     recommended to use with TDD LoRA </li> <li>Detailer:</li> <li>add explicit detailer prompt and negative prompt  </li> <li>add explicit detailer steps setting  </li> <li>move steps, strength, prompt, negative from settings into ui params  </li> <li>set/restore detailer metadata  </li> <li>new detailer wiki</li> <li>Preview</li> <li>since different TAESD versions produce different results and latest is not necessarily greatest     you can choose TAESD version in settings -&gt; live preview     also added is support for another finetuned version of TAESD Hybrid TinyVAE </li> <li>Video </li> <li>all video create/save code is now unified  </li> <li>add support for video formats: GIF, PNG, MP4/MP4V, MP4/AVC1, MP4/JVT3, MKV/H264, AVI/DIVX, AVI/RGBA, MJPEG/MJPG, MPG/MPG1, AVR/AVR1</li> <li>note: video format support is platform dependent and not all formats may be available on all platforms</li> <li>note: avc1 and h264 need custom opencv due to oss licensing issues  </li> <li>ZLUDA v3.8.7  </li> <li>new runtime compiler implementation: complex types, JIT are now available  </li> <li>fast fourier transformation is implemented  </li> <li>experimental BLASLt support via nightly build  <ul> <li>set <code>ZLUDA_NIGHTLY=1</code> to install nightly ZLUDA: newer torch such as 2.4.x (default) and 2.5.x are now available  </li> <li>requirements: unofficial hipBLASLt  </li> </ul> </li> <li>Other</li> <li>XYZ Grid: add prompt search&amp;replace options: primary, refine, detailer, all</li> <li>SysInfo: update to collected data and benchmarks  </li> <li>Fixes:</li> <li>explict clear caches on model load  </li> <li>lock adetailer commit: <code>#a89c01d</code> </li> <li>xyzgrid progress calculation  </li> <li>xyzgrid detailer</li> <li>vae tiling use default value if not set  </li> <li>sd35 img2img</li> <li>samplers test for scale noise before using  </li> <li>scheduler api  </li> <li>sampler create error handling  </li> <li>controlnet with hires  </li> <li>controlnet with batch count  </li> <li>apply settings skip hidden settings  </li> <li>lora diffusers method apply only once  </li> <li>lora diffusers method set prompt tags and metadata  </li> <li>flux support on-the-fly quantization for bnb of unet only  </li> <li>control restore pipeline before running hires  </li> <li>restore args after batch run  </li> <li>flux controlnet  </li> <li>zluda installer  </li> <li>control inherit parent pipe settings  </li> <li>control logging  </li> <li>hf cache folder settings  </li> <li>fluxfill should not require base model</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-12-31","title":"Update for 2024-12-31","text":"<p>NYE refresh release with quite a few optimizatios and bug fixes... Commit hash: <code>master: #dcfc9f3</code> <code>dev: #935cac6</code> </p> <ul> <li>LoRA:  </li> <li>LoRA load/apply/unapply methods have been changed in 12/2024 Xmass release and further tuned in this release</li> <li>for details on available methods, see https://github.com/vladmandic/automatic/wiki/Lora#lora-loader </li> <li>Sana support  </li> <li>quantized models support  </li> <li>add fuse support with on-demand apply/unapply (new default)  </li> <li>add legacy option in settings -&gt; networks </li> <li>HunyuanVideo:  </li> <li>optimizations: full offload, quantization and tiling support  </li> <li>LTXVideo:  </li> <li>optimizations: full offload, quantization and tiling support  </li> <li>TeaCache integration  </li> <li>VAE:  </li> <li>tiling granular options in settings -&gt; Variational Auto Encoder </li> <li>UI:  </li> <li>live preview optimizations and error handling  </li> <li>live preview high quality output, thanks @Disty0  </li> <li>CSS optimizations when log view is disabled  </li> <li>Samplers:  </li> <li>add flow shift options and separate dynamic thresholding from dynamic shifting  </li> <li>autodetect matching sigma capabilities  </li> <li>API:  </li> <li>better default values for generate  </li> <li>Refactor:  </li> <li>remove all LDM imports if running in native mode  </li> <li>startup optimizatios  </li> <li>Torch:  </li> <li>support for <code>torch==2.6.0</code> </li> <li>OpenVINO:  </li> <li>disable re-compile on resolution change  </li> <li>fix shape mismatch on resolution change  </li> <li>Fixes:  </li> <li>flux pipeline switches: txt/img/inpaint  </li> <li>flux custom unet loader for bnb  </li> <li>flux do not requantize already quantized model</li> <li>interrogate caption with T5  </li> <li>on-the-fly quantization using TorchAO  </li> <li>remove concurrent preview requests  </li> <li>xyz grid recover on error  </li> <li>hires batch  </li> <li>sdxl refiner  </li> <li>increase progress timeout</li> <li>kandinsky matmul  </li> <li>do not show disabled networks  </li> <li>enable debug logging by default</li> <li>image width/height calculation when doing img2img  </li> <li>corrections with batch processing  </li> <li>hires with refiner prompt and batch processing  </li> <li>processing with nested calls  </li> <li>ui networks initial sort  </li> <li>esrgan on cpu devices  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-12-24","title":"Update for 2024-12-24","text":""},{"location":"CHANGELOG/#highlights-for-2024-12-24","title":"Highlights for 2024-12-24","text":""},{"location":"CHANGELOG/#sdnext-xmass-edition-whats-new","title":"SD.Next Xmass edition: What's new?","text":"<p>While we have several new supported models, workflows and tools, this release is primarily about quality-of-life improvements: - New memory management engine   list of changes that went into this one is long: changes to GPU offloading, brand new LoRA loader, system memory management, on-the-fly quantization, improved gguf loader, etc.   but main goal is enabling modern large models to run on standard consumer GPUs   without performance hits typically associated with aggressive memory swapping and needs for constant manual tweaks - New documentation website   with full search and tons of new documentation - New settings panel with simplified and streamlined configuration  </p> <p>We've also added support for several new models such as highly anticipated NVLabs Sana (see supported models for full list) And several new SOTA video models: Lightricks LTX-Video, Hunyuan Video and Genmo Mochi.1 Preview </p> <p>And a lot of Control and IPAdapter goodies - for SDXL there is new ProMax, improved Union and Tiling models - for FLUX.1 there are Flux Tools as well as official Canny and Depth models,   a cool Redux model as well as XLabs IP-adapter - for SD3.5 there are official Canny, Blur and Depth models in addition to existing 3rd party models   as well as InstantX IP-adapter  </p> <p>Plus couple of new integrated workflows such as FreeScale and Style Aligned Image Generation </p> <p>And it wouldn't be a Xmass edition without couple of custom themes: Snowflake and Elf-Green! All-in-all, we're around ~180 commits worth of updates, check the changelog for full list  </p> <p>ReadMe | ChangeLog | Docs | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2024-12-24","title":"Details for 2024-12-24","text":""},{"location":"CHANGELOG/#new-models-and-integrations","title":"New models and integrations","text":"<ul> <li>NVLabs Sana   support for 1.6B 2048px, 1.6B 1024px and 0.6B 512px models Sana can synthesize high-resolution images with strong text-image alignment by using Gemma2 as text-encoder   and its fast - typically at least 2x faster than sd-xl even for 1.6B variant and maintains performance regardless of resolution   e.g., rendering at 4k is possible in less than 8GB vram   to use, select from networks -&gt; models -&gt; reference and models will be auto-downloaded on first use reference values: sampler: default (or any flow-match variant), steps: 20, width/height: 1024, guidance scale: 4.5 note like other LLM-based text-encoders, sana prefers long and descriptive prompts   any short prompt below 300 characters will be auto-expanded using built in Gemma LLM before encoding while long prompts will be passed as-is  </li> <li>ControlNet</li> <li>improved support for Union controlnets with granular control mode type</li> <li>added support for latest Xinsir ProMax all-in-one controlnet  </li> <li>added support for multiple Tiling controlnets, for example Xinsir Tile note: when selecting tiles in control settings, you can also specify non-square ratios     in which case it will use context-aware image resize to maintain overall composition note: available tiling options can be set in settings -&gt; control  </li> <li>IP-Adapter </li> <li>FLUX.1 XLabs v1 and v2 IP-adapter  </li> <li>FLUX.1 secondary guidance, enabled using Attention guidance in advanced menu  </li> <li>SD 3.5 InstantX IP-adapter  </li> <li>Flux Tools Redux is actually a tool, Fill is inpaint/outpaint optimized version of Flux-dev Canny &amp; Depth are optimized versions of Flux-dev for their respective tasks: they are not ControlNets that work on top of a model   to use, go to image or control interface and select Flux Tools in scripts   all models are auto-downloaded on first use note: All models are gated and require acceptance of terms and conditions via web page recommended: Enable on-the-fly quantization or compression to reduce resource usage todo: support for Canny/Depth LoRAs  </li> <li>Redux: ~0.1GB     works together with existing model and basically uses input image to analyze it and use that instead of prompt optional can use prompt to combine guidance with input image recommended: low denoise strength levels result in more variety  </li> <li>Fill: ~23.8GB, replaces currently loaded model note: can be used in inpaint/outpaint mode only  </li> <li>Canny: ~23.8GB, replaces currently loaded model recommended: guidance scale 30  </li> <li>Depth: ~23.8GB, replaces currently loaded model recommended: guidance scale 10  </li> <li>Flux ControlNet LoRA   alternative to standard ControlNets, FLUX.1 also allows LoRA to help guide the generation process   both Depth and Canny LoRAs are available in standard control menus  </li> <li>StabilityAI SD35 ControlNets</li> <li>In addition to previously released <code>InstantX</code> and <code>Alimama</code>, we now have official ones from StabilityAI  </li> <li>Style Aligned Image Generation   enable in scripts, compatible with sd-xl   enter multiple prompts in prompt field separated by new line   style-aligned applies selected attention layers uniformly to all images to achive consistency   can be used with or without input image in which case first prompt is used to establish baseline note: all prompts are processes as a single batch, so vram is limiting factor  </li> <li>FreeScale   enable in scripts, compatible with sd-xl for text and img2img   run iterative generation of images at different scales to achieve better results   can render 4k sdxl images note: disable live preview to avoid memory issues when generating large images  </li> </ul>"},{"location":"CHANGELOG/#video-models","title":"Video models","text":"<ul> <li>Lightricks LTX-Video   model size: 27.75gb   support for 0.9.0, 0.9.1 and custom safetensor-based models with full quantization and offloading support   support for text-to-video and image-to-video, to use, select in scripts -&gt; ltx-video refrence values: steps 50, width 704, height 512, frames 161, guidance scale 3.0  </li> <li>Hunyuan Video   model size: 40.92gb   support for text-to-video, to use, select in scripts -&gt; hunyuan video   basic support only refrence values: steps 50, width 1280, height 720, frames 129, guidance scale 6.0  </li> <li>Genmo Mochi.1 Preview   support for text-to-video, to use, select in scripts -&gt; mochi.1 video   basic support only refrence values: steps 64, width 848, height 480, frames 19, guidance scale 4.5  </li> </ul> <p>Notes: - all video models are very large and resource intensive!   any use on gpus below 16gb and systems below 48gb ram is experimental at best - sdnext support for video models is relatively basic with further optimizations pending community interest   any future optimizations would likely have to go into partial loading and excecution instead of offloading inactive parts of the model - new video models use generic llms for prompting and due to that requires very long and descriptive prompt - you may need to enable sequential offload for maximum gpu memory savings - optionally enable pre-quantization using bnb for additional memory savings - reduce number of frames and/or resolution to reduce memory usage  </p>"},{"location":"CHANGELOG/#ui-and-workflow-improvements","title":"UI and workflow improvements","text":"<ul> <li>Docs:</li> <li>New documentation site! https://vladmandic.github.io/sdnext-docs/</li> <li>Additional Wiki content: Styles, Wildcards, etc.</li> <li>LoRA handler rewrite:  </li> <li>LoRA weights are no longer calculated on-the-fly during model execution, but are pre-calculated at the start     this results in perceived overhead on generate startup, but results in overall faster execution as LoRA does not need to be processed on each step     thanks @AI-Casanova  </li> <li>LoRA weights can be applied/unapplied as on each generate or they can store weights backups for later use     this setting has large performance and resource implications, see Offload wiki for details  </li> <li>LoRA name in prompt can now also be an absolute path to a LoRA file, even if LoRA is not indexed     example: <code>&lt;lora:/test/folder/my-lora.safetensors:1.0&gt;</code></li> <li>LoRA name in prompt can now also be path to a LoRA file op <code>huggingface</code>     example: <code>&lt;lora:/huggingface.co/vendor/repo/my-lora.safetensors:1.0&gt;</code></li> <li>Model loader improvements:  </li> <li>detect model components on model load fail  </li> <li>allow passing absolute path to model loader  </li> <li>Flux, SD35: force unload model  </li> <li>Flux: apply <code>bnb</code> quant when loading unet/transformer </li> <li>Flux: all-in-one safetensors     example: https://civitai.com/models/646328?modelVersionId=1040235 </li> <li>Flux: do not recast quants  </li> <li>Memory improvements:  </li> <li>faster and more compatible balanced offload mode  </li> <li>balanced offload: units are now in percentage instead of bytes  </li> <li>balanced offload: add both high and low watermark, defaults as below <code>0.25</code> for low-watermark: skip offload if memory usage is below 25% <code>0.70</code> high-watermark: must offload if memory usage is above 70%  </li> <li>balanced offload will attempt to run offload as non-blocking and force gc at the end  </li> <li>change-in-behavior:     low-end systems, triggered by either <code>lowvrwam</code> or by detection of &lt;=4GB will use sequential offload     all other systems use balanced offload by default (can be changed in settings)     previous behavior was to use model offload on systems with &lt;=8GB and <code>medvram</code> and no offload by default  </li> <li>VAE upcase is now disabled by default on all systems     if you have issues with image decode, you'll need to enable it manually  </li> <li>UI:  </li> <li>improved stats on generate completion  </li> <li>improved live preview display and performance  </li> <li>improved accordion behavior  </li> <li>auto-size networks height for sidebar  </li> <li>control: hide preview column by default</li> <li>control: optionn to hide input column</li> <li>control: add stats</li> <li>settings: reorganized and simplified  </li> <li>browser -&gt; server logging framework  </li> <li>add addtional themes: <code>black-reimagined</code>, thanks @Artheriax  </li> <li>Batch</li> <li>image batch processing will use caption files if they exist instead of default prompt  </li> </ul>"},{"location":"CHANGELOG/#updates","title":"Updates","text":"<ul> <li>Quantization</li> <li>Add <code>TorchAO</code> pre (during load) and post (during execution) quantization torchao supports 4 different int-based and 3 float-based quantization schemes   This is in addition to existing support for:  </li> <li><code>BitsAndBytes</code> with 3 float-based quantization schemes  </li> <li><code>Optimium.Quanto</code> with 3 int-based and 2 float-based quantizations schemes  </li> <li><code>GGUF</code> with pre-quantized weights  </li> <li>Switch <code>GGUF</code> loader from custom to diffuser native</li> <li>IPEX: update to IPEX 2.5.10+xpu  </li> <li>OpenVINO:  </li> <li>update to 2024.6.0  </li> <li>disable model caching by default  </li> <li>Sampler improvements  </li> <li>UniPC, DEIS, SA, DPM-Multistep: allow FlowMatch sigma method and prediction type  </li> <li>Euler FlowMatch: add sigma methods (karras/exponential/betas)  </li> <li>Euler FlowMatch: allow using timestep presets to set sigmas  </li> <li>DPM FlowMatch: update all and add sigma methods  </li> <li>BDIA-DDIM: experimental new scheduler  </li> <li>UFOGen: experimental new scheduler  </li> </ul>"},{"location":"CHANGELOG/#fixes","title":"Fixes","text":"<ul> <li>add <code>SD_NO_CACHE=true</code> env variable to disable file/folder caching  </li> <li>add settings -&gt; networks -&gt; embeddings -&gt; enable/disable</li> <li>update <code>diffusers</code> </li> <li>fix README links  </li> <li>fix sdxl controlnet single-file loader  </li> <li>relax settings validator  </li> <li>improve js progress calls resiliency  </li> <li>fix text-to-video pipeline  </li> <li>avoid live-preview if vae-decode is running  </li> <li>allow xyz-grid with multi-axis s&amp;r  </li> <li>fix xyz-grid with lora  </li> <li>fix api script callbacks  </li> <li>fix gpu memory monitoring  </li> <li>simplify img2img/inpaint/sketch canvas handling  </li> <li>fix prompt caching  </li> <li>fix xyz grid skip final pass  </li> <li>fix sd upscale script  </li> <li>fix cogvideox-i2v  </li> <li>lora auto-apply tags remove duplicates  </li> <li>control load model on-demand if not already loaded  </li> <li>taesd limit render to 2024px  </li> <li>taesd downscale preview to 1024px max: configurable in settings -&gt; live preview  </li> <li>uninstall conflicting <code>wandb</code> package  </li> <li>dont skip diffusers version check if quick is specified  </li> <li>notify on torch install  </li> <li>detect pipeline fro diffusers folder-style model  </li> <li>do not recast flux quants  </li> <li>fix xyz-grid with lora none  </li> <li>fix svd image2video  </li> <li>fix gallery display during generate  </li> <li>fix wildcards replacement to be unique  </li> <li>fix animatediff-xl  </li> <li>fix pag with batch count  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-11-21","title":"Update for 2024-11-21","text":""},{"location":"CHANGELOG/#highlights-for-2024-11-21","title":"Highlights for 2024-11-21","text":"<p>Three weeks is a long time in Generative AI world - and we're back with ~140 commits worth of updates!</p> <p>What's New?</p> <p>First, a massive update to docs including new UI top-level info tab with access to changelog and wiki, many updates and new articles AND full built-in documentation search capabilities</p>"},{"location":"CHANGELOG/#new-integrations","title":"New integrations","text":"<ul> <li>PuLID: Pure and Lightning ID Customization via Contrastive Alignment</li> <li>InstantX InstantIR: Blind Image Restoration with Instant Generative Reference</li> <li>nVidia Labs ConsiStory: Consistent Image Generation</li> <li>MiaoshouAI PromptGen v2.0 VQA captioning</li> </ul>"},{"location":"CHANGELOG/#workflow-improvements","title":"Workflow Improvements","text":"<ul> <li>Native Docker support</li> <li>SD3x &amp; Flux.1: more ControlNets, all-in-one-safetensors, DPM samplers, skip-layer-guidance, etc.</li> <li>XYZ grid: benchmarking, video creation, etc.</li> <li>Enhanced prompt parsing</li> <li>UI improvements</li> <li>Installer self-healing <code>venv</code></li> </ul> <p>And quite a few more improvements and fixes since the last update! For full list and details see changelog...</p> <p>README | CHANGELOG | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2024-11-21","title":"Details for 2024-11-21","text":"<ul> <li>Docs:  </li> <li>new top-level info tab with access to changelog and wiki </li> <li>UI built-in changelog search     since changelog is the best up-to-date source of info     go to info -&gt; changelog and search/highligh/navigate directly in UI!  </li> <li>UI built-in wiki     go to info -&gt; wiki and search wiki pages directly in UI!  </li> <li>major Wiki and Home updates  </li> <li>updated API swagger docs for at <code>/docs</code> </li> <li>Integrations:  </li> <li>PuLID: Pure and Lightning ID Customization via Contrastive Alignment  <ul> <li>advanced method of face id transfer with better quality as well as control over identity and appearance   try it out, likely the best quality available for sdxl models  </li> <li>select in scripts -&gt; pulid </li> <li>compatible with sdxl for text-to-image, image-to-image, inpaint, refine, detailer workflows  </li> <li>can be used in xyz grid  </li> <li>note: this module contains several advanced features on top of original implementation  </li> </ul> </li> <li>InstantIR: Blind Image Restoration with Instant Generative Reference  <ul> <li>alternative to traditional <code>img2img</code> with more control over restoration process  </li> <li>select in image -&gt; scripts -&gt; instantir </li> <li>compatible with sdxl </li> <li>note: after used once it cannot be unloaded without reloading base model  </li> </ul> </li> <li>ConsiStory: Consistent Image Generation  <ul> <li>create consistent anchor image and then generate images that are consistent with anchor  </li> <li>select in scripts -&gt; consistory </li> <li>compatible with sdxl </li> <li>note: very resource intensive and not compatible with model offloading  </li> <li>note: changing default parameters can lead to unexpected results and/or failures  </li> <li>note: after used once it cannot be unloaded without reloading base model  </li> </ul> </li> <li> <p>MiaoshouAI PromptGen v2.0 base and large:  </p> <ul> <li>in process -&gt; visual query </li> <li>caption modes: <code>&lt;GENERATE_TAGS&gt;</code> generate tags <code>&lt;CAPTION&gt;</code>, <code>&lt;DETAILED_CAPTION&gt;</code>, <code>&lt;MORE_DETAILED_CAPTION&gt;</code> caption image <code>&lt;ANALYZE&gt;</code> image composition <code>&lt;MIXED_CAPTION&gt;</code>, <code>&lt;MIXED_CAPTION_PLUS&gt;</code> detailed caption and tags with optional analyze  </li> </ul> </li> <li> <p>Model improvements:  </p> </li> <li>SD35: ControlNets:  <ul> <li>InstantX Canny, Pose, Depth, Tile </li> <li>Alimama Inpainting, SoftEdge </li> <li>note: that just like with FLUX.1 or any large model, ControlNet are also large and can push your system over the limit   e.g. SD3 controlnets vary from 1GB to over 4GB in size  </li> </ul> </li> <li>SD35: All-in-one safetensors  <ul> <li>examples: large, medium </li> <li>note: enable bnb on-the-fly quantization for even bigger gains  </li> </ul> </li> <li>SD35: skip-layer-guidance <ul> <li>enable in scripts -&gt; slg</li> <li>allows for granular strength/start/stop control of guidance for each layer of the model  </li> </ul> </li> <li> <p>NoobAI XL ControlNets, thanks @lbeltrame</p> </li> <li> <p>Workflow improvements:  </p> </li> <li>Native Docker support with pre-defined Dockerfile</li> <li>Samplers:<ul> <li>FlowMatch samplers:</li> <li>Applicable to SD 3.x and Flux.1 models</li> <li>Complete family: DPM2, DPM2a, DPM2++, DPM2++ 2M, DPM2++ 2S, DPM2++ SDE, DPM2++ 2M SDE, DPM2++ 3M SDE</li> <li>Beta and Exponential sigma method enabled for all samplers</li> </ul> </li> <li>XYZ grid:  <ul> <li>optional time benchmark info to individual images  </li> <li>optional add params to individual images  </li> <li>create video from generated grid images   supports all standard video types and interpolation  </li> </ul> </li> <li>Prompt parser:  <ul> <li>support for prompt scheduling  </li> <li>renamed parser options: <code>native</code>, <code>xhinker</code>, <code>compel</code>, <code>a1111</code>, <code>fixed</code> </li> <li>parser options are available in xyz grid  </li> <li>improved caching  </li> </ul> </li> <li>UI:  <ul> <li>better gallery and networks sidebar sizing  </li> <li>add additional hotkeys </li> <li>add show networks on startup setting  </li> <li>better mapping of networks previews  </li> <li>optimize networks display load  </li> </ul> </li> <li>Image2image:  <ul> <li>integrated refine/upscale/hires workflow  </li> </ul> </li> <li>Other:  </li> <li>Installer:  <ul> <li>Log <code>venv</code> and package search paths  </li> <li>Auto-remove invalid packages from <code>venv/site-packages</code>   e.g. packages starting with <code>~</code> which are left-over due to windows access violation  </li> <li>Requirements: update  </li> </ul> </li> <li>Scripts:  <ul> <li>More verbose descriptions for all scripts  </li> </ul> </li> <li>Model loader:  <ul> <li>Report modules included in safetensors when attempting to load a model  </li> </ul> </li> <li>CLI:  <ul> <li>refactor command line params   run <code>webui.sh</code>/<code>webui.bat</code> with <code>--help</code> to see all options  </li> <li>added <code>cli/model-metadata.py</code> to display metadata in any safetensors file  </li> <li>added <code>cli/model-keys.py</code> to quicky display content of any safetensors file  </li> </ul> </li> <li> <p>Internal:  </p> <ul> <li>Auto pipeline switching coveres wrapper classes and nested pipelines  </li> <li>Full settings validation on load of <code>config.json</code> </li> <li>Refactor of all params in main processing classes  </li> <li>Improve API scripts usage resiliency  </li> </ul> </li> <li> <p>Fixes:  </p> </li> <li>custom watermark add alphablending  </li> <li>fix xyz grid include images  </li> <li>fix xyz skip on interrupted  </li> <li>fix vqa models ignoring hfcache folder setting  </li> <li>fix network height in standard vs modern ui  </li> <li>fix k-diff enum on startup  </li> <li>fix text2video scripts  </li> <li>multiple xyz-grid fixes  </li> <li>dont uninstall flash-attn  </li> <li>ui css fixes  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-11-01","title":"Update for 2024-11-01","text":"<p>Smaller release just 3 days after the last one, but with some important fixes and improvements. This release can be considered an LTS release before we kick off the next round of major updates.  </p> <ul> <li>Other:</li> <li>Repo: move screenshots to GH pages</li> <li>Update requirements</li> <li>Fixes:</li> <li>detailer min/max size as fractions of image size  </li> <li>ipadapter load on-demand  </li> <li>ipadapter face use correct yolo model  </li> <li>list diffusers remove duplicates  </li> <li>fix legacy extensions access to shared objects  </li> <li>fix diffusers load from folder  </li> <li>fix lora enum logging on windows  </li> <li>fix xyz grid with batch count  </li> <li>move dowwloads of some auxillary models to hfcache instead of models folder  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-10-29","title":"Update for 2024-10-29","text":""},{"location":"CHANGELOG/#highlights-for-2024-10-29","title":"Highlights for 2024-10-29","text":"<ul> <li>Support for all SD3.x variants SD3.0-Medium, SD3.5-Medium, SD3.5-Large, SD3.0-Large-Turbo</li> <li>Allow quantization using <code>bitsandbytes</code> on-the-fly during models load   Load any variant of SD3.x or FLUX.1 and apply quantization during load without the need for pre-quantized models  </li> <li>Allow for custom model URL in standard model selector   Can be used to specify any model from HuggingFace or CivitAI </li> <li>Full support for <code>torch==2.5.1</code></li> <li>New wiki articles: Gated Access, Quantization, Offloading </li> </ul> <p>Plus tons of smaller improvements and cumulative fixes reported since last release  </p> <p>README | CHANGELOG | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2024-10-29","title":"Details for 2024-10-29","text":"<ul> <li>model selector:</li> <li>change-in-behavior</li> <li>when typing, it will auto-load model as soon as exactly one match is found</li> <li>allows entering model that are not on the list which triggers huggingface search     e.g. <code>stabilityai/stable-diffusion-xl-base-1.0</code>     partial search hits are displayed in the log     if exact model is found, it will be auto-downloaded and loaded  </li> <li>allows entering civitai direct download link which triggers model download     e.g. <code>https://civitai.com/api/download/models/72396?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16</code> </li> <li>auto-search-and-download can be disabled in settings -&gt; models -&gt; auto-download     this also disables reference models as they are auto-downloaded on first use as well  </li> <li>sd3 enhancements:  </li> <li>allow on-the-fly bnb quantization during load</li> <li>report when loading incomplete model  </li> <li>handle missing model components during load  </li> <li>handle component preloading  </li> <li>native lora handler  </li> <li>support for all sd35 variants: medium/large/large-turbo</li> <li>gguf transformer loader (prototype)  </li> <li>flux.1 enhancements:  </li> <li>allow on-the-fly bnb quantization during load</li> <li>samplers:</li> <li>support for original k-diffusion samplers     select via scripts -&gt; k-diffusion -&gt; sampler </li> <li>ipadapter:</li> <li>list available adapters based on loaded model type</li> <li>add adapter <code>ostris consistency</code> for sd15/sdxl</li> <li>detailer:</li> <li>add <code>[prompt]</code> to refine/defailer prompts as placeholder referencing original prompt  </li> <li>torch</li> <li>use <code>torch==2.5.1</code> by default on supported platforms</li> <li>CUDA set device memory limit     in settings -&gt; compute settings -&gt; torch memory limit     default=0 meaning no limit, if set torch will limit memory usage to specified fraction note: this is not a hard limit, torch will try to stay under this value  </li> <li>compute backends:</li> <li>OpenVINO: add accuracy option  </li> <li>ZLUDA: guess GPU arch  </li> <li>major model load refactor</li> <li>wiki: new articles</li> <li>Gated Access Wiki </li> <li>Quantization Wiki </li> <li>Offloading Wiki </li> </ul> <p>fixes: - fix send-to-control - fix k-diffusion - fix sd3 img2img and hires - fix ipadapter supported model detection - fix t2iadapter auto-download - fix omnigen dynamic attention - handle a1111 prompt scheduling - handle omnigen image placeholder in prompt  </p>"},{"location":"CHANGELOG/#update-for-2024-10-23","title":"Update for 2024-10-23","text":""},{"location":"CHANGELOG/#highlights-for-2024-10-23","title":"Highlights for 2024-10-23","text":"<p>A month later and with nearly 300 commits, here is the latest SD.Next update!  </p>"},{"location":"CHANGELOG/#workflow-highlights-for-2024-10-23","title":"Workflow highlights for 2024-10-23","text":"<ul> <li>Reprocess: New workflow options that allow you to generate at lower quality and then   reprocess at higher quality for select images only or generate without hires/refine and then reprocess with hires/refine   and you can pick any previous latent from auto-captured history!  </li> <li>Detailer Fully built-in detailer workflow with support for all standard models  </li> <li>Built-in model analyzer   See all details of your currently loaded model, including components, parameter count, layer count, etc.  </li> <li>Extract LoRA: load any LoRA(s) and play with generate as usual   and once you like the results simply extract combined LoRA for future use!  </li> </ul>"},{"location":"CHANGELOG/#new-models-for-2024-10-23","title":"New models for 2024-10-23","text":"<ul> <li>New fine-tuned CLiP-ViT-L 1st stage text-encoders used by most models (SD15/SDXL/SD3/Flux/etc.) brings additional details to your images  </li> <li>New models: Stable Diffusion 3.5 Large OmniGen CogView 3 Plus Meissonic </li> <li>Additional integration: Ctrl+X which allows for control of structure and appearance without the need for extra models, APG: Adaptive Projected Guidance for optimal guidance control, LinFusion for on-the-fly distillation of any sd15/sdxl model  </li> </ul>"},{"location":"CHANGELOG/#what-else-for-2024-10-23","title":"What else for 2024-10-23","text":"<ul> <li>Tons of work on dynamic quantization that can be applied on-the-fly during model load to any model type (you do not need to use pre-quantized models)   Supported quantization engines include <code>BitsAndBytes</code>, <code>TorchAO</code>, <code>Optimum.quanto</code>, <code>NNCF</code> compression, and more...  </li> <li>Auto-detection of best available device/dtype settings for your platform and GPU reduces neeed for manual configuration Note: This is a breaking change to default settings and its recommended to check your preferred settings after upgrade  </li> <li>Full rewrite of sampler options, not far more streamlined with tons of new options to tweak scheduler behavior  </li> <li>Improved LoRA detection and handling for all supported models  </li> <li>Several of Flux.1 optimizations and new quantization types  </li> </ul> <p>Oh, and we've compiled a full table with list of top-30 (how many have you tried?) popular text-to-image generative models, their respective parameters and architecture overview: Models Overview </p> <p>And there are also other goodies like multiple XYZ grid improvements, additional Flux ControlNets, additional Interrogate models, better LoRA tags support, and more... README | CHANGELOG | WiKi | Discord</p>"},{"location":"CHANGELOG/#details-for-2024-10-23","title":"Details for 2024-10-23","text":"<ul> <li>reprocess</li> <li>new top-level button: reprocess latent from your history of generated image(s)  </li> <li>generate using full-quality:off and then reprocess using full quality decode </li> <li>generate without hires/refine and then reprocess with hires/refine note: you can change hires/refine settings and run-reprocess again!  </li> <li> <p>reprocess using detailer </p> </li> <li> <p>history</p> </li> <li>by default, reprocess will pick last latent, but you can select any latent from history!  </li> <li>history is under networks -&gt; history     each history item includes info on operations that were used, timestamp and metadata  </li> <li>any latent operation during workflow automatically adds one or more items to history     e.g. generate base + upscale + hires + detailer  </li> <li>history size: settings -&gt; execution -&gt; latent history size     memory usage is ~130kb of ram for 1mp image  </li> <li> <p>note list of latents in history is not auto-refreshed, use refresh button  </p> </li> <li> <p>model analyzer </p> </li> <li>see all details of your currently loaded model, including components, parameter count, layer count, etc.  </li> <li> <p>in models -&gt; current -&gt; analyze  </p> </li> <li> <p>text encoder:  </p> </li> <li>allow loading different custom text encoders: clip-vit-l, clip-vit-g, t5     will automatically find appropriate encoder in the loaded model and replace it with loaded text encoder     download text encoders into folder set in settings -&gt; system paths -&gt; text encoders     default <code>models/Text-encoder</code> folder is used if no custom path is set     finetuned clip-vit-l models: Detailed, Smooth, LongCLIP     reference clip-vit-l and clip-vit-g models: OpenCLIP-Laion2b note sd/sdxl contain heavily distilled versions of reference models, so switching to reference model produces vastly different results  </li> <li>xyz grid support for text encoder  </li> <li> <p>full prompt parser now correctly works with different prompts in batch  </p> </li> <li> <p>detailer:  </p> </li> <li>replaced face-hires with detailer which can run any number of standard detailing models  </li> <li>includes face/hand/person/eyes predefined detailer models plus support for manually downloaded models     set path in settings -&gt; system paths -&gt; yolo </li> <li>select one or more models in detailer menu and thats it!  </li> <li>to avoid duplication of ui elements, detailer will use following values from refiner: sampler, steps, prompts </li> <li>when using multiple detailers and prompt is multi-line, each line is applied to corresponding detailer  </li> <li>adjustable settings: strength, max detected objects, edge padding, edge blur, min detection confidence, max detection overlap, min and max size of detected object </li> <li>image metadata includes info on used detailer models  </li> <li>note detailer defaults are not save in ui settings, they are saved in server settings     to apply your defaults, set ui values and apply via system -&gt; settings -&gt; apply settings </li> <li> <p>if using models trained on multiple classes, you can specify which classes you want to detail     e.g. original yolo detection model is trained on coco dataset with 80 predefined classes     if you leave field blank, it will use any class found in the model     you can see classes defined in the model while model itself is loaded for the first time  </p> </li> <li> <p>extract lora: extract combined lora from current memory state, thanks @AI-Casanova   load any LoRA(s) and play with generate as usual and once you like the results simply extract combined LoRA for future use!   in models -&gt; extract lora </p> </li> <li> <p>sampler options: full rewrite  </p> </li> </ul> <p>sampler notes:   - pick a sampler and then pick values, all values have \"default\" as a choice to make it simpler   - a lot of options are new, some are old but moved around     e.g. karras checkbox is replaced with a choice of different sigma methods   - not every combination of settings is valid   - some settings are specific to model types     e.g. sd15/sdxl typically use epsilon prediction   - quite a few well-known schedulers are just variations of settings, for example:     - sampler sgm is sampler with trailing spacing and sample prediction type     - dpm 2m or 3m are dpm 1s with orders of 2 or 3     - dpm 2m sde is dpm 2m with sde as solver     - sampler simple is sampler with trailing spacing and linear beta schedule   - xyz grid support for sampler options   - metadata updates for sampler options   - modernui updates for sampler options   - note sampler options defaults are not saved in ui settings, they are saved in server settings     to apply your defaults, set ui values and apply via system -&gt; settings -&gt; apply settings </p> <p>sampler options:   - sigma method: karas, beta, exponential   - timesteps spacing: linspace, leading, trailing   - beta schedule: linear, scaled, cosine   - prediction type: epsilon, sample, v-prediction   - timesteps presents: none, ays-sd15, ays-sdxl   - timesteps override:    - sampler order: 0=default, 1-5   - options: dynamic, low order, rescale <ul> <li>Ctrl+X:</li> <li>control structure (similar to controlnet) and appearance (similar to ipadapter)     without the need for extra models, all via code feed-forwards!</li> <li>can run in structure-only or appearance-only or both modes</li> <li>when providing structure and appearance input images, its best to provide a short prompts describing them  </li> <li>structure image can be almost anything: actual photo, openpose-style stick man, 3d render, sketch, depth-map, etc.     just describe what it is in a structure prompt so it can be de-structured and correctly applied  </li> <li> <p>supports sdxl in both txt2img and img2img, simply select from scripts</p> </li> <li> <p>APG: Adaptive Projected Guidance</p> </li> <li>latest algo to provide better guidance for image generation, can be used instead of existing guidance rescale and/or PAG  </li> <li>in addtion to stronger guidance and reduction of burn at high guidance values, it can also increase image details  </li> <li>compatible with sd15/sdxl/sc </li> <li>select in scripts -&gt; apg  </li> <li>for low    cfg scale, use positive momentum: e.g. cfg=2 =&gt; momentum=0.6</li> <li>for normal cfg scale, use negative momentum: e.g. cfg=6 =&gt; momentum=-0.3</li> <li> <p>for high   cfg scale, use neutral  momentum: e.g. cfg=10 =&gt; momentum=0.0</p> </li> <li> <p>LinFusion </p> </li> <li>apply liner distillation to during load to any sd15/sdxl model  </li> <li>can reduce vram use for high resolutions and increase performance</li> <li> <p>note: use lower cfg scales as typical for distilled models  </p> </li> <li> <p>Flux </p> </li> <li>see wiki for details on <code>gguf</code> </li> <li>support for <code>gguf</code> binary format for loading unet/transformer component  </li> <li>support for <code>gguf</code> binary format for loading t5/text-encoder component: requires transformers pr  </li> <li>additional controlnets: JasperAI Depth, Upscaler, Surface, thanks @EnragedAntelope  </li> <li>additional controlnets: XLabs-AI Canny, Depth, HED </li> <li>mark specific unet as unavailable if load failed  </li> <li>fix diffusers local model name parsing  </li> <li>full prompt parser will auto-select <code>xhinker</code> for flux models  </li> <li>controlnet support for img2img and inpaint (in addition to previous txt2img controlnet)  </li> <li>allow separate vae load  </li> <li>support for both kohya and onetrainer loras in native load mode for fp16/nf4/fp4, thanks @AI-Casanova  </li> <li>support for differential diffusion  </li> <li>added native load mode for qint8/qint4 models</li> <li> <p>avoid unet load if unchanged  </p> </li> <li> <p>OmniGen </p> </li> <li>Radical new model with pure LLM architecture based on Phi-3  </li> <li>Select from networks -&gt; models -&gt; reference </li> <li>Can be used for text-to-image and image-to-image  </li> <li>Image-to-image is very different, you need to specify in prompt what do you want to do     and add <code>|image|</code> placeholder where input image is used!     examples: <code>in |image| remove glasses from face</code>, <code>using depth map from |image|, create new image of a cute robot</code> </li> <li> <p>Params used: prompt, steps, guidance scale for prompt guidance, refine guidance scale for image guidance     Recommended: guidance=3.0, refine-guidance=1.6  </p> </li> <li> <p>Stable Diffusion 3.5 Large </p> </li> <li>New/improved variant of Stable Diffusion 3  </li> <li>Select from networks -&gt; models -&gt; reference </li> <li>Available in standard and turbo variations  </li> <li> <p>Note: Access to to both variations of SD3.5 model is gated, you must accept the conditions and use HF login  </p> </li> <li> <p>CogView 3 Plus</p> </li> <li>Select from networks -&gt; models -&gt; reference </li> <li>resolution width and height can be from 512px to 2048px and must be divisible by 32  </li> <li> <p>precision: bf16 or fp32     fp16 is not supported due to internal model overflows  </p> </li> <li> <p>Meissonic </p> </li> <li>Select from networks -&gt; models -&gt; reference </li> <li>Experimental as upstream implemenation code is unstable</li> <li> <p>Must set scheduler:default, generator:unset</p> </li> <li> <p>SageAttention </p> </li> <li>new 8-bit attention implementation on top of SDP that can provide acceleration for some models, thanks @Disty0  </li> <li>enable in settings -&gt; compute settings -&gt; sdp options -&gt; sage attention</li> <li>compatible with DiT-based models: e.g. Flux.1, AuraFlow, CogVideoX </li> <li> <p>not compatible with UNet-based models, e.g. SD15, SDXL </p> </li> <li> <p>gpu</p> </li> <li>previously <code>cuda_dtype</code> in settings defaulted to <code>fp16</code> if available  </li> <li>now <code>cuda_type</code> defaults to Auto which executes <code>bf16</code> and <code>fp16</code> tests on startup and selects best available dtype     if you have specific requirements, you can still set to fp32/fp16/bf16 as desired     if you have gpu that incorrectly identifies bf16 or fp16 availablity, let us know so we can improve the auto-detection  </li> <li> <p>support for torch expandable segments     enable in settings -&gt; compute -&gt; torch expandable segments     can provide significant memory savings for some models     not enabled by default as its only supported on latest versions of torch and some gpus  </p> </li> <li> <p>xyz grid full refactor  </p> </li> <li>multi-mode: selectable-script and alwayson-script </li> <li>allow usage combined with other scripts  </li> <li>allow unet selection  </li> <li>allow passing model args directly:     allowed params will be checked against models call signature     example: <code>width=768; height=512, width=512; height=768</code> </li> <li>allow passing processing args directly:     params are set directly on main processing object and can be known or new params     example: <code>steps=10, steps=20; test=unknown</code> </li> <li>enable working with different resolutions     now you can adjust width/height in the grid just as any other param  </li> <li>renamed options to include section name and adjusted cost of each option  </li> <li> <p>added additional metadata  </p> </li> <li> <p>interrogate </p> </li> <li>add additional blip models: blip-base, blip-large, blip-t5-xl, blip-t5-xxl, opt-2.7b, opt-6.7b </li> <li>change default params for better memory utilization  </li> <li>lock commits for miaoshouAI-promptgen  </li> <li>add optional advanced params  </li> <li> <p>update logging  </p> </li> <li> <p>lora auto-apply tags to prompt  </p> </li> <li>controlled via settings -&gt; networks -&gt; lora_apply_tags 0:disable, -1:all-tags, n:top-n-tags </li> <li>uses tags from both model embedded data and civitai downloaded data  </li> <li>if lora contains no tags, lora name itself will be used as a tag  </li> <li>if prompt contains <code>_tags_</code> it will be used as placeholder for replacement, otherwise tags will be appended  </li> <li>used tags are also logged and registered in image metadata  </li> <li>loras are no longer filtered per detected type vs loaded model type as its unreliable  </li> <li>loras display in networks now shows possible version in top-left corner  </li> <li>correct using of <code>extra_networks_default_multiplier</code> if not scale is specified  </li> <li>improve lora base model detection  </li> <li>improve lora error handling and logging  </li> <li> <p>setting <code>lora_load_gpu</code> to load LoRA directly to GPU default: true unless lovwram  </p> </li> <li> <p>quantization </p> </li> <li>new top level settings group as we have quite a few quantization options now!     configure in settings -&gt; quantization </li> <li>in addition to existing <code>optimum.quanto</code> and <code>nncf</code>, we now have <code>bitsandbytes</code> and <code>torchao</code> </li> <li>bitsandbytes: fp8, fp4, nf4  <ul> <li>quantization can be applied on-the-fly during model load  </li> <li>currently supports <code>transformers</code> and <code>t5</code> in sd3 and flux </li> </ul> </li> <li> <p>torchao: int8, int4, fp8, fp4, fpx  </p> <ul> <li>configure in settings -&gt; quantization  </li> <li>can be applied to any model on-the-fly during load  </li> </ul> </li> <li> <p>huggingface:  </p> </li> <li>force logout/login on token change  </li> <li> <p>unified handling of cache folder: set via <code>HF_HUB</code> or <code>HF_HUB_CACHE</code> or via settings -&gt; system paths  </p> </li> <li> <p>cogvideox:  </p> </li> <li>add support for image2video (in addition to previous text2video and video2video)  </li> <li> <p>note: image2video requires separate 5b model variant  </p> </li> <li> <p>torch </p> </li> <li> <p>due to numerous issues with torch 2.5.0 which was just released as stable, we are sticking with 2.4.1 for now  </p> </li> <li> <p>backend=original is now marked as in maintenance-only mode  </p> </li> <li>python 3.12 improved compatibility, automatically handle <code>setuptools</code> </li> <li>control</li> <li>persist/reapply units current state on server restart  </li> <li>better handle size before/after metadata  </li> <li>video add option <code>gradio_skip_video</code> to avoid gradio issues with displaying generated videos  </li> <li>add support for manually downloaded diffusers models from huggingface  </li> <li>ui </li> <li>move checkboxes <code>full quality, tiling, hidiffusion</code> to advanced section  </li> <li>hide token counter until tokens are known  </li> <li>minor ui optimizations  </li> <li>fix update infotext on image select  </li> <li>fix imageviewer exif parser  </li> <li>selectable info view in image viewer, thanks @ZeldaMaster501  </li> <li>setting to enable browser autolaunch, thanks @brknsoul  </li> <li>free-u check if device/dtype are fft compatible and cast as necessary  </li> <li>rocm</li> <li>additional gpu detection and auto-config code, thanks @lshqqytiger  </li> <li>experimental triton backend for flash attention, thanks @lshqqytiger  </li> <li>update to rocm 6.2, thanks @Disty0</li> <li>directml </li> <li>update <code>torch</code> to 2.4.1, thanks @lshqqytiger  </li> <li>extensions </li> <li>add mechanism to lock-down extension to specific working commit  </li> <li>added <code>sd-webui-controlnet</code> and <code>adetailer</code> last-known working commits  </li> <li>upscaling </li> <li>interruptible operations</li> <li>refactor </li> <li>general lora apply/unapply process  </li> <li>modularize main process loop  </li> <li>massive log cleanup  </li> <li>full lint pass  </li> <li>improve inference mode handling  </li> <li>unify quant lib loading  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-09-13","title":"Update for 2024-09-13","text":""},{"location":"CHANGELOG/#highlights-for-2024-09-13","title":"Highlights for 2024-09-13","text":"<p>Major refactor of FLUX.1 support: - Full ControlNet support, better LoRA support, full prompt attention implementation - Faster execution, more flexible loading, additional quantization options, and more... - Added image-to-image, inpaint, outpaint, hires modes - Added workflow where FLUX can be used as refiner for other models - Since both Optimum-Quanto and BitsAndBytes libraries are limited in their platform support matrix,   try enabling NNCF for quantization/compression on-the-fly!  </p> <p>Few image related goodies... - Context-aware resize that allows for img2img/inpaint even at massively different aspect ratios without distortions! - LUT Color grading apply professional color grading to your images using industry-standard .cube LUTs! - Auto HDR image create for SD and SDXL with both 16ch true-HDR and 8-ch HDR-effect images ;)  </p> <p>And few video related goodies... - CogVideoX 2b and 5b variants   with support for text-to-video and video-to-video! - AnimateDiff prompt travel and long context windows!   create video which travels between different prompts and at long video lengths!  </p> <p>Plus tons of other items and fixes - see changelog for details! Examples: - Built-in prompt-enhancer, TAESD optimizations, new DC-Solver scheduler, global XYZ grid management, etc. - Updates to ZLUDA, IPEX, OpenVINO...</p>"},{"location":"CHANGELOG/#details-for-2024-09-13","title":"Details for 2024-09-13","text":"<p>Major refactor of FLUX.1 support: - allow configuration of individual FLUX.1 model components: transformer, text-encoder, vae   model load will load selected components first and then initialize model using pre-loaded components   components that were not pre-loaded will be downloaded and initialized as needed   as usual, components can also be loaded after initial model load note: use of transformer/unet is recommended as those are flux.1 finetunes note: manually selecting vae and text-encoder is not recommended note: mix-and-match of different quantizations for different components can lead to unexpected errors   - transformer/unet is list of manually downloaded safetensors   - vae is list of manually downloaded safetensors   - text-encoder is list of predefined and manually downloaded text-encoders - controlnet support:   support for InstantX/Shakker-Labs models including Union-Pro   note that flux controlnet models are large, up to 6.6GB on top of already large base model!   as such, you may need to use offloading:sequential which is not as fast, but uses far less memory   when using union model, you must also select control mode in the control unit   flux does not yet support img2img so to use controlnet, you need to set contronet input via control unit override - model support loading all-in-one safetensors   not recommended due to massive duplication of components, but added due to popular demand   each such model is 20-32GB in size vs ~11GB for typical unet fine-tune - improve logging, warn when attempting to load unet as base model - refiner support   FLUX.1 can be used as refiner for other models such as sd/sdxl   simply load sd/sdxl model as base and flux model as refiner and use as usual refiner workflow - img2img, inpaint and outpaint support note flux may require higher denoising strength than typical sd/sdxl models note: img2img is not yet supported with controlnet - transformer/unet support fp8/fp4 quantization   this brings supported quants to: nf4/fp8/fp4/qint8/qint4 - vae support fp16 - lora support additional training tools - face-hires support - support fuse-qkv projections   can speed up generate   enable via settings -&gt; compute -&gt; fused projections </p> <p>Other improvements &amp; Fixes: - CogVideoX   - support for both 2B and 5B variations   - support for both text2video and video2video modes   - simply select in scripts -&gt; cogvideox   - as with any video modules, includes additional frame interpolation using RIFE   - if init video is used, it will be automatically resized and interpolated to desired number of frames - AnimateDiff:   - prompt travel      create video which travels between different prompts at different steps!      example prompt:       &gt; 0: dog       &gt; 5: cat       &gt; 10: bird   - support for v3 model (finally)   - support for LCM model   - support for free-noise rolling context window     allow for creation of much longer videos, automatically enabled if frames &gt; 16 - Context-aware image resize, thanks @AI-Casanova!   based on seam-carving   allows for img2img/inpaint even at massively different aspect ratios without distortions!   simply select as resize method when using img2img or control tabs - HDR high-dynamic-range image create for SD and SDXL   create hdr images from in multiple exposures by latent-space modifications during generation   use via scripts -&gt; hdr   option save hdr images creates images in standard 8bit/channel (hdr-effect) and 16bit/channel (full-hdr) PNG format   ui result is always 8bit/channel hdr-effect image plus grid of original images used to create hdr   grid image can be disabled via settings -&gt; user interface -&gt; show grid   actual full-hdr image is not displayed in ui, only optionally saved to disk - new scheduler: DC Solver - color grading apply professional color grading to your images   using industry-standard .cube LUTs!   enable via scripts -&gt; color-grading - hires workflow now allows for full resize options   not just limited width/height/scale - xyz grid is now availabe as both local and global script! - prompt enhance: improve quality and/or verbosity of your prompts   simply select in scripts -&gt; prompt enhance   uses gokaygokay/Flux-Prompt-Enhance model - decode   - auto-set upcast if first decode fails   - restore dtype on upcast - taesd configurable number of layers   can be used to speed-up taesd decoding by reducing number of ops   e.g. if generating 1024px image, reducing layers by 1 will result in preview being 512px   set via settings -&gt; live preview -&gt; taesd decode layers - xhinker prompt parser handle offloaded models - control better handle offloading - upscale will use resize-to if set to non-zero values over resize-by   applies to any upscale options, including refine workflow - networks add option to choose if mouse-over on network should attempt to fetch additional info   option:<code>extra_networks_fetch</code> enable/disable in settings -&gt; networks - speed up some garbage collection ops - sampler settings add dynamic shift   used by flow-matching samplers to adjust between structure and details - sampler settings force base shift   improves quality of the flow-matching samplers - t5 support manually downloaded models   applies to all models that use t5 transformer - modern-ui add override field - full lint updates - use <code>diffusers</code> from main branch, no longer tied to release - improve diffusers/transformers/huggingface_hub progress reporting - use unique identifiers for all ui components - visual query (a.ka vqa or vlm) added support for several models   - MiaoshouAI PromptGen 1.5 Base   - MiaoshouAI PromptGen 1.5 Large   - CogFlorence 2.2 Large - modernui update - zluda update to 3.8.4, thanks @lshqqytiger! - ipex update to 2.3.110+xpu on linux, thanks @Disty0! - openvino update to 2024.3.0, thanks @Disty0! - update <code>requirements</code> - fix AuraFlow - fix handling of model configs if offline config is not available - fix vae decode in backend original - fix model path typos - fix guidance end handler - fix script sorting - fix vae dtype during load - fix all ui labels are unique</p>"},{"location":"CHANGELOG/#update-for-2024-08-31","title":"Update for 2024-08-31","text":""},{"location":"CHANGELOG/#highlights-for-2024-08-31","title":"Highlights for 2024-08-31","text":"<p>Summer break is over and we are back with a massive update!  </p> <p>Support for all of the new models: - Black Forest Labs FLUX.1 - AuraFlow 0.3 - AlphaVLLM Lumina-Next-SFT - Kwai Kolors - HunyuanDiT 1.2 </p> <p>What else? Just a bit... ;)  </p> <p>New fast-install mode, new Optimum Quanto and BitsAndBytes based quantization modes, new balanced offload mode that dynamically offloads GPU&lt;-&gt;CPU as needed, and more... And from previous service-pack: new ControlNet-Union all-in-one model, support for DoRA networks, additional VLM models, new AuraSR upscaler  </p> <p>Breaking Changes...</p> <p>Due to internal changes, you'll need to reset your attention and offload settings! But...For a good reason, new balanced offload is magic when it comes to memory utilization while sacrificing minimal performance!</p>"},{"location":"CHANGELOG/#details-for-2024-08-31","title":"Details for 2024-08-31","text":"<p>New Models...</p> <p>To use and of the new models, simply select model from Networks -&gt; Reference and it will be auto-downloaded on first use  </p> <ul> <li>Black Forest Labs FLUX.1   FLUX.1 models are based on a hybrid architecture of multimodal and parallel diffusion transformer blocks, scaled to 12B parameters and builing on flow matching   This is a very large model at ~32GB in size, its recommended to use a) offloading, b) quantization   For more information on variations, requirements, options, and how to donwload and use FLUX.1, see Wiki   SD.Next supports:  </li> <li>FLUX.1 Dev and FLUX.1 Schnell original variations  </li> <li>additional qint8 and qint4 quantized variations  </li> <li>additional nf4 quantized variation  </li> <li>AuraFlow   AuraFlow v0.3 is the fully open-sourced largest flow-based text-to-image generation model   This is a very large model at 6.8B params and nearly 31GB in size, smaller variants are expected in the future   Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  </li> <li>AlphaVLLM Lumina-Next-SFT   Lumina-Next-SFT is a Next-DiT model containing 2B parameters, enhanced through high-quality supervised fine-tuning (SFT)   This model uses T5 XXL variation of text encoder (previous version of Lumina used Gemma 2B as text encoder)   Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  </li> <li>Kwai Kolors   Kolors is a large-scale text-to-image generation model based on latent diffusion   This is an SDXL style model that replaces standard CLiP-L and CLiP-G text encoders with a massive <code>chatglm3-6b</code> encoder supporting both English and Chinese prompting  </li> <li>HunyuanDiT 1.2   Hunyuan-DiT is a powerful multi-resolution diffusion transformer (DiT) with fine-grained Chinese understanding  </li> <li>AnimateDiff   support for additional models: SD 1.5 v3 (Sparse), SD Lightning (4-step), SDXL Beta </li> </ul> <p>New Features...</p> <ul> <li>support for Balanced Offload, thanks @Disty0!   balanced offload will dynamically split and offload models from the GPU based on the max configured GPU and CPU memory size   model parts that dont fit in the GPU will be dynamically sliced and offloaded to the CPU   see Settings -&gt; Diffusers Settings -&gt; Max GPU memory and Max CPU memory note: recommended value for max GPU memory is ~80% of your total GPU memory note: balanced offload will force loading LoRA with Diffusers method note: balanced offload is not compatible with Optimum Quanto  </li> <li>support for Optimum Quanto with 8 bit and 4 bit quantization options, thanks @Disty0 and @Trojaner!   to use, go to Settings -&gt; Compute Settings and enable \"Quantize Model weights with Optimum Quanto\" option note: Optimum Quanto requires PyTorch 2.4  </li> <li>new prompt attention mode: xhinker which brings support for prompt attention to new models such as FLUX.1 and SD3   to use, enable in Settings -&gt; Execution -&gt; Prompt attention</li> <li>use PEFT for LoRA handling on all models other than SD15/SD21/SDXL   this improves LoRA compatibility for SC, SD3, AuraFlow, Flux, etc.  </li> </ul> <p>Changes &amp; Fixes...</p> <ul> <li>default resolution bumped from 512x512 to 1024x1024, time to move on ;)</li> <li>convert Dynamic Attention SDP into a global SDP option, thanks @Disty0! note: requires reset of selected attention option</li> <li>update default CUDA version from 12.1 to 12.4</li> <li>update <code>requirements</code></li> <li>samplers now prefers the model defaults over the diffusers defaults, thanks @Disty0!  </li> <li>improve xyz grid for lora handling and add lora strength option  </li> <li>don't enable Dynamic Attention by default on platforms that support Flash Attention, thanks @Disty0!  </li> <li>convert offload options into a single choice list, thanks @Disty0! note: requires reset of selected offload option  </li> <li>control module allows reszing of indivudual process override images to match input image   for example: set size-&gt;before-&gt;method:nearest, mode:fixed or mode:fill  </li> <li>control tab includes superset of txt and img scripts</li> <li>automatically offload disabled controlnet units  </li> <li>prioritize specified backend if <code>--use-*</code> option is used, thanks @lshqqytiger</li> <li>ipadapter option to auto-crop input images to faces to improve efficiency of face-transfter ipadapters  </li> <li>update IPEX to 2.1.40+xpu on Linux, thanks @Disty0!  </li> <li>general ROCm fixes, thanks @lshqqytiger!  </li> <li>support for HIP SDK 6.1 on ZLUDA backend, thanks @lshqqytiger!</li> <li>fix full vae previews, thanks @Disty0!  </li> <li>fix default scheduler not being applied, thanks @Disty0!  </li> <li>fix Stable Cascade with custom schedulers, thanks @Disty0!  </li> <li>fix LoRA apply with force-diffusers</li> <li>fix LoRA scales with force-diffusers</li> <li>fix control API</li> <li>fix VAE load refrerencing incorrect configuration</li> <li>fix NVML gpu monitoring</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-07-08","title":"Update for 2024-07-08","text":"<p>This release is primary service release with cumulative fixes and several improvements, but no breaking changes.</p> <p>New features... - massive updates to Wiki   with over 20 new pages and articles, now includes guides for nearly all major features note: this is work-in-progress, if you have any feedback or suggestions, please let us know!   thanks @GenesisArtemis! - support for DoRA networks, thanks @AI-Casanova! - support for uv, extremely fast installer, thanks @Yoinky3000!   to use, simply add <code>--uv</code> to your command line params - Xinsir ControlNet++ Union   new SDXL all-in-one controlnet that can process any kind of preprocessors! - CogFlorence 2 Large VLM model   to use, simply select in process -&gt; visual query - AuraSR high-quality 4x GAN-style upscaling model   note: this is a large upscaler at 2.5GB  </p> <p>And fixes... - enable Florence VLM  for all platforms, thanks @lshqqytiger! - improve ROCm detection under WSL2, thanks @lshqqytiger! - add SD3 with FP16 T5 to list of detected models - fix executing extensions with zero params - add support for embeddings bundled in LoRA, thanks @AI-Casanova! - fix executing extensions with zero params - fix nncf for lora, thanks @Disty0! - fix diffusers version detection for SD3 - fix current step for higher order samplers - fix control input type video - fix reset pipeline at the end of each iteration - fix faceswap when no faces detected - fix civitai search - multiple ModernUI fixes</p>"},{"location":"CHANGELOG/#update-for-2024-06-23","title":"Update for 2024-06-23","text":""},{"location":"CHANGELOG/#highlights-for-2024-06-23","title":"Highlights for 2024-06-23","text":"<p>Following zero-day SD3 release, a 10 days later heres a refresh with 10+ improvements including full prompt attention, support for compressed weights, additional text-encoder quantization modes.  </p> <p>But theres more than SD3: - support for quantized T5 text encoder FP16/FP8/FP4/INT8 in all models that use T5: SD3, PixArt-\u03a3, etc. - support for PixArt-Sigma in small/medium/large variants - support for HunyuanDiT 1.1 - additional NNCF weights compression support: SD3, PixArt, ControlNet, Lora - integration of MS Florence VLM/VQA Base and Large models - (finally) new release of Torch-DirectML - additional efficiencies for users with low VRAM GPUs - over 20 overall fixes  </p>"},{"location":"CHANGELOG/#model-improvements-for-2024-06-23","title":"Model Improvements for 2024-06-23","text":"<ul> <li>SD3: enable tiny-VAE (TAESD) preview and non-full quality mode  </li> <li>SD3: enable base LoRA support  </li> <li>SD3: add support for FP4 quantized T5 text encoder   simply select in settings -&gt; model -&gt; text encoder note for SD3 with T5, set SD.Next to use FP16 precision, not BF16 precision  </li> <li>SD3: add support for INT8 quantized T5 text encoder, thanks @Disty0!  </li> <li>SD3: enable cpu-offloading for T5 text encoder, thanks @Disty0!  </li> <li>SD3: simplified loading of model in single-file safetensors format   model load can now be performed fully offline  </li> <li>SD3: full support for prompt parsing and attention, thanks @AI-Casanova!</li> <li>SD3: ability to target different prompts to each of text-encoders, thanks @AI-Casanova!   example: <code>dog TE2: cat TE3: bird</code></li> <li>SD3: add support for sampler shift for Euler FlowMatch   see settings -&gt; samplers, also available as param in xyz grid   higher shift means model will spend more time on structure and less on details  </li> <li>SD3: add support for selecting T5 text encoder variant in XYZ grid</li> <li>Pixart-\u03a3: Add small (512px) and large (2k) variations, in addition to existing medium (1k)  </li> <li>Pixart-\u03a3: Add support for 4/8bit quantized t5 text encoder note by default pixart-\u03a3 uses full fp16 t5 encoder with large memory footprint   simply select in settings -&gt; model -&gt; text encoder before or after model load  </li> <li>HunyuanDiT: support for model version 1.1  </li> <li>MS Florence: integration of Microsoft Florence VLM/VQA Base and Large models   simply select in process -&gt; visual query!</li> </ul>"},{"location":"CHANGELOG/#general-improvements-for-2024-06-23","title":"General Improvements for 2024-06-23","text":"<ul> <li>support FP4 quantized T5 text encoder, in addition to existing FP8 and FP16</li> <li>support for T5 text-encoder loader in all models that use T5 example: load FP4 or FP8 quantized T5 text-encoder into PixArt Sigma!</li> <li>support for <code>torch-directml</code> 0.2.2, thanks @lshqqytiger! note: new directml is finally based on modern <code>torch</code> 2.3.1!  </li> <li>xyz grid: add support for LoRA selector</li> <li>vae load: store original vae so it can be restored when set to none</li> <li>extra networks: info display now contains link to source url if model if its known   works for civitai and huggingface models  </li> <li>force gc for lowvram users and improve gc logging</li> <li>improved google.colab support</li> <li>css tweaks for standardui</li> <li>css tweaks for modernui</li> <li>additional torch gc checks, thanks @Disty0!</li> </ul> <p>Improvements: NNCF, thanks @Disty0! - SD3 and PixArt support - moved the first compression step to CPU - sequential cpu offload (lowvram) support - Lora support without reloading the model - ControlNet compression support  </p>"},{"location":"CHANGELOG/#fixes-for-2024-06-23","title":"Fixes for 2024-06-23","text":"<ul> <li>fix unsaturated outputs, force apply vae config on model load  </li> <li>fix hidiffusion handling of non-square aspect ratios, thanks @ShenZhang-Shin!</li> <li>fix control second pass resize  </li> <li>fix hunyuandit set attention processor</li> <li>fix civitai download without name</li> <li>fix compatibility with latest adetailer</li> <li>fix invalid sampler warning</li> <li>fix starting from non git repo</li> <li>fix control api negative prompt handling</li> <li>fix saving style without name provided</li> <li>fix t2i-color adapter</li> <li>fix sdxl \"has been incorrectly initialized\"</li> <li>fix api face-hires</li> <li>fix api ip-adapter</li> <li>fix memory exceptions with ROCm, thanks @Disty0!</li> <li>fix face-hires with lowvram, thanks @Disty0!</li> <li>fix pag incorrectly resetting pipeline</li> <li>cleanup image metadata</li> <li>restructure api examples: <code>cli/api-*</code></li> <li>handle theme fallback when invalid theme is specified</li> <li>remove obsolete training code leftovers</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-06-13","title":"Update for 2024-06-13","text":""},{"location":"CHANGELOG/#highlights-for-2024-06-13","title":"Highlights for 2024-06-13","text":"<p>First, yes, it is here and supported: StabilityAI Stable Diffusion 3 Medium for details on how to download and use, see Wiki</p>"},{"location":"CHANGELOG/#what-else-2024-06-13","title":"What else 2024-06-13?","text":"<p>A lot of work on state-of-the-art multi-lingual models with both Tenecent HunyuanDiT and MuLan Plus tons of minor features such as optimized initial install experience, T-Gate and ResAdapter, additional ModernUI themes (both light and dark) and fixes since the last release which was only 2 weeks ago!</p>"},{"location":"CHANGELOG/#full-changelog-for-2024-06-13","title":"Full Changelog for 2024-06-13","text":""},{"location":"CHANGELOG/#new-models-for-2024-06-23","title":"New Models for 2024-06-23","text":"<ul> <li>StabilityAI Stable Diffusion 3 Medium   yup, supported!   quote: \"Stable Diffusion 3 Medium is a multimodal diffusion transformer (MMDiT) model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency\"   sdnext also supports switching optional T5 text encoder on-the-fly as well as loading model from either diffusers repo or safetensors single-file   for details, see Wiki</li> <li>Tenecent HunyuanDiT bilingual english/chinese diffusion transformer model   note: this is a very large model at ~17GB, but can be used with less VRAM using model offloading   simply select from networks -&gt; models -&gt; reference, model will be auto-downloaded on first use  </li> </ul>"},{"location":"CHANGELOG/#new-functionality-for-2024-06-23","title":"New Functionality for 2024-06-23","text":"<ul> <li>MuLan Multi-language prompts   write your prompts in ~110 auto-detected languages!   compatible with SD15 and SDXL   enable in scripts -&gt; MuLan and set encoder to <code>InternVL-14B-224px</code> encoder note: right now this is more of a proof-of-concept before smaller and/or quantized models are released   model will be auto-downloaded on first use: note its huge size of 27GB   even executing it in FP16 will require ~16GB of VRAM for text encoder alone   examples:  </li> <li>English: photo of a beautiful woman wearing a white bikini on a beach with a city skyline in the background</li> <li>Croatian: fotografija lijepe \u017eene u bijelom bikiniju na pla\u017ei s gradskim obzorom u pozadini</li> <li>Italian: Foto di una bella donna che indossa un bikini bianco su una spiaggia con lo skyline di una citt\u00e0 sullo sfondo</li> <li>Spanish: Foto de una hermosa mujer con un bikini blanco en una playa con un horizonte de la ciudad en el fondo</li> <li>German: Foto einer sch\u00f6nen Frau in einem wei\u00dfen Bikini an einem Strand mit einer Skyline der Stadt im Hintergrund</li> <li>Arabic: \u0635\u0648\u0631\u0629 \u0644\u0627\u0645\u0631\u0623\u0629 \u062c\u0645\u064a\u0644\u0629 \u062a\u0631\u062a\u062f\u064a \u0628\u064a\u0643\u064a\u0646\u064a \u0623\u0628\u064a\u0636 \u0639\u0644\u0649 \u0634\u0627\u0637\u0626 \u0645\u0639 \u0623\u0641\u0642 \u0627\u0644\u0645\u062f\u064a\u0646\u0629 \u0641\u064a \u0627\u0644\u062e\u0644\u0641\u064a\u0629</li> <li>Japanese: \u8857\u306e\u30b9\u30ab\u30a4\u30e9\u30a4\u30f3\u3092\u80cc\u666f\u306b\u30d3\u30fc\u30c1\u3067\u767d\u3044\u30d3\u30ad\u30cb\u3092\u7740\u305f\u7f8e\u3057\u3044\u5973\u6027\u306e\u5199\u771f</li> <li>Chinese: \u4e00\u4e2a\u7f8e\u4e3d\u7684\u5973\u4eba\u5728\u6d77\u6ee9\u4e0a\u7a7f\u7740\u767d\u8272\u6bd4\u57fa\u5c3c\u7684\u7167\u7247, \u80cc\u666f\u662f\u57ce\u5e02\u5929\u9645\u7ebf</li> <li>Korean: \ub3c4\uc2dc\uc758 \uc2a4\uce74\uc774\ub77c\uc778\uc744 \ubc30\uacbd\uc73c\ub85c \ud574\ubcc0\uc5d0\uc11c \ud770\uc0c9 \ube44\ud0a4\ub2c8\ub97c \uc785\uc740 \uc544\ub984 \ub2e4\uc6b4 \uc5ec\uc131\uc758 \uc0ac\uc9c4</li> <li>T-Gate Speed up generations by gating at which step cross-attention is no longer needed   enable via scripts -&gt; t-gate   compatible with SD15 </li> <li>PCM LoRAs allow for fast denoising using less steps with standard SD15 and SDXL models   download from https://huggingface.co/Kijai/converted_pcm_loras_fp16/tree/main</li> <li>ByteDance ResAdapter resolution-free model adapter   allows to use resolutions from 0.5 to 2.0 of original model resolution, compatible with SD15 and SDXL   enable via scripts -&gt; resadapter and select desired model</li> <li>Kohya HiRes Fix allows for higher resolution generation using standard SD15 models   enable via scripts -&gt; kohya-hires-fix note: alternative to regular hidiffusion method, but with different approach to scaling  </li> <li>additional built-in 4 great custom trained ControlNet SDXL models from Xinsir: OpenPose, Canny, Scribble, AnimePainter   thanks @lbeltrame</li> <li>add torch full deterministic mode   enable in settings -&gt; compute -&gt; use deterministic mode   typical differences are not large and its disabled by default as it does have some performance impact  </li> <li>new sampler: Euler FlowMatch </li> </ul>"},{"location":"CHANGELOG/#improvements-fixes-2024-06-13","title":"Improvements Fixes 2024-06-13","text":"<ul> <li>additional modernui themes</li> <li>reintroduce prompt attention normalization, disabled by default, enable in settings -&gt; execution   this can drastically help with unbalanced prompts  </li> <li>further work on improving python 3.12 functionality and remove experimental flag   note: recommended version remains python 3.11 for all users, except if you are using directml in which case its python 3.10  </li> <li>improved installer for initial installs   initial install will do single-pass install of all required packages with correct versions   subsequent runs will check package versions as necessary  </li> <li>add env variable <code>SD_PIP_DEBUG</code> to write <code>pip.log</code> for all pip operations   also improved installer logging  </li> <li>add python version check for <code>torch-directml</code> </li> <li>do not install <code>tensorflow</code> by default  </li> <li>improve metadata/infotext parser   add <code>cli/image-exif.py</code> that can be used to view/extract metadata from images  </li> <li>lower overhead on generate calls  </li> <li>auto-synchronize modernui and core branches  </li> <li>add option to pad prompt with zeros, thanks @Disty</li> </ul>"},{"location":"CHANGELOG/#fixes-2024-06-13","title":"Fixes 2024-06-13","text":"<ul> <li>cumulative fixes since the last release  </li> <li>fix apply/unapply hidiffusion for sd15  </li> <li>fix controlnet reference enabled check  </li> <li>fix face-hires with control batch count  </li> <li>install pynvml on-demand  </li> <li>apply rollback-vae option to latest torch versions, thanks @Iaotle  </li> <li>face hires skip if strength is 0  </li> <li>restore all sampler configuration on sampler change  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-06-02","title":"Update for 2024-06-02","text":"<ul> <li>fix textual inversion loading</li> <li>fix gallery mtime display</li> <li>fix extra network scrollable area when using modernui</li> <li>fix control prompts list handling</li> <li>fix restore variation seed and strength</li> <li>fix negative prompt parsing from metadata</li> <li>fix stable cascade progress monitoring</li> <li>fix variation seed with hires pass</li> <li>fix loading models trained with onetrainer</li> <li>add variation seed info to metadata</li> <li>workaround for scale-by when using modernui</li> <li>lock torch-directml version</li> <li>improve xformers installer</li> <li>improve ultralytics installer (face-hires)</li> <li>improve triton installer (compile)</li> <li>improve insightface installer (faceip)</li> <li>improve mim installer (dwpose)</li> <li>add dpm++ 1s and dpm++ 3m aliases for dpm++ 2m scheduler with different orders</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-05-28","title":"Update for 2024-05-28","text":""},{"location":"CHANGELOG/#highlights-for-2024-05-28","title":"Highlights for 2024-05-28","text":"<p>New SD.Next release has been baking in <code>dev</code> for a longer than usual, but changes are massive - about 350 commits for core and 300 for UI...</p> <p>Starting with the new UI - yup, this version ships with a preview of the new ModernUI For details on how to enable and use it, see Home and WiKi ModernUI is still in early development and not all features are available yet, please report issues and feedback Thanks to @BinaryQuantumSoul for his hard work on this project!  </p> <p>What else?</p>"},{"location":"CHANGELOG/#new-built-in-features","title":"New built-in features","text":"<ul> <li>PWA SD.Next is now installable as a web-app</li> <li>Gallery: extremely fast built-in gallery viewer   List, preview, search through all your images and videos!  </li> <li>HiDiffusion allows generating very-high resolution images out-of-the-box using standard models  </li> <li>Perturbed-Attention Guidance (PAG) enhances sample quality in addition to standard CFG scale  </li> <li>LayerDiffuse simply create transparent (foreground-only) images  </li> <li>IP adapter masking allows to use multiple input images for each segment of the input image  </li> <li>IP adapter InstantStyle implementation  </li> <li>Token Downsampling (ToDo) provides significant speedups with minimal-to-none quality loss  </li> <li>Samplers optimizations that allow normal samplers to complete work in 1/3 of the steps!   Yup, even popular DPM++2M can now run in 10 steps with quality equaling 30 steps using AYS presets  </li> <li>Native wildcards support  </li> <li>Improved built-in Face HiRes </li> <li>Better outpainting </li> <li>And much more...   For details of above features and full list, see Changelog</li> </ul>"},{"location":"CHANGELOG/#new-models","title":"New models","text":"<p>While still waiting for Stable Diffusion 3.0, there have been some significant models released in the meantime:</p> <ul> <li>PixArt-\u03a3, high end diffusion transformer model (DiT) capable of directly generating images at 4K resolution  </li> <li>SDXS, extremely fast 1-step generation consistency model  </li> <li>Hyper-SD, 1-step, 2-step, 4-step and 8-step optimized models  </li> </ul> <p>Note SD.Next is no longer marked as a fork of A1111 and github project has been fully detached Given huge number of changes with +3443/-3342 commits diff (at the time of fork detach) over the past year, a completely different backend/engine and a change of focus, it is time to give credit to original author,  and move on!  </p>"},{"location":"CHANGELOG/#full-changelog-for-2024-05-28","title":"Full ChangeLog for 2024-05-28","text":"<ul> <li>Features:</li> <li>ModernUI preview of the new ModernUI </li> <li>PWA SD.Next is now installable as a web-app and includes verified manifest  </li> <li>**Gallery</li> <li>Gallery: list, preview, search through all your images and videos!     Implemented as infinite-scroll with client-side-caching and lazy-loading while being fully async and non-blocking     Search or sort by path, name, size, width, height, mtime or any image metadata item, also with extended syntax like width &gt; 1000 Settings: optional additional user-defined folders, thumbnails in fixed or variable aspect-ratio  </li> <li>HiDiffusion:     Generate high-resolution images using your standard models without duplicates/distorsions AND improved performance     For example, SD15 can now go up to 2024x2048 and SDXL up to 4k natively     Simply enable checkbox in advanced menu and set desired resolution     Additional settings are available in settings -&gt; inference settings -&gt; hidiffusion     And can also be set and used via xyz grid Note: HiDiffusion resolution sensitive, so if you get error, set resolution to be multiples of 128  </li> <li>Perturbed-Attention Guidance     PAG enhances sample quality by utilizing self-attention in formation of latent in addition to standard CFG scale     Simply set advanced -&gt; attention guidance and advanced -&gt; adaptive scaling     Additional options are available in settings -&gt; inference settings -&gt; pag Note: PAG has replaced SAG as attention guidance method in SD.Next  </li> <li>LayerDiffuse     Create transparent images with foreground-only being generated     Simply select from scripts -&gt; apply to current model     All necessary files will be auto-downloaded on first use  </li> <li>IP Adapter Masking:     Powerful method of using masking with ip-adapters     When combined with multiple ip-adapters, it allows for different inputs guidance for each segment of the input image Hint: to create masks, you can use manually created masks or control-&gt;mask module with auto-segment to create masks and later upload them  </li> <li>IP Adapter advanced layer configuration:     Allows for more control over how each layer of ip-adapter is applied, requires a valid dict to be passed as input     See InstantStyle for details  </li> <li>OneDiff: new optimization/compile engine, thanks @aifartist     As with all other compile engines, enable via settings -&gt; compute settings -&gt; compile </li> <li>ToDo Token Downsampling for Efficient Generation of High-Resolution Images     Newer alternative method to ToMe that can provide speed-up with minimal quality loss     Enable in settings -&gt; inference settings -&gt; token merging     Also available in XYZ grid  </li> <li>Outpaint:     New method of outpainting that uses a combination of auto-masking and edge generation to create seamless transitions between original and generated image     Use on control tab:<ul> <li>input -&gt; denoising strength: 0.5 or higher</li> <li>select image -&gt; outpaint -&gt; expand edges or zoom out to desired size</li> <li>size -&gt; mode: outpaint, method: nearest</li> <li>mask -&gt; inpaint masked only (if you want to keep original image)</li> </ul> </li> <li>Wildcards:<ul> <li>native support of standard file-based wildcards in prompt  </li> <li>enabled by default, can be disabled in settings -&gt; extra networks if you want to use 3rd party extension  </li> <li>wildcards folder is set in settings -&gt; system paths and can be flat-file list or complex folder structure  </li> <li>matches strings <code>\"__*__\"</code> in positive and negative prompts  </li> <li>supports filename and path-based wildcards  </li> <li>supports nested wildcards (wildcard can refer to another wildcard, etc.)  </li> <li>supports wildcards files in one-choice per line or multiple choices per line separated by <code>|</code> format  </li> <li>note: this is in addition to previously released style-based wildcards  </li> </ul> </li> <li>Models:</li> <li>Load UNET: ability to override/load external UNET to a selected model     Works similar to how VAE is selected and loaded: Set UNet folder and UNet model in settings     Can be replaced on-the-fly, not just during initial model load     Enables usage of fine-tunes such as DPO-SD15 or DPO-SDXL Note: if there is a <code>JSON</code> file with the same name as the model it will be used as Unet config, otherwise Unet config from currently loaded model will be used  </li> <li>PixArt-\u03a3     pixart-\u03a3 is a high end diffusion Transformer model (DiT) with a T5 encoder/decoder capable of directly generating images at 4K resolution     to use, simply select from networks -&gt; models -&gt; reference -&gt; PixArt-\u03a3 note: this is a very large model at ~22GB     set parameters: sampler: Default </li> <li>SDXS     sdxs is an extremely fast 1-step generation consistency model that also uses TAESD as quick VAE out-of-the-box     to use, simply select from networks -&gt; models -&gt; reference -&gt; SDXS     set parameters: sampler: CMSI, steps: 1, cfg_scale: 0.0</li> <li>Hyper-SD     sd15 and sdxl 1-step, 2-step, 4-step and 8-step optimized models using lora     set parameters: sampler: TCD or LCM, steps: 1/2/4/8, cfg_scale: 0.0 </li> <li>UI:</li> <li>Faster UI load times</li> <li>Theme types: Standard (built-in themes), Modern (experimental nextgen ui), None (used for Gradio and Huggingface 3rd party themes)     Specifying a theme type updates list of available themes     For example, Gradio themes will not appear as available if theme type is set to Standard </li> <li>Redesign of base txt2img interface  </li> <li>Minor tweaks to styles: refresh/apply/save</li> <li>See details in WiKi</li> <li>API:</li> <li>Add API endpoint <code>/sdapi/v1/control</code> and CLI util <code>cli/simple-control.py</code>     (in addition to previously added <code>/sdapi/v1/preprocessors</code> and <code>/sdapi/v1/masking</code>)     example:     &gt; simple-control.py --prompt 'woman in the city' --sampler UniPC --steps 20     &gt; --input \\~/generative/Samples/cutie-512.png --output /tmp/test.png --processed /tmp/proc.png     &gt; --control 'Canny:Canny FP16:0.7, OpenPose:OpenPose FP16:0.8' --type controlnet     &gt; --ipadapter 'Plus:~/generative/Samples/cutie-512.png:0.5'  </li> <li>Add API endpoint <code>/sdapi/v1/vqa</code> and CLI util <code>cli/simple-vqa.py</code></li> <li>Changes:</li> <li>Due to change in Diffusers model loading     initial model load will now fetch config files required for the model     from the Huggingface site instead of using predefined YAML files</li> <li>Removed built-in extensions: ControlNet and Image-Browser     as both image-browser and controlnet have native built-in equivalents     both can still be installed by user if desired  </li> <li>Different defaults depending on available GPU, thanks @Disty0<ul> <li>4GB and below: lowvram</li> <li>8GB and below: medvram</li> <li>Cross-attention: Dynamic Attention SDP with medvram or lowvram, otherwise SDP  </li> <li>VAE Tiling enabled with medvram and lowvram</li> <li>Disable Extract EMA by default</li> <li>Disable forced VAE Slicing for lowvram</li> </ul> </li> <li>Upscaler compile disabled by default with OpenVINO backend  </li> <li>Hypernetwork support disabled by default, can be enabled in settings  </li> <li>Improvements:</li> <li>Faster server startup  </li> <li>Styles apply wildcards to params</li> <li>Face HiRes fully configurable and higher quality when using high-resolution models  </li> <li>Extra networks persistent sort order in settings  </li> <li>Add option to make batch generations use fully random seed vs sequential  </li> <li>Make metadata in full screen viewer optional</li> <li>Add VAE civitai scan metadata/preview</li> <li>More efficient in-browser callbacks</li> <li>Updated all system requirements  </li> <li>UI log monitor will auto-reconnect to server on server restart  </li> <li>UI styles includes indicator for active styles  </li> <li>UI reduce load on browser  </li> <li>Secondary sampler add option \"same as primary\"  </li> <li>Change attention mechanism on-the-fly without model reload, thanks @Disty0  </li> <li>Update stable-fast with support for torch 2.2.2 and 2.3.0, thanks @Aptronymist</li> <li>Add torch cudaMallocAsync in compute options     Can improve memory utilization on compatible GPUs (RTX and newer)  </li> <li>Torch dynamic profiling     You can enable/disable full torch profiling in settings top menu on-the-fly  </li> <li>Prompt caching - if you use the same prompt multiple times, no need to re-parse and encode it     Useful for batches as prompt processing is ~0.1sec on each pass  </li> <li>Enhance <code>SD_PROMPT_DEBUG</code> to show actual tokens used</li> <li>Support controlnet manually downloads models in both standalone and diffusers format     For standalone, simply copy safetensors file to <code>models/control/controlnet</code> folder     For diffusers format, create folder with model name in <code>models/control/controlnet/</code>     and copy <code>model.json</code> and <code>diffusion_pytorch_model.safetensors</code> to that folder  </li> <li>Samplers</li> <li>Add Euler SGM variation (e.g. SGM Uniform), optimized for SDXL-Lightning models note: you can use other samplers as well with SDXL-Lightning models  </li> <li>Add CMSI sampler, optimized for consistency models  </li> <li>Add option timestep spacing to sampler settings and sampler section in main ui     Note: changing timestep spacing changes behavior of sampler and can help to make any sampler turbo/lightning compatibile</li> <li>Add option timesteps to manually set timesteps instead of relying on steps+spacing     Additionally, presets from nVidias align-you-steps reasearch are provided     Result is that perfectly aligned steps can drastically reduce number of steps needed!     For example, AYS preset alows DPM++2M to run in ~10 steps with quality equallying ~30 steps!  </li> <li>IPEX, thanks @Disty0</li> <li>Update to IPEX 2.1.20 on Linux     requires removing the venv folder to update properly  </li> <li>Removed 1024x1024 workaround  </li> <li>Disable ipexrun by default, set <code>IPEXRUN=True</code> if you want to use <code>ipexrun</code> </li> <li>ROCm, thanks @Disty0  </li> <li>Add support for ROCm 6.1 nighthly builds  </li> <li>Switch to stable branch of PyTorch  </li> <li>Compatibility improvenments  </li> <li>Add MIGraphX torch compile engine  </li> <li>ZLUDA, thanks @lshqqytiger</li> <li>Rewrite ZLUDA installer</li> <li>ZLUDA v3.8 updates: Runtime API support</li> <li>Add <code>--reinstall-zluda</code> (to download the latest ZLUDA)</li> <li>Fixes:</li> <li>Update requirements</li> <li>Installer automatically handle detached git states  </li> <li>Prompt params parser</li> <li>Allowing forcing LoRA loading method for some or all models</li> <li>Image save without metadata</li> <li>API generate save metadata</li> <li>Face/InstantID faults</li> <li>CivitAI update model info for all models</li> <li>FP16/BF16 test on model load</li> <li>Variation seed possible NaNs</li> <li>Enumerate diffusers model with multiple variants</li> <li>Diffusers skip non-models on enum</li> <li>Face-HiRes compatibility with control modules</li> <li>Face-HiRes avoid doule save in some scenarios</li> <li>Loading safetensors embeddings</li> <li>CSS fixes</li> <li>Check if attention processor is compatible with model</li> <li>SD Upscale when used with control module</li> <li>Noise sampler seed, thanks @leppie</li> <li>Control module with ADetailer and active ControlNet</li> <li>Control module restore button full functionality</li> <li>Control improved handling with multiple control units and different init images</li> <li>Control add correct metadata to image</li> <li>Time embeddings load part of model load</li> <li>A1111 update OptionInfo properties</li> <li>MOTD exception handling</li> <li>Notifications not triggering</li> <li>Prompt cropping on copy</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-03-19","title":"Update for 2024-03-19","text":""},{"location":"CHANGELOG/#highlights-2024-03-19","title":"Highlights 2024-03-19","text":"<p>New models: - Stable Cascade Full and Lite - Playground v2.5 - KOALA 700M - Stable Video Diffusion XT 1.1 - VGen </p> <p>New pipelines and features: - Img2img using LEdit++, context aware method with image analysis and positive/negative prompt handling - Trajectory Consistency Distillation TCD for processing in even less steps - Visual Query &amp; Answer using moondream2 as an addition to standard interrogate methods - Face-HiRes: simple built-in detailer for face refinements - Even simpler outpaint: when resizing image, simply pick outpaint method and if image has different aspect ratio, blank areas will be outpainted! - UI aspect-ratio controls and other UI improvements - User controllable invisibile and visible watermarking - Native composable LoRA</p> <p>What else?</p> <ul> <li>Reference models: Networks -&gt; Models -&gt; Reference: All reference models now come with recommended settings that can be auto-applied if desired  </li> <li>Styles: Not just for prompts! Styles can apply generate parameters as templates and can be used to apply wildcards to prompts improvements, Additional API endpoints  </li> <li>Given the high interest in ZLUDA engine introduced in last release weve updated much more flexible/automatic install procedure (see wiki for details)  </li> <li>Plus Additional Improvements such as: Smooth tiling, Refine/HiRes workflow improvements, Control workflow  </li> </ul> <p>Further details: - For basic instructions, see README - For more details on all new features see full CHANGELOG - For documentation, see WiKi - Discord server  </p>"},{"location":"CHANGELOG/#full-changelog-2024-03-19","title":"Full Changelog 2024-03-19","text":"<ul> <li>Stable Cascade Full and Lite</li> <li>large multi-stage high-quality model from warp-ai/wuerstchen team and released by stabilityai  </li> <li>download using networks -&gt; reference</li> <li>see wiki for details</li> <li>Playground v2.5</li> <li>new model version from Playground: based on SDXL, but with some cool new concepts</li> <li>download using networks -&gt; reference</li> <li>set sampler to DPM++ 2M EDM or Euler EDM</li> <li>KOALA 700M</li> <li>another very fast &amp; light sdxl model where original unet was compressed and distilled to 54% of original size  </li> <li>download using networks -&gt; reference</li> <li>note to download fp16 variant (recommended), set settings -&gt; diffusers -&gt; preferred model variant  </li> <li>LEdit++</li> <li>context aware img2img method with image analysis and positive/negative prompt handling  </li> <li>enable via img2img -&gt; scripts -&gt; ledit</li> <li>uses following params from standard img2img: cfg scale (recommended ~3), steps (recommended ~50), denoise strength (recommended ~0.7)</li> <li>can use postive and/or negative prompt to guide editing process<ul> <li>positive prompt: what to enhance, strength and threshold for auto-masking</li> <li>negative prompt: what to remove, strength and threshold for auto-masking  </li> </ul> </li> <li>note: not compatible with model offloading</li> <li>Second Pass / Refine</li> <li>independent upscale and hires options: run hires without upscale or upscale without hires or both</li> <li>upscale can now run 0.1-8.0 scale and will also run if enabled at 1.0 to allow for upscalers that simply improve image quality</li> <li>update ui section to reflect changes</li> <li>note: behavior using backend:original is unchanged for backwards compatibilty</li> <li>Visual Query visual query &amp; answer in process tab  </li> <li>go to process -&gt; visual query  </li> <li>ask your questions, e.g. \"describe the image\", \"what is behind the subject\", \"what are predominant colors of the image?\"</li> <li>primary model is moondream2, a tiny 1.86B vision language model note: its still 3.7GB in size, so not really tiny  </li> <li>additional support for multiple variations of several base models: GIT, BLIP, ViLT, PIX, sizes range from 0.3 to 1.7GB  </li> <li>Video</li> <li>Image2Video<ul> <li>new module for creating videos from images  </li> <li>simply enable from img2img -&gt; scripts -&gt; image2video </li> <li>model is auto-downloaded on first use</li> <li>based on VGen </li> </ul> </li> <li>Stable Video Diffusion<ul> <li>updated with SVD 1.0, SVD XT 1.0 and SVD XT 1.1</li> <li>models are auto-downloaded on first use</li> <li>simply enable from img2img -&gt; scripts -&gt; stable video diffusion </li> <li>for svd 1.0, use frames=~14, for xt models use frames=~25</li> </ul> </li> <li>Composable LoRA, thanks @AI-Casanova</li> <li>control lora strength for each step     for example: <code>&lt;xxx:0.1@0,0.9@1&gt;</code> means strength=0.1 for step at 0% and intepolate towards strength=0.9 for step at 100%</li> <li>note: this is a very experimental feature and may not work as expected</li> <li>Control</li> <li>added refiner/hires workflows</li> <li>added resize methods to before/after/mask: fixed, crop, fill</li> <li>Styles: styles are not just for prompts!</li> <li>new styles editor: networks -&gt; styles -&gt; edit</li> <li>styles can apply generate parameters, for example to have a style that enables and configures hires:     parameters=<code>enable_hr: True, hr_scale: 2, hr_upscaler: Latent Bilinear antialias, hr_sampler_name: DEIS, hr_second_pass_steps: 20, denoising_strength: 0.5</code></li> <li>styles can apply wildcards to prompts, for example:     wildcards=<code>movie=mad max, dune, star wars, star trek; intricate=realistic, color sketch, pencil sketch, intricate</code></li> <li>as usual, you can apply any number of styles so you can choose which settings are applied and in which order and which wildcards are used</li> <li>UI</li> <li>aspect-ratio add selector and lock to width/height control     allowed aspect ration can be configured via settings -&gt; user interface*  </li> <li>interrogate tab is now merged into process tab  </li> <li>image viewer now displays image metadata</li> <li>themes improve on-the-fly switching</li> <li>log monitor flag server warnings/errors and overall improve display</li> <li>control separate processor settings from unit settings</li> <li>Face HiRes</li> <li>new face restore option, works similar to well-known adetailer by running an inpaint on detected faces but with just a checkbox to enable/disable  </li> <li>set as default face restorer in settings -&gt; postprocessing  </li> <li>disabled by default, to enable simply check face restore in your generate advanced settings  </li> <li>strength, steps and sampler are set using by hires section in refine menu  </li> <li>strength can be overriden in settings -&gt; postprocessing  </li> <li>will use secondary prompt and secondary negative prompt if present in refine  </li> <li>Watermarking</li> <li>SD.Next disables all known watermarks in models, but does allow user to set custom watermark  </li> <li>see settings -&gt; image options -&gt; watermarking </li> <li>invisible watermark: using steganogephy  </li> <li>image watermark: overlaid on top of image  </li> <li>Reference models</li> <li>additional reference models available for single-click download &amp; run:     Stable Cascade, Stable Cascade lite, Stable Video Diffusion XT 1.1 </li> <li>reference models will now download fp16 variation by default  </li> <li>reference models will print recommended settings to log if present</li> <li>new setting in extra network: use reference values when available     disabled by default, if enabled will force use of reference settings for models that have them</li> <li>Samplers</li> <li>TCD: Trajectory Consistency Distillation     new sampler that produces consistent results in a very low number of steps (comparable to LCM but without reliance on LoRA)     for best results, use with TCD LoRA: https://huggingface.co/h1t/TCD-SDXL-LoRA</li> <li>DPM++ 2M EDM and Euler EDM     EDM is a new solver algorithm currently available for DPM++2M and Euler samplers     Note that using EDM samplers with non-EDM optimized models will provide just noise and vice-versa  </li> <li>Improvements</li> <li>FaceID extend support for LoRA, HyperTile and FreeU, thanks @Trojaner</li> <li>Tiling now extends to both Unet and VAE producing smoother outputs, thanks @AI-Casanova</li> <li>new setting in image options: include mask in output</li> <li>improved params parsing from from prompt string and styles</li> <li>default theme updates and additional built-in theme black-gray</li> <li>support models with their own YAML model config files</li> <li>support models with their own JSON per-component config files, for example: <code>playground-v2.5_vae.config</code></li> <li>prompt can have comments enclosed with <code>/*</code> and <code>*/</code>     comments are extracted from prompt and added to image metadata  </li> <li>ROCm </li> <li>add ROCm 6.0 nightly option to installer, thanks @jicka</li> <li>add flash attention support for rdna3, thanks @Disty0     install flash_attn package for rdna3 manually and enable flash attention from compute settings     to install flash_attn, activate the venv and run <code>pip install -U git+https://github.com/ROCm/flash-attention@howiejay/navi_support</code> </li> <li>IPEX</li> <li>disabled IPEX Optimize by default  </li> <li>API</li> <li>add preprocessor api endpoints     GET:<code>/sdapi/v1/preprocessors</code>, POST:<code>/sdapi/v1/preprocess</code>, sample script:<code>cli/simple-preprocess.py</code></li> <li>add masking api endpoints     GET:<code>/sdapi/v1/masking</code>, POST:<code>/sdapi/v1/mask</code>, sample script:<code>cli/simple-mask.py</code></li> <li>Internal</li> <li>improved vram efficiency for model compile, thanks @Disty0</li> <li>stable-fast compatibility with torch 2.2.1  </li> <li>remove obsolete textual inversion training code</li> <li>remove obsolete hypernetworks training code</li> <li>Refiner validated workflows:</li> <li>Fully functional: SD15 + SD15, SDXL + SDXL, SDXL + SDXL-R</li> <li>Functional, but result is not as good: SD15 + SDXL, SDXL + SD15, SD15 + SDXL-R</li> <li>SDXL Lightning models just-work, just makes sure to set CFG Scale to 0     and choose a best-suited sampler, it may not be the one youre used to (e.g. maybe even basic Euler)  </li> <li>Fixes</li> <li>improve model cpu offload compatibility</li> <li>improve model sequential offload compatibility</li> <li>improve bfloat16 compatibility</li> <li>improve xformers installer to match cuda version and install triton</li> <li>fix extra networks refresh</li> <li>fix sdp memory attention in backend original</li> <li>fix autodetect sd21 models</li> <li>fix api info endpoint</li> <li>fix sampler eta in xyz grid, thanks @AI-Casanova</li> <li>fix requires_aesthetics_score errors</li> <li>fix t2i-canny</li> <li>fix differenital diffusion for manual mask, thanks @23pennies</li> <li>fix ipadapter apply/unapply on batch runs</li> <li>fix control with multiple units and override images</li> <li>fix control with hires</li> <li>fix control-lllite</li> <li>fix font fallback, thanks @NetroScript</li> <li>update civitai downloader to handler new metadata</li> <li>improve control error handling</li> <li>use default model variant if specified variant doesnt exist</li> <li>use diffusers lora load override for lcm/tcd/turbo loras</li> <li>exception handler around vram memory stats gather</li> <li>improve ZLUDA installer with <code>--use-zluda</code> cli param, thanks @lshqqytiger</li> </ul>"},{"location":"CHANGELOG/#update-for-2024-02-22","title":"Update for 2024-02-22","text":"<p>Only 3 weeks since last release, but heres another feature-packed one! This time release schedule was shorter as we wanted to get some of the fixes out faster.</p>"},{"location":"CHANGELOG/#highlights-2024-02-22","title":"Highlights 2024-02-22","text":"<ul> <li>IP-Adapters &amp; FaceID: multi-adapter and multi-image suport  </li> <li>New optimization engines: DeepCache, ZLUDA and Dynamic Attention Slicing </li> <li>New built-in pipelines: Differential diffusion and Regional prompting </li> <li>Big updates to: Outpainting (noised-edge-extend), Clip-skip (interpolate with non-integrer values!), CFG end (prevent overburn on high CFG scales), Control module masking functionality  </li> <li>All reported issues since the last release are addressed and included in this release  </li> </ul> <p>Further details: - For basic instructions, see README - For more details on all new features see full CHANGELOG - For documentation, see WiKi - Discord server  </p>"},{"location":"CHANGELOG/#full-changelog-for-2024-02-22","title":"Full ChangeLog for 2024-02-22","text":"<ul> <li>Improvements:</li> <li>IP Adapter major refactor  <ul> <li>support for multiple input images per each ip adapter  </li> <li>support for multiple concurrent ip adapters note: you cannot mix &amp; match ip adapters that use different CLiP models, for example <code>Base</code> and <code>Base ViT-G</code> </li> <li>add adapter start/end to settings, thanks @AI-Casanova   having adapter start late can help with better control over composition and prompt adherence   having adapter end early can help with overal quality and performance  </li> <li>unified interface in txt2img, img2img and control  </li> <li>enhanced xyz grid support  </li> </ul> </li> <li>FaceID now also works with multiple input images!  </li> <li>Differential diffusion     img2img generation where you control strength of each pixel or image area     can be used with manually created masks or with auto-generated depth-maps     uses general denoising strength value     simply enable from img2img -&gt; scripts -&gt; differential diffusion note: supports sd15 and sdxl models  </li> <li>Regional prompting as a built-in solution     usage is same as original implementation from @hako-mikan     click on title to open docs and see examples of full syntax on how to use it     simply enable from scripts -&gt; regional prompting note: supports sd15 models only  </li> <li>DeepCache model acceleration     it can produce massive speedups (2x-5x) with no overhead, but with some loss of quality settings -&gt; compute -&gt; model compile -&gt; deep-cache and settings -&gt; compute -&gt; model compile -&gt; cache interval </li> <li>ZLUDA experimental support, thanks @lshqqytiger  <ul> <li>ZLUDA is CUDA wrapper that can be used for GPUs without native support</li> <li>best use case is AMD GPUs on Windows, see wiki for details  </li> </ul> </li> <li>Outpaint control outpaint now uses new alghorithm: noised-edge-extend     new method allows for much larger outpaint areas in a single pass, even outpaint 512-&gt;1024 works well     note that denoise strength should be increased for larger the outpaint areas, for example outpainting 512-&gt;1024 works well with denoise 0.75     outpaint can run in img2img mode (default) and inpaint mode where original image is masked (if inpaint masked only is selected)  </li> <li>Clip-skip reworked completely, thanks @AI-Casanova &amp; @Disty0     now clip-skip range is 0-12 where previously lowest value was 1 (default is still 1)     values can also be decimal to interpolate between different layers, for example <code>clip-skip: 1.5</code>, thanks @AI-Casanova  </li> <li>CFG End new param to control image generation guidance, thanks @AI-Casanova     sometimes you want strong control over composition, but you want it to stop at some point     for example, when used with ip-adapters or controlnet, high cfg scale can overpower the guided image  </li> <li>Control<ul> <li>when performing inpainting, you can specify processing resolution using size-&gt;mask </li> <li>units now have extra option to re-use current preview image as processor input  </li> </ul> </li> <li>Cross-attention refactored cross-attention methods, thanks @Disty0  <ul> <li>for backend:original, its unchanged: SDP, xFormers, Doggettxs, InvokeAI, Sub-quadratic, Split attention  </li> <li>for backend:diffuers, list is now: SDP, xFormers, Batch matrix-matrix, Split attention, Dynamic Attention BMM, Dynamic Attention SDP   note: you may need to update your settings! Attention Slicing is renamed to Split attention  </li> <li>for ROCm, updated default cross-attention to Scaled Dot Product  </li> </ul> </li> <li>Dynamic Attention Slicing, thanks @Disty0  <ul> <li>dynamically slices attention queries in order to keep them under the slice rate   slicing gets only triggered if the query size is larger than the slice rate to gain performance Dynamic Attention Slicing BMM uses Batch matrix-matrix Dynamic Attention Slicing SDP uses Scaled Dot Product </li> <li>settings -&gt; compute settings -&gt; attention -&gt; dynamic attention slicing </li> </ul> </li> <li>ONNX:  <ul> <li>allow specify onnx default provider and cpu fallback settings -&gt; diffusers </li> <li>allow manual install of specific onnx flavor settings -&gt; onnx </li> <li>better handling of <code>fp16</code> models/vae, thanks @lshqqytiger  </li> </ul> </li> <li>OpenVINO update to <code>torch 2.2.0</code>, thanks @Disty0  </li> <li>HyperTile additional options thanks @Disty0  <ul> <li>add swap size option  </li> <li>add use only for hires pass option  </li> </ul> </li> <li>add <code>--theme</code> cli param to force theme on startup  </li> <li>add <code>--allow-paths</code> cli param to add additional paths that are allowed to be accessed via web, thanks @OuticNZ  </li> <li>Wiki:</li> <li>added benchmark notes for IPEX, OpenVINO and Olive  </li> <li>added ZLUDA wiki page  </li> <li>Internal</li> <li>update dependencies  </li> <li>refactor txt2img/img2img api  </li> <li>enhanced theme loader  </li> <li>add additional debug env variables  </li> <li>enhanced sdp cross-optimization control     see settings -&gt; compute settings </li> <li>experimental support for python 3.12 </li> <li>Fixes:  </li> <li>add variation seed to diffusers txt2img, thanks @AI-Casanova  </li> <li>add cmd param <code>--skip-env</code> to skip setting of environment parameters during sdnext load  </li> <li>handle extensions that install conflicting versions of packages <code>onnxruntime</code>, <code>opencv2-python</code> </li> <li>installer refresh package cache on any install  </li> <li>fix embeddings registration on server startup, thanks @AI-Casanova  </li> <li>ipex handle dependencies, thanks @Disty0  </li> <li>insightface handle dependencies  </li> <li>img2img mask blur and padding  </li> <li>xyz grid handle ip adapter name and scale  </li> <li>lazy loading of image may prevent metadata from being loaded on time  </li> <li>allow startup without valid models folder  </li> <li>fix interrogate api endpoint  </li> <li>control fix resize causing runtime errors  </li> <li>control fix processor override image after processor change  </li> <li>control fix display grid with batch  </li> <li>control restore pipeline before running scripts/extensions  </li> <li>handle pipelines that return dict instead of object  </li> <li>lora use strict name matching if preferred option is by-filename  </li> <li>fix inpaint mask only for diffusers  </li> <li>fix vae dtype mismatch, thanks @Disty0  </li> <li>fix controlnet inpaint mask  </li> <li>fix theme list refresh  </li> <li>fix extensions update information in ui  </li> <li>fix taesd with bfloat16</li> <li>fix model merge manual merge settings, thanks @AI-Casanova  </li> <li>fix gradio instant update issues for textboxes in quicksettings  </li> <li>fix rembg missing dependency  </li> <li>bind controlnet extension to last known working commit, thanks @Aptronymist  </li> <li>prompts-from-file fix resizable prompt area  </li> </ul>"},{"location":"CHANGELOG/#update-for-2024-02-07","title":"Update for 2024-02-07","text":"<p>Another big release just hit the shelves!</p>"},{"location":"CHANGELOG/#highlights-2024-02-07","title":"Highlights 2024-02-07","text":"<ul> <li>A lot more functionality in the Control module:</li> <li>Inpaint and outpaint support, flexible resizing options, optional hires  </li> <li>Built-in support for many new processors and models, all auto-downloaded on first use  </li> <li>Full support for scripts and extensions  </li> <li>Complete Face module   implements all variations of FaceID, FaceSwap and latest PhotoMaker and InstantID </li> <li>Much enhanced IPAdapter modules  </li> <li>Brand new Intelligent masking, manual or automatic   Using ML models (LAMA object removal, REMBG background removal, SAM segmentation, etc.) and with live previews   With granular blur, erode and dilate controls  </li> <li>New models and pipelines: Segmind SegMoE, Mixture Tiling, InstaFlow, SAG, BlipDiffusion </li> <li>Massive work integrating latest advances with OpenVINO, IPEX and ONNX Olive</li> <li>Full control over brightness, sharpness and color shifts and color grading during generate process directly in latent space  </li> <li>Documentation! This was a big one, with a lot of new content and updates in the WiKi </li> </ul> <p>Plus welcome additions to UI performance, usability and accessibility and flexibility of deployment as well as API improvements And it also includes fixes for all reported issues so far  </p> <p>As of this release, default backend is set to diffusers as its more feature rich than original and supports many additional models (original backend does remain as fully supported)  </p> <p>Also, previous versions of SD.Next were tuned for balance between performance and resource usage. With this release, focus is more on performance. See Benchmark notes for details, but as a highlight, we are now hitting ~110-150 it/s on a standard nVidia RTX4090 in optimal scenarios!  </p> <p>Further details: - For basic instructions, see README - For more details on all new features see full CHANGELOG - For documentation, see WiKi</p>"},{"location":"CHANGELOG/#full-changelog-2024-02-07","title":"Full ChangeLog 2024-02-07","text":"<ul> <li>Heavily updated Wiki </li> <li>Control:  </li> <li>new docs:<ul> <li>Control overview </li> <li>Control guide, thanks @Aptronymist  </li> </ul> </li> <li>add inpaint support     applies to both img2img and controlnet workflows  </li> <li>add outpaint support     applies to both img2img and controlnet workflows note: increase denoising strength since outpainted area is blank by default  </li> <li>new mask module  <ul> <li>granular blur (gaussian), erode (reduce or remove noise) and dilate (pad or expand)  </li> <li>optional live preview </li> <li>optional auto-segmentation using ml models   auto-segmentation can be done using segment-anything models or rembg models note: auto segmentation will automatically expand user-masked area to segments that include current user mask  </li> <li>optional auto-mask   if you dont provide mask or mask is empty, you can instead use auto-mask to automatically generate mask   this is especially useful if you want to use advanced masking on batch or video inputs and dont want to manually mask each image note: such auto-created mask is also subject to all other selected settings such as auto-segmentation, blur, erode and dilate  </li> <li>optional object removal using LaMA model   remove selected objects from images with a single click   works best when combined with auto-segmentation to remove smaller objects  </li> <li>masking can be combined with control processors in which case mask is applied before processor  </li> <li>unmasked part of can is optionally applied to final image as overlay, see settings <code>mask_apply_overlay</code> </li> </ul> </li> <li>support for many additional controlnet models     now built-in models include 30+ SD15 models and 15+ SDXL models  </li> <li>allow resize both before and after generate operation     this allows for workflows such as: image -&gt; upscale or downscale -&gt; generate -&gt; upscale or downscale -&gt; output     providing more flexibility and than standard hires workflow note: resizing before generate can be done using standard upscalers or latent</li> <li>implicit hires     since hires is only used for txt2img, control reuses existing resize functionality     any image size is used as txt2img target size     but if resize scale is also set its used to additionally upscale image after initial txt2img and for hires pass  </li> <li>add support for scripts and extensions     you can now combine control workflow with your favorite script or extension note extensions that are hard-coded for txt2img or img2img tabs may not work until they are updated  </li> <li>add depth-anything depth map processor and trained controlnet  </li> <li>add marigold depth map processor     this is state-of-the-art depth estimation model, but its quite heavy on resources  </li> <li>add openpose xl controlnet  </li> <li>add blip/booru interrogate functionality to both input and output images  </li> <li>configurable output folder in settings  </li> <li>auto-refresh available models on tab activate  </li> <li>add image preview for override images set per-unit  </li> <li>more compact unit layout  </li> <li>reduce usage of temp files  </li> <li>add context menu to action buttons  </li> <li>move ip-adapter implementation to control tabs  </li> <li>resize by now applies to input image or frame individually     allows for processing where input images are of different sizes  </li> <li>support controlnets with non-default yaml config files  </li> <li>implement resize modes for override images  </li> <li>allow any selection of units  </li> <li>dynamically install depenencies required by specific processors  </li> <li>fix input image size  </li> <li>fix video color mode  </li> <li>fix correct image mode  </li> <li>fix batch/folder/video modes  </li> <li>fix processor switching within same unit  </li> <li>fix pipeline switching between different modes  </li> <li>Face module   implements all variations of FaceID, FaceSwap and latest PhotoMaker and InstantID   simply select from scripts and choose your favorite method and model note: all models are auto-downloaded on first use  </li> <li>FaceID <ul> <li>faceid guides image generation given the input image  </li> <li>full implementation for SD15 and SD-XL, to use simply select from Scripts Base (93MB) uses InsightFace to generate face embeds and OpenCLIP-ViT-H-14 (2.5GB) as image encoder Plus (150MB) uses InsightFace to generate face embeds and CLIP-ViT-H-14-laion2B (3.8GB) as image encoder SDXL (1022MB) uses InsightFace to generate face embeds and OpenCLIP-ViT-bigG-14 (3.7GB) as image encoder  </li> </ul> </li> <li>FaceSwap <ul> <li>face swap performs face swapping at the end of generation  </li> <li>based on InsightFace in-swapper  </li> </ul> </li> <li>PhotoMaker <ul> <li>for SD-XL only  </li> <li>new model from TenencentARC using similar concept as IPAdapter, but with different implementation and   allowing full concept swaps between input images and generated images using trigger words  </li> <li>note: trigger word must match exactly one term in prompt for model to work  </li> </ul> </li> <li>InstantID <ul> <li>for SD-XL only  </li> <li>based on custom trained ip-adapter and controlnet combined concepts  </li> <li>note: controlnet appears to be heavily watermarked  </li> </ul> </li> <li>enable use via api, thanks @trojaner  </li> <li>IPAdapter </li> <li>additional models for SD15 and SD-XL, to use simply select from Scripts: SD15: Base, Base ViT-G, Light, Plus, Plus Face, Full Face SDXL: Base SDXL, Base ViT-H SDXL, Plus ViT-H SDXL, Plus Face ViT-H SDXL  </li> <li>enable use via api, thanks @trojaner  </li> <li>Segmind SegMoE </li> <li>initial support for reference models     download&amp;load via network -&gt; models -&gt; reference -&gt; SegMoE SD 4x2 (3.7GB), SegMoE XL 2x1 (10GB), SegMoE XL 4x2 </li> <li>note: since segmoe is basically sequential mix of unets from multiple models, it can get large     SD 4x2 is ~4GB, XL 2x1 is ~10GB and XL 4x2 is 18GB  </li> <li>supports lora, thanks @AI-Casanova</li> <li>support for create and load custom mixes will be added in the future  </li> <li>Mixture Tiling </li> <li>uses multiple prompts to guide different parts of the grid during diffusion process  </li> <li>can be used ot create complex scenes with multiple subjects  </li> <li>simply select from scripts  </li> <li>Self-attention guidance </li> <li>simply select scale in advanced menu  </li> <li>can drastically improve image coherence as well as reduce artifacts  </li> <li>note: only compatible with some schedulers  </li> <li>FreeInit for AnimateDiff</li> <li>greatly improves temporal consistency of generated outputs  </li> <li>all options are available in animateddiff script  </li> <li>SalesForce BlipDiffusion </li> <li>model can be used to place subject in a different context  </li> <li>requires input image  </li> <li>last word in prompt and negative prompt will be used as source and target subjects  </li> <li>sampler must be set to default before loading the model  </li> <li>InstaFlow </li> <li>another take on super-fast image generation in a single step  </li> <li>set sampler:default, steps:1, cfg-scale:0 </li> <li>load from networks -&gt; models -&gt; reference  </li> <li>Improvements </li> <li>ui <ul> <li>check version and update SD.Next via UI   simply go to: settings -&gt; update</li> <li>globally configurable font size   will dynamically rescale ui depending on settings -&gt; user interface  </li> <li>built-in themes can be changed on-the-fly   this does not work with gradio-default themes as css is created by gradio itself  </li> <li>two new themes: simple-dark and simple-light </li> <li>modularized blip/booru interrogate   now appears as toolbuttons on image/gallery output  </li> <li>faster browser page load  </li> <li>update hints, thanks @brknsoul  </li> <li>cleanup settings  </li> </ul> </li> <li>server<ul> <li>all move/offload options are disable by default for optimal performance   enable manually if low on vram  </li> </ul> </li> <li>server startup: performance  <ul> <li>reduced module imports   ldm support is now only loaded when running in backend=original  </li> <li>faster extension load  </li> <li>faster json parsing  </li> <li>faster lora indexing  </li> <li>lazy load optional imports  </li> <li>batch embedding load, thanks @midcoastal and @AI-Casanova   10x+ faster embeddings load for large number of embeddings, now works for 1000+ embeddings  </li> <li>file and folder list caching, thanks @midcoastal   if you have a lot of files and and/or are using slower or non-local storage, this speeds up file access a lot  </li> <li>add <code>SD_INSTALL_DEBUG</code> env variable to trace all <code>git</code> and <code>pip</code> operations</li> </ul> </li> <li>extra networks <ul> <li>4x faster civitai metadata and previews lookup  </li> <li>better display and selection of tags &amp; trigger words   if hashes are calculated, trigger words will only be displayed for actual model version  </li> <li>better matching of previews  </li> <li>better search, including searching for multiple keywords or using full regex   see wiki page for more details on syntax   thanks @NetroScript  </li> <li>reduce html overhead  </li> </ul> </li> <li>model compression, thanks @Disty0  <ul> <li>using built-in NNCF model compression, you can reduce the size of your models significantly   example: up to 3.4GB of VRAM saved for SD-XL model!  </li> <li>see wiki for details  </li> </ul> </li> <li>embeddings     you can now use sd 1.5 embeddings with your sd-xl models!, thanks @AI-Casanova     conversion is done on-the-fly, is completely transparent and result is an approximation of embedding     to enable: settings-&gt;extra networks-&gt;auto-convert embeddings  </li> <li>offline deployment: allow deployment without git clone     for example, you can now deploy a zip of the sdnext folder  </li> <li>latent upscale: updated latent upscalers (some are new) nearest, nearest-exact, area, bilinear, bicubic, bilinear-antialias, bicubic-antialias</li> <li>scheduler: added <code>SA Solver</code> </li> <li>model load to gpu     new option in settings-&gt;diffusers allowing models to be loaded directly to GPU while keeping RAM free     this option is not compatible with any kind of model offloading as model is expected to stay in GPU     additionally, all model-moves can now be traced with env variable <code>SD_MOVE_DEBUG</code> </li> <li>xyz grid<ul> <li>range control   example: <code>5.0-6.0:3</code> will generate 3 images with values <code>5.0,5.5,6.0</code>   example: <code>10-20:4</code> will generate 4 images with values <code>10,13,16,20</code> </li> <li>continue on error   now you can use xyz grid with different params and test which ones work and which dont  </li> <li>correct font scaling, thanks @nCoderGit  </li> </ul> </li> <li>hypertile <ul> <li>enable vae tiling  </li> <li>add autodetect optimial value   set tile size to 0 to use autodetected value  </li> </ul> </li> <li>cli <ul> <li><code>sdapi.py</code> allow manual api invoke   example: <code>python cli/sdapi.py /sdapi/v1/sd-models</code> </li> <li><code>image-exif.py</code> improve metadata parsing  </li> <li><code>install-sf</code> helper script to automatically find best available stable-fast package for the platform  </li> </ul> </li> <li>memory: add ram usage monitoring in addition to gpu memory usage monitoring  </li> <li>vae: enable taesd batch decode     enable/disable with settings -&gt; diffusers &gt; vae slicing  </li> <li>compile</li> <li>new option: fused projections     pretty much free 5% performance boost for compatible models     enable in settings -&gt; compute settings  </li> <li>new option: dynamic quantization (experimental)     reduces memory usage and increases performance     enable in settings -&gt; compute settings     best used together with torch compile: inductor     this feature is highly experimental and will evolve over time     requires nightly versions of <code>torch</code> and <code>torchao</code>     &gt; <code>pip install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121</code>     &gt; <code>pip install -U git+https://github.com/pytorch-labs/ao</code> </li> <li>new option: compile text encoder (experimental)  </li> <li>correction </li> <li>new section in generate, allows for image corrections during generataion directly in latent space  </li> <li>adds brightness, sharpness and color controls, thanks @AI-Casanova</li> <li>adds color grading controls, thanks @AI-Casanova</li> <li>replaces old hdr section</li> <li>IPEX, thanks @disty0  </li> <li>see wiki for details  </li> <li>rewrite ipex hijacks without CondFunc     improves compatibilty and performance     fixes random memory leaks  </li> <li>out of the box support for Intel Data Center GPU Max Series  </li> <li>remove IPEX / Torch 2.0 specific hijacks  </li> <li>add <code>IPEX_SDPA_SLICE_TRIGGER_RATE</code>, <code>IPEX_ATTENTION_SLICE_RATE</code> and <code>IPEX_FORCE_ATTENTION_SLICE</code> env variables  </li> <li>disable 1024x1024 workaround if the GPU supports 64 bit  </li> <li>fix lock-ups at very high resolutions  </li> <li>OpenVINO, thanks @disty0  </li> <li>see wiki for details  </li> <li>quantization support with NNCF     run 8 bit directly without autocast     enable OpenVINO Quantize Models with NNCF from Compute Settings </li> <li>4-bit support with NNCF     enable Compress Model weights with NNCF from Compute Settings and set a 4-bit NNCF mode     select both CPU and GPU from the device selection if you want to use 4-bit or 8-bit modes on GPU  </li> <li>experimental support for Text Encoder compiling     OpenVINO is faster than IPEX now  </li> <li>update to OpenVINO 2023.3.0  </li> <li>add device selection to <code>Compute Settings</code>     selecting multiple devices will use <code>HETERO</code> device  </li> <li>remove <code>OPENVINO_TORCH_BACKEND_DEVICE</code> env variable  </li> <li>reduce system memory usage after compile  </li> <li>fix cache loading with multiple models  </li> <li>Olive support, thanks @lshqqytiger</li> <li>fully merged in in wiki, see wiki for details  </li> <li>as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  </li> <li>fixes </li> <li>civitai model download: enable downloads of embeddings</li> <li>ipadapter: allow changing of model/image on-the-fly  </li> <li>ipadapter: fix fallback of cross-attention on unload  </li> <li>rebasin iterations, thanks @AI-Casanova</li> <li>prompt scheduler, thanks @AI-Casanova</li> <li>python: fix python 3.9 compatibility  </li> <li>sdxl: fix positive prompt embeds</li> <li>img2img: clip and blip interrogate  </li> <li>img2img: sampler selection offset  </li> <li>img2img: support variable aspect ratio without explicit resize  </li> <li>cli: add <code>simple-upscale.py</code> script  </li> <li>cli: fix cmd args parsing  </li> <li>cli: add <code>run-benchmark.py</code> script  </li> <li>api: add <code>/sdapi/v1/version</code> endpoint</li> <li>api: add <code>/sdapi/v1/platform</code> endpoint</li> <li>api: return current image in progress api if requested  </li> <li>api: sanitize response object  </li> <li>api: cleanup error logging  </li> <li>api: fix api-only errors  </li> <li>api: fix image to base64</li> <li>api: fix upscale  </li> <li>refiner: fix use of sd15 model as refiners in second pass  </li> <li>refiner: enable none as option in xyz grid  </li> <li>sampler: add sampler options info to metadata</li> <li>sampler: guard against invalid sampler index  </li> <li>sampler: add img2img_extra_noise option</li> <li>config: reset default cfg scale to 6.0  </li> <li>hdr: fix math, thanks @AI-Casanova</li> <li>processing: correct display metadata  </li> <li>processing: fix batch file names  </li> <li>live preview: fix when using <code>bfloat16</code> </li> <li>live preview: add thread locking  </li> <li>upscale: fix ldsr</li> <li>huggingface: handle fallback model variant on load  </li> <li>reference: fix links to models and use safetensors where possible  </li> <li>model merge: unbalanced models where not all keys are present, thanks @AI-Casanova</li> <li>better sdxl model detection</li> <li>global crlf-&gt;lf switch  </li> <li>model type switch if there is loaded submodels  </li> <li>cleanup samplers use of compute devices, thanks @Disty0  </li> <li>other </li> <li>extensions <code>sd-webui-controlnet</code> is locked to commit <code>ecd33eb</code> due to breaking changes  </li> <li>extension <code>stable-diffusion-webui-images-browser</code> is locked to commit <code>27fe4a7</code> due to breaking changes  </li> <li>updated core requirements  </li> <li>fully dynamic pipelines     pipeline switch is now done on-the-fly and does not require manual initialization of individual components     this allows for quick implementation of new pipelines     see <code>modules/sd_models.py:switch_pipe</code> for details  </li> <li>major internal ui module refactoring     this may cause compatibility issues if an extension is doing a direct import from <code>ui.py</code>     in which case, report it so we can add a compatibility layer  </li> <li>major public api refactoring     this may cause compatibility issues if an extension is doing a direct import from <code>api.py</code> or <code>models.py</code>     in which case, report it so we can add a compatibility layer  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-12-29","title":"Update for 2023-12-29","text":"<p>To wrap up this amazing year, were releasing a new version of SD.Next, this one is absolutely massive!  </p>"},{"location":"CHANGELOG/#highlights-2023-12-29","title":"Highlights 2023-12-29","text":"<ul> <li>Brand new Control module for text, image, batch and video processing   Native implementation of all control methods for both SD15 and SD-XL   \u25b9 ControlNet | ControlNet XS | Control LLLite | T2I Adapters | IP Adapters   For details, see Wiki documentation:  </li> <li>Support for new models types out-of-the-box   This brings number of supported t2i/i2i model families to 13!   \u25b9 Stable Diffusion 1.5/2.1 | SD-XL | LCM | Segmind | Kandinsky | Pixart-\u03b1 | W\u00fcrstchen | aMUSEd | DeepFloyd IF | UniDiffusion | SD-Distilled | BLiP Diffusion | etc. </li> <li>New video capabilities:   \u25b9 AnimateDiff | SVD | ModelScope | ZeroScope </li> <li>Enhanced platform support   \u25b9 Windows | Linux | MacOS with nVidia | AMD | IntelArc | DirectML | OpenVINO | ONNX+Olive backends  </li> <li>Better onboarding experience (first install)   with all model types available for single click download &amp; load (networks -&gt; reference)  </li> <li>Performance optimizations!   For comparisment of different processing options and compile backends, see Wiki   As a highlight, were reaching ~100 it/s (no tricks, this is with full features enabled and end-to-end on a standard nVidia RTX4090)  </li> <li>New custom pipelines framework for quickly porting any new pipeline  </li> </ul> <p>And others improvements in areas such as: Upscaling (up to 8x now with 40+ available upscalers), Inpainting (better quality), Prompt scheduling, new Sampler options, new LoRA types, additional UI themes, better HDR processing, built-in Video interpolation, parallel Batch processing, etc.  </p> <p>Plus some nifty new modules such as FaceID automatic face guidance using embeds during generation and Depth 3D image to 3D scene</p>"},{"location":"CHANGELOG/#full-changelog-2023-12-29","title":"Full ChangeLog 2023-12-29","text":"<ul> <li>Control </li> <li>native implementation of all image control methods: ControlNet, ControlNet XS, Control LLLite, T2I Adapters and IP Adapters </li> <li>top-level Control next to Text and Image generate  </li> <li>supports all variations of SD15 and SD-XL models  </li> <li>supports Text, Image, Batch and Video processing  </li> <li>for details and list of supported models and workflows, see Wiki documentation: https://github.com/vladmandic/automatic/wiki/Control </li> <li>Diffusers </li> <li>Segmind Vega model support  <ul> <li>small and fast version of SDXL, only 3.1GB in size!  </li> <li>select from networks -&gt; reference </li> </ul> </li> <li>aMUSEd 256 and aMUSEd 512 model support  <ul> <li>lightweigt models that excel at fast image generation  </li> <li>note: must select: settings -&gt; diffusers -&gt; generator device: unset</li> <li>select from networks -&gt; reference</li> </ul> </li> <li>Playground v1, Playground v2 256, Playground v2 512, Playground v2 1024 model support  <ul> <li>comparable to SD15 and SD-XL, trained from scratch for highly aesthetic images  </li> <li>simply select from networks -&gt; reference and use as usual  </li> </ul> </li> <li>BLIP-Diffusion <ul> <li>img2img model that can replace subjects in images using prompt keywords  </li> <li>download and load by selecting from networks -&gt; reference -&gt; blip diffusion</li> <li>in image tab, select <code>blip diffusion</code> script</li> </ul> </li> <li>DemoFusion run your SDXL generations at any resolution!  <ul> <li>in Text tab select script -&gt; demofusion </li> <li>note: GPU VRAM limits do not automatically go away so be careful when using it with large resolutions   in the future, expect more optimizations, especially related to offloading/slicing/tiling,   but at the moment this is pretty much experimental-only  </li> </ul> </li> <li>AnimateDiff <ul> <li>overall improved quality  </li> <li>can now be used with second pass - enhance, upscale and hires your videos!  </li> </ul> </li> <li>IP Adapter <ul> <li>add support for ip-adapter-plus_sd15, ip-adapter-plus-face_sd15 and ip-adapter-full-face_sd15 </li> <li>can now be used in xyz-grid </li> </ul> </li> <li>Text-to-Video <ul> <li>in text tab, select <code>text-to-video</code> script  </li> <li>supported models: ModelScope v1.7b, ZeroScope v1, ZeroScope v1.1, ZeroScope v2, ZeroScope v2 Dark, Potat v1 if you know of any other t2v models youd like to see supported, let me know! </li> <li>models are auto-downloaded on first use  </li> <li>note: current base model will be unloaded to free up resources  </li> </ul> </li> <li>Prompt scheduling now implemented for Diffusers backend, thanks @AI-Casanova</li> <li>Custom pipelines contribute by adding your own custom pipelines!  <ul> <li>for details, see fully documented example: https://github.com/vladmandic/automatic/blob/dev/scripts/example.py </li> </ul> </li> <li>Schedulers <ul> <li>add timesteps range, changing it will make scheduler to be over-complete or under-complete  </li> <li>add rescale betas with zero SNR option (applicable to Euler, Euler a and DDIM, allows for higher dynamic range)  </li> </ul> </li> <li>Inpaint <ul> <li>improved quality when using mask blur and padding  </li> </ul> </li> <li>UI <ul> <li>3 new native UI themes: orchid-dreams, emerald-paradise and timeless-beige, thanks @illu_Zn</li> <li>more dynamic controls depending on the backend (original or diffusers)   controls that are not applicable in current mode are now hidden  </li> <li>allow setting of resize method directly in image tab   (previously via settings -&gt; upscaler_for_img2img)  </li> </ul> </li> <li>Optional</li> <li>FaceID face guidance during generation  <ul> <li>also based on IP adapters, but with additional face detection and external embeddings calculation  </li> <li>calculates face embeds based on input image and uses it to guide generation  </li> <li>simply select from scripts -&gt; faceid </li> <li>experimental module: requirements must be installed manually:     &gt; pip install insightface ip_adapter  </li> </ul> </li> <li>Depth 3D image to 3D scene<ul> <li>delivered as an extension, install from extensions tab https://github.com/vladmandic/sd-extension-depth3d </li> <li>creates fully compatible 3D scene from any image by using depth estimation   and creating a fully populated mesh  </li> <li>scene can be freely viewed in 3D in the UI itself or downloaded for use in other applications  </li> </ul> </li> <li>ONNX/Olive <ul> <li>major work continues in olive branch, see wiki for details, thanks @lshqqytiger   as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  </li> </ul> </li> <li>General </li> <li>new onboarding <ul> <li>if no models are found during startup, app will no longer ask to download default checkpoint   instead, it will show message in UI with options to change model path or download any of the reference checkpoints  </li> <li>extra networks -&gt; models -&gt; reference section is now enabled for both original and diffusers backend  </li> </ul> </li> <li>support for Torch 2.1.2 (release) and Torch 2.3 (dev)  </li> <li>Process create videos from batch or folder processing       supports GIF, PNG and MP4 with full interpolation, scene change detection, etc.  </li> <li>LoRA <ul> <li>add support for block weights, thanks @AI-Casanova   example <code>&lt;lora:SDXL_LCM_LoRA:1.0:in=0:mid=1:out=0&gt;</code> </li> <li>add support for LyCORIS GLora networks  </li> <li>add support for LoRA PEFT (Diffusers) networks  </li> <li>add support for Lora-OFT (Kohya) and Lyco-OFT (Kohaku) networks  </li> <li>reintroduce alternative loading method in settings: <code>lora_force_diffusers</code> </li> <li>add support for <code>lora_fuse_diffusers</code> if using alternative method   use if you have multiple complex loras that may be causing performance degradation   as it fuses lora with model during load instead of interpreting lora on-the-fly  </li> </ul> </li> <li>CivitAI downloader allow usage of access tokens for download of gated or private models  </li> <li>Extra networks new settting -&gt; extra networks -&gt; build info on first access     indexes all networks on first access instead of server startup  </li> <li>IPEX, thanks @disty0  <ul> <li>update to Torch 2.1   if you get file not found errors, set <code>DISABLE_IPEXRUN=1</code> and run the webui with <code>--reinstall</code> </li> <li>built-in MKL and DPCPP for IPEX, no need to install OneAPI anymore  </li> <li>StableVideoDiffusion is now supported with IPEX  </li> <li>8 bit support with NNCF on Diffusers backend  </li> <li>fix IPEX Optimize not applying with Diffusers backend  </li> <li>disable 32bit workarounds if the GPU supports 64bit  </li> <li>add <code>DISABLE_IPEXRUN</code> and <code>DISABLE_IPEX_1024_WA</code> environment variables  </li> <li>performance and compatibility improvements  </li> </ul> </li> <li>OpenVINO, thanks @disty0  <ul> <li>8 bit support for CPUs </li> <li>reduce System RAM usage  </li> <li>update to Torch 2.1.2  </li> <li>add Directory for OpenVINO cache option to System Paths </li> <li>remove Intel ARC specific 1024x1024 workaround  </li> </ul> </li> <li>HDR controls <ul> <li>batch-aware for enhancement of multiple images or video frames  </li> <li>available in image tab  </li> </ul> </li> <li>Logging<ul> <li>additional TRACE logging enabled via specific env variables   see https://github.com/vladmandic/automatic/wiki/Debug for details  </li> <li>improved profiling   use with <code>--debug --profile</code> </li> <li>log output file sizes  </li> </ul> </li> <li>Other <ul> <li>API several minor but breaking changes to API behavior to better align response fields, thanks @Trojaner</li> <li>Inpaint add option <code>apply_overlay</code> to control if inpaint result should be applied as overlay or as-is   can remove artifacts and hard edges of inpaint area but also remove some details from original  </li> <li>chaiNNer fix <code>NaN</code> issues due to autocast  </li> <li>Upscale increase limit from 4x to 8x given the quality of some upscalers  </li> <li>Networks fix sort  </li> <li>reduced default CFG scale from 6 to 4 to be more out-of-the-box compatibile with LCM/Turbo models</li> <li>disable google fonts check on server startup  </li> <li>fix torchvision/basicsr compatibility  </li> <li>fix styles quick save  </li> <li>add hdr settings to metadata  </li> <li>improve handling of long filenames and filenames during batch processing  </li> <li>do not set preview samples when using via api  </li> <li>avoid unnecessary resizes in img2img and inpaint  </li> <li>safe handling of config updates avoid file corruption on I/O errors  </li> <li>updated <code>cli/simple-txt2img.py</code> and <code>cli/simple-img2img.py</code> scripts  </li> <li>save <code>params.txt</code> regardless of image save status  </li> <li>update built-in log monitor in ui, thanks @midcoastal  </li> <li>major CHANGELOG doc cleanup, thanks @JetVarimax  </li> <li>major INSTALL doc cleanup, thanks JetVarimax  </li> </ul> </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-12-04","title":"Update for 2023-12-04","text":"<p>Whats new? Native video in SD.Next via both AnimateDiff and Stable-Video-Diffusion - and including native MP4 encoding and smooth video outputs out-of-the-box, not just animated-GIFs. Also new is support for SDXL-Turbo as well as new Kandinsky 3 models and cool latent correction via HDR controls for any txt2img workflows, best-of-class SDXL model merge using full ReBasin methods and further mobile UI optimizations.  </p> <ul> <li>Diffusers</li> <li>IP adapter<ul> <li>lightweight native implementation of T2I adapters which can guide generation towards specific image style  </li> <li>supports most T2I models, not limited to SD 1.5  </li> <li>models are auto-downloaded on first use</li> </ul> </li> <li>AnimateDiff<ul> <li>lightweight native implementation of AnimateDiff models: AnimateDiff 1.4, 1.5 v1, 1.5 v2, AnimateFace</li> <li>supports SD 1.5 only  </li> <li>models are auto-downloaded on first use  </li> <li>for video saving support, see video support section</li> <li>can be combined with IP-Adapter for even better results!  </li> </ul> </li> <li>HDR latent control, based on article <ul> <li>in Advanced params</li> <li>allows control of latent clamping, color centering and range maximization </li> <li>supported by XYZ grid </li> </ul> </li> <li>SD21 Turbo and SDXL Turbo support  <ul> <li>just set CFG scale (0.0-1.0) and steps (1-3) to a very low value  </li> <li>compatible with original StabilityAI SDXL-Turbo or any of the newer merges</li> <li>download safetensors or select from networks -&gt; reference</li> </ul> </li> <li>Stable Video Diffusion and Stable Video Diffusion XT support  <ul> <li>download using built-in model downloader or simply select from networks -&gt; reference   support for manually downloaded safetensors models will be added later  </li> <li>for video saving support, see video support section</li> <li>go to image tab, enter input image and select script -&gt; stable video diffusion</li> </ul> </li> <li>Kandinsky 3 support  <ul> <li>download using built-in model downloader or simply select from networks -&gt; reference </li> <li>this model is absolutely massive at 27.5GB at fp16, so be patient  </li> <li>model params count is at 11.9B (compared to SD-XL at 3.3B) and its trained on mixed resolutions from 256px to 1024px  </li> <li>use either model offload or sequential cpu offload to be able to use it  </li> </ul> </li> <li>better autodetection of inpaint and instruct pipelines  </li> <li>support long seconary prompt for refiner  </li> <li>Video support</li> <li>applies to any model that supports video generation, e.g. AnimateDiff and StableVideoDiffusion  </li> <li>support for animated-GIF, animated-PNG and MP4 </li> <li>GIF and PNG can be looped  </li> <li>MP4 can have additional padding at the start/end as well as motion-aware interpolated frames for smooth playback     interpolation is done using RIFE with native implementation in SD.Next     And its fast - interpolation from 16 frames with 10x frames to target 160 frames results takes 2-3sec</li> <li>output folder for videos is in settings -&gt; image paths -&gt; video </li> <li>General </li> <li>redesigned built-in profiler  <ul> <li>now includes both <code>python</code> and <code>torch</code> and traces individual functions  </li> <li>use with <code>--debug --profile</code> </li> </ul> </li> <li>model merge add SD-XL ReBasin support, thanks @AI-Casanova  </li> <li>further UI optimizations for mobile devices, thanks @iDeNoh  </li> <li>log level defaults to info for console and debug for log file  </li> <li>better prompt display in process tab  </li> <li>increase maximum lora cache values  </li> <li>fix extra networks sorting</li> <li>fix controlnet compatibility issues in original backend  </li> <li>fix img2img/inpaint paste params  </li> <li>fix save text file for manually saved images  </li> <li>fix python 3.9 compatibility issues  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-11-23","title":"Update for 2023-11-23","text":"<p>New release, primarily focused around three major new features: full LCM support, completely new Model Merge functionality and Stable-fast compile support Also included are several other improvements and large number of hotfixes - see full changelog for details  </p> <ul> <li>Diffusers </li> <li>LCM support for any SD 1.5 or SD-XL model!  <ul> <li>download lcm-lora-sd15 and/or lcm-lora-sdxl </li> <li>load for favorite SD 1.5 or SD-XL model (original LCM was SD 1.5 only, this is both) </li> <li>load lcm lora (note: lcm lora is processed differently than any other lora) </li> <li>set sampler to LCM </li> <li>set number of steps to some low number, for SD-XL 6-7 steps is normally sufficient   note: LCM scheduler does not support steps higher than 50  </li> <li>set CFG to between 1 and 2  </li> </ul> </li> <li>Add <code>cli/lcm-convert.py</code> script to convert any SD 1.5 or SD-XL model to LCM model     by baking in LORA and uploading to Huggingface, thanks @Disty0  </li> <li>Support for Stable Fast model compile on Windows/Linux/WSL2 with CUDA     See Wiki:Benchmark for details and comparison     of different backends, precision modes, advanced settings and compile modes Hint: 70+ it/s is possible on RTX4090 with no special tweaks  </li> <li>Add additional pipeline types for manual model loads when loading from <code>safetensors</code> </li> <li>Updated logic for calculating steps when using base/hires/refiner workflows  </li> <li>Improve model offloading for both model and sequential cpu offload when dealing with meta tensors</li> <li>Safe model offloading for non-standard models  </li> <li>Fix DPM SDE scheduler  </li> <li>Better support for SD 1.5 inpainting models  </li> <li>Add support for OpenAI Consistency decoder VAE</li> <li>Enhance prompt parsing with long prompts and support for BREAK keyword     Change-in-behavior: new line in prompt now means BREAK </li> <li>Add alternative Lora loading algorithm, triggered if <code>SD_LORA_DIFFUSERS</code> is set  </li> <li>Models</li> <li>Model merge<ul> <li>completely redesigned, now based on best-of-class <code>meh</code> by @s1dlx   and heavily modified for additional functionality and fully integrated by @AI-Casanova (thanks!)  </li> <li>merge SD or SD-XL models using simple merge (12 methods),   using one of presets (20 built-in presets) or custom block merge values  </li> <li>merge with ReBasin permutations and/or clipping protection  </li> <li>fully multithreaded for fastest merge possible  </li> </ul> </li> <li>Model update <ul> <li>under UI -&gt; Models - Update  </li> <li>scan existing models for updated metadata on CivitAI and   provide download functionality for models with available  </li> </ul> </li> <li>Extra networks </li> <li>Use multi-threading for 5x load speedup  </li> <li>Better Lora trigger words support  </li> <li>Auto refresh styles on change  </li> <li>General </li> <li>Many mobile UI optimizations, thanks @iDeNoh</li> <li>Support for Torch 2.1.1 with CUDA 12.1 or CUDA 11.8  </li> <li>Configurable location for HF cache folder     Default is standard <code>~/.cache/huggingface/hub</code> </li> <li>Reworked parser when pasting previously generated images/prompts     includes all <code>txt2img</code>, <code>img2img</code> and <code>override</code> params  </li> <li>Reworked model compile</li> <li>Support custom upscalers in subfolders  </li> <li>Add additional image info when loading image in process tab  </li> <li>Better file locking when sharing config and/or models between multiple instances  </li> <li>Handle custom API endpoints when using auth  </li> <li>Show logged in user in log when accessing via UI and/or API  </li> <li>Support <code>--ckpt none</code> to skip loading a model  </li> <li>XYZ grid</li> <li>Add refiner options to XYZ Grid  </li> <li>Add option to create only subgrids in XYZ grid, thanks @midcoastal</li> <li>Allow custom font, background and text color in settings</li> <li>Fixes </li> <li>Fix <code>params.txt</code> saved before actual image</li> <li>Fix inpaint  </li> <li>Fix manual grid image save  </li> <li>Fix img2img init image save  </li> <li>Fix upscale in txt2img for batch counts when no hires is used  </li> <li>More uniform models paths  </li> <li>Safe scripts callback execution  </li> <li>Improved extension compatibility  </li> <li>Improved BF16 support  </li> <li>Match previews for reference models with downloaded models</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-11-06","title":"Update for 2023-11-06","text":"<p>Another pretty big release, this time with focus on new models (3 new model types), new backends and optimizations Plus quite a few fixes  </p> <p>Also, Wiki has been updated with new content, so check it out! Some highlights: OpenVINO, IntelArc, DirectML, ONNX/Olive</p> <ul> <li>Diffusers</li> <li>since now SD.Next supports 12 different model types, weve added reference model for each type in Extra networks -&gt; Reference for easier select &amp; auto-download     Models can still be downloaded manually, this is just a convenience feature &amp; a showcase for supported models  </li> <li>new model type: Segmind SSD-1B     its a distilled model trained at 1024px, this time 50% smaller and faster version of SD-XL!     (and quality does not suffer, its just more optimized)     test shows batch-size:4 with 1k images at full quality used less than 6.5GB of VRAM     and for further optimization, you can use built-in TAESD decoder,     which results in batch-size:16 with 1k images using 7.9GB of VRAM     select from extra networks -&gt; reference or download using built-in Huggingface downloader: <code>segmind/SSD-1B</code> </li> <li>new model type: Pixart-\u03b1 XL 2     in medium/512px and large/1024px variations     comparable in quality to SD 1.5 and SD-XL, but with better text encoder and highly optimized training pipeline     so finetunes can be done in as little as 10% compared to SD/SD-XL (note that due to much larger text encoder, it is a large model)     select from extra networks -&gt; reference or download using built-in Huggingface downloader: <code>PixArt-alpha/PixArt-XL-2-1024-MS</code> </li> <li>new model type: LCM: Latent Consistency Models     trained at 512px, but with near-instant generate in a as little as 3 steps!     combined with OpenVINO, generate on CPU takes less than 5-10 seconds: https://www.youtube.com/watch?v=b90ESUTLsRo     and absolute beast when combined with HyperTile and TAESD decoder resulting in 28 FPS     (on RTX4090 for batch 16x16 at 512px)     note: set sampler to Default before loading model as LCM comes with its own LCMScheduler sampler     select from extra networks -&gt; reference or download using built-in Huggingface downloader: <code>SimianLuo/LCM_Dreamshaper_v7</code> </li> <li>support for Custom pipelines, thanks @disty0     download using built-in Huggingface downloader     think of them as plugins for diffusers not unlike original extensions that modify behavior of <code>ldm</code> backend     list of community pipelines: https://github.com/huggingface/diffusers/blob/main/examples/community/README.md </li> <li>new custom pipeline: <code>Disty0/zero123plus-pipeline</code>, thanks @disty0     generate 4 output images with different camera positions: front, side, top, back!     for more details, see https://github.com/vladmandic/automatic/discussions/2421 </li> <li>new backend: ONNX/Olive (experimental), thanks @lshqqytiger     for details, see WiKi</li> <li>extend support for Free-U     improve generations quality at no cost (other than finding params that work for you)  </li> <li>General </li> <li>attempt to auto-fix invalid samples which occur due to math errors in lower precision     example: <code>RuntimeWarning: invalid value encountered in cast: sample = sample.astype(np.uint8)</code>     begone black images (note: if it proves as working, this solution will need to be expanded to cover all scenarios) </li> <li>add Lora OFT support, thanks @antis0007 and @ai-casanova  </li> <li>Upscalers <ul> <li>compile option, thanks @disty0  </li> <li>chaiNNer add high quality models from Helaman </li> </ul> </li> <li>redesigned Progress bar with full details on current operation  </li> <li>new option: settings -&gt; images -&gt; keep incomplete     can be used to skip vae decode on aborted/skipped/interrupted image generations  </li> <li>new option: settings -&gt; system paths -&gt; models     can be used to set custom base path for all models (previously only as cli option)  </li> <li>remove external clone of items in <code>/repositories</code> </li> <li>Interrogator module has been removed from <code>extensions-builtin</code>     and fully implemented (and improved) natively  </li> <li>UI </li> <li>UI tweaks for default themes  </li> <li>UI switch core font in default theme to noto-sans     previously default font was simply system-ui, but it lead to too much variations between browsers and platforms  </li> <li>UI tweaks for mobile devices, thanks @iDeNoh  </li> <li>updated Context menu     right-click on any button in action menu (e.g. generate button)  </li> <li>Extra networks </li> <li>sort by name, size, date, etc.  </li> <li>switch between gallery and list views  </li> <li>add tags from user metadata (in addition to tags in model metadata) for lora </li> <li>added Reference models for diffusers backend  </li> <li>faster enumeration of all networks on server startup  </li> <li>Packages</li> <li>updated <code>diffusers</code> to 0.22.0, <code>transformers</code> to 4.34.1  </li> <li>update openvino, thanks @disty0  </li> <li>update directml, @lshqqytiger  </li> <li>Compute </li> <li>OpenVINO:  <ul> <li>updated to mainstream <code>torch</code> 2.1.0 </li> <li>support for ESRGAN upscalers  </li> </ul> </li> <li>Fixes </li> <li>fix freeu for backend original and add it to xyz grid  </li> <li>fix loading diffuser models in huggingface format from non-standard location  </li> <li>fix default styles looking in wrong location  </li> <li>fix missing upscaler folder on initial startup  </li> <li>fix handling of relative path for models  </li> <li>fix simple live preview device mismatch  </li> <li>fix batch img2img  </li> <li>fix diffusers samplers: dpm++ 2m, dpm++ 1s, deis  </li> <li>fix new style filename template  </li> <li>fix image name template using model name  </li> <li>fix image name sequence  </li> <li>fix model path using relative path  </li> <li>fix safari/webkit layour, thanks @eadnams22</li> <li>fix <code>torch-rocm</code> and <code>tensorflow-rocm</code> version detection, thanks @xangelix  </li> <li>fix chainner upscalers color clipping  </li> <li>fix for base+refiner workflow in diffusers mode: number of steps, diffuser pipe mode  </li> <li>fix for prompt encoder with refiner in diffusers mode  </li> <li>fix prompts-from-file saving incorrect metadata  </li> <li>fix add/remove extra networks to prompt</li> <li>fix before-hires step  </li> <li>fix diffusers switch from invalid model  </li> <li>force second requirements check on startup  </li> <li>remove lyco, multiple_tqdm  </li> <li>enhance extension compatibility for extensions directly importing codeformers  </li> <li>enhance extension compatibility for extensions directly accessing processing params  </li> <li>css fixes  </li> <li>clearly mark external themes in ui  </li> <li>update <code>typing-extensions</code> </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-10-17","title":"Update for 2023-10-17","text":"<p>This is a major release, with many changes and new functionality...  </p> <p>Changelog is massive, but do read through or youll be missing on some very cool new functionality or even free speedups and quality improvements (regardless of which workflows youre using)!  </p> <p>Note that for this release its recommended to perform a clean install (e.g. fresh <code>git clone</code>) Upgrades are still possible and supported, but clean install is recommended for best experience  </p> <ul> <li>UI </li> <li>added change log to UI     see System -&gt; Changelog </li> <li>converted submenus from checkboxes to accordion elements     any ui state including state of open/closed menus can be saved as default!     see System -&gt; User interface -&gt; Set menu states </li> <li>new built-in theme invoked     thanks @BinaryQuantumSoul  </li> <li>add compact view option in settings -&gt; user interface  </li> <li>small visual indicator bottom right of page showing internal server job state  </li> <li>Extra networks:  </li> <li>Details <ul> <li>new details interface to view and save data about extra networks   main ui now has a single button on each en to trigger details view  </li> <li>details view includes model/lora metadata parser!  </li> <li>details view includes civitai model metadata!  </li> </ul> </li> <li>Metadata:  <ul> <li>you can scan civitai   for missing metadata and previews directly from extra networks   simply click on button in top-right corner of extra networks page  </li> </ul> </li> <li>Styles <ul> <li>save/apply icons moved to extra networks  </li> <li>can be edited in details view  </li> <li>support for single or multiple styles per json  </li> <li>support for embedded previews  </li> <li>large database of art styles included by default   can be disabled in settings -&gt; extra networks -&gt; show built-in </li> <li>styles can also be used in a prompt directly: <code>&lt;style:style_name&gt;</code>   if style if an exact match, it will be used   otherwise it will rotate between styles that match the start of the name   that way you can use different styles as wildcards when processing batches  </li> <li>styles can have extra fields, not just prompt and negative prompt   for example: \"Extra: sampler: Euler a, width: 480, height: 640, steps: 30, cfg scale: 10, clip skip: 2\"</li> </ul> </li> <li>VAE <ul> <li>VAEs are now also listed as part of extra networks  </li> <li>Image preview methods have been redesigned: simple, approximate, taesd, full   please set desired preview method in settings  </li> <li>both original and diffusers backend now support \"full quality\" setting   if you desired model or platform does not support FP16 and/or you have a low-end hardware and cannot use FP32   you can disable \"full quality\" in advanced params and it will likely reduce decode errors (infamous black images)  </li> </ul> </li> <li>LoRA <ul> <li>LoRAs are now automatically filtered based on compatibility with currently loaded model   note that if lora type cannot be auto-determined, it will be left in the list  </li> </ul> </li> <li>Refiner <ul> <li>you can load model from extra networks as base model or as refiner   simply select button in top-right of models page  </li> </ul> </li> <li>General <ul> <li>faster search, ability to show/hide/sort networks  </li> <li>refactored subfolder handling note: this will trigger model hash recalculation on first model use  </li> </ul> </li> <li>Diffusers:  </li> <li>better pipeline auto-detect when loading from safetensors  </li> <li>SDXL Inpaint <ul> <li>although any model can be used for inpainiting, there is a case to be made for   dedicated inpainting models as they are tuned to inpaint and not generate  </li> <li>model can be used as base model for img2img or refiner model for txt2img   To download go to Models -&gt; Huggingface:  </li> <li><code>diffusers/stable-diffusion-xl-1.0-inpainting-0.1</code> (6.7GB) </li> </ul> </li> <li>SDXL Instruct-Pix2Pix <ul> <li>model can be used as base model for img2img or refiner model for txt2img   this model is massive and requires a lot of resources!   to download go to Models -&gt; Huggingface:  </li> <li><code>diffusers/sdxl-instructpix2pix-768</code> (11.9GB) </li> </ul> </li> <li>SD Latent Upscale <ul> <li>you can use SD Latent Upscale models as refiner models   this is a bit experimental, but it works quite well!   to download go to Models -&gt; Huggingface:  </li> <li><code>stabilityai/sd-x2-latent-upscaler</code> (2.2GB) </li> <li><code>stabilityai/stable-diffusion-x4-upscaler</code> (1.7GB) </li> </ul> </li> <li>better Prompt attention     should better handle more complex prompts     for sdxl, choose which part of prompt goes to second text encoder - just add <code>TE2:</code> separator in the prompt     for hires and refiner, second pass prompt is used if present, otherwise primary prompt is used     new option in settings -&gt; diffusers -&gt; sdxl pooled embeds     thanks @AI-Casanova  </li> <li>better Hires support for SD and SDXL  </li> <li>better TI embeddings support for SD and SDXL     faster loading, wider compatibility and support for embeddings with multiple vectors     information about used embedding is now also added to image metadata     thanks @AI-Casanova  </li> <li>better Lora handling     thanks @AI-Casanova  </li> <li>better SDXL preview quality (approx method)     thanks @BlueAmulet</li> <li>new setting: settings -&gt; diffusers -&gt; force inpaint     as some models behave better when in inpaint mode even for normal img2img tasks  </li> <li>Upscalers:</li> <li>pretty much a rewrite and tons of new upscalers - built-in list is now at 42 </li> <li>fix long outstanding memory leak in legacy code, amazing this went undetected for so long  </li> <li>more high quality upscalers available by default SwinIR (2), ESRGAN (12), RealESRGAN (6), SCUNet (2)  </li> <li>if that is not enough, there is new chaiNNer integration:     adds 15 more upscalers from different families out-of-the-box: HAT (6), RealHAT (2), DAT (1), RRDBNet (1), SPSRNet (1), SRFormer (2), SwiftSR (2)     and yes, you can download and add your own, just place them in <code>models/chaiNNer</code> </li> <li>two additional latent upscalers based on SD upscale models when using Diffusers backend SD Upscale 2x, SD Upscale 4x     note: Recommended usage for SD Upscale* is by using second pass instead of upscaler     as it allows for tuning of prompt, seed, sampler settings which are used to guide upscaler  </li> <li>upscalers are available in xyz grid </li> <li>simplified settings-&gt;postprocessing-&gt;upscalers     e.g. all upsamplers share same settings for tiling  </li> <li>allow upscale-only as part of txt2img and img2img workflows     simply set denoising strength to 0 so hires does not get triggered  </li> <li>unified init/download/execute/progress code  </li> <li>easier installation  </li> <li>Samplers:  </li> <li>moved ui options to submenu  </li> <li>default list for new installs is now all samplers, list can be modified in settings  </li> <li>simplified samplers configuration in settings     plus added few new ones like sigma min/max which can highly impact sampler behavior  </li> <li>note that list of samplers is now different since keeping a flat-list of all possible     combinations results in 50+ samplers which is not practical     items such as algorithm (e.g. karras) is actually a sampler option, not a sampler itself  </li> <li>CivitAI:</li> <li>civitai model download is now multithreaded and resumable     meaning that you can download multiple models in parallel     as well as resume aborted/incomplete downloads  </li> <li>civitai integration in models -&gt; civitai can now find most     previews AND metadata for most models (checkpoints, loras, embeddings)     metadata is now parsed and saved in [model].json     typical hit rate is &gt;95% for models, loras and embeddings  </li> <li>description from parsed model metadata is used as model description if there is no manual     description file present in format of [model].txt </li> <li>to enable search for models, make sure all models have set hash values Models -&gt; Valida -&gt; Calculate hashes </li> <li>LoRA</li> <li>new unified LoRA handler for all LoRA types (lora, lyco, loha, lokr, locon, ia3, etc.)     applies to both original and diffusers backend     thanks @AI-Casanova for diffusers port  </li> <li>for backend:original, separate lyco handler has been removed  </li> <li>Compute </li> <li>CUDA:  <ul> <li>default updated to <code>torch</code> 2.1.0 with cuda 12.1 </li> <li>testing moved to <code>torch</code> 2.2.0-dev/cu122 </li> <li>check out generate context menu -&gt; show nvml for live gpu stats (memory, power, temp, clock, etc.)</li> </ul> </li> <li>Intel Arc/IPEX:  <ul> <li>tons of optimizations, built-in binary wheels for Windows   i have to say, intel arc/ipex is getting to be quite a player, especially with openvino   thanks @Disty0 @Nuullll  </li> </ul> </li> <li>AMD ROCm:  <ul> <li>updated installer to support detect <code>ROCm</code> 5.4/5.5/5.6/5.7 </li> <li>support for <code>torch-rocm-5.7</code></li> </ul> </li> <li>xFormers:<ul> <li>default updated to 0.0.23 </li> <li>note that latest xformers are still not compatible with cuda 12.1   recommended to use torch 2.1.0 with cuda 11.8   if you attempt to use xformers with cuda 12.1, it will force a full xformers rebuild on install   which can take a very long time and may/may-not work  </li> <li>added cmd param <code>--use-xformers</code> to force usage of exformers  </li> </ul> </li> <li>GC:  <ul> <li>custom garbage collect threshold to reduce vram memory usage, thanks @Disty0   see settings -&gt; compute -&gt; gc </li> </ul> </li> <li>Inference </li> <li>new section in settings <ul> <li>HyperTile: new!   available for diffusers and original backends   massive (up to 2x) speed-up your generations for free :) note: hypertile is not compatible with any extension that modifies processing parameters such as resolution   thanks @tfernd</li> <li>Free-U: new!   available for diffusers and original backends   improve generations quality at no cost (other than finding params that work for you) note: temporarily disabled for diffusers pending release of diffusers==0.22   thanks @ljleb  </li> <li>Token Merging: not new, but updated   available for diffusers and original backends   speed-up your generations by merging redundant tokens   speed up will depend on how aggressive you want to be with token merging  </li> <li>Batch mode   new option settings -&gt; inference -&gt; batch mode   when using img2img process batch, optionally process multiple images in batch in parallel   thanks @Symbiomatrix</li> </ul> </li> <li>NSFW Detection/Censor </li> <li>install extension: NudeNet     body part detection, image metadata, advanced censoring, etc...     works for text, image and process workflows     more in the extension notes  </li> <li>Extensions</li> <li>automatic discovery of new extensions on github     no more waiting for them to appear in index!</li> <li>new framework for extension validation     extensions ui now shows actual status of extensions for reviewed extensions     if you want to contribute/flag/update extension status, reach out on github or discord  </li> <li>better overall compatibility with A1111 extensions (up to a point)  </li> <li>MultiDiffusion     has been removed from list of built-in extensions     you can still install it manually if desired  </li> <li>[LyCORIS]https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris     has been removed from list of built-in extensions     it is considered obsolete given that all functionality is now built-in  </li> <li>General </li> <li>Startup <ul> <li>all main CLI parameters can now be set as environment variable as well   for example <code>--data-dir &lt;path&gt;</code> can be specified as <code>SD_DATADIR=&lt;path&gt;</code> before starting SD.Next  </li> </ul> </li> <li>XYZ Grid<ul> <li>more flexibility to use selection or strings  </li> </ul> </li> <li>Logging <ul> <li>get browser session info in server log  </li> <li>allow custom log file destination   see <code>webui --log</code> </li> <li>when running with <code>--debug</code> flag, log is force-rotated   so each <code>sdnext.log.*</code> represents exactly one server run  </li> <li>internal server job state tracking  </li> </ul> </li> <li>Launcher <ul> <li>new <code>webui.ps1</code> powershell launcher for windows (old <code>webui.bat</code> is still valid)   thanks @em411  </li> </ul> </li> <li>API<ul> <li>add end-to-end example how to use API: <code>cli/simple-txt2img.js</code>   covers txt2img, upscale, hires, refiner  </li> </ul> </li> <li>train.py<ul> <li>wrapper script around built-in kohyas lora training script   see <code>cli/train.py --help</code>   new support for sd and sdxl, thanks @evshiron   new support for full offline mode (without sdnext server running)  </li> </ul> </li> <li>Themes</li> <li>all built-in themes are fully supported:  <ul> <li>black-teal (default), light-teal, black-orange, invoked, amethyst-nightfall, midnight-barbie </li> </ul> </li> <li>if youre using any gradio default themes or a 3rd party theme or  that are not optimized for SD.Next, you may experience issues     default minimal style has been updated for compatibility, but actual styling is completely outside of SD.Next control  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-09-13","title":"Update for 2023-09-13","text":"<p>Started as a mostly a service release with quite a few fixes, but then... Major changes how hires works as well as support for a very interesting new model Wuerstchen </p> <ul> <li>tons of fixes  </li> <li>changes to hires </li> <li>enable non-latent upscale modes (standard upscalers)  </li> <li>when using latent upscale, hires pass is run automatically  </li> <li>when using non-latent upscalers, hires pass is skipped by default     enabled using force hires option in ui     hires was not designed to work with standard upscalers, but i understand this is a common workflow  </li> <li>when using refiner, upscale/hires runs before refiner pass  </li> <li>second pass can now also utilize full/quick vae quality  </li> <li>note that when combining non-latent upscale, hires and refiner output quality is maximum,     but operations are really resource intensive as it includes: base-&gt;decode-&gt;upscale-&gt;encode-&gt;hires-&gt;refine</li> <li>all combinations of: decode full/quick + upscale none/latent/non-latent + hires on/off + refiner on/off     should be supported, but given the number of combinations, issues are possible  </li> <li>all operations are captured in image metadata</li> <li>diffusers:</li> <li>allow loading of sd/sdxl models from safetensors without online connectivity</li> <li>support for new model: wuerstchen     its a high-resolution model (1024px+) thats ~40% faster than sd-xl with a bit lower resource requirements     go to models -&gt; huggingface -&gt; search \"warp-ai/wuerstchen\" -&gt; download     its nearly 12gb in size, so be patient :)</li> <li>minor re-layout of the main ui  </li> <li>updated ui hints </li> <li>updated models -&gt; civitai </li> <li>search and download loras  </li> <li>find previews for already downloaded models or loras  </li> <li>new option inference mode </li> <li>default is standard <code>torch.no_grad</code>     new option is <code>torch.inference_only</code> which is slightly faster and uses less vram, but only works on some gpus  </li> <li>new cmdline param <code>--no-metadata</code>   skips reading metadata from models that are not already cached  </li> <li>updated gradio </li> <li>styles support for subfolders  </li> <li>css optimizations</li> <li>clean-up logging </li> <li>capture system info in startup log  </li> <li>better diagnostic output  </li> <li>capture extension output  </li> <li>capture ldm output  </li> <li>cleaner server restart  </li> <li>custom exception handling</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-09-06","title":"Update for 2023-09-06","text":"<p>One week later, another large update!</p> <ul> <li>system:  </li> <li>full python 3.11 support     note that changing python version does require reinstall     and if youre already on python 3.10, really no need to upgrade  </li> <li>themes:  </li> <li>new default theme: black-teal </li> <li>new light theme: light-teal </li> <li>new additional theme: midnight-barbie, thanks @nyxia  </li> <li>extra networks:  </li> <li>support for tags     show tags on hover, search by tag, list tags, add to prompt, etc.  </li> <li>styles are now also listed as part of extra networks     existing <code>styles.csv</code> is converted upon startup to individual styles inside <code>models/style</code>     this is stage one of new styles functionality     old styles interface is still available, but will be removed in future  </li> <li>cache file lists for much faster startup     speedups are 50+% for large number of extra networks  </li> <li>ui refresh button now refreshes selected page, not all pages  </li> <li>simplified handling of descriptions     now shows on-mouse-over without the need for user interaction  </li> <li>metadata and info buttons only show if there is actual content  </li> <li>diffusers:  </li> <li>add full support for textual inversions (embeddings)     this applies to both sd15 and sdxl     thanks @ai-casanova for porting compel/sdxl code  </li> <li>mix&amp;match base and refiner models (experimental):     most of those are \"because why not\" and can result in corrupt images, but some are actually useful     also note that if youre not using actual refiner model, you need to bump refiner steps     as normal models are not designed to work with low step count     and if youre having issues, try setting prompt parser to \"fixed attention\" as majority of problems     are due to token mismatches when using prompt attention  <ul> <li>any sd15 + any sd15  </li> <li>any sd15 + sdxl-refiner  </li> <li>any sdxl-base + sdxl-refiner  </li> <li>any sdxl-base + any sd15  </li> <li>any sdxl-base + any sdxl-base  </li> </ul> </li> <li>ability to interrupt (stop/skip) model generate  </li> <li>added aesthetics score setting (for sdxl)     used to automatically guide unet towards higher pleasing images     highly recommended for simple prompts  </li> <li>added force zeros setting     create zero-tensor for prompt if prompt is empty (positive or negative)  </li> <li>general:  </li> <li><code>rembg</code> remove backgrounds support for is-net model  </li> <li>settings now show markers for all items set to non-default values  </li> <li>metadata refactored how/what/when metadata is added to images     should result in much cleaner and more complete metadata  </li> <li>pre-create all system folders on startup  </li> <li>handle model load errors gracefully  </li> <li>improved vram reporting in ui  </li> <li>improved script profiling (when running in debug mode)  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-08-30","title":"Update for 2023-08-30","text":"<p>Time for a quite a large update that has been leaking bit-by-bit over the past week or so... Note: due to large changes, it is recommended to reset (delete) your <code>ui-config.json</code> </p> <ul> <li>diffusers:  </li> <li>support for distilled sd models     just go to models/huggingface and download a model, for example: <code>segmind/tiny-sd</code>, <code>segmind/small-sd</code>, <code>segmind/portrait-finetuned</code>     those are lower quality, but extremely small and fast     up to 50% faster than sd 1.5 and execute in as little as 2.1gb of vram  </li> <li>general:  </li> <li>redesigned settings <ul> <li>new layout with separated sections: settings, ui config, licenses, system info, benchmark, models </li> <li>system info tab is now part of settings   when running outside of sdnext, system info is shown in main ui  </li> <li>all system and image paths are now relative by default  </li> <li>add settings validation when performing load/save  </li> <li>settings tab in ui now shows settings that are changed from default values  </li> <li>settings tab switch to compact view  </li> </ul> </li> <li>update gradio major version     this may result in some smaller layout changes since its a major version change     however, browser page load is now much faster  </li> <li>optimizations:<ul> <li>optimize model hashing  </li> <li>add cli param <code>--skip-all</code> that skips all installer checks   use at personal discretion, but it can be useful for bulk deployments  </li> <li>add model precompile option (when model compile is enabled)  </li> <li>extra network folder info caching   results in much faster startup when you have large number of extra networks  </li> <li>faster xyz grid switching   especially when using different checkpoints  </li> </ul> </li> <li>update second pass options for clarity</li> <li>models:<ul> <li>civitai download missing model previews</li> </ul> </li> <li>add openvino (experimental) cpu optimized model compile and inference     enable with <code>--use-openvino</code>     thanks @disty0  </li> <li>enable batch img2img scale-by workflows     now you can batch process with rescaling based on each individual original image size  </li> <li>fixes:<ul> <li>fix extra networks previews  </li> <li>css fixes  </li> <li>improved extensions compatibility (e.g. sd-cn-animation)  </li> <li>allow changing vae on-the-fly for both original and diffusers backend</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-08-20","title":"Update for 2023-08-20","text":"<p>Another release thats been baking in dev branch for a while...</p> <ul> <li>general:</li> <li>caching of extra network information to enable much faster create/refresh operations     thanks @midcoastal</li> <li>diffusers:</li> <li>add hires support (experimental)     applies to all model types that support img2img, including sd and sd-xl     also supports all hires upscaler types as well as standard params like steps and denoising strength     when used with sd-xl, it can be used with or without refiner loaded     how to enable - there are no explicit checkboxes other than second pass itself:<ul> <li>hires: upscaler is set and target resolution is not at default  </li> <li>refiner: if refiner model is loaded  </li> </ul> </li> <li>images save options: before hires, before refiner</li> <li>redo <code>move model to cpu</code> logic in settings -&gt; diffusers to be more reliable     note that system defaults have also changed, so you may need to tweak to your liking  </li> <li>update dependencies</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-08-17","title":"Update for 2023-08-17","text":"<p>Smaller update, but with some breaking changes (to prepare for future larger functionality)...</p> <ul> <li>general:</li> <li>update all metadata saved with images     see https://github.com/vladmandic/automatic/wiki/Metadata for details  </li> <li>improved amd installer with support for navi 2x &amp; 3x and rocm 5.4/5.5/5.6     thanks @evshiron  </li> <li>fix img2img resizing (applies to original, diffusers, hires)  </li> <li>config change: main <code>config.json</code> no longer contains entire configuration     but only differences from defaults (similar to recent change performed to <code>ui-config.json</code>)  </li> <li>diffusers:</li> <li>enable batch img2img workflows  </li> <li>original:  </li> <li>new samplers: dpm++ 3M sde (standard and karras variations)     enable in settings -&gt; samplers -&gt; show samplers</li> <li>expose always/never discard penultimate sigma     enable in settings -&gt; samplers </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-08-11","title":"Update for 2023-08-11","text":"<p>This is a big one thats been cooking in <code>dev</code> for a while now, but finally ready for release...</p> <ul> <li>diffusers:</li> <li>pipeline autodetect     if pipeline is set to autodetect (default for new installs), app will try to autodetect pipeline based on selected model     this should reduce user errors such as loading sd-xl model when sd pipeline is selected  </li> <li>quick vae decode as alternative to full vae decode which is very resource intensive     quick decode is based on <code>taesd</code> and produces lower quality, but its great for tests or grids as it runs much faster and uses far less vram     disabled by default, selectable in txt2img/img2img -&gt; advanced -&gt; full quality </li> <li>prompt attention for sd and sd-xl     supports both <code>full parser</code> and native <code>compel</code>     thanks @ai-casanova  </li> <li>advanced lora load/apply methods     in addition to standard lora loading that was recently added to sd-xl using diffusers, now we have  <ul> <li>sequential apply (load &amp; apply multiple loras in sequential manner) and  </li> <li>merge and apply (load multiple loras and merge before applying to model) see settings -&gt; diffusers -&gt; lora methods thanks @hameerabbasi and @ai-casanova  </li> </ul> </li> <li>sd-xl vae from safetensors now applies correct config     result is that 3rd party vaes can be used without washed out colors  </li> <li>options for optimized memory handling for lower memory usage     see settings -&gt; diffusers</li> <li>general:</li> <li>new civitai model search and download     native support for civitai, integrated into ui as models -&gt; civitai </li> <li>updated requirements     this time its a bigger change so upgrade may take longer to install new requirements</li> <li>improved extra networks performance with large number of networks</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-08-05","title":"Update for 2023-08-05","text":"<p>Another minor update, but it unlocks some cool new items...</p> <ul> <li>diffusers:</li> <li>vaesd live preview (sd and sd-xl)  </li> <li>fix inpainting (sd and sd-xl)  </li> <li>general:</li> <li>new torch 2.0 with ipex (intel arc)  </li> <li>additional callbacks for extensions     enables latest comfyui extension  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-30","title":"Update for 2023-07-30","text":"<p>Smaller release, but IMO worth a post...</p> <ul> <li>diffusers:</li> <li>sd-xl loras are now supported!</li> <li>memory optimizations: Enhanced sequential CPU offloading, model CPU offload, FP16 VAE<ul> <li>significant impact if running SD-XL (for example, but applies to any model) with only 8GB VRAM</li> </ul> </li> <li>update packages</li> <li>minor bugfixes</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-26","title":"Update for 2023-07-26","text":"<p>This is a big one, new models, new diffusers, new features and updated UI...</p> <p>First, SD-XL 1.0 is released and yes, SD.Next supports it out of the box!</p> <ul> <li>SD-XL Base</li> <li>SD-XL Refiner</li> </ul> <p>Also fresh is new Kandinsky 2.2 model that does look quite nice:</p> <ul> <li>Kandinsky Decoder</li> <li>Kandinsky Prior</li> </ul> <p>Actual changelog is:</p> <ul> <li>general:</li> <li>new loading screens and artwork</li> <li>major ui simplification for both txt2img and img2img     nothing is removed, but you can show/hide individual sections     default is very simple interface, but you can enable any sections and save it as default in settings  </li> <li>themes: add additional built-in theme, <code>amethyst-nightfall</code></li> <li>extra networks: add add/remove tags to prompt (e.g. lora activation keywords)</li> <li>extensions: fix couple of compatibility items</li> <li>firefox compatibility improvements</li> <li>minor image viewer improvements</li> <li> <p>add backend and operation info to metadata</p> </li> <li> <p>diffusers:</p> </li> <li>were out of experimental phase and diffusers backend is considered stable  </li> <li>sd-xl: support for sd-xl 1.0 official model</li> <li>sd-xl: loading vae now applies to both base and refiner and saves a bit of vram  </li> <li>sd-xl: denoising_start/denoising_end</li> <li>sd-xl: enable dual prompts     dual prompt is used if set regardless if refiner is enabled/loaded     if refiner is loaded &amp; enabled, refiner prompt will also be used for refiner pass  <ul> <li>primary prompt goes to OpenAI CLIP-ViT/L-14</li> <li>refiner prompt goes to OpenCLIP-ViT/bigG-14</li> </ul> </li> <li>kandinsky 2.2 support     note: kandinsky model must be downloaded using model downloader, not as safetensors due to specific model format  </li> <li>refiner: fix batch processing</li> <li>vae: enable loading of pure-safetensors vae files without config     also enable automatic selection to work with diffusers  </li> <li>sd-xl: initial lora support     right now this applies to official lora released by stability-ai, support for kohyas lora is expected soon  </li> <li>implement img2img and inpainting (experimental)     actual support and quality depends on model     it works as expected for sd 1.5, but not so much for sd-xl for now  </li> <li>implement limited stop/interrupt for diffusers     works between stages, not within steps  </li> <li>add option to save image before refiner pass  </li> <li>option to set vae upcast in settings  </li> <li> <p>enable fp16 vae decode when using optimized vae     this pretty much doubles performance of decode step (delay after generate is done)  </p> </li> <li> <p>original</p> </li> <li>fix hires secondary sampler     this now fully obsoletes <code>fallback_sampler</code> and <code>force_hr_sampler_name</code> </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-18","title":"Update for 2023-07-18","text":"<p>While were waiting for official SD-XL release, heres another update with some fixes and enhancements...</p> <ul> <li>global</li> <li>image save: option to add invisible image watermark to all your generated images     disabled by default, can be enabled in settings -&gt; image options     watermark information will be shown when loading image such as in process image tab     also additional cli utility <code>/cli/image-watermark.py</code> to read/write/strip watermarks from images  </li> <li>batch processing: fix metadata saving, also allow to drag&amp;drop images for batch processing  </li> <li>ui configuration: you can modify all ui default values from settings as usual,     but only values that are non-default will be written to <code>ui-config.json</code> </li> <li>startup: add cmd flag to skip all <code>torch</code> checks  </li> <li>startup: force requirements check on each server start     there are too many misbehaving extensions that change system requirements  </li> <li>internal: safe handling of all config file read/write operations     this allows sdnext to run in fully shared environments and prevents any possible configuration corruptions  </li> <li>diffusers:</li> <li>sd-xl: remove image watermarks autocreated by 0.9 model  </li> <li>vae: enable loading of external vae, documented in diffusers wiki     and mix&amp;match continues, you can even use sd-xl vae with sd 1.5 models!  </li> <li>samplers: add concept of default sampler to avoid needing to tweak settings for primary or second pass     note that sampler details will be printed in log when running in debug level  </li> <li>samplers: allow overriding of sampler beta values in settings  </li> <li>refiner: fix refiner applying only to first image in batch  </li> <li>refiner: allow using direct latents or processed output in refiner  </li> <li>model: basic support for one more model: UniDiffuser     download using model downloader: <code>thu-ml/unidiffuser-v1</code>     and set resolution to 512x512  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-14","title":"Update for 2023-07-14","text":"<p>Trying to unify settings for both original and diffusers backend without introducing duplicates...</p> <ul> <li>renamed hires fix to second pass   as that is what it actually is, name hires fix is misleading to start with  </li> <li>actual hires fix and refiner are now options inside second pass section  </li> <li>obsoleted settings -&gt; sampler -&gt; force_hr_sampler_name   it is now part of second pass options and it works the same for both original and diffusers backend   which means you can use different scheduler settings for txt2img and hires if you want  </li> <li>sd-xl refiner will run if its loaded and if second pass is enabled   so you can quickly enable/disable refiner by simply enabling/disabling second pass  </li> <li>you can mix&amp;match model and refiner   for example, you can generate image using sd 1.5 and still use sd-xl refiner as second pass  </li> <li>reorganized settings -&gt; samplers to show which section refers to which backend  </li> <li>added diffusers lmsd sampler  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-13","title":"Update for 2023-07-13","text":"<p>Another big one, but now improvements to both diffusers and original backends as well plus ability to dynamically switch between them!</p> <ul> <li>swich backend between diffusers and original on-the-fly</li> <li>you can still use <code>--backend &lt;backend&gt;</code> and now that only means in which mode app will start,     but you can change it anytime in ui settings</li> <li>for example, you can even do things like generate image using sd-xl,     then switch to original backend and perform inpaint using a different model  </li> <li>diffusers backend:</li> <li>separate ui settings for refiner pass with sd-xl     you can specify: prompt, negative prompt, steps, denoise start  </li> <li>fix loading from pure safetensors files     now you can load sd-xl from safetensors file or from huggingface folder format  </li> <li>fix kandinsky model (2.1 working, 2.2 was just released and will be soon)  </li> <li>original backend:</li> <li>improvements to vae/unet handling as well as cross-optimization heads     in non-technical terms, this means lower memory usage and higher performance     and you should be able to generate higher resolution images without any other changes</li> <li>other:</li> <li>major refactoring of the javascript code     includes fixes for text selections and navigation  </li> <li>system info tab now reports on nvidia driver version as well  </li> <li>minor fixes in extra-networks  </li> <li>installer handles origin changes for submodules  </li> </ul> <p>big thanks to @huggingface team for great communication, support and fixing all the reported issues asap!</p>"},{"location":"CHANGELOG/#update-for-2023-07-10","title":"Update for 2023-07-10","text":"<p>Service release with some fixes and enhancements:</p> <ul> <li>diffusers:</li> <li>option to move base and/or refiner model to cpu to free up vram  </li> <li>model downloader options to specify model variant / revision / mirror  </li> <li>now you can download <code>fp16</code> variant directly for reduced memory footprint  </li> <li>basic img2img workflow (sketch and inpaint are not supported yet)     note that sd-xl img2img workflows are architecturaly different so it will take longer to implement  </li> <li>updated hints for settings  </li> <li>extra networks:</li> <li>fix corrupt display on refesh when new extra network type found  </li> <li>additional ui tweaks  </li> <li>generate thumbnails from previews only if preview resolution is above 1k</li> <li>image viewer:</li> <li>fixes for non-chromium browsers and mobile users and add option to download image  </li> <li>option to download image directly from image viewer</li> <li>general</li> <li>fix startup issue with incorrect config  </li> <li>installer should always check requirements on upgrades</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-08","title":"Update for 2023-07-08","text":"<p>This is a massive update which has been baking in a <code>dev</code> branch for a while now</p> <ul> <li>merge experimental diffusers support  </li> </ul> <p>TL;DR: Yes, you can run SD-XL model in SD.Next now For details, see Wiki page: Diffusers Note this is still experimental, so please follow Wiki Additional enhancements and fixes will be provided over the next few days Thanks to @huggingface team for making this possible and our internal @team for all the early testing</p> <p>Release also contains number of smaller updates:</p> <ul> <li>add pan &amp; zoom controls (touch and mouse) to image viewer (lightbox)  </li> <li>cache extra networks between tabs   this should result in neat 2x speedup on building extra networks  </li> <li>add settings -&gt; extra networks -&gt; do not automatically build extra network pages   speeds up app start if you have a lot of extra networks and you want to build them manually when needed  </li> <li>extra network ui tweaks  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-07-01","title":"Update for 2023-07-01","text":"<p>Small quality-of-life updates and bugfixes:</p> <ul> <li>add option to disallow usage of ckpt checkpoints</li> <li>change lora and lyco dir without server restart</li> <li>additional filename template fields: <code>uuid</code>, <code>seq</code>, <code>image_hash</code> </li> <li>image toolbar is now shown only when image is present</li> <li>image <code>Zip</code> button gone and its not optional setting that applies to standard <code>Save</code> button</li> <li>folder <code>Show</code> button is present only when working on localhost,   otherwise its replaced with <code>Copy</code> that places image URLs on clipboard so they can be used in other apps</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-30","title":"Update for 2023-06-30","text":"<p>A bit bigger update this time, but contained to specific areas...</p> <ul> <li>change in behavior   extensions no longer auto-update on startup   using <code>--upgrade</code> flag upgrades core app as well as all submodules and extensions  </li> <li>live server log monitoring in ui   configurable via settings -&gt; live preview  </li> <li>new extra networks interface note: if youre using a 3rd party ui extension for extra networks, it will likely need to be updated to work with new interface</li> <li>display in front of main ui, inline with main ui or as a sidebar  </li> <li>lazy load thumbnails     drastically reduces load times for large number of extra networks  </li> <li>auto-create thumbnails from preview images in extra networks in a background thread     significant load time saving on subsequent restarts  </li> <li>support for info files in addition to description files  </li> <li>support for variable aspect-ratio thumbnails  </li> <li>new folder view  </li> <li>extensions sort by trending  </li> <li>add requirements check for training  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-26","title":"Update for 2023-06-26","text":"<ul> <li>new training tab interface  </li> <li>redesigned preprocess, train embedding, train hypernetwork  </li> <li>new models tab interface  </li> <li>new model convert functionality, thanks @akegarasu  </li> <li>new model verify functionality  </li> <li>lot of ipex specific fixes/optimizations, thanks @disty0  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-20","title":"Update for 2023-06-20","text":"<p>This one is less relevant for standard users, but pretty major if youre running an actual server But even if not, it still includes bunch of cumulative fixes since last release - and going by number of new issues, this is probably the most stable release so far... (next one is not going to be as stable, but it will be fun :) )</p> <ul> <li>minor improvements to extra networks ui  </li> <li>more hints/tooltips integrated into ui  </li> <li>new dedicated api server  </li> <li>but highly promising for high throughput server  </li> <li>improve server logging and monitoring with  </li> <li>server log file rotation  </li> <li>ring buffer with api endpoint <code>/sdapi/v1/log</code> </li> <li>real-time status and load endpoint <code>/sdapi/v1/system-info/status</code></li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-14","title":"Update for 2023-06-14","text":"<p>Second stage of a jumbo merge from upstream plus few minor changes...</p> <ul> <li>simplify token merging  </li> <li>reorganize some settings  </li> <li>all updates from upstream: A1111 v1.3.2 [df004be] (latest release)   pretty much nothing major that i havent released in previous versions, but its still a long list of tiny changes  </li> <li>skipped/did-not-port:     add separate hires prompt: unnecessarily complicated and spread over large number of commits due to many regressions     allow external scripts to add cross-optimization methods: dangerous and i dont see a use case for it so far     load extension info in threads: unnecessary as other optimizations ive already put place perform equally good  </li> <li>broken/reverted:     sub-quadratic optimization changes  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-13","title":"Update for 2023-06-13","text":"<p>Just a day later and one bigger update... Both some new functionality as well as massive merges from upstream  </p> <ul> <li>new cache for models/lora/lyco metadata: <code>metadata.json</code>   drastically reduces disk access on app startup  </li> <li>allow saving/resetting of ui default values   settings -&gt; ui defaults</li> <li>ability to run server without loaded model   default is to auto-load model on startup, can be changed in settings -&gt; stable diffusion   if disabled, model will be loaded on first request, e.g. when you click generate   useful when you want to start server to perform other tasks like upscaling which do not rely on model  </li> <li>updated <code>accelerate</code> and <code>xformers</code></li> <li>huge nubmer of changes ported from A1111 upstream   this was a massive merge, hopefully this does not cause any regressions   and still a bit more pending...</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-12","title":"Update for 2023-06-12","text":"<ul> <li>updated ui labels and hints to improve clarity and provide some extra info   this is 1st stage of the process, more to come...   if you want to join the effort, see https://github.com/vladmandic/automatic/discussions/1246</li> <li>new localization and hints engine   how hints are displayed can be selected in settings -&gt; ui  </li> <li>reworked installer sequence   as some extensions are loading packages directly from their preload sequence   which was preventing some optimizations to take effect  </li> <li>updated settings tab functionality, thanks @gegell   with real-time monitor for all new and/or updated settings  </li> <li>launcher will now warn if application owned files are modified   you are free to add any user files, but do not modify app files unless youre sure in what youre doing  </li> <li>add more profiling for scripts/extensions so you can see what takes time   this applies both to initial load as well as execution  </li> <li>experimental <code>sd_model_dict</code> setting which allows you to load model dictionary   from one model and apply weights from another model specified in <code>sd_model_checkpoint</code>   results? who am i to judge :)</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-05","title":"Update for 2023-06-05","text":"<p>Few new features and extra handling for broken extensions that caused my phone to go crazy with notifications over the weekend...</p> <ul> <li>added extra networks to xyz grid options   now you can have more fun with all your embeddings and loras :)  </li> <li>new vae decode method to help with larger batch sizes, thanks @bigdog  </li> <li>new setting -&gt; lora -&gt; use lycoris to handle all lora types   this is still experimental, but the goal is to obsolete old built-in lora module   as it doesnt understand many new loras and built-in lyco module can handle it all  </li> <li>somewhat optimize browser page loading   still slower than id want, but gradio is pretty bad at this  </li> <li>profiling of scripts/extensions callbacks   you can now see how much or pre/post processing is done, not just how long generate takes  </li> <li>additional exception handling so bad exception does not crash main app  </li> <li>additional background removal models  </li> <li>some work on bfloat16 which nobody really should be using, but why not \ud83d\ude42</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-06-02","title":"Update for 2023-06-02","text":"<p>Some quality-of-life improvements while working on larger stuff in the background...</p> <ul> <li>redesign action box to be uniform across all themes  </li> <li>add pause option next to stop/skip  </li> <li>redesigned progress bar  </li> <li>add new built-in extension: agent-scheduler   very elegant way to getting full queueing capabilities, thank @artventurdev  </li> <li>enable more image formats   note: not all are understood by browser so previews and images may appear as blank   unless you have some browser extensions that can handle them   but they are saved correctly. and cant beat raw quality of 32-bit <code>tiff</code> or <code>psd</code> :)  </li> <li>change in behavior: <code>xformers</code> will be uninstalled on startup if they are not active   if you do have <code>xformers</code> selected as your desired cross-optimization method, then they will be used   reason is that a lot of libaries try to blindly import xformers even if they are not selected or not functional  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-30","title":"Update for 2023-05-30","text":"<p>Another bigger one...And more to come in the next few days...</p> <ul> <li>new live preview mode: taesd   i really like this one, so its enabled as default for new installs  </li> <li>settings search feature  </li> <li>new sampler: dpm++ 2m sde  </li> <li>fully common save/zip/delete (new) options in all tabs   which (again) meant rework of process image tab  </li> <li>system info tab: live gpu utilization/memory graphs for nvidia gpus  </li> <li>updated controlnet interface  </li> <li>minor style changes  </li> <li>updated lora, swinir, scunet and ldsr code from upstream  </li> <li>start of merge from a1111 v1.3  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-26","title":"Update for 2023-05-26","text":"<p>Some quality-of-life improvements...</p> <ul> <li>updated README</li> <li>created CHANGELOG   this will be the source for all info about new things moving forward   and cross-posted to Discussions#99 as well as discord announcements</li> <li>optimize model loading on startup   this should reduce startup time significantly  </li> <li>set default cross-optimization method for each platform backend   applicable for new installs only  </li> <li><code>cuda</code> =&gt; Scaled-Dot-Product</li> <li><code>rocm</code> =&gt; Sub-quadratic</li> <li><code>directml</code> =&gt; Sub-quadratic</li> <li><code>ipex</code> =&gt; invokeais</li> <li><code>mps</code> =&gt; Doggettxs</li> <li><code>cpu</code> =&gt; Doggettxs</li> <li>optimize logging  </li> <li>optimize profiling   now includes startup profiling as well as <code>cuda</code> profiling during generate  </li> <li>minor lightbox improvements  </li> <li>bugfixes...i dont recall when was a release with at least several of those  </li> </ul> <p>other than that - first stage of Diffusers integration is now in master branch i dont recommend anyone to try it (and dont even think reporting issues for it) but if anyone wants to contribute, take a look at project page</p>"},{"location":"CHANGELOG/#update-for-2023-05-23","title":"Update for 2023-05-23","text":"<p>Major internal work with perhaps not that much user-facing to show for it ;)</p> <ul> <li>update core repos: stability-ai, taming-transformers, k-diffusion, blip, codeformer   note: to avoid disruptions, this is applicable for new installs only</li> <li>tested with torch 2.1, cuda 12.1, cudnn 8.9   (production remains on torch2.0.1+cuda11.8+cudnn8.8)  </li> <li>fully extend support of <code>--data-dir</code>   allows multiple installations to share pretty much everything, not just models   especially useful if you want to run in a stateless container or cloud instance  </li> <li>redo api authentication   now api authentication will use same user/pwd (if specified) for ui and strictly enforce it using httpbasicauth   new authentication is also fully supported in combination with ssl for both sync and async calls   if you want to use api programatically, see examples in <code>cli/sdapi.py</code> </li> <li>add dark/light theme mode toggle  </li> <li>redo some <code>clip-skip</code> functionality  </li> <li>better matching for vae vs model  </li> <li>update to <code>xyz grid</code> to allow creation of large number of images without creating grid itself  </li> <li>update <code>gradio</code> (again)  </li> <li>more prompt parser optimizations  </li> <li>better error handling when importing image settings which are not compatible with current install   for example, when upscaler or sampler originally used is not available  </li> <li>fixes...amazing how many issues were introduced by porting a1111 v1.20 code without adding almost no new functionality   next one is v1.30 (still in dev) which does bring a lot of new features  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-17","title":"Update for 2023-05-17","text":"<p>This is a massive one due to huge number of changes, but hopefully it will go ok...</p> <ul> <li>new prompt parsers   select in UI -&gt; Settings -&gt; Stable Diffusion  </li> <li>Full: my new implementation  </li> <li>A1111: for backward compatibility  </li> <li>Compel: as used in ComfyUI and InvokeAI (a.k.a Temporal Weighting)  </li> <li>Fixed: for really old backward compatibility  </li> <li>monitor extensions install/startup and   log if they modify any packages/requirements   this is a deep-experimental python hack, but i think its worth it as extensions modifying requirements   is one of most common causes of issues</li> <li>added <code>--safe</code> command line flag mode which skips loading user extensions   please try to use it before opening new issue  </li> <li>reintroduce <code>--api-only</code> mode to start server without ui  </li> <li>port all upstream changes from A1111   up to today - commit hash <code>89f9faa</code> </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-15","title":"Update for 2023-05-15","text":"<ul> <li>major work on prompt parsing   this can cause some differences in results compared to what youre used to, but its all about fixes &amp; improvements</li> <li>prompt parser was adding commas and spaces as separate words and tokens and/or prefixes</li> <li>negative prompt weight using <code>[word:weight]</code> was ignored, it was always <code>0.909</code></li> <li>bracket matching was anything but correct. complex nested attention brackets are now working.</li> <li>btw, if you run with <code>--debug</code> flag, youll now actually see parsed prompt &amp; schedule</li> <li>updated all scripts in <code>/cli</code> </li> <li>add option in settings to force different latent sampler instead of using primary only</li> <li>add interrupt/skip capabilities to process images</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-13","title":"Update for 2023-05-13","text":"<p>This is mostly about optimizations...</p> <ul> <li>improved <code>torch-directml</code> support   especially interesting for amd users on windows  where torch+rocm is not yet available   dont forget to run using <code>--use-directml</code> or default is cpu </li> <li>improved compatibility with nvidia rtx 1xxx/2xxx series gpus  </li> <li>fully working <code>torch.compile</code> with torch 2.0.1   using <code>inductor</code> compile takes a while on first run, but does result in 5-10% performance increase  </li> <li>improved memory handling   for highest performance, you can also disable aggressive gc in settings  </li> <li>improved performance   especially after generate as image handling has been moved to separate thread  </li> <li>allow per-extension updates in extension manager  </li> <li>option to reset configuration in settings  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-11","title":"Update for 2023-05-11","text":"<ul> <li>brand new extension manager   this is pretty much a complete rewrite, so new issues are possible</li> <li>support for <code>torch</code> 2.0.1   note that if you are experiencing frequent hangs, this may be a worth a try  </li> <li>updated <code>gradio</code> to 3.29.0</li> <li>added <code>--reinstall</code> flag to force reinstall of all packages  </li> <li>auto-recover &amp; re-attempt when <code>--upgrade</code> is requested but fails</li> <li>check for duplicate extensions  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-08","title":"Update for 2023-05-08","text":"<p>Back online with few updates:</p> <ul> <li>bugfixes. yup, quite a lot of those  </li> <li>auto-detect some cpu/gpu capabilities on startup   this should reduce need to tweak and tune settings like no-half, no-half-vae, fp16 vs fp32, etc  </li> <li>configurable order of top level tabs  </li> <li>configurable order of scripts in txt2img and img2img   for both, see sections in ui-&gt; settings -&gt; user interface</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-04","title":"Update for 2023-05-04","text":"<p>Again, few days later...</p> <ul> <li>reviewed/ported all commits from A1111 upstream   some a few are not applicable as i already have alternative implementations   and very few i choose not to implement (save/restore last-known-good-config is a bad hack)   otherwise, were fully up to date (it doesnt show on fork status as code merges were mostly manual due to conflicts)   but...due to sheer size of the updates, this may introduce some temporary issues  </li> <li>redesigned server restart function   now available and working in ui   actually, since server restart is now a true restart and not ui restart, it can be used much more flexibly  </li> <li>faster model load   plus support for slower devices via stream-load function (in ui settings)  </li> <li>better logging   this includes new <code>--debug</code> flag for more verbose logging when troubleshooting  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-05-01","title":"Update for 2023-05-01","text":"<p>Been a bit quieter for last few days as changes were quite significant, but finally here we are...</p> <ul> <li>Updated core libraries: Gradio, Diffusers, Transformers</li> <li>Added support for Intel ARC GPUs via Intel OneAPI IPEX (auto-detected)</li> <li>Added support for TorchML (set by default when running on non-compatible GPU or on CPU)</li> <li>Enhanced support for AMD GPUs with ROCm</li> <li>Enhanced support for Apple M1/M2</li> <li>Redesigned command params: run <code>webui --help</code> for details</li> <li>Redesigned API and script processing</li> <li>Experimental support for multiple Torch compile options</li> <li>Improved sampler support</li> <li>Google Colab: https://colab.research.google.com/drive/126cDNwHfifCyUpCCQF9IHpEdiXRfHrLN   Maintained by https://github.com/Linaqruf/sd-notebook-collection</li> <li>Fixes, fixes, fixes...</li> </ul> <p>To take advantage of new out-of-the-box tunings, its recommended to delete your <code>config.json</code> so new defaults are applied. its not necessary, but otherwise you may need to play with UI Settings to get the best of Intel ARC, TorchML, ROCm or Apple M1/M2.</p>"},{"location":"CHANGELOG/#update-for-2023-04-27","title":"Update for 2023-04-27","text":"<p>a bit shorter list as:</p> <ul> <li>ive been busy with bugfixing   there are a lot of them, not going to list each here.   but seems like critical issues backlog is quieting down and soon i can focus on new features development.  </li> <li>ive started collaboration with couple of major projects,   hopefully this will accelerate future development.</li> </ul> <p>whats new:</p> <ul> <li>ability to view/add/edit model description shown in extra networks cards  </li> <li>add option to specify fallback sampler if primary sampler is not compatible with desired operation  </li> <li>make clip skip a local parameter  </li> <li>remove obsolete items from UI settings  </li> <li>set defaults for AMD ROCm   if you have issues, you may want to start with a fresh install so configuration can be created from scratch</li> <li>set defaults for Apple M1/M2   if you have issues, you may want to start with a fresh install so configuration can be created from scratch</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-25","title":"Update for 2023-04-25","text":"<ul> <li>update process image -&gt; info</li> <li>add VAE info to metadata</li> <li>update GPU utility search paths for better GPU type detection</li> <li>update git flags for wider compatibility</li> <li>update environment tuning</li> <li>update ti training defaults</li> <li>update VAE search paths</li> <li>add compatibility opts for some old extensions</li> <li>validate script args for always-on scripts   fixes: deforum with controlnet  </li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-24","title":"Update for 2023-04-24","text":"<ul> <li>identify race condition where generate locks up while fetching preview</li> <li>add pulldowns to x/y/z script</li> <li>add VAE rollback feature in case of NaNs</li> <li>use samples format for live preview</li> <li>add token merging</li> <li>use Approx NN for live preview</li> <li>create default <code>styles.csv</code></li> <li>fix setup not installing <code>tensorflow</code> dependencies</li> <li>update default git flags to reduce number of warnings</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-23","title":"Update for 2023-04-23","text":"<ul> <li>fix VAE dtype   should fix most issues with NaN or black images  </li> <li>add built-in Gradio themes  </li> <li>reduce requirements  </li> <li>more AMD specific work</li> <li>initial work on Apple platform support</li> <li>additional PR merges</li> <li>handle torch cuda crashing in setup</li> <li>fix setup race conditions</li> <li>fix ui lightbox</li> <li>mark tensorflow as optional</li> <li>add additional image name templates</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-22","title":"Update for 2023-04-22","text":"<ul> <li>autodetect which system libs should be installed   this is a first pass of autoconfig for nVidia vs AMD environments  </li> <li>fix parse cmd line args from extensions  </li> <li>only install <code>xformers</code> if actually selected as desired cross-attention method</li> <li>do not attempt to use <code>xformers</code> or <code>sdp</code> if running on cpu</li> <li>merge tomesd token merging  </li> <li>merge 23 PRs pending from a1111 backlog (!!)</li> </ul> <p>expect shorter updates for the next few days as ill be partially ooo</p>"},{"location":"CHANGELOG/#update-for-2023-04-20","title":"Update for 2023-04-20","text":"<ul> <li>full CUDA tuning section in UI Settings</li> <li>improve exif/pnginfo metadata parsing   it can now handle 3rd party images or images edited in external software</li> <li>optimized setup performance and logging</li> <li>improve compatibility with some 3rd party extensions   for example handle extensions that install packages directly from github urls</li> <li>fix initial model download if no models found</li> <li>fix vae not found issues</li> <li>fix multiple git issues</li> </ul> <p>note: if you previously had command line optimizations such as --no-half, those are now ignored and moved to ui settings</p>"},{"location":"CHANGELOG/#update-for-2023-04-19","title":"Update for 2023-04-19","text":"<ul> <li>fix live preview</li> <li>fix model merge</li> <li>fix handling of user-defined temp folders</li> <li>fix submit benchmark</li> <li>option to override <code>torch</code> and <code>xformers</code> installer</li> <li>separate benchmark data for system-info extension</li> <li>minor css fixes</li> <li>created initial merge backlog from pending prs on a1111 repo   see #258 for details</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-18","title":"Update for 2023-04-18","text":"<ul> <li>reconnect ui to active session on browser restart   this is one of most frequently asked for items, finally figured it out   works for text and image generation, but not for process as there is no progress bar reported there to start with  </li> <li>force unload <code>xformers</code> when not used   improves compatibility with AMD/M1 platforms  </li> <li>add <code>styles.csv</code> to UI settings to allow customizing path  </li> <li>add <code>--skip-git</code> to cmd flags for power users that want   to skip all git checks and operations and perform manual updates</li> <li>add <code>--disable-queue</code> to cmd flags that disables Gradio queues (experimental)   this forces it to use HTTP instead of WebSockets and can help on unreliable network connections  </li> <li>set scripts &amp; extensions loading priority and allow custom priorities   fixes random extension issues: <code>ScuNet</code> upscaler disappearing, <code>Additional Networks</code> not showing up on XYZ axis, etc.</li> <li>improve html loading order</li> <li>remove some <code>asserts</code> causing runtime errors and replace with user-friendly messages</li> <li>update README.md</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-17","title":"Update for 2023-04-17","text":"<ul> <li>themes are now dynamic and discovered from list of available gradio themes on huggingface   its quite a list of 30+ supported themes so far  </li> <li>added option to see theme preview without the need to apply it or restart server</li> <li>integrated image info functionality into process image tab and removed separate image info tab</li> <li>more installer improvements</li> <li>fix urls</li> <li>updated github integration</li> <li>make model download as optional if no models found</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-16","title":"Update for 2023-04-16","text":"<ul> <li>support for ui themes! to to settings -&gt; user interface -&gt; \"ui theme*   includes 12 predefined themes</li> <li>ability to restart server from ui</li> <li>updated requirements</li> <li>removed <code>styles.csv</code> from repo, its now fully under user control</li> <li>removed model-keyword extension as overly aggressive</li> <li>rewrite of the fastapi middleware handlers</li> <li>install bugfixes, hopefully new installer is now ok  \\   i really want to focus on features and not troubleshooting installer</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-15","title":"Update for 2023-04-15","text":"<ul> <li>update default values</li> <li>remove <code>ui-config.json</code> from repo, its now fully under user control</li> <li>updated extensions manager</li> <li>updated locon/lycoris plugin</li> <li>enable quick launch by default</li> <li>add multidiffusion upscaler extensions</li> <li>add model keyword extension</li> <li>enable strong linting</li> <li>fix circular imports</li> <li>fix extensions updated</li> <li>fix git update issues</li> <li>update github templates</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-14","title":"Update for 2023-04-14","text":"<ul> <li>handle duplicate extensions</li> <li>redo exception handler</li> <li>fix generate forever</li> <li>enable cmdflags compatibility</li> <li>change default css font</li> <li>fix ti previews on initial start</li> <li>enhance tracebacks</li> <li>pin transformers version to last known good version</li> <li>fix extension loader</li> </ul>"},{"location":"CHANGELOG/#update-for-2023-04-12","title":"Update for 2023-04-12","text":"<p>This has been pending for a while, but finally uploaded some massive changes</p> <ul> <li>New launcher</li> <li><code>webui.bat</code> and <code>webui.sh</code>:     Platform specific wrapper scripts that starts <code>launch.py</code> in Python virtual environment Note: Server can run without virtual environment, but it is recommended to use it     This is carry-over from original repo If youre unsure which launcher to use, this is the one you want </li> <li><code>launch.py</code>:     Main startup script     Can be used directly to start server in manually activated <code>venv</code> or to run it without <code>venv</code> </li> <li><code>installer.py</code>:     Main installer, used by <code>launch.py</code> </li> <li><code>webui.py</code>:     Main server script  </li> <li>New logger</li> <li>New exception handler</li> <li>Built-in performance profiler</li> <li>New requirements handling</li> <li>Move of most of command line flags into UI Settings</li> </ul>"},{"location":"CLI-Arguments/","title":"Command Line Arguments","text":"<p>Tip</p> <p>All command line arguments can also be set as environment flags, for example <code>--debug</code> is equivalent to <code>SD_DEBUG=True</code> All options listed here are available as arguments to use from the command line or as environment variables, there's no need to do both.</p>"},{"location":"CLI-Arguments/#list","title":"List","text":"<p>webui --help</p> <pre><code>Configuration:\n  --config CONFIG                                    Use specific server configuration file, default: config.json\n  --ui-config UI_CONFIG                              Use specific UI configuration file, default: ui-config.json\n  --medvram                                          Split model stages and keep only active part in VRAM, default: False\n  --lowvram                                          Split model components and keep only active part in VRAM, default: False\n  --freeze                                           Disable editing settings\n\nPaths:\n  --ckpt CKPT                                        Path to model checkpoint to load immediately, default: None\n  --data-dir DATA_DIR                                Base path where all user data is stored, default:\n  --models-dir MODELS_DIR                            Base path where all models are stored, default: models\n\nDiagnostics:\n  --no-hashing                                       Disable hashing of checkpoints, default: False\n  --no-metadata                                      Disable reading of metadata from models, default: False\n  --disable-queue                                    Disable queues, default: False\n  --device-id DEVICE_ID                              Select the default CUDA device to use, default: None\n\nHTTP:\n  --server-name SERVER_NAME                          Sets hostname of server, default: None\n  --tls-keyfile TLS_KEYFILE                          Enable TLS and specify key file, default: None\n  --tls-certfile TLS_CERTFILE                        Enable TLS and specify cert file, default: None\n  --tls-selfsign                                     Enable TLS with self-signed certificates, default: False\n  --cors-origins CORS_ORIGINS                        Allowed CORS origins as comma-separated list, default: None\n  --cors-regex CORS_REGEX                            Allowed CORS origins as regular expression, default: None\n  --subpath SUBPATH                                  Customize the URL subpath for usage with reverse proxy\n  --autolaunch                                       Open the UI URL in the system's default browser upon launch\n  --auth AUTH                                        Set access authentication like \"user:pwd,user:pwd\"\"\n  --auth-file AUTH_FILE                              Set access authentication using file, default: None\n  --api-only                                         Run in API only mode without starting UI\n  --allowed-paths ALLOWED_PATHS [ALLOWED_PATHS ...]  add additional paths to paths allowed for web access\n  --share                                            Enable UI accessible through Gradio site, default: False\n  --insecure                                         Enable extensions tab regardless of other options, default: False\n  --listen                                           Launch web server using public IP address, default: False\n  --port PORT                                        Launch web server with given server port, default: 7860\n\nSetup:\n  --reset                                            Reset main repository to latest version, default: False\n  --upgrade, --update                                Upgrade main repository to latest version, default: False\n  --requirements                                     Force re-check of requirements, default: False\n  --reinstall                                        Force reinstallation of all requirements, default: False\n  --uv                                               Use uv instead of pip to install the packages\n\nStartup:\n  --quick                                            Bypass version checks, default: False\n  --skip-requirements                                Skips checking and installing requirements, default: False\n  --skip-extensions                                  Skips running individual extension installers, default: False\n  --skip-git                                         Skips running all GIT operations, default: False\n  --skip-torch                                       Skips running Torch checks, default: False\n  --skip-all                                         Skips running all checks, default: False\n  --skip-env                                         Skips setting of env variables during startup, default: False\n\nCompute Engine:\n  --use-directml                                     Use DirectML if no compatible GPU is detected, default: False\n  --use-openvino                                     Use Intel OpenVINO backend, default: False\n  --use-ipex                                         Force use Intel OneAPI XPU backend, default: False\n  --use-cuda                                         Force use nVidia CUDA backend, default: False\n  --use-rocm                                         Force use AMD ROCm backend, default: False\n  --use-zluda                                        Force use ZLUDA, AMD GPUs only, default: False\n  --use-xformers                                     Force use xFormers cross-optimization, default: False\n\nDiagnostics:\n  --safe                                             Run in safe mode with no user extensions\n  --experimental                                     Allow unsupported versions of libraries, default: False\n  --test                                             Run test only and exit\n  --version                                          Print version information\n  --ignore                                           Ignore any errors and attempt to continue\n\nLogging:\n  --log LOG                                          Set log file, default: None\n  --debug                                            Run installer with debug logging, default: False\n  --profile                                          Run profiler, default: False\n  --monitor PERIOD                                   Monitor load and log in specific periods, default: 0\n  --docs                                             Mount API docs, default: False\n  --api-log                                          Enable logging of all API requests, default: False\n</code></pre>"},{"location":"CLI-Arguments/#details","title":"Details","text":""},{"location":"CLI-Arguments/#general-options","title":"General Options","text":"<ul> <li><code>--config</code>: Specify the server configuration file. This option allows you to use a specific server configuration file. The default is set to <code>&lt;path to data&gt;/config.json</code>. You can customize this by providing a different file path or by setting the environment variable <code>SD_CONFIG</code>.</li> <li><code>--ui-config</code>: Specify the UI configuration file. This option allows you to use a specific UI configuration file. The default is set to <code>&lt;path to data&gt;/ui-config.json</code>. You can customize this by providing a different file path or by setting the environment variable <code>SD_UICONFIG</code>.</li> <li><code>--autolaunch</code>: Open the UI URL in the system's default browser upon launch. Enabling this flag (<code>True</code>) automatically opens the UI URL in the system's default browser upon launch. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_AUTOLAUNCH</code>.</li> <li><code>--upgrade</code>: Upgrade the main repository to the latest version. Use this option when you want to ensure that you are using the most recent version of the application. Enabling this flag (<code>True</code>) upgrades the main repository to the latest version.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_UPGRADE</code>. This is relatively safe to use with master branch. Dev branch is another story, be wary.</li> <li><code>--debug</code>: Run SDNext with debug logging to the console. Enabling this flag (<code>True</code>) runs SDNext with debug logging. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DEBUG</code>. The sdnext.log files always show debug output for troubleshooting purposes.</li> <li><code>--test</code>: Run the tests only and exit. This is useful for checking the integrity of the application without starting the actual service. Also useful for doing installation or reinstallation activities. Enabling this flag (<code>True</code>) runs the application in test mode, executing tests and exiting without launching the full application.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_TEST</code>. Be careful setting this option as an environment variable as it will only test on every startup, you will never get inside SDNext. </li> </ul>"},{"location":"CLI-Arguments/#skip-options","title":"Skip Options","text":"<ul> <li><code>--quick</code>: Run with startup sequence only, does not check requirements, extensions, git, or torch tests. Enabling this flag (<code>True</code>) runs with the startup sequence only. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_QUICK</code>.</li> <li><code>--skip-requirements</code>: Skips checking and installing requirements. Enabling this flag (<code>True</code>) during setup skips checking and installing requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPREQUIREMENTS</code>.</li> <li><code>--skip-extensions</code>: Skips running individual extension installers. Enabling this flag (<code>True</code>) during setup skips running individual extension installers. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPEXTENSION</code>.</li> <li><code>--skip-git</code>: Skips running all GIT operations. Enabling this flag (<code>True</code>) during setup skips running all GIT operations. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPGIT</code>.</li> <li><code>--skip-torch</code>: Skips running Torch checks. Enabling this flag (<code>True</code>) during setup skips running Torch checks. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPTORCH</code>.</li> <li><code>--skip-env</code>: Skips setting env variables. Enabling this flag (<code>True</code>) during setup skips setting of any and all env variables used for tuning. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SKIPENV</code>.</li> </ul>"},{"location":"CLI-Arguments/#memory-management","title":"Memory Management","text":"<ul> <li><code>--medvram</code>: Split model stages and keep only the active part in VRAM. Enabling this flag (<code>True</code>) allows the application to split model stages, conserving GPU memory by keeping only the active part in VRAM. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_MEDVRAM</code>.</li> <li><code>--lowvram</code>: Split model components and keep only the active part in VRAM. Enabling this flag (<code>True</code>) allows the application to split model components. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_LOWVRAM</code>.</li> </ul>"},{"location":"CLI-Arguments/#hardware-backends-use-during-installation-or-reinstall-operations","title":"Hardware Backends (Use During Installation or <code>--reinstall</code> operations)","text":"<ul> <li><code>--use-directml</code>: Use DirectML if no compatible GPU is detected. Enabling this flag (<code>True</code>) allows the use of DirectML if no compatible GPU is detected. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEDIRECTML</code>.</li> <li><code>--use-openvino</code>: Use Intel OpenVINO backend. Enabling this flag (<code>True</code>) allows the use of the Intel OpenVINO backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEOPENVINO</code>.</li> <li><code>--use-ipex</code>: Force use Intel OneAPI XPU backend. Enabling this flag (<code>True</code>) forces the use of the Intel OneAPI XPU backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEIPEX</code>.</li> <li><code>--use-cuda</code>: Force use NVIDIA CUDA backend. Enabling this flag (<code>True</code>) forces the use of the NVIDIA CUDA backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USECUDA</code>.</li> <li><code>--use-rocm</code>: Force use AMD ROCm backend. Enabling this flag (<code>True</code>) forces the use of the AMD ROCm backend. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEROCM</code>.</li> <li><code>--use-zluda</code>: Force use ZLUDA backend. Enabling this flag (<code>True</code>) forces the use of the AMD ROCm backend wrapped with ZLUDA. The default is <code>False</code> and you can set it to <code>True</code> using the environment variable <code>SD_USEZLUDA</code>. You should not have <code>torch</code> installed before enabling this flag.</li> </ul>"},{"location":"CLI-Arguments/#ipex-environment-variables","title":"IPEX Environment Variables","text":"<ul> <li><code>IPEX_SDPA_SLICE_TRIGGER_RATE</code>: Specify when dynamic attention slicing for Scaled Dot Product Attention should get triggered for Intel ARC. This environment variable allows you to set the trigger rate in gigabytes (GB). The default is <code>1</code>.</li> <li><code>IPEX_ATTENTION_SLICE_RATE</code>: Specify the dynamic attention slicing rate for 32 bit GPUs. This environment variable allows you to set the slicing rate in gigabytes (GB). The default is <code>0.5</code>.</li> <li><code>IPEX_FORCE_ATTENTION_SLICE</code>: Specify to enable or disable Dynamic Attention. The default is <code>1</code>.</li> <li><code>1</code> will force enable dynamic attention slicing even if the GPU supports 64 bit.</li> <li><code>-1</code> will force disable dynamic attention slicing even if the GPU doesn't support 64 bit.</li> <li><code>0</code> will automatically enable or disable dynamic attention based on the GPU.</li> <li><code>IPEXRUN</code>: Specify to launch the webui with ipexrun. Set it to <code>True</code> to use ipexrun. The default is unset.</li> </ul>"},{"location":"CLI-Arguments/#pathing","title":"Pathing","text":"<ul> <li><code>--log</code>: Set log file name and path. This argument allows you to set the log filename and location. The default is <code>sdnext.log</code> in the base directory, and you can set it with the environment variable <code>SD_LOG</code>.</li> <li><code>--ckpt</code>: Path to the model checkpoint to load immediately. This option allows you to specify the path to a model checkpoint for immediate loading. The default is <code>None</code>, and you can set a custom path by providing the argument value or by setting the environment variable <code>SD_MODEL</code>.</li> <li><code>--vae</code>: Path to the VAE checkpoint to load immediately. This option allows you to specify the path to a VAE checkpoint for immediate loading. The default is <code>None</code>, and you can set a custom path by providing the argument value or by setting the environment variable <code>SD_VAE</code>.</li> <li><code>--data-dir</code>: Base path where all user data is stored. You can set the base path where all user data is stored using this option. The default is an empty string (<code>''</code>). Customize this by providing a different path or by setting the environment variable <code>SD_DATADIR</code>.</li> <li><code>--models-dir</code>: Base path where all models are stored. This option sets the base path where all models are stored. The default is <code>'models'</code>. Customize this by providing a different path or by setting the environment variable <code>SD_MODELSDIR</code>.</li> </ul>"},{"location":"CLI-Arguments/#troubleshooting-options","title":"Troubleshooting Options","text":"<p>Also see Troubleshooting Wiki page</p> <ul> <li><code>--safe</code>: Run in safe mode with no user extensions. Safe mode can be useful when troubleshooting or when you want to restrict the execution of potentially unsafe user-provided code. Enabling this flag (<code>True</code>) runs the application in safe mode, disabling user extensions.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SAFE</code>. Try this first.</li> <li><code>--requirements</code>: Force re-check of python (pip) package requirements and installs any that are not up-to-date. Enabling this flag (<code>True</code>) forces a re-check of requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_REQUIREMENTS</code>. Try this second.</li> <li><code>--reinstall</code>: Force reinstallation of all requirements. Use this option when you want to ensure that all dependencies are freshly installed, potentially resolving any issues related to outdated or corrupted installations. Also useful to change hardware backends, such as to OpenVINO or DML. Enabling this flag (<code>True</code>) forces the reinstallation of all requirements. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_REINSTALL</code>. Try this third. Be careful setting this option as an environment variable as it will reinstall on every startup.</li> <li><code>--reset</code>: Reset main repository to latest version. Enabling this flag (<code>True</code>) resets the main repository to the latest version. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_RESET</code>. This is more serious with consequences, but you can try this fourth. Be careful setting this option as an environment variable as it will reset on every startup.</li> <li><code>--experimental</code>: Allow unsupported versions of libraries. This is useful for testing or trying out features that may not be officially supported yet. Enabling this flag (<code>True</code>) allows the application to use unsupported versions of libraries.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_EXPERIMENTAL</code>. Be careful setting this option as an environment variable as it can easily cause issues.</li> <li><code>--ignore</code>: Try to ignore any errors and attempt to continue. This can be useful in scenarios where certain errors are known and can be safely bypassed. Enabling this flag (<code>True</code>) instructs the application to ignore any errors encountered during the setup process and attempt to continue. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_IGNORE</code>.</li> </ul>"},{"location":"CLI-Arguments/#advanced-options","title":"Advanced Options","text":"<ul> <li><code>--device-id</code>: Select the default CUDA device to use. This option allows you to select the default CUDA device for GPU operations. The default is <code>None</code>, and you can set a custom device ID by providing the argument value or by setting the environment variable <code>SD_DEVICEID</code>.</li> <li><code>--use-xformers</code>: Forces the installation and use of xFormers cross-optimization. Enabling this flag (<code>True</code>) forces the installation and use of xFormers. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_USEXFORMERS</code>.</li> <li><code>--no-hashing</code>: Disable hashing of checkpoints. Enabling this flag (<code>True</code>) disables the hashing of checkpoints. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_NOHASHING</code>.</li> <li><code>--no-metadata</code>: Disable reading of metadata from models. Enabling this flag (<code>True</code>) disables the reading of metadata from models. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_NOMETADATA</code>.</li> <li><code>--profile</code>: Run profiler. Enabling this flag (<code>True</code>) runs the profiler. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_PROFILE</code>.</li> <li><code>--disable-queue</code>: Disable queues. Enabling this flag (<code>True</code>) disables the use of queues. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DISABLEQUEUE</code>.</li> <li><code>--allow-code</code>: Allow custom script execution. This is useful for scenarios where users may want to run their own code. Enabling this flag (<code>True</code>) allows the execution of custom scripts.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_ALLOWCODE</code>.</li> <li><code>--use-cpu</code>: This option forces the use of CPU for specified modules. You can provide a list of modules as arguments. The default is an empty list (<code>[]</code>). Customize this by providing module names or by setting the environment variable <code>SD_USECPU</code>. Not advised, legacy code with poor performance.</li> <li><code>--freeze</code>: Disable editing settings. This is useful to lock down configurations. Enabling this flag (<code>True</code>) prevents editing of settings.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_FREEZE</code>.</li> </ul>"},{"location":"CLI-Arguments/#api","title":"API","text":"<ul> <li><code>--docs</code>: Mount API docs at /docs i.e., <code>https://127.0.0.1/docs</code>. Enabling this flag (<code>True</code>) mounts API documentation at the <code>/docs</code> endpoint. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_DOCS</code>.</li> <li><code>--api-only</code>: Run in API only mode without starting UI. Enabling this flag (<code>True</code>) runs the application in API-only mode without starting the UI. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_APIONLY</code>.</li> <li><code>--api-log</code>: Enable logging of all API requests. Enabling this flag (<code>True</code>) logs all API requests. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_APILOG</code>.</li> </ul>"},{"location":"CLI-Arguments/#networking","title":"Networking","text":"<ul> <li><code>--share</code>: Enable UI to be accessible through the Gradio site. This is useful for sharing your SDNext with others. Enabling this flag (<code>True</code>) allows the UI to be accessible through the Gradio site.  The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_SHARE</code>. The shared URL will be in your console log.</li> <li><code>--insecure</code>: Enable extensions tab regardless of other options. This should only be used when you want local network or web accessible control of your extensions, potentially dangerous on the web. Enabling this flag (<code>True</code>) allows the extensions tab to be enabled regardless of other specified options. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_INSECURE</code>. Use with care when using <code>--share</code>.</li> <li><code>--listen</code>: Launch web server to be accessible from local network. Enabling this flag (<code>True</code>) allows the web server to launch for use on your own network. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_LISTEN</code>.</li> <li><code>--auth</code>: Set access authentication like \"user:pwd,user:pwd\". This option allows you to set access authentication with a specified username and password combination. The default is <code>None</code>, and you can set a custom authentication string by providing the argument value or by setting the environment variable <code>SD_AUTH</code>.</li> <li><code>--auth-file</code>: Set access authentication using file. This option allows you to set access authentication using a file. The default is <code>None</code>, and you can set a custom file path by providing the argument value or by setting the environment variable <code>SD_AUTHFILE</code>.</li> <li><code>--server-name</code>: Sets hostname of server. This option sets the hostname of the server. The default is <code>None</code>, and you can set a custom server name by providing the argument value or by setting the environment variable <code>SD_SERVERNAME</code>.</li> <li><code>--subpath</code>: Customize the URL subpath for usage with reverse proxy. This option allows you to customize the URL subpath for usage with a reverse proxy. The default is <code>None</code>, and you can set a custom subpath by providing the argument value or by setting the environment variable <code>SD_SUBPATH</code>.</li> <li><code>--cors-origins</code>: Allowed CORS origins as comma-separated list. This option sets the allowed CORS origins as a comma-separated list. The default is <code>None</code>, and you can set custom origins by providing the argument value or by setting the environment variable <code>SD_CORSORIGINS</code>.</li> <li><code>--cors-regex</code>: Allowed CORS origins as regular expression. This option sets the allowed CORS origins as a regular expression. The default is <code>None</code>, and you can set a custom regular expression by providing the argument value or by setting the environment variable <code>SD_CORSREGEX</code>.</li> <li><code>--tls-keyfile</code>: Enable TLS and specify key file. This option enables TLS (Transport Layer Security) and specifies the key file. The default is <code>None</code>, and you can set a custom key file path by providing the argument value or by setting the environment variable <code>SD_TLSKEYFILE</code>.</li> <li><code>--tls-certfile</code>: Enable TLS and specify cert file. This option enables TLS (Transport Layer Security) and specifies the certificate file. The default is <code>None</code>, and you can set a custom certificate file path by providing the argument value or by setting the environment variable <code>SD_TLSCERTFILE</code>.</li> <li><code>--tls-selfsign</code>: Enable TLS with self-signed certificates. Enabling this flag (<code>True</code>) enables TLS with self-signed certificates. The default is <code>False</code>, and you can set it to <code>True</code> using the environment variable <code>SD_TLSSELFSIGN</code>.</li> </ul>"},{"location":"CLI-Arguments/#cors-in-depth","title":"CORS in depth","text":"<p>These CORS options are important for controlling which domains are permitted to access the resources of your server. It helps in enhancing the security of your web application by preventing unauthorized cross-origin requests. When configuring these options, ensure that you only allow origins that you trust to interact with your server, as allowing any origin (<code>*</code>) can introduce security vulnerabilities. Always specify the origins explicitly or using a secure regular expression pattern.</p> <ul> <li><code>--cors-origins</code>: Allowed CORS origins as a comma-separated list. CORS is a security feature implemented by web browsers that restricts web pages from making requests to a different domain than the one that served the web page. This option allows you to specify a list of origins (domains) that are allowed to access resources on your server. For example, if your application is hosted on <code>http://example.com</code>, and you want to allow access from <code>http://client.example.com</code>, you would set <code>--cors-origins http://client.example.com</code>. For local network use, often with other tools, you can just set <code>--cors-origins *</code>.</li> <li><code>--cors-regex</code>: Allowed CORS origins as a regular expression. This option provides more flexibility by allowing you to specify CORS origins using a regular expression. The regular expression should match the origin(s) you want to allow. This is useful when you have a dynamic set of origins that follow a certain pattern. For instance, if you want to allow any subdomain under <code>example.com</code>, you could set <code>--cors-regex \"^https?://[a-z0-9-]+\\.example\\.com$\"</code>.</li> </ul>"},{"location":"CLI-Arguments/#tls-in-depth","title":"TLS in depth","text":"<p>When setting up TLS, it's important to use valid certificates from a trusted CA in a production environment to ensure secure and encrypted communication. In a development or testing environment, self-signed certificates can be used, but users should be cautious when accessing the application as browsers may show security warnings due to the self-signed nature. Always ensure that your TLS setup meets security best practices.</p> <ul> <li><code>--tls-keyfile</code>: Enable TLS and specify the key file. TLS is a cryptographic protocol that ensures the secure transmission of data over a network. Enabling TLS in your application secures the communication between the client and the server. The <code>--tls-keyfile</code> option allows you to specify the path to the private key file used for encryption. This private key file should be in PEM format.</li> </ul> <p>Example:</p> <pre><code>--tls-keyfile /path/to/private-key.pem\n</code></pre> <ul> <li><code>--tls-certfile</code>: Enable TLS and specify the cert file. Along with the private key, you need to specify the TLS certificate file. The certificate file contains the public key and information about the server. It should be in PEM format.</li> </ul> <p>Example:</p> <pre><code>--tls-certfile /path/to/certificate.pem\n</code></pre> <ul> <li><code>--tls-selfsign</code>: Enable TLS with self-signed certificates. If you don't have a certificate issued by a Certificate Authority (CA), you can enable this option to create self-signed certificates. Self-signed certificates are useful for development and testing but should not be used in production environments where security is crucial.</li> </ul> <p>Example:</p> <pre><code>--tls-selfsign /path/to/self-signed-certificate.pem\n</code></pre>"},{"location":"CLI-Tools/","title":"Stable-Diffusion Productivity Scripts","text":""},{"location":"CLI-Tools/#generate","title":"Generate","text":"<ul> <li><code>api-txt2img.py</code>: run generate using text-to-image</li> <li><code>api-img2img.py</code>: run generate using image-to-image</li> <li><code>api-control.py</code>: run generate using control-interface</li> <li><code>api-faceid.py</code>: run generate using face id</li> <li><code>api-pulid.js</code>: run generate using pulid</li> <li><code>api-grid.py</code>: run generate using text-to-image   while allowing to vary parameters for x and y to create an image grid  </li> </ul>"},{"location":"CLI-Tools/#information","title":"Information","text":"<ul> <li><code>api-progress.py</code>: get progress of a job</li> <li><code>api-info.py</code>: get image metadata</li> </ul>"},{"location":"CLI-Tools/#other-api-examples","title":"Other API examples","text":"<ul> <li><code>api-upscale.py</code>: run image upscaling</li> <li><code>api-detect.py</code>: detect faces in images</li> <li><code>api-json.py</code>: send json to any api endpoint</li> <li><code>api-mask.py</code>: run image masking with advanced options</li> <li><code>api-preprocess.py</code>: run image preprocessing</li> <li><code>api-vqa.py</code>: run visual question answering</li> <li><code>api-interrogate.py</code>: interrogate images using clip</li> </ul>"},{"location":"CLI-Tools/#-run-benchmarkpy-run-benchmark-tests","title":"- <code>run-benchmark.py</code>: run benchmark tests","text":""},{"location":"CLI-Tools/#javascript","title":"JavaScript","text":"<ul> <li><code>api-txt2img.js</code>: run generate using text-to-image</li> <li><code>api-model.js</code>: load model</li> </ul>"},{"location":"CLI-Tools/#helper-scripts","title":"Helper scripts","text":"<ul> <li><code>validate-locale.py</code>: validate locale files</li> <li><code>lcm-convert.py</code>: convert lcm models</li> <li><code>video-extract.py</code>: extract frames from video files</li> <li><code>install-sf.py</code>: install stablefast</li> <li><code>zluda-python.py</code>: install zluda</li> </ul>"},{"location":"CLI-Tools/#utilities","title":"Utilities","text":"<ul> <li><code>nvidia-smi.py</code>: get GPU information</li> <li><code>image-grid.py</code>: create image grid</li> <li><code>image-exif.py</code>: extract exif data from images</li> <li><code>image-palette.py</code>: extract color palette from images</li> <li><code>image-watermark.py</code>: create invisible image watermark</li> <li><code>gen-styles.py</code>: generate styles from lines in text file</li> <li><code>model-metadata.py</code>: get model metadata</li> </ul>"},{"location":"CLI-Tools/#api-examples","title":"API Examples","text":""},{"location":"CLI-Tools/#generate_1","title":"Generate","text":"<p>Text-to-image with all of the possible parameters Supports upsampling, face restoration and grid creation  </p> <p>python cli/generate.py</p> <p>By default uses parameters from  <code>generate.json</code></p> <p>Parameters that are not specified will be randomized:</p> <ul> <li>Prompt will be dynamically created from template of random samples: <code>random.json</code></li> <li>Sampler/Scheduler will be randomly picked from available ones</li> <li>CFG Scale set to 5-10</li> </ul> <p></p>"},{"location":"CLI-Tools/#utility-scripts","title":"Utility Scripts","text":""},{"location":"CLI-Tools/#sdapi","title":"SDAPI","text":"<p>Utility module that handles async communication to Automatic API endpoints Note: Requires SD API  </p> <p>Can be used to manually execute specific commands:</p> <p>python sdapi.py progress python sdapi.py interrupt python sdapi.py shutdown</p>"},{"location":"CLiP-Skip/","title":"CLiP Skip","text":"<p>[!WARNING] Using CLiP skip values without understanding their implications can lead to unexpected and most likely corrupt image outputs.</p>"},{"location":"CLiP-Skip/#what-is-a-clip-skip","title":"What is a CLiP skip","text":"<p>The CLiP text encoder model consists of multiple layers, each providing a different level of specificity Lower Clip Skip values mean that more layers are used, resulting in images that closely match the prompts Higher values skip some of these layers, leading to potentially more creative outputs but with less adherence to the original prompt  </p> <p>Why did it became popular? Because original NovelAI SD 1.5 model was trained with CLiP skip value of 2 And that model was used as basis for many other anime-style models, leading to a common practice of using CLiP skip value of 2 for anime content  </p> <p>However, anything newer than that, like pretty much ANY SDXL or FLUX or other recent model, is trained with CLiP skip value of 1, and using CLiP skip value of 2 on these models will lead to bad results  </p> <p>However, there are still many recommendations on the internet to use CLiP skip value of 2 - and most of them are outdated and wrong (unless we're talking about NAI SD 1.5 derivatives) The problem arises from the fact that many SD applications such as original Automatic1111 do not even implement CLiP skip for SDXL and other newer models, so setting it to 2 does not change anything and does not lead to any issues  </p> <p>SD.Next, on the other hand, does implement CLiP skip for all models, so setting it to 2 will lead to bad results with SDXL and other newer models  </p> <p>If you want to experiment with CLiP skip values, you can do so, but be aware of the implications To change the CLiP skip value, you need to unlock it in settings and then set it to the desired value  </p> <p>Note</p> <p>SD.Next also supports non-integer CLiP skip values, such as 1.2, 1.5, etc.</p>"},{"location":"Caption/","title":"Caption","text":"<p>Caption tab includes functions regarding image interrogation and captioning Its separated into two main sections: VLM and CLiP </p> <p>Captionining/interrogate can be performed on a single image, on list of uploaded images or on a folder containing images Captioning results can be saved to a file in which case, they will be saved next to original image file with a filename with <code>.txt</code> extension  </p>"},{"location":"Caption/#vlm-caption","title":"VLM Caption","text":"<p>Uses VLM (Vision Language Model) models to generate captions for images. VLM model is LLM (large language model) with additional vision component to allow it to analyze input images. You can use a predefined prompt such as MORE DETAILED CAPTION or enter your own prompt to generate captions. For example: - describe the background of the image - *does the image cointain a person?\"  </p> <p>SD.Next supports many VLM models such as: Florence, MoonDream, Gemma, Qwen, JoyCaption, etc. VLM model will be auto-downloaded on first use  </p>"},{"location":"Caption/#clip-interrogate","title":"CLiP Interrogate","text":"<p>CLiP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of image and text pairs. Different CLiP models are commonly used for text-encoding task during generate image using many popular models such as SD15/SD-XL/SD3.5/etc.</p> <p>SD.Next supports over 50+ CLiP models CLiP model will be auto-downloaded on first use  </p> <p>In addition to Interrogate, CLiP tab also includes Analyze feature which allows to predict image categories such as: medium, artist, movement, trending and flavor.</p>"},{"location":"Control-HowTo/","title":"How to use ControlNet","text":"<p>ControlNet, from its name, can \"control\" the Diffusion model you are using and force it to represent images in a specific way depending on the information from an additional model supplied by the user.  </p> <p>These models can influence the image in different ways: for example, you can pose people or characters, draw a scene basing on spatial information (how far is a person from a car behind, for example), on pre-existing lineart, and so on. They work by supplying a \"control image\" (which can be pre existing, or created on the fly before generation) which represents in a special way the information the ControlNet model needs.  </p> <p>Note</p> <p>ControlNet models can be large Using ControlNet decreases generation speed and increases resource usage, especially GPU VRAM</p>"},{"location":"Control-HowTo/#the-choice-of-model","title":"The choice of model","text":"<p>Choosing the model is important because different models can influence in different ways the image you are going to generate (strongly or weakly). Depending on the model you're using (SD 1.5, SDXL, SD 3.5, Flux...) there are different models available.  </p> <p>Here we'll cover the commonly used ones:</p> <ul> <li>Openpose   This is used to pose the people or characters in an image, and uses \"stick-like\" images that represent eyes, nose, ears, and limbs (and occasionally fingers). It's a widely used model for poses. Effect on composition: Light to medium</li> <li>Depth   This model uses depth maps, that is images that show how close / far items and people are based on the intensity of a grey color scale (white: closest, black: farthest) in a specific scene. It can be used to replicate settings, or to ensure items and people are placed appropriately. It has a very strong impact on the composition. Effect on composition: Strong  </li> <li>Lineart   This model uses pre-existing lineart to guide the generation of an image (imagine, for example, a lineart representation of a caf\u00e9). It can use lineart generated from photorealistic images, or from anime-like images. It has the weakest impact on the image composition. Effect on composition: Weak</li> <li>Canny   The Canny model uses control images that have gone through a process of \"edge detection\" (that is, identifying edges and contours in an image) and are represented as something close to lineart. It can be used effectively to represent scenes where lineart would be too complex, or where lineart isn't really possible to use. The effect on the composition depends on the model used. SD 1.5 models have a strong impact, while SDXL based ones tend to be weaker. Effect on composition: Variable (from weak to strong)</li> <li>Segmentation   This model uses control images that have been \"segmented\" (a technique from computer vision that identifies distinct elements in an image), to identify the various elements in an image by different colors: for example, people are shown as red silhouetters, buildings as light blue, and so on (from about 50 to 100 distinct colors are used). It's very useful when the model has no knowledge of specific objects, to avoid other concepts from \"bleeding in\". Effect on composition: Medium  </li> <li>Tiling   This is a special model type in a sense its not really a controlnet, but instead allows you to use a pre-existing image to create large output image from \"tiles\" of the original image. This can be used instead of normal img2img process with resizing as it allows much larger output sizes since each tile is generated separately.  </li> <li>Union and ProMax   Those are special models that combine multiple types of control in a single model and can be used instead of any of the above listed models. When selecting Union or ProMax models, you also need to select control mode which is going to be used  </li> </ul>"},{"location":"Control-HowTo/#how-to-generate-a-control-image","title":"How to generate a control image","text":"<p>Control images are required for ControlNet to work: you can't use it without one. The problem of course is, how to make one? There are two different ways.</p>"},{"location":"Control-HowTo/#generate-control-images-on-the-fly","title":"Generate control images on the fly","text":"<p>SD.Next can generate the appropriate control image from any input image you supply using a \"preprocessor\", which, depending on the model used, turns your input image in way suitable for use in ControlNet. There are as many preprocessors as ControlNet models, so use the model choice section to guide yourself. </p> <p>Note that preprocessors are additional models, so using them consumes more VRAM (you can choose to unload or move them to the CPU after use in the SD.Next options, if this is a concern). Also, depending on the data they have been trained on, the accuracy of the resulting image vary.</p> <p>Canny, Depth, Segmentation and Lineart preprocessors are recommended in case you do not have control images at hand. </p>"},{"location":"Control-HowTo/#use-a-pre-existing-control-image","title":"Use a pre-existing control image","text":"<p>In particular for Openpose, the accuracy of the preprocessor may not be enough, or you know how to generate images yourself, so you can supply a pre-made image. You will not require the additional VRAM for preprocessing, but of course you need to know how to make one.</p> <p>Several examples of the available software that can be used to generate controlnet input images:</p> <ul> <li>PoseMyArt can export images in OpenPose format</li> <li>Depth Anything HuggingFace page can generate depth maps</li> <li>Clip Studio Paint Ex can be used to generate lineart from 3D models with the \"Convert lines\" feature</li> </ul>"},{"location":"Control-HowTo/#using-contronet-with-sdnext","title":"Using ControNet with SD.Next","text":"<p>Note</p> <p>Following step-by-step guide is created using SD.Next ModernUI Same options exist in StandardUI as well althrough their location in the UI may differ</p> <p>First, enable Control by clicking on the \"Control\" checkbox near the preview area. </p> <p></p> <p>A new tab will appear, make sure \"ControlNet\" is selected. </p> <p></p> <p>Now you have to decide how many \"units\" (ControlNet models) to use. For most uses one is sufficient, but for particularly complex scenarios you may need more than one. You can control the number of units by increasing the \"Units\" number. This guide assumes you use one unit. The workflow is the same the more units you add. Bear in mind that the more units you use, the more VRAM will be used. </p> <p></p> <p>Ensure the unit is enabled by checking if the checkbox under \"ControlNet Unit 1\" is checked. If it is not, click on it to enable it.  </p> <p></p> <p>Now you have to select the ControlNet model you want to use. Click on the \"reload\" icon to load the available ControlNet models and select the one you want from the list. It will be automatically downloaded and made available to SD.Next. You can check the console output or the log for progress information. </p> <p></p> <p></p> <p>Note</p> <p>No items will display unless you have loaded a checkpoint first.</p> <p>Should you want to use a preprocessor, select it from the list next to the ControlNet combo box.  </p> <p></p> <p>Now you have to decide how much your ControlNet model will affect the generation. The \"CN\" strength slider  goes from 0 (no effect) to 1 (complete effect). You might want to experiment here depending on your needs: for OpenPose models, 1 is usually fine, but for depth and canny, lower strength may be required.  </p> <p></p> <p>For specific uses (out of scope of this guide) you can also decide for how long ControlNet will be active during the generation process. This can be adjusted by changing the values on \"CN start\" and \"CN end\" (0, start of the generation, 1 end).  </p> <p></p> <p>Click on the upper arrow icon to upload your control image. Note: if you don't use a preprocessor it must be in the same aspect ratio as the image that will be generated.  </p> <p></p> <p>If you have used a preprocessor, you can hit the preview icon to see how the image will be preprocessed.</p> <p></p> <p>Specific preprocessor settings can be changed in the Control settings section.</p> <p></p> <p>Tip</p> <p>If you have messed up, hit the \"Reset\" icon and the values will be all reset to default.</p> <p>Once everything is set up, write your prompt, set your image parameters, and hit Generate. You will get a preview of your control image and generation will begin. </p>"},{"location":"Control-Settings/","title":"Control Guide","text":"<p>SDNext's Control tab is our effort to bring ControlNet, IP-Adapters, T2I Adapter, ControlNet XS, and ControlNet LLLite to our users.  </p> <p>Note that this document is a work in progress, it's all quite complex and will take some time to write up a bit more as well as smooth out the rough edges and correct any issues and bugs that pop up, expect frequent updates! </p> <p>This guide will attempt to explain how to use it so that anyone can understand it and put it to work for themselves.  </p>"},{"location":"Control-Settings/#controls","title":"Controls","text":""},{"location":"Control-Settings/#input","title":"Input","text":"<p>The Input control is exactly what it sounds like, it controls what input images (or videos) are contributing to your image generation, by default that is just the image in the Control input pane, however if you select <code>Separate init image</code>, another image pane will appear below, allowing you to use that as well.  </p> <p>Note: When using a Control input image as well as a Init input image, the Init input dominates. Adjusting denoise to &gt;=0.9 is recommended, as that will allow the Control input to balance with the Init input. Higher values will increase the strength of Control input further, giving it dominance.  </p> <p></p> <p><code>Show Preview</code> is simple, it controls the visibility of the preview window in the far right of the middle row. You'll want this on if you're doing any kind of masking or manipulations that you would want to preview before generating.</p> <p>There are 3 different Input types:</p> <ul> <li><code>Control only</code>: This uses only the Control input below as a source for any ControlNet or IP Adapter type tasks based on any of our various options.  </li> <li><code>Init image same as control</code>: This option will additionally treat any image placed into the <code>Control input</code> pane as a source for img2img type tasks, an image to modify for example.  </li> <li><code>Separate init image</code>: This option creates an additional window next to <code>Control input</code> labeled <code>Init input</code>, so you can have a separate image for both Control operations and an init source.</li> </ul> <p><code>Denoising strength</code> is the same as if you were doing any img2img operation. The higher the value, the more denoising that will take place, and the greater any source image will be modified.</p>"},{"location":"Control-Settings/#size","title":"Size","text":"<p>This can be a little confusing at first because of the <code>Before</code> and <code>After</code> subtabs, however it's really quite simple and extremely powerful. The Control size menu allows you to manipulate the size of your input images before and after inference takes place.  </p> <p></p> <p>The <code>Before</code> subtab does 2 things:</p> <ul> <li> <p>If you do not select any <code>Resize method</code>, it is only controlling the output image size width and height in pixels as it would in any text2img or img2img operation.</p> </li> <li> <p>However, if you do select a <code>Resize method</code>, Nearest for example, you can upscale or downscale the <code>Control input</code> image before any other operations take place. This will be the size of any image used in further operations. Second Pass is not entirely functional yet, but will be part of this.</p> </li> </ul> <p>For example, you might have a much larger image, such as 2048x3072, that you want to use with canny or depth map, but you do not want an image that large to manipulate or guide your generation, that would be prohibitive, slower, and possibly cause an OOM.  </p> <p>This is where <code>Resize method</code> comes in, you would simply select a resize method, typically Nearest or Lanczos, and then either set the pixel width or height you want to resize to under Fixed, or switch over to Scale and select a number below 1. A setting of 0.5 would make your input image effectively 1024x1536 pixels, which would be used as input for later operations.  </p> <p>The <code>After</code> subtab controls any upscaling or downscaling that would take place at the end of your image generation process, most commonly this would either be latent upscaling, and ESRGAN model such as 4x Ultrasharp, or one of the various chaiNNer models we provide. This is the same as it would be in a standard upscaling via text2img or img2img.</p>"},{"location":"Control-Settings/#mask","title":"Mask","text":"<p>The Mask controls are where we start getting into the real meat of Control, not only does it allow a plethora of different options to mask, segment, and control the view of your masking with various preview types, but it comes with 22 different colormaps for your viewing pleasure! (And I think vlad made some of those words up \ud83e\udd2b)  </p> <p></p> <ul> <li> <p><code>Live update</code>: With this checked, your masking will update as you make changes to it, if this is off, you will need to hit the <code>Refresh</code> button to the right to have your preview pane update, making more changes to it while it is processing may lead to it being desynchronized, just hit the refresh button if it does not look correct.</p> </li> <li> <p><code>Inpaint masked only</code>: Inpainting will apply only to areas you have masked if this is checked. You must actually inpaint something, otherwise it's just img2img.</p> </li> <li> <p><code>Invert mask</code>: Inverts the masking, things you mark with the brush will be excluded from a full mask of the image.</p> </li> <li> <p><code>Auto-mask</code>: There are three options here, Threshold, Edge, and Greyscale. Each provides a different method of auto-masking your images.</p> </li> <li> <p><code>Auto-segment</code>: Just like Auto-mask, we have provided an extensive list of Auto-segmentation models, they don't require ControlNet to handle the process, but may take a few seconds to process, depending on your GPU.  </p> </li> <li> <p><code>Preview</code>: You can select the preview type here, we have provided 5 modes, Masked, Binary, Greyscale, Color, and Composite, which is the default.</p> </li> <li> <p><code>Colormap</code>: You can select the style/color scheme of the preview here. There are 22 fantastic color schemes!</p> </li> <li> <p><code>Blur</code>: This blurs the edges of what you have masked, to allow some flexibility. Play with it.</p> </li> <li> <p><code>Erode</code>: This slider controls the reduction of your auto-masking or auto-segmentation border.</p> </li> <li> <p><code>Dilate</code>: This slider controls the expansion of your auto-masking or auto-segmentation border.</p> </li> </ul>"},{"location":"Control-Settings/#video","title":"Video","text":"<p>The Video controls are quite exciting and fun to play with, with our tools now you can, if you wished, turn any video into an anime version for example, frame by frame. There are three output options, GIF, PNG, and MP4. You must select one of these to have video output. With these simple controls, you can tweak your video output with surprising flexibility. Some video output methods provide more controls, try them all.</p> <p></p> <ul> <li> <p><code>Skip input frames</code>: This setting controls how many frames are processed from input instead of every frame. Setting it to 0 would mean processing every frame, a setting of 1 would process every other frame, a setting of 2 would process every third frame, cutting the number of total frames by 2/3rds, and so on.</p> </li> <li> <p><code>Video file</code>: You select the type of output you want here, animated GIF (not JIF!), animated PNG, or MP4 video, all provided via FFMPEG of course.  </p> </li> <li> <p><code>Duration</code>: The length in seconds you want your output video to be.  </p> </li> <li> <p><code>Pad frames</code>: Determine how many frames to add to the beginning and end of the video. This feature is particularly useful when used with interpolation.</p> </li> <li> <p><code>Interpolate frames</code>: The number of frames you want interpolated (via RIFE) between existing frames (filtered by skip input frames) in a video sequence. This smoothens the video output, especially if you're skipping frames to avoid choppy motion or low frame rates.</p> </li> <li> <p><code>Loop</code>: This is purely for animated GIF and PNG output, it enables the classic looping that you would expect.</p> </li> </ul> <p>When you're using interpolation, the software also detects scene changes. If the scene changes significantly, it will insert pad frames instead of interpolating between two unrelated frames. This ensures a seamless transition between scenes and maintains the overall quality of the video output.</p>"},{"location":"Control-Settings/#extensions","title":"Extensions","text":"<p>These are some nice goodies that we have cooked up so that no actual installed extensions are necessary, you may even find that our version works better!</p>"},{"location":"Control-Settings/#ip-adapter","title":"IP-Adapter","text":"<p>This is our IP Adapter implementation, with 10 available models for your image or face cloning needs!</p> <p></p>"},{"location":"Control-Settings/#image-panes","title":"Image Panes","text":"<p>You may notice small icons above the image panes that look like pencils, these are Interrogate buttons. The left one is BLIP, and the right one is DeepBooru. Click one of the buttons to interrogate the image in the pane below it. The results will appear in your prompt area.</p> <p></p>"},{"location":"Control-Settings/#control-input","title":"Control Input","text":"<p>This is the heart of Control, you may put any image or even video here to be processed by our system, that means any and all scripts, extensions, even the various Controlnet variants below, though you can individually add guidance images to each of those. If an image is placed here, the system will assume you are performing an img2img process of some sort. If you upload a video to SDNext via the Control input pane, you will see that you can play the video, both input and resultant output. Batching and folders should work as expected.</p> <p>Note below there are 2 other buttons, Inpainting and Outpainting, below.</p> <p></p>"},{"location":"Control-Settings/#controlnet","title":"ControlNet+","text":"<p>At the very bottom of the Control page, we have what you've all been waiting for, full ControlNet! I do mean full too, we have it all! This includes SD and SD-XL. at last! You won't ever need the ControlNet extension ever again.  </p> <p>This will take a bit more work to document example workflows, but there are tooltips, and if you've used ControlNet before, you shouldn't have any problems! However if you do, hop on by our Discord server and we're happy to help.</p>"},{"location":"Control-Technical/","title":"Control Overview","text":"<p>Native control module for SD.Next for Diffusers backend Can be used for Control generation as well as Image and Text workflows  </p> <p>For a guide on the options and settings, as well as explanations for the controls themselves, see the Control Howto and Control Settings pages.</p>"},{"location":"Control-Technical/#supported-control-models","title":"Supported Control Models","text":"<ul> <li>lllyasviel ControlNet for SD 1.5 and SD-XL models   Includes ControlNets as well as Reference-only mode and any compatible 3rd party models   Original ControlNets for SD15 are 1.4GB each and for SDXL its at massive 4.9GB  </li> <li>VisLearn ControlNet XS for SD-XL models   Lightweight ControlNet models for SDXL at 165MB only with near-identical results  </li> <li>TencentARC T2I-Adapter for SD 1.5 and SD-XL models   T2I-Adapters provide similar functionality at much lower resource cost at only 300MB each  </li> <li>Kohya Control LLite for SD-XL models   LLLite models for SDXL at 46MB only provide lightweight image control  </li> <li>TenecentAILab IP-Adapter for SD 1.5 and SD-XL models   IP-Adapters provides great style transfer functionality at much lower resource cost at below 100MB for SD15 and 700MB for SDXL   IP-Adapters can be combined with ControlNet for more stable results, especially when doing batch/video processing  </li> <li>CiaraRowles TemporalNet for SD 1.5 models   ControlNet model designed to enhance temporal consistency and reduce flickering for batch/video processing  </li> </ul> <p>All built-in models are downloaded upon first use and stored stored in: <code>/models/controlnet</code>, <code>/models/adapter</code>, <code>/models/xs</code>, <code>/models/lite</code>, <code>/models/processor</code></p> <p>Listed below are all models that are supported out-of-the-box:</p>"},{"location":"Control-Technical/#controlnet","title":"ControlNet","text":"<ul> <li>SD15:   Canny, Depth, IP2P, LineArt, LineArt Anime, MLDS, NormalBae, OpenPose,   Scribble, Segment, Shuffle, SoftEdge, TemporalNet, HED, Tile  </li> <li>SDXL:   Canny Small XL, Canny Mid XL, Canny XL, Depth Zoe XL, Depth Mid XL</li> </ul> <p>Note: only models compatible with currently loaded base model are listed Additional ControlNet models in safetensors can be downloaded manually and placed into corresponding folder: <code>/models/control/controlnet</code> </p>"},{"location":"Control-Technical/#controlnet-xs","title":"ControlNet XS","text":"<ul> <li>SDXL:   Canny, Depth  </li> </ul>"},{"location":"Control-Technical/#controlnet-lllite","title":"ControlNet LLLite","text":"<ul> <li>SDXL:   Canny, Canny anime, Depth anime, Blur anime, Pose anime, Replicate anime</li> </ul> <p>Note: control-lllite is implemented using unofficial implementation and its considered experimental Additional ControlNet models in safetensors can be downloaded manually and placed into corresponding folder: <code>/models/control/lite</code> </p>"},{"location":"Control-Technical/#t2i-adapter","title":"T2I-Adapter","text":"<pre><code>'Segment': 'TencentARC/t2iadapter_seg_sd14v1',\n'Zoe Depth': 'TencentARC/t2iadapter_zoedepth_sd15v1',\n'OpenPose': 'TencentARC/t2iadapter_openpose_sd14v1',\n'KeyPose': 'TencentARC/t2iadapter_keypose_sd14v1',\n'Color': 'TencentARC/t2iadapter_color_sd14v1',\n'Depth v1': 'TencentARC/t2iadapter_depth_sd14v1',\n'Depth v2': 'TencentARC/t2iadapter_depth_sd15v2',\n'Canny v1': 'TencentARC/t2iadapter_canny_sd14v1',\n'Canny v2': 'TencentARC/t2iadapter_canny_sd15v2',\n'Sketch v1': 'TencentARC/t2iadapter_sketch_sd14v1',\n'Sketch v2': 'TencentARC/t2iadapter_sketch_sd15v2',\n</code></pre> <ul> <li>SD15:   Segment, Zoe Depth, OpenPose, KeyPose, Color, Depth v1, Depth v2, Canny v1, Canny v2, Sketch v1, Sketch v2  </li> <li>SDXL:   Canny XL, Depth Zoe XL, Depth Midas XL, LineArt XL, OpenPose XL, Sketch XL  </li> </ul> <p>Note: Only models compatible with currently loaded base model are listed</p>"},{"location":"Control-Technical/#processors","title":"Processors","text":"<ul> <li>Pose style: OpenPose, DWPose, MediaPipe Face</li> <li>Outline style: Canny, Edge, LineArt Realistic, LineArt Anime, HED, PidiNet</li> <li>Depth style: Midas Depth Hybrid, Zoe Depth, Leres Depth, Normal Bae</li> <li>Segmentation style: SegmentAnything</li> <li>Other: MLSD, Shuffle</li> </ul> <p>Note: Processor sizes can vary from none for built-in ones to anywhere between 200MB up to 4.2GB for ZoeDepth-Large</p>"},{"location":"Control-Technical/#segmentation-models","title":"Segmentation Models","text":"<p>There are 8 Auto-segmentation models available:  </p> <ul> <li>Facebook SAM ViT Base (357MB)  </li> <li>Facebook SAM ViT Large (1.16GB)</li> <li>Facebook SAM ViT Huge (2.56GB)</li> <li>SlimSAM Uniform (106MB)</li> <li>SlimSAM Uniform Tiny (37MB)</li> <li>Rembg Silueta</li> <li>Rembg U2Net  </li> <li>Rembg ISNet</li> </ul>"},{"location":"Control-Technical/#reference","title":"Reference","text":"<p>Reference mode is its own pipeline, so it cannot have multiple units or processors  </p>"},{"location":"Control-Technical/#workflows","title":"Workflows","text":""},{"location":"Control-Technical/#inputs-outputs","title":"Inputs &amp; Outputs","text":"<ul> <li>Image -&gt; Image</li> <li>Batch: list of images -&gt; Gallery and/or Video</li> <li>Folder: folder with images -&gt; Gallery and/or Video</li> <li>Video -&gt; Gallery and/or Video</li> </ul> <p>Notes: - Input/Output/Preview panels can be minimized by clicking on them - For video output, make sure to set video options  </p>"},{"location":"Control-Technical/#unit","title":"Unit","text":"<ul> <li>Unit is: input plus process plus control</li> <li>Pipeline consists of any number of configured units   If unit is using using control modules, all control modules inside pipeline must be of same type   e.g. ControlNet, ControlNet-XS, T2I-Adapter or Reference</li> <li>Each unit can use primary input or its own override input  </li> <li>Each unit can have no processor in which case it will run control on input directly   Use when you're using predefined input templates  </li> <li>Unit can have no control in which case it will run processor only  </li> <li>Any combination of input, processor and control is possible   For example, two enabled units with process only will produce compound processed image but without control  </li> </ul>"},{"location":"Control-Technical/#what-if","title":"What-if?","text":"<ul> <li>If no input is provided then pipeline will run in txt2img mode   Can be freely used instead of standard <code>txt2img</code> </li> <li>If none of units have control or adapter, pipeline will run in img2img mode using input image   Can be freely used instead of standard <code>img2img</code> </li> <li>If you have processor enabled, but no controlnet or adapter loaded,   pipeline will run in img2img mode using processed input</li> <li>If you have multiple processors enabled, but no controlnet or adapter loaded,   pipeline will run in img2img mode on blended processed image  </li> <li>Output resolution is by default set to input resolution,   Use resize settings to force any resolution  </li> <li>Resize operation can run before (on input image) or after processing (on output image)  </li> <li>Using video input will run pipeline on each frame unless skip frames is set   Video output is standard list of images (gallery) and can be optionally encoded into a video file   Video file can be interpolated using RIFE for smoother playback  </li> </ul>"},{"location":"Control-Technical/#overrides","title":"Overrides","text":"<ul> <li>Control can be based on main input or each individual unit can have its own override input</li> <li>By default, control runs in default control+txt2img mode</li> <li>If init image is provided, it runs in control+img2img mode   Init image can be same as control image or separate</li> <li>IP adapter can be applied to any workflow</li> <li>IP adapter can use same input as control input or separate</li> </ul>"},{"location":"Control-Technical/#inpaint","title":"Inpaint","text":"<ul> <li>Inpaint workflow is triggered when input image is provided in inpaint mode</li> <li>Inpaint mode can be used with image-to-image or controlnet workflows</li> <li>Other unit types such as T2I, XS or Lite do not support inpaint mode</li> </ul>"},{"location":"Control-Technical/#outpaint","title":"Outpaint","text":"<ul> <li>Outpaint workflow is triggered when input image is provided in outpaint mode</li> <li>Outpaint mode can be used with image-to-image or controlnet workflows</li> <li>Other unit types such as T2I, XS or Lite do not support outpaint mode</li> <li>Recommendation is to increase denoising strength to at least 0.8 since outpained area is blank and needs to be filled with noise</li> <li>Outpaint following input image can be controled by overlap setting - higher overlap and more of original image will be part of the outpaint process</li> </ul>"},{"location":"Control-Technical/#logging","title":"Logging","text":"<p>To enable extra logging for troubleshooting purposes, set environment variables before running SD.Next</p> <ul> <li> <p>Linux:</p> <p>export SD_CONTROL_DEBUG=true export SD_PROCESS_DEBUG=true ./webui.sh --debug  </p> </li> <li> <p>Windows:</p> <p>set SD_CONTROL_DEBUG=true set SD_PROCESS_DEBUG=true webui.bat --debug  </p> </li> </ul> <p>Note: Starting with debug info enabled also enables Test mode in Control module</p>"},{"location":"Control-Technical/#known-issues","title":"Known issues","text":""},{"location":"Control-Technical/#dwpose","title":"DWPose","text":"<p>DWPose preprocessor internally uses <code>openmin/mmengine/mmpose/mmdet</code> packages which have not been updated in several years and compatibility with latest versions of <code>torch</code> and other system packages is limited  </p> <p>You may try to manually install DWPose dependencies using following procedure: - Install full CUDA Toolkit as mmengine requires nvidia compiler (nvcc) note: CUDA version should match version of CUDA that comes with <code>torch</code> in in the SD.Next log: </p> <p>Torch: torch==2.6.0+cu126 torchvision==0.21.0+cu126   here <code>cu126</code> means CUDA version 12.6 - Install build tools for your platform   Linux: <code>build-essentials/gcc/make</code> or Windows: <code>Visual Studio Build Tools</code> - Activate your <code>venv</code>   Linux: <code>source venv/bin/activate</code> or Windows: <code>venv\\Scripts\\activate</code> - Install requirements:</p> <p>pip install --upgrade --no-deps --force-reinstall termcolor xtcocotools terminaltables pycocotools munkres shapely openmim==0.3.9 mmengine==0.10.5 mmcv==2.2.0 mmpose==1.3.2 mmdet==3.3.0</p> <p>Note that this can take a long time  </p>"},{"location":"Debug/","title":"Debug","text":"<p>To run SD.Next in debug mode, start it with <code>--debug</code> flag This has no overhead and can be safely used in daily operations as it just prints additional information to logs  </p> <p>Example:</p> <p>webui.bat --debug webui.sh --debug  </p>"},{"location":"Debug/#extra-debug","title":"Extra Debug","text":"<p>Some debug information would be too much for regular use, so it can be enabled by use of environment variables:</p>"},{"location":"Debug/#install-load","title":"Install &amp; load","text":"<ul> <li><code>SD_INSTALL_DEBUG</code>: report detailed information related to packages installation  </li> <li><code>SD_PATH_DEBUG</code>: report all used paths as they are parsed</li> <li><code>SD_SCRIPT_DEBUG</code>: increase verbosity of script and extension load and execution</li> <li><code>SD_MOVE_DEBUG</code>: trace all model moves from and to cpu/gpu  </li> <li><code>SD_EXT_DEBUG</code>: trace extensions load/install/update operations  </li> <li><code>SD_LOAD_DEBUG</code>: report all model loading operations as they happen</li> <li><code>SD_PIP_DEBUG</code>: report details about all pip operations</li> <li><code>SD_CONFIG_DEBUG</code>: report all configuration processing requests</li> </ul>"},{"location":"Debug/#core-processing","title":"Core processing","text":"<ul> <li><code>SD_DISABLE_PBAR</code>: disable progress bar for all operations</li> <li><code>SD_PROCESS_DEBUG</code>: print detailed processing information</li> <li><code>SD_DIFFUSERS_DEBUG</code>: increase verbosity of diffusers processing</li> <li><code>SD_LDM_DEBUG</code>: increase verbosity of LDM processing</li> <li><code>SD_CONTROL_DEBUG</code>: report all debug information related to control module</li> <li><code>SD_PROMPT_DEBUG</code>: print all prompt parsing and encoding information</li> <li><code>SD_SAMPLER_DEBUG</code>: report all possible sampler settings for selected sampler</li> <li><code>SD_STYLES_DEBUG</code>: report styles processing information</li> <li><code>SD_STEPS_DEBUG</code>: report calculations done to scheduler steps</li> <li><code>SD_VAE_DEBUG</code>: report details on all VAE operations</li> <li><code>SD_VIDEO_DEBUG</code>: report all video processing operations as they happen</li> </ul>"},{"location":"Debug/#networks","title":"Networks","text":"<ul> <li><code>SD_EN_DEBUG</code>: report all extra networks operations as they happen</li> <li><code>SD_LORA_DEBUG</code>: increase verbosity of LoRA loading and execution</li> </ul>"},{"location":"Debug/#other","title":"Other","text":"<ul> <li><code>SD_SAVE_DEBUG</code>: report all params and metadata save operations as they happen</li> <li><code>SD_PASTE_DEBUG</code>: report all params paste and parse operations as they happen</li> <li><code>SD_HDR_DEBUG</code>: print HDR processing information</li> <li><code>SD_MASK_DEBUG</code>: reported detailed information on image masking operations as they happen  </li> <li><code>SD_DOWNLOAD_DEBUG</code>: report detailed information on model download operations as they happen</li> <li><code>SD_CALLBACK_DEBUG</code>: report each step as it executes with full details  </li> <li><code>SD_BROWSER_DEBUG</code>: report all gallery operations as they happen</li> <li><code>SD_NAMEGEN_DEBUG</code>: report all filename generation operations as they happen</li> <li><code>SD_PULID_DEBUG</code>: debug logging for pulid operations</li> <li><code>SD_PREVIEW_DEBUG</code>: debug logging for live preview operations</li> <li><code>SD_LLM_DEBUG</code>: debug logging for LLM operations</li> </ul> <p>Example Windows:</p> <p>set SD_PROCESS_DEBUG=true webui.bat --debug  </p> <p>Example Linux:</p> <p>export SD_PROCESS_DEBUG=true webui.sh --debug  </p> <p>Additional information enabled via env variables will show in log with level <code>TRACE</code></p>"},{"location":"Debug/#profiling","title":"Profiling","text":"<p>To run SD.Next in profiling mode, start it with <code>--profile</code> flag This does have overhead, both on processing and memory side, so its not recommended for daily use SD.Next will collect profiling information from both Python, Torch and CUDA and print it upon completion of specific operations  </p> <p>Example:</p> <p>webui.bat --debug --profile webui.sh --debug --profile</p>"},{"location":"Debug/#monitoring","title":"Monitoring","text":"<p>SD.Next memory usage can be monitored using <code>--monitor PERIOD</code> flag in which case, it will print detailed memory statistics for both CPU and GPU ever n seconds  </p> <p>Example:</p> <p>webui.sh --debug --monitor</p>"},{"location":"Debug/#crashes","title":"Crashes","text":"<p>If SD.Next crashes during its code, it will produce traceback in the console output If traceback is not present and process simply exited that typically means that process crashed on the system side and app-level traceback cannot be captured</p> <p>In that case, please check system logs for more information:</p> <ul> <li>Linux: kernel logs <code>dmesg -ku</code></li> <li>Windows: Event Viewer -&gt; Windows Logs -&gt; Application/System</li> </ul>"},{"location":"Debug/#tracebacks","title":"Tracebacks","text":"<p>If runtime error occurs during SD.Next processing, traceback will be printed in the console output Level of information in the traceback can be further increased by setting env variables: - <code>SD_TRACELINES</code>: number of extra lines to show around the error line - <code>SD_TRACEFRAMES</code>: number of frames to show in the traceback - <code>SD_TRACEWIDTH</code>: width of the traceback output - <code>SD_TRACEWRAP</code>: enable word wrapping in the traceback - <code>SD_TRACEINDENT</code>: enable indent guides in the traceback - <code>SD_TRACELOCALS</code>: show local variables in the traceback - <code>SD_TRACEDUNDER</code>: show dunder variables in the traceback locals - <code>SD_TRACESUNDER</code>: show single underscore variables in the traceback locals</p>"},{"location":"Detailer/","title":"Detailer","text":"<p>Detailer is an extra workflow task that can be enabled for any text/image/control generation by selecting it from the menu</p> <p>What does detailer do?</p>"},{"location":"Detailer/#detect-objects","title":"Detect objects","text":"<p>Using specified model(s) and following the specified settings, detect objects in the image Settings that apply are: - Minimum confidence of detected object - Maximum number of detected objects - Max overlap between detected objects - Minimum and Maximum size of detected objects</p>"},{"location":"Detailer/#detail-object","title":"Detail object","text":"<ol> <li>Crops detected object from the image and resizes it to resolution ideal for loaded model  </li> <li>Runs inpaint operation for each cropped object using specified rules  </li> <li>Resizes inpainted object to original size and places it back in the original image  </li> </ol> <p>Settings that apply are: - Prompt and Negative prompt - Steps and Strength of inpaint operation - Edge padding and Edge blur  </p>"},{"location":"Detailer/#notes","title":"Notes","text":""},{"location":"Detailer/#prompt","title":"Prompt","text":"<p>If detailer prompt is specified, it will be used, otherwise primary prompt will be used. In case you want to add detailer-specific prompts but also keep the primary prompt, you can use <code>[prompt]</code> in the detailer prompt text box to refer to it.</p> <p>For example, <code>[prompt]. blue eyes</code></p>"},{"location":"Detailer/#multiple-detailers","title":"Multiple Detailers","text":"<p>If you're using multiple detailer models, you can specify prompt for each model separately by placing them on separate lines: 1st model will use prompt from 1st line, etc.  </p>"},{"location":"Detailer/#multi-class-models","title":"Multi-class models","text":"<p>Most object detection models are trained on a single class of objects, e.g. face or hands Some models are trained on multiple classes in which case you can specify which classes to detect Example multi-class model is <code>yolo11m</code> model When using multi-class model, available classes will be displayed in the log when you run the model  </p>"},{"location":"Detailer/#models","title":"Models","text":"<p>SD.Next comes with several pre-defined detailer single-class models: <code>face-yolo8n</code>, <code>hand_yolov8n</code>, <code>person_yolov8n-seg</code>, <code>eyes-v1</code>, <code>eyes-full-v1</code> And one multi-class model: <code>yolo11m</code></p> <p>Additional models can be added by downloading pretrained model and placing it into folder specified in Settings -&gt; System paths -&gt; Yolo models (default: <code>models/yolo</code></p> <p>Compatible models are any from Yolo model family Models from different families such as Mediapipe are not supported  </p>"},{"location":"Detailer/#expert-mode","title":"Expert Mode","text":"<p>List of models is typically presented as drowpdown list where one or more models can be selected Alternatively, you can use button next to list to convert it to a text box which allows manual inputs of models and (optionally) their parameters Text is parsed as list of models which can be separated by comma <code>,</code>, newline <code>\\n</code> or semicolon <code>;</code>  Each model can have parameters added with colon <code>:</code> character</p> <p>Example:</p> <p>face-yolo8n:steps=5:denoising_strength=0.3, eyes-v1:mask_blur=0.5 hand_yolov8n:sampler_name=DDIM  </p> <p>For full list of parameters, see Parameters page</p> <p>Warning</p> <p>Expert mode is indended for advanced use and SD.Next does not validate the input Overriding some parameter values may result in unexpected behavior or errors</p>"},{"location":"DirectML/","title":"DirectML","text":"<p>SD.Next includes support for PyTorch-DirectML.</p>"},{"location":"DirectML/#how-to","title":"How to","text":"<p>Add <code>--use-directml</code> on commandline arguments.</p> <p>For details, go to Installation.</p>"},{"location":"DirectML/#performance","title":"Performance","text":"<p>The performance is quite bad compared to ZLUDA and ROCm.</p> <p>If your card is relatively new and you prefer Windows system, we recommend ZLUDA.</p> <p>If you are familiar with Linux system, we recommend ROCm.</p>"},{"location":"DirectML/#faq","title":"FAQ","text":""},{"location":"DirectML/#directml-does-not-collect-garbage-memory","title":"DirectML does not collect garbage memory","text":"<p>PyTorch-DirectML does not access graphics memory by indexing. Because PyTorch-DirectML's tensor implementation extends OpaqueTensorImpl, we cannot access the actual storage of a tensor.</p>"},{"location":"DirectML/#an-error-occurs-with-no-error-message","title":"An error occurs with no error message","text":"<p>If you met <code>RuntimeError</code> with no error message (or empty), please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#it-does-not-work-properly-with-fp16","title":"It does not work properly with FP16","text":"<p>If it works with FP32, please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#the-terminal-is-suddenly-frozen-during-generation","title":"The terminal is suddenly frozen during generation","text":"<p>Please report us via GitHub issue or Discord. (please check whether there's a duplicated issue)</p>"},{"location":"DirectML/#olive-experimental-support","title":"Olive (experimental support)","text":"<p>Refer to ONNX Runtime</p>"},{"location":"Docker/","title":"Docker","text":"<p>SD.Next includes basic Dockerfiles for use with for use with following platforms: - nVidia CUDA - AMD ROCm - Intel IPEX - OpenVINO </p> <p>Other system may require different configurations and base images, but principle remains the same  </p> <p>Goal of containerized SD.Next is to provide a fully stateless environment that can be easily deployed and scaled  </p> <p>It is recommended to build your own docker image to include any customizations or extensions you may require</p> <p>Build process is very simple and fast, typically around ~1min for initial build (plus time required to download base image and any dependencies which can take a while depending on your internet connection) to just couple of seconds for any incremental builds using previously cached images  </p> <p>If you want to skip the build process, you can use Prebuilt images provided by community members which are offered on best-effort basis  </p>"},{"location":"Docker/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>If you already have functional Docker on your host, you can skip this section See-also: Manual-Install example</p> <ul> <li>Docker itself https://docs.docker.com/get-started/get-docker/</li> <li>nVidia Container ToolKit to enable GPU support https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</li> </ul>"},{"location":"Docker/#build-image","title":"Build Image","text":"<p>First build will also need to download the base image, which can take a while depending on your connection If you make changes to <code>Dockerfile</code> or update SD.Next, you will need to rebuild the image  </p> <p>Important</p> <p>Build process should be done on a system where SD.Next was started at least once to download all required submodules before docker copy process</p> <p>Tip</p> <p>If you want to include any extensions in the docker image, install/clone them into <code>/extensions</code> folder before building the image</p>"},{"location":"Docker/#cuda","title":"CUDA","text":"<p>SD.Next docker template is based on official base image with <code>python==3.11.11</code>, <code>torch==2.8.0</code> and <code>cuda==12.8</code> Base image <code>pytorch/pytorch:2.8.0-cuda12.8-cudnn9-runtime</code> is 3.99GB And full SD.Next resulting image is ~11.0GB and contains all required dependencies  </p> <pre><code>SD_FOLDER=/home/sdnext # path to sdnext home folder\ndocker build \\\n  --debug \\\n  --tag sdnext/sdnext-cuda \\\n  --progress=plain \\\n  --file $SD_FOLDER/configs/Dockerfile.cuda \\\n  $SD_FOLDER\n</code></pre>"},{"location":"Docker/#rocm","title":"ROCm","text":"<p>Base image <code>rocm/dev-ubuntu-24.04</code> is 3.15GB And full SD.Next resulting image is ~23.15GB and contains all required dependencies</p> <pre><code>SD_FOLDER=/home/sdnext # path to sdnext home folder\ndocker build \\\n  --debug \\\n  --tag sdnext/sdnext-rocm \\\n  --progress=plain \\\n  --file $SD_FOLDER/configs/Dockerfile.rocm \\\n  $SD_FOLDER\n</code></pre>"},{"location":"Docker/#ipex","title":"IPEX","text":"<p>Base image <code>https://hub.docker.com/_/ubuntu</code> is 1.1GB And full SD.Next resulting image is ~9.1GB and contains all required dependencies</p> <pre><code>SD_FOLDER=/home/sdnext # path to sdnext home folder\ndocker build \\\n  --debug \\\n  --tag sdnext/sdnext-ipex \\\n  --progress=plain \\\n  --file $SD_FOLDER/configs/Dockerfile.ipex \\\n  $SD_FOLDER\n</code></pre>"},{"location":"Docker/#openvino","title":"OpenVINO","text":"<p>Base image <code>https://hub.docker.com/_/ubuntu</code> is 1.1GB And full SD.Next resulting image is ~3.6GB  </p> <pre><code>SD_FOLDER=/home/sdnext # path to sdnext home folder\ndocker build \\\n  --debug \\\n  --tag sdnext/sdnext-openvino \\\n  --progress=plain \\\n  --file $SD_FOLDER/configs/Dockerfile.openvino \\\n  $SD_FOLDER\n</code></pre>"},{"location":"Docker/#prebuilt","title":"Prebuilt","text":"<p>SD.Next community members have provided prebuilt docker images for various platforms: - nVidia CUDA   tag: <code>vladmandic/sdnext-cuda:latest</code> compressed size: 3.98GB - AMD ROCm   tag: <code>disty0/sdnext-rocm:latest</code> compressed size: 1.05GB - Intel IPEX   tag: <code>disty0/sdnext-ipex:latest</code> compressed size: 0.4GB - OpenVINO   tag: <code>disty0/sdnext-openvino:latest</code> compressed size: 0.4GB  </p>"},{"location":"Docker/#notes","title":"Notes","text":""},{"location":"Docker/#log-example","title":"Log Example","text":"<pre><code>[+] Building 54.1s (12/12) FINISHED                                                                   docker:default\n =&gt; [internal] load build definition from Dockerfile.cuda                                                   0.0s\n =&gt; =&gt; transferring dockerfile: 2.24kB                                                                      0.0s\n =&gt; [internal] load metadata for docker.io/pytorch/pytorch:2.6.0-cuda12.6-cudnn9-runtime                    0.2s\n =&gt; [internal] load .dockerignore                                                                           0.0s\n =&gt; =&gt; transferring context: 366B                                                                           0.0s\n =&gt; [1/7] FROM docker.io/pytorch/pytorch:2.6.0-cuda12.6-cudnn9-runtime@sha256:xxx                           0.0s\n =&gt; [internal] load build context                                                                           0.1s\n =&gt; =&gt; transferring context: 279.77kB                                                                       0.1s\n =&gt; CACHED [2/7] RUN [\"apt-get\", \"-y\", \"update\"]                                                            0.0s\n =&gt; CACHED [3/7] RUN [\"apt-get\", \"-y\", \"install\", \"git\", \"build-essential\", \"google-perftools\", \"curl\"]     0.0s\n =&gt; CACHED [4/7] RUN [\"/usr/sbin/ldconfig\"]                                                                 0.0s\n =&gt; CACHED [5/7] COPY . /app                                                                                0.0s\n =&gt; CACHED [6/7] WORKDIR /app                                                                               0.0s\n =&gt; [7/7] RUN [\"python\", \"/app/launch.py\", \"--debug\", \"--uv\", \"--use-cuda\", \"--test\", \"--optional\"]        51.0s\n =&gt; exporting to image                                                                                      2.8s\n =&gt; =&gt; exporting layers                                                                                     2.8s\n =&gt; =&gt; writing image sha256:xxx                                                                             0.0s\n =&gt; =&gt; naming to docker.io/sdnext/sdnext-cuda                                                               0.0s\n</code></pre>"},{"location":"Docker/#state","title":"State","text":"<p>As mentioned, the goal of SD.Next docker deployment is fully stateless operations. By default, SD.Next docker containers is stateless: any data stored inside the container is lost when the container stops.  </p> <p>All state items and outputs will be read from and written to <code>/server/data</code> This includes: - Configuration files: <code>config.json</code>, <code>ui-config.json</code> - Cache information: <code>cache.json</code>, <code>metadata.json</code> - Outputs of all generated images: <code>outputs/</code></p>"},{"location":"Docker/#persistence","title":"Persistence","text":"<p>If you plan to customize SD.Next deployment with additional extensions, you may want to create and map docker volume to avoid constaint reinstalls on each startup.  </p>"},{"location":"Docker/#healthchecks","title":"Healthchecks","text":"<p>By default, SD.Next docker container does not include Docker healthchecks, but they can be enabled. Simply remove comment from <code>HEALTHCHECK</code> line in <code>Dockerfile</code> and rebuild the image.  </p>"},{"location":"Docker/#run-local","title":"Run Local","text":"<p>Containers built using above commands are local and can be used directly on the host system  </p> <p>Note</p> <p>This is an EXAMPLE run command, modify as needed for your environment! - Republishes port from container to host directly   You may need to remap ports if you have multiple containers running on the same host - Maps local server folder <code>/server/data</code> to be used by the container as data root   This is where all state items and outputs will be read from and written to - Maps local server folder <code>/server/models</code> to be used by the container as model root   This is where models will be read from and written to - Locations <code>/mnt/data</code> and <code>/mnt/models</code> are configured inside Dockerfile itself,   so either edit those values and rebuild container or make sure those are available - If you're using network attached storage instead of local folders,   you can use those directly and skip mounting local folders</p> <pre><code>docker run \\\n  --name sdnext-container \\\n  --rm \\\n  --gpus all \\\n  --publish 7860:7860 \\\n  --mount type=bind,source=/server/models,target=/mnt/models \\\n  --mount type=bind,source=/server/data,target=/mnt/data \\\n  --detach \\\n  sdnext/sdnext-cuda\n</code></pre> <p>Warning</p> <p>Parameter <code>--gpus all</code> is required to expose nVidia CUDA GPU from parent host to the container For other platforms, use refer to official documentation and use appropriate parameters For example, AMD uses <code>--device /dev/dri</code> and <code>--device /dev/kfd</code></p> <p>Tip</p> <p>Param <code>--detach</code> will run container in background If you want troubleshoot startup and see logs in the console directly, remove <code>--detach</code> parameter</p> <p>Typical SDNext container will start in ~6sec and will be ready to accept connections on port <code>7860</code></p>"},{"location":"Docker/#publish","title":"Publish","text":"<p>If you want to share the containers with others or deploy it on some cloud compute platform, you will need to publish the container to a container registry  </p> <p>There are many container registries with Docker Hub being the most popular and widely used Alternatively, check out GitHub Packages, AWS ECR, Azure ACR, Google AR, Quay, etc  </p> <p>Example using Docker Hub 1. Create an account on Docker Hub 2. Create a peronal access token in your Docker Hub account 3. Login to Docker Hub from your terminal  </p> <p>docker login --username  --password  4. Tag your local container with your Docker Hub username and repository name docker tag sdnext/sdnext-cuda /sdnext-cuda 5. Push your container to Docker Hub docker push /sdnext-cuda <p>Your container is now available on Docker Hub and can be seen on https://hub.docker.com/repository/docker/_username_/sdnext-cuda/!  </p>"},{"location":"Docker/#run-cloud","title":"Run Cloud","text":"<p>Once your container is published, you can run it on any cloud compute platform that supports Docker containers For example RunPod or AWS ECS</p> <p>Example using RunPod: 1. Login to RunPod 2. Create Pod    1. Select platform with desired GPU    2. Edit template:       &gt; Container image: username/sdnext-cuda:latest       &gt; Expose HTTP port: 7860 3. Deploy Pod    Wait for deployment to complete 4. Connect to Pod    RunPod will provide with a public hostname over which you can access your SD.Next instance  </p>"},{"location":"Docker/#extra","title":"Extra","text":"<p>Additional docker commands that may be useful</p> <p>Tip</p> <p>Inspect image</p> <pre><code>docker image inspect sdnext/sdnext-cuda  \n</code></pre> <p>Tip</p> <p>Clean Up</p> <pre><code>docker image ls --all\ndocker image rm &lt;id&gt;\ndocker builder prune --force  \n</code></pre> <p>Tip</p> <p>List Containers</p> <pre><code>docker container ls --all\ndocker ps --all\n</code></pre> <p>Tip</p> <p>View Log</p> <pre><code>&gt; docker container logs --follow &lt;id&gt;\n</code></pre> <p>Tip</p> <p>Stop Container</p> <pre><code>&gt; docker container stop &lt;id&gt;\n</code></pre> <p>Tip</p> <p>Test GPU</p> <pre><code>docker info  \ndocker run --name cudatest --rm --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark  \n</code></pre> <p>Tip</p> <p>Test Torch</p> <pre><code>docker pull pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime  \ndocker run --name pytorch --rm --gpus all -it pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime  \n</code></pre>"},{"location":"Docker/#manual-install","title":"Manual Install","text":"<p>Warning</p> <p>URLs below are examples for Ubuntu 24 Check your Linux distribution and version and use appropriate packages instead</p> <p>Docker</p> <pre><code>wget https://download.docker.com/linux/ubuntu/dists/oracular/pool/stable/amd64/containerd.io_1.7.25-1_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/oracular/pool/stable/amd64/docker-ce_27.5.1-1~ubuntu.24.10~oracular_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/oracular/pool/stable/amd64/docker-ce-cli_27.5.1-1~ubuntu.24.10~oracular_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/oracular/pool/stable/amd64/docker-buildx-plugin_0.20.0-1~ubuntu.24.10~oracular_amd64.deb\nwget https://download.docker.com/linux/ubuntu/dists/oracular/pool/stable/amd64/docker-compose-plugin_2.32.4-1~ubuntu.24.10~oracular_amd64.deb\nsudo dpkg -i *.deb\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nsystemctl status docker\nsystemctl status containerd\n</code></pre> <p>nVidia Container ToolKit</p> <pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt update\nsudo apt install nvidia-container-toolkit\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\ndocker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre>"},{"location":"Docs/","title":"SD.Next Wiki &amp; Docs","text":""},{"location":"Docs/#wiki","title":"Wiki","text":"<p>Any standard SD.Next installation already includes cloned copy of wiki repo, simply go to you SD.Next folder under <code>/wiki</code> and every article will be present as a separte markdown file (with the extension <code>.md</code>)</p> <p>If you want to create separate copy, SD.Next Wiki is a sub-repository to main SD.Next Github repo and can be cloned by simply adding <code>.wiki</code> to the end of the URL  </p> <p>git clone https://github.com/vladmandic/sdnext.wiki  </p> <p>Standard format for all Wiki/Docs documents is Markdown </p>"},{"location":"Docs/#contributing","title":"Contributing","text":"<p>You can edit any existing article by editing its markdown file or create a new article by creating a new markdown file  </p> <p>[!NOTE] GitHub does not support creation of PRs directly towards Wiki repositories</p> <p>[!IMPORTANT] To avoid out-of-sync edits, always make sure you're using latest copy of wiki before performing any edits</p> <p>Once you've completed the edits, you can submit your file using whatever is simplest to you:</p> <ul> <li>Create a Discussion thread in SD.Next GitHub repo and upload your edited MD file</li> <li>Reach out via Discord server - #Contribute channel </li> </ul>"},{"location":"Docs/#docs","title":"Docs","text":"<p>SD.Next Documentation site is automatically built from Wiki and does not need to be manually updated with extension of the menu layout  </p> <p>Any updates to Wiki will trigger automatic rebuild, but not more frequent than every 6 hours - thus any Wiki updates may take several hours to be displayed in docs site  </p> <p>Docs site is hosted in GitHub repo Which contains all the necessary scripts  </p> <p>Menu layout is defined in menu using YAML format and any updates should be submited as a regular PR to that file  </p>"},{"location":"Extensions/","title":"Extensions Development","text":""},{"location":"Extensions/#common-mistakes","title":"Common Mistakes","text":"<ul> <li>Execution on import:   There should be NO CODE that executes on import in any of the extension files (with exception of allowed trivial <code>install.py</code> code)   All code should be in functions/classes and executed on application callbacks   Failure to do so results in slow server startup (at best) or even server crashes (at worst)  </li> <li>Keeping all Python code in <code>/scripts</code>:   That folder should have a single entry point file and that file should import any other file that may reside anywhere else EXCEPT <code>/scripts</code> folder   Failure to do so results server loading and executing each and every file during startup and then extension itself performs imports again   If extension fails to do so results in slow server startup (at best) or even server crashes (at worst)  </li> <li>Browser namespace collisions:   All JS code is imported as-is and lives in an global browser namespace   Do not define functions like <code>log()</code> - use a unique prefix for all your functions and variables  </li> <li>Incorrect usage of callbacks:   For example, JS code can be executed <code>onUiLoaded</code> or <code>onUiUpdated</code>   First one is triggered once, second one is triggered hundreds of times during server startup   Choosing wrong callback to perform initialization work results in initializaton work being performed hundreds of times resulting in slow page loads   Typical problem is why is my browser page loading so slow or why is autolaunch not working? Because browser is asked to do the same things hundreds of times   This applies to both Python and JS code - always check if you're using correct callbacks  </li> <li>Executing code when disabled:   If extension is not enabled, it should not perform any work in callbacks it is registered for and always perform early-exit when work is not needed   This can result in pre/during/post generate delays for no apparent reasons   Typical problem would be ~0.5sec delay before generate start - which quickly adds up to overall time-to-generate regardless of how fast generate actually is  </li> <li>Unsafe references:   Extensions can access server variables either directly or as provided via callbacks, but content of those variables should be handled with care   For example, extension can access <code>image.info</code> property, but there is no guarantee that property will exist for all images - always use safe access methods like <code>get()</code> or <code>getattr()</code>   This also applies to server settings - extensions cannot assume settings never change, otherwise its not viable to ever improve underlying server at all  </li> <li>Unsafe patching:   Extensions can patch server code by providing overrides for some methods, but should do so with care   For example, extension can replace <code>forward</code> functions, but always consider there may be other extensions that do the same   So patching should never be done globally on in a way that breaks other extensions  </li> <li>Running platform specific code:   Not everyone runs the same OS or compute backend and using platform or hardware specific code or relying on packages which are not cross-platform is just bad   For example, never use platform specific designators such as <code>torch.to('cuda')</code> or <code>torch.float16</code>, always use well-defined server variables such as <code>devices.device</code> and <code>devices.dtype</code> </li> <li>Assuming values:   Never assume values, always check a well-defined variable what is the current value and handle it accordingly   For example, extension may not be installed in <code>/extensions</code> folder, it may be relocated to a different folder</li> </ul> <p>Unfortunately, looking at top-20 most popular extensions, most of them are guilty of not just one or two, but majority of the above cases and this is not isolated only to extensions that are considered broken Always remember that by installing extension you give it full access to do anything and you're relying on extension author to perform all the work correctly and safely  </p>"},{"location":"Extensions/#extension-vs-script","title":"Extension vs Script","text":"<ul> <li>Script is a single module that implements a script</li> <li>Script object is inherited from <code>Script</code> class and implements several mandatory and any of the optional methods for well-defined callbacks</li> <li>Extension is a larger implementation that exists as its own folder with a well-defined folder structure</li> <li>Extension code can further define any number of Scripts or work on its own</li> </ul>"},{"location":"Extensions/#extension-folder-structure","title":"Extension Folder Structure","text":"<p>Note: any of the files are optional</p> <ul> <li><code>/preload.py</code>   Loaded early during server startup to provide additional command line parameters   Extension should define a <code>preload(parser: argparser)</code> function to extend parser as needed   Preload should perform no other work or access any other modules</li> <li><code>/install.py</code>   Loaded early during server startup to install any optional requirements   Must use well defined server functions such as <code>launch.run_pip</code> and access only modules explicitly marked as safe at the early stages of server startup   Do not do direct OS calls or perform any other work other than basic installation</li> <li><code>/javascript/*.js</code>   All JS files in a folder are added as-is   There should be no work done in JS scripts other than defining functions/variables and registering callbacks that will be executed at appropriate time</li> <li><code>/style.css</code>   Style is added as-is  </li> <li><code>/scripts/*.py</code>   This is intended as a main entry-point for extension and every file is loaded by the server during startup   There should be no work done in Python scripts other than defining functions/variables and registering callbacks that will be executed at appropriate time</li> </ul>"},{"location":"FAQ/","title":"FAQ: Frequently asked Questions","text":""},{"location":"FAQ/#where-is-the-png-info-tab","title":"Where is the \"PNG Info\" tab?","text":"<p>The functionality is integrated into the \"Process Image\" tabs, removing the need for a separate \"PNG Info\" tab.</p>"},{"location":"FAQ/#where-are-the-command-line-flags-like-xformers","title":"Where are the command-line flags like --xformers?","text":"<p>Most command-line flags have been moved to UI \u2192 Settings. For a complete list, start the web UI with the --help flag.</p>"},{"location":"FAQ/#how-can-i-get-more-information-about-whats-happening-with-my-app","title":"How can I get more information about what\u2019s happening with my app?","text":"<p>Launch the app with the --debug flag and review the setup.log file for detailed insights.</p>"},{"location":"FAQ/#how-do-i-add-command-line-options-to-webuibat-in-windows","title":"How do I add command-line options to webui.bat in Windows?","text":"<p>1. Right-click on webui.bat and select \"Create Shortcut\". 2. Right-click the shortcut and open \"Properties\". 3. Add the options at the end of the \"Target\" field after a space. Example: <code>\"C:\\path\\to\\webui.bat\" --medvram --autolaunch</code> 4. Click OK and use this shortcut to start the app.</p>"},{"location":"FAQ/#how-do-i-use-an-amd-gpu-on-windows","title":"How do I use an AMD GPU on Windows?","text":"<p>Add the --use-zluda command-line flag when starting the app. Checkout the Zluda wiki for more info.  </p>"},{"location":"FAQ/#how-can-i-create-large-images-eg-2048x2048-with-limited-vram","title":"How can I create large images (e.g., 2048x2048) with limited VRAM?","text":"<p>Render a smaller image (e.g., 512x512) and upscale it using the \"Process Image\" tab with an upscaler like SwinIR_4x. This method tiles the image to minimize VRAM usage.</p>"},{"location":"FAQ/#why-are-my-images-dull-or-have-incorrect-colors","title":"Why are my images dull or have incorrect colors?","text":"<p>Try a different VAE file. Add ,sd_vae to the Settings \u2192 User Interface \u2192 Quicksettings list to enable quick VAE selection. Place new VAE files in the models\\VAE folder.</p>"},{"location":"FAQ/#how-do-i-update-to-the-latest-version","title":"How do I update to the latest version?","text":"<p>Run webui.bat --upgrade. This pulls the latest updates and applies them to your installation.</p>"},{"location":"FAQ/#something-broke-how-do-i-restore-my-sdnext-installation","title":"Something broke. How do I restore my SD.Next installation?","text":"<p>Use webui.bat --reinstall to reinstall required components or webui.bat --reset to reset to the latest version.</p>"},{"location":"FAQ/#my-image-folder-is-too-large-how-do-i-manage-storage","title":"My image folder is too large. How do I manage storage?","text":"<p>In Settings \u2192 Image Options, switch from PNG to JPG for smaller file sizes and disable \"Always save all generated images.\" Save only desired images manually.</p>"},{"location":"FAQ/#i-keep-getting-out-of-memory-errors-what-can-i-do","title":"I keep getting out-of-memory errors. What can I do?","text":"<p>1. Render smaller images (512x512) and upscale later. 2. In Settings \u2192 Compute Settings, enable FP16 precision. 3. Use memory-efficient Cross-Attention Optimization methods like Sub-quadratic attention. Disable \"SDP disable memory attention\" if enabled.</p>"},{"location":"FAQ/#why-are-my-images-distorted-eg-two-heads-merged-people","title":"Why are my images distorted (e.g., two heads, merged people)?","text":"<p>Stable Diffusion models are typically trained on 512x512 images. Rendering at different aspect ratios or sizes may cause artifacts. Stick to standard sizes or upscale afterward.</p>"},{"location":"FAQ/#what-does-the-hires-fix-option-do","title":"What does the \"Hires Fix\" option do?","text":"<p>\"Hires Fix\" upscales images during generation, avoiding post-generation artifacts from traditional upscalers. It complements, but does not replace, external upscalers.</p>"},{"location":"FAQ/#what-is-clip-skip","title":"What is CLIP Skip?","text":"<p>CLIP Skip adjusts how detailed image generation becomes. Higher skip values can result in less specific but potentially higher-quality images. Some models benefit from specific settings. PS: CLIP skip is not needed while using SDXL in most cases.</p>"},{"location":"FAQ/#what-is-the-best-sampler-to-use","title":"What is the best sampler to use?","text":"<p>There isn\u2019t a definitive best, but \"DPM++ 2M\" with \"Karras\" is commonly recommended as a reliable general-purpose sampler.</p>"},{"location":"FAQ/#how-do-i-organize-checkpoint-ckpt-files","title":"How do I organize checkpoint (ckpt) files?","text":"<p>Create subfolders in the models directory and move ckpt files there. Restart the UI for changes to take effect. Use meaningful categories like \"Photorealistic\" or \"Anime\".</p>"},{"location":"FAQ/#how-do-i-enable-auto-updates-on-startup","title":"How do I enable auto-updates on startup?","text":"<p>Add the --upgrade flag to your webui.bat launch parameters to automatically update on every startup.</p>"},{"location":"FAQ/#why-do-i-get-an-error-related-to-typing-extensions-when-running-sdnext-for-the-first-time","title":"Why do I get an error related to typing-extensions when running SD.Next for the first time?","text":"<p>This error is caused by a recent upstream Python library conflict.</p> <p>Solution: Simply re-run webui.bat or webui.sh. The issue will resolve automatically, and SD.Next will work as expected.</p>"},{"location":"FAQ/#why-is-my-clip-interrogator-not-working-or-causing-errors","title":"Why is my clip-interrogator not working or causing errors?","text":"<p>The old clip-interrogator is incompatible with the newer transformers package required by SD.Next.</p> <p>Solution: If you manually installed clip-interrogator as an extension, remove it. SD.Next now includes an updated version of clip-interrogator for new installations. To manually update, run: <code>git submodule set-url extensions-builtin/clip-interrogator-ext Dahvikiin/clip-interrogator-ext.git webui.bat --upgrade</code> Then, re-launch SD.Next.</p>"},{"location":"FAQ/#why-do-i-get-an-error-stablediffusionxlpipeline-object-has-no-attribute-decode_first_stage","title":"Why do I get an error: 'StableDiffusionXLPipeline' object has no attribute 'decode_first_stage'?","text":"<p>This issue is related to the Image Previews feature.</p> <p>Solution: Ensure the preview method is not set to Full VAE. If the error persists, disable previews.</p>"},{"location":"FAQ/#what-does-this-error-mean-module-diffusers-has-no-attribute-stablediffusionxlpipeline","title":"What does this error mean: `module 'diffusers' has no attribute 'StableDiffusionXLPipeline'?","text":"<p>This happens when an extension has downgraded the required diffusers package.</p> <p>Solution: Disable all user-installed extensions and re-launch SD.Next.</p>"},{"location":"FAQ/#why-do-i-see-errors-with-xyz_gridpy-or-options-object-has-no-attribute-uni_pc_order","title":"Why do I see errors with xyz_grid.py or 'Options' object has no attribute 'uni_pc_order'?","text":"<p>These errors occur because some legacy variables are not initialized when using specific --backend arguments.</p> <p>Solution: Start SD.Next without any --backend arguments to initialize the variables. This issue will be addressed in future updates.</p>"},{"location":"FLUX/","title":"Black Forest Labs FLUX.1","text":"<p>FLUX.1 family consists of 3 variations: - Pro   Model weights are NOT released, model is available only via Black Forest Labs - Dev   Open-weight, guidance-distilled from Pro variation, available for non-commercial applications - Schnell   Open-weight, timestep-distilled from Dev variation, available under Apache2.0 license  </p> <p></p> <p>Important</p> <p>Allow gated access This is a gated model, you need to accept the terms and conditions to use it For more information see Gated Access Wiki</p> <p>Important</p> <p>Set offloading Set appropriate offloading setting before loading the model to avoid out-of-memory errors For more information see Offloading Wiki</p> <p>Important</p> <p>Choose quantization Check compatibility of different quantizations with your platform and GPU! For more information see Quantization Wiki</p> <p>Tip</p> <p>Use reference models Use of reference models is recommended over manually downloaded models! Simply select it from Networks -&gt; Models -&gt; Reference and the model will be auto-downloaded on first use</p> <p>Important</p> <p>Do not attempt to assemble a full model by loading all individual components That may be how some other apps are designed to work, but its not how SD.Next works Always load full model and then replace individual components as needed</p> <p>Warning</p> <p>If you're getting error message during model load: <code>file=xxx is not a complete model</code> It means exactly that - you're trying to load a model component instead of full model</p>"},{"location":"FLUX/#components","title":"Components","text":"<p>FLUX.1 model consists of: - Unet/Transformer: MMDiT - Text encoder 1: CLIP-ViT/L, - Text encoder 2: T5-XXL Version 1.1 - VAE</p> <p>When using reference models, all components will be loaded as needed. If using manually downloaded model, you need to ensure that all components are correctly configured and available. Note that majority of available downloads are not actually all-in-one models and are instead just a part of the full model with individual components.</p> <p>Tip</p> <p>For convience, you can add setting that allow quick replacements of model components to your quicksettings by adding Settings -&gt; User Interface -&gt; Quicksettings  list -&gt; sd_model_checkpoint, sd_unet, sd_vae, sd_text_encoder</p> <p></p>"},{"location":"FLUX/#fine-tunes","title":"Fine-tunes","text":""},{"location":"FLUX/#diffusers","title":"Diffusers","text":"<p>There are already many FLUX.1 unofficial variations available Any Diffuser-based variation can be downloaded and loaded into SD.Next using Models -&gt; Huggingface -&gt; Download For example, interesting variation is a merge of Dev and Schnell variations by sayakpaul: sayakpaul/FLUX.1-merged </p>"},{"location":"FLUX/#loras","title":"LoRAs","text":"<p>SD.Next includes support for FLUX.1 LoRAs  </p> <p>Since LoRA keys vary significantly between tools used to train LoRA as well as LoRA types, support for additional LoRAs will be added as needed - please report any non-functional LoRAs!</p> <p>Also note that compatibility of LoRA depends on the quantization type! If you have issues loading LoRA, try switching your FLUX.1 base model to different quantization type  </p>"},{"location":"FLUX/#all-in-one","title":"All-in-one","text":"<p>Typical all-in-one safetensors file is over 20GB in size and contains full model with transformer, both text-encoders and VAE Since text encoders and VAE are same between all FLUX.1 models, using all-in-one safetensors is not recommended due to large duplication of data  </p>"},{"location":"FLUX/#unettransformer","title":"Unet/Transformer","text":"<p>Unet/Transformer component of FLUX.1 is a typical model fine-tune and is around 11GB in size  </p> <p>To load a Unet/Transformer safetensors file: 1. Download <code>safetensors</code> or <code>gguf</code> file from desired source and place it in <code>models/UNET</code> folder    example: FastFlux Unchained 2. Load FLUX.1 model as usual and then 3. Replace transformer with one in desired safetensors file using: Settings -&gt; Execution &amp; Models -&gt; UNet </p>"},{"location":"FLUX/#text-encoder","title":"Text Encoder","text":"<p>SD.Next allows changing optional text encoder on-the-fly  </p> <p>Go to Settings -&gt; Models -&gt; Text encoder and select the desired text encoder T5 enhances text rendering and some details, but its otherwise very lightly used and optional Loading lighter T5 will greatly decrease model resource usage, but may not be compatible with all offloading modes  </p> <p>Tip</p> <p>To use prompt attention syntax with FLUX.1, set Settings -&gt; Execution -&gt; Prompt attention to xhinker**</p> <p>Example image with different encoder quantization options </p>"},{"location":"FLUX/#vae","title":"VAE","text":"<p>SD.Next allows changing VAE model used by FLUX.1 on-the-fly There are no alternative VAE models released, so this setting is mostly for future use  </p> <p>Tip</p> <p>To enable image previews during generate, set Settings -&gt; Live Preview -&gt; Method to TAESD** To further speed up generation, you can disable \"full quality\" which triggers use of TAESD instead of full VAE to decode final image</p>"},{"location":"FLUX/#scheduler","title":"Scheduler","text":"<p>Due to specifics of flow-matching methods, number of steps also has strong influence on the image composition, not just on the way how its resolved</p> <p>Example image at different steps </p> <p>Additionally, sampler can be tuned with shift parameter which roughly modifies how long does model spend on composition vs actual diffusion  </p> <p>Example image with different sampler shift values </p>"},{"location":"FLUX/#controlnet","title":"ControlNet","text":"<p>Support for all InstantX/Shakker-Labs models including Union-Pro</p> <p>FLUX.1 ControlNets are large at over 6GB on top of already very large FLUX.1 model as such, you may need to use offloading:sequential which is not as fast, but uses far less memory  </p> <p>When using union model, you must also select control mode in the control unit  </p>"},{"location":"FLUX/#flux-tools","title":"Flux Tools","text":"<p>Link to Flux Tools announcement - Redux is actually a tool - Fill is inpaint/outpaint optimized version of Flux-dev - Canny/Depth are optimized versions of Flux-dev for their respective tasks: they are not ControlNets that work on top of a model  </p> <p>To use, go to image or control interface and select Flux Tools in scripts All models are auto-downloaded on first use note: All models are gated and require acceptance of terms and conditions via web page recommended: Enable on-the-fly quantization to reduce resource usage - Redux: ~0.1GB   works together with existing model and basically uses input image to analyze it and use that instead of prompt recommended: low denoise strength levels result in more variety - Fill: ~23.8GB, replaces currently loaded model note: can be used in inpaint/outpaint mode only - Canny: ~23.8GB, replaces currently loaded model recommended: guidance scale 30 - Depth: ~23.8GB, replaces currently loaded model recommended: guidance scale 10  </p>"},{"location":"FLUX/#notes","title":"Notes","text":""},{"location":"FLUX/#performance","title":"Performance","text":"<p>Performance and memory usage of different FLUX.1 variations:</p> dtype time (sec) performance memory offload note bf16 &gt;32 GB none *1 bf16 50.47 0.40 it/s balanced *2 bf16 94.28 0.21 it/s 1.89 GB sequential nf4 14.69 1.36 it/s 17.92 GB none nf4 21.02 0.95 it/s balanced *2 nf4 sequential err qint8 15.42 1.30 it/s 18.85 GB none qint8 balanced err qint8 sequential err qint4 18.37 1.09 it/s 11.38 GB none qint4 balanced err qint4 sequential err <p>Notes: - 1: Memory usage exceeeds 32GB and is not recommended - 2: Balanced offload VRAM usage is not included since it depends on desired threshold  </p>"},{"location":"Features/","title":"Features","text":""},{"location":"Features/#control","title":"Control","text":"<p>SDNext's Control tab is our long awaited effort to bring ControlNet, IP-Adapters, T2I Adapter, ControlNet XS, and ControlNet LLLite to our users.  </p> <p>After doing that, we decided that we would add everything else under the sun that we could squeeze in there, and place it directly into your hands with greater options and flexibility than ever before, to allow you to Control your image and video generation with as little effort, as as much power, as possible.</p> <p>Note that this document is a work in progress, it's all quite complex and will take some time to write up a bit more as well as smooth out the rough edges and correct any issues and bugs that pop up, expect frequent updates! </p> <p>This guide will attempt to explain how to use it so that anyone can understand it and put it to work for themselves.  </p> <p>Be sure to also check out the Control resource page which has more technical information as well as some general tips and suggestions. Its usage information will be merged into this page soon\u2122\ufe0f.</p> <p>We'll start with the... Control Controls!</p>"},{"location":"Features/#controls","title":"Controls","text":""},{"location":"Features/#input","title":"Input","text":"<p>The Input control is exactly what it sounds like, it controls what input images (or videos) are contributing to your image generation, by default that is just the image in the Control input pane, however if you select <code>Separate init image</code>, another image pane will appear below, allowing you to use that as well.  </p> <p>Note: When using a Control input image as well as a Init input image, the Init input dominates. Adjusting denoise to &gt;=0.9 is recommended, as that will allow the Control input to balance with the Init input. Higher values will increase the strength of Control input further, giving it dominance.  </p> <p></p> <p><code>Show Preview</code> is simple, it controls the visibility of the preview window in the far right of the middle row. You'll want this on if you're doing any kind of masking or manipulations that you would want to preview before generating.</p> <p>There are 3 different Input types:</p> <ul> <li> <p><code>Control only</code>: This uses only the Control input below as a source for any ControlNet or IP Adapter type tasks based on any of our various options.  </p> </li> <li> <p><code>Init image same as control</code>: This option will additionally treat any image placed into the <code>Control input</code> pane as a source for img2img type tasks, an image to modify for example.  </p> </li> <li> <p><code>Separate init image</code>: This option creates an additional window next to <code>Control input</code> labeled <code>Init input</code>, so you can have a separate image for both Control operations and an init source.</p> </li> </ul> <p><code>Denoising strength</code> is the same as if you were doing any img2img operation. The higher the value, the more denoising that will take place, and the greater any source image will be modified.</p>"},{"location":"Features/#size","title":"Size","text":"<p>This can be a little confusing at first because of the <code>Before</code> and <code>After</code> subtabs, however it's really quite simple and extremely powerful. The Control size menu allows you to manipulate the size of your input images before and after inference takes place.  </p> <p></p> <p>The <code>Before</code> subtab does 2 things:</p> <ul> <li> <p>If you do not select any <code>Resize method</code>, it is only controlling the output image size width and height in pixels as it would in any text2img or img2img operation.</p> </li> <li> <p>However, if you do select a <code>Resize method</code>, Nearest for example, you can upscale or downscale the <code>Control input</code> image before any other operations take place. This will be the size of any image used in further operations. Second Pass is not entirely functional yet, but will be part of this.</p> </li> </ul> <p>For example, you might have a much larger image, such as 2048x3072, that you want to use with canny or depth map, but you do not want an image that large to manipulate or guide your generation, that would be prohibitive, slower, and possibly cause an OOM.  </p> <p>This is where <code>Resize method</code> comes in, you would simply select a resize method, typically Nearest or Lanczos, and then either set the pixel width or height you want to resize to under Fixed, or switch over to Scale and select a number below 1. A setting of 0.5 would make your input image effectively 1024x1536 pixels, which would be used as input for later operations.  </p> <p>The <code>After</code> subtab controls any upscaling or downscaling that would take place at the end of your image generation process, most commonly this would either be latent upscaling, and ESRGAN model such as 4x Ultrasharp, or one of the various chaiNNer models we provide. This is the same as it would be in a standard upscaling via text2img or img2img.</p>"},{"location":"Features/#mask","title":"Mask","text":"<p>The Mask controls are where we start getting into the real meat of Control, not only does it allow a plethora of different options to mask, segment, and control the view of your masking with various preview types, but it comes with 22 different colormaps for your viewing pleasure! (And I think vlad made some of those words up \ud83e\udd2b)  </p> <p></p> <ul> <li> <p><code>Live update</code>: With this checked, your masking will update as you make changes to it, if this is off, you will need to hit the <code>Refresh</code> button to the right to have your preview pane update, making more changes to it while it is processing may lead to it being desynchronized, just hit the refresh button if it does not look correct.</p> </li> <li> <p><code>Inpaint masked only</code>: Inpainting will apply only to areas you have masked if this is checked. You must actually inpaint something, otherwise it's just img2img.</p> </li> <li> <p><code>Invert mask</code>: Inverts the masking, things you mark with the brush will be excluded from a full mask of the image.</p> </li> <li> <p><code>Auto-mask</code>: There are three options here, Threshold, Edge, and Greyscale. Each provides a different method of auto-masking your images.</p> </li> <li> <p><code>Auto-segment</code>: Just like Auto-mask, we have provided an extensive list of Auto-segmentation models, they don't require ControlNet to handle the process, but may take a few seconds to process, depending on your GPU.  </p> </li> <li> <p><code>Preview</code>: You can select the preview type here, we have provided 5 modes, Masked, Binary, Greyscale, Color, and Composite, which is the default.</p> </li> <li> <p><code>Colormap</code>: You can select the style/color scheme of the preview here. There are 22 fantastic color schemes!</p> </li> <li> <p><code>Blur</code>: This blurs the edges of what you have masked, to allow some flexibility. Play with it.</p> </li> <li> <p><code>Erode</code>: This slider controls the reduction of your auto-masking or auto-segmentation border.</p> </li> <li> <p><code>Dilate</code>: This slider controls the expansion of your auto-masking or auto-segmentation border.</p> </li> </ul>"},{"location":"Features/#video","title":"Video","text":"<p>The Video controls are quite exciting and fun to play with, with our tools now you can, if you wished, turn any video into an anime version for example, frame by frame. There are three output options, GIF, PNG, and MP4. You must select one of these to have video output. With these simple controls, you can tweak your video output with surprising flexibility. Some video output methods provide more controls, try them all.</p> <p></p> <ul> <li> <p><code>Skip input frames</code>: This setting controls how many frames are processed from input instead of every frame. Setting it to 0 would mean processing every frame, a setting of 1 would process every other frame, a setting of 2 would process every third frame, cutting the number of total frames by 2/3rds, and so on.</p> </li> <li> <p><code>Video file</code>: You select the type of output you want here, animated GIF (not JIF!), animated PNG, or MP4 video, all provided via FFMPEG of course.  </p> </li> <li> <p><code>Duration</code>: The length in seconds you want your output video to be.  </p> </li> <li> <p><code>Pad frames</code>: Determine how many frames to add to the beginning and end of the video. This feature is particularly useful when used with interpolation.</p> </li> <li> <p><code>Interpolate frames</code>: The number of frames you want interpolated (via RIFE) between existing frames (filtered by skip input frames) in a video sequence. This smoothens the video output, especially if you're skipping frames to avoid choppy motion or low frame rates.</p> </li> <li> <p><code>Loop</code>: This is purely for animated GIF and PNG output, it enables the classic looping that you would expect.</p> </li> </ul> <p>When you're using interpolation, the software also detects scene changes. If the scene changes significantly, it will insert pad frames instead of interpolating between two unrelated frames. This ensures a seamless transition between scenes and maintains the overall quality of the video output.</p>"},{"location":"Features/#extensions","title":"Extensions","text":"<p>These are some nice goodies that we have cooked up so that no actual installed extensions are necessary, you may even find that our version works better!</p>"},{"location":"Features/#animatediff","title":"AnimateDiff","text":"<p>This is the new home of our Vlad-created implementation of the AnimateDiff extension. Now with FREE FreeInit!  </p> <p>I honestly don't know how to use this, so I'll update this when I do. My apologies! But if you already do, enjoy!</p> <p></p>"},{"location":"Features/#ip-adapter","title":"IP-Adapter","text":"<p>This is our IP Adapter implementation, with 10 available models for your image or face cloning needs!</p> <p></p>"},{"location":"Features/#image-panes","title":"Image Panes","text":"<p>You may notice small icons above the image panes that look like pencils, these are Interrogate buttons. The left one is BLIP, and the right one is DeepBooru. Click one of the buttons to interrogate the image in the pane below it. The results will appear in your prompt area.</p> <p></p>"},{"location":"Features/#control-input","title":"Control Input","text":"<p>This is the heart of Control, you may put any image or even video here to be processed by our system, that means any and all scripts, extensions, even the various Controlnet variants below, though you can individually add guidance images to each of those. If an image is placed here, the system will assume you are performing an img2img process of some sort. If you upload a video to SDNext via the Control input pane, you will see that you can play the video, both input and resultant output. Batching and folders should work as expected.</p> <p>Note below there are 2 other buttons, Inpainting and Outpainting, below.</p> <p></p>"},{"location":"Features/#controlnet","title":"ControlNet+","text":"<p>At the very bottom of the Control page, we have what you've all been waiting for, full ControlNet! I do mean full too, we have it all! This includes SD and SD-XL. at last! You won't ever need the ControlNet extension ever again.  </p> <p>This will take a bit more work to document example workflows, but there are tooltips, and if you've used ControlNet before, you shouldn't have any problems! However if you do, hop on by our Discord server and we're happy to help.</p> <p> </p>"},{"location":"Features/#processvisual-query","title":"Process/Visual query","text":"<p>Visual query subsection of the Process tab contains tools to use Visual Question Answering interrogation of images using Vision Language Models.</p> <p>Currently supported models:</p> <ul> <li>Moondream 2</li> <li>GiT Textcaps</li> <li>GIT VQA</li> <li>Base</li> <li>Large</li> <li>Blip</li> <li>Base</li> <li>Large</li> <li>ViLT Base</li> <li>Pix Textcaps</li> <li>MS Florence 2</li> <li>Base</li> <li>Large</li> </ul> <p> </p>"},{"location":"Features/#lcm","title":"LCM","text":"<p>LCM (Latent Consistency Model) is a new feature that provides support for SD 1.5 and SD-XL models.</p>"},{"location":"Features/#installation","title":"Installation","text":"<p>Download the LCM LoRA models and place them in your LoRA folder (models/lora or custom):</p> <ul> <li>For SD 1.5: lcm-lora-sdv1-5</li> <li>For SD-XL: lcm-lora-sdxl</li> </ul> <p>As they have the same name, we recommend doing them one at a time and then renaming it before downloading the next.  </p>"},{"location":"Features/#usage","title":"Usage","text":"<ol> <li>Load your preferred SD 1.5 or SD-XL model that you want to use LCM with</li> <li>Load the correct LCM lora (lcm-lora-sdv1-5 or lcm-lora-sdxl) into your prompt, ex: <code>&lt;lora:lcm-lora-sdv1-5:1&gt;</code></li> <li>Set your sampler to LCM</li> <li>Set number of steps to a low number, e.g. 4-6 steps for SD 1.5, 2-8 steps for SD-XL</li> <li>Set your CFG Scale to 1 or 2 (or somewhere between, play with it for best quality)</li> <li>Optionally, turning on Hypertile and/or FreeU will greatly increase speed and quality of output images</li> <li>???</li> <li>Generate!</li> </ol>"},{"location":"Features/#notes","title":"Notes","text":"<ul> <li>This also works with latent upscaling, as a second pass/hires fix.</li> <li>LCM scheduler does not support steps higher than 50</li> <li>The <code>cli/lcm-convert.py</code> script can convert any SD 1.5 or SD-XL model to an LCM model by baking in the LoRA and uploading to Huggingface</li> </ul>"},{"location":"Features/#lora","title":"LoRa","text":""},{"location":"Features/#introduction","title":"Introduction","text":"<p>LoRA models are small Stable Diffusion models that apply tiny changes to standard checkpoint models. They are usually 10 to 100 times smaller than checkpoint models. That makes them very attractive to people who have an extensive collection of models.</p> <p>This is a tutorial for beginners who haven\u2019t used LoRA models before. You will learn what LoRA models are, where to find them, and how to use them in the SD.NEXT WebUI.</p> <p>You can place your LoRA models in <code>./*Your SD.NEXT directory*/models/Lora</code>. (You can change the path of the LoRa directory in your settings in the system paths tab)</p> <p>You can either access your LoRA models by clicking on Networks and then on Lora and select the lora model you want to add to your prompt or by typing:  <code>&lt;lora:*lora file name*:*preferred weight*&gt;</code></p> <p>The weight indicates the amount of effect it has on your image generation.</p>"},{"location":"Features/#notes_1","title":"Notes","text":"<ul> <li>Some LoRa's are for different diffusers pipelines, for example you have SD1.5 LoRa's and you have SDXL LoRa's, if you try to use one of these while using the wrong type diffusers pipeline it will give an error.</li> </ul>"},{"location":"Features/#hidiffusion","title":"HiDiffusion","text":""},{"location":"Features/#introduction_1","title":"Introduction","text":"<p>Diffusion models are great for high-resolution image synthesis but struggle with object duplication and longer generation times at higher resolutions. HiDiffusion, a solution with two key components: Resolution-Aware U-Net (RAU-Net), which prevents object duplication by adjusting feature map sizes, and Modified Shifted Window Multi-head Self-Attention (MSW-MSA), which reduces computation time. HiDiffusion can be added to existing models to generate images up to 4096\u00d74096 pixels at 1.5-6 times faster speeds. Experiments show that HiDiffusion effectively tackles these issues and sets new standards for high-resolution image synthesis.</p>"},{"location":"Features/#reference","title":"Reference","text":"<p>You can read more about HiDiffusion in the link below:</p> <ul> <li>https://hidiffusion.github.io/</li> </ul>"},{"location":"Features/#benefits-of-using-hidiffusion","title":"Benefits of using HiDiffusion","text":"<ul> <li>Increases the resolution and speed of pretrained diffusion models.</li> <li>Supports txt2image, img2img, inpainting and more.</li> </ul>"},{"location":"Features/#how-to-enable","title":"How to enable","text":"<p>Check the HiDiffusion checkbox in the SD.NEXT webUI in either the Text, Image or Control tab.</p> <p> </p>"},{"location":"Features/#face-restore","title":"Face restore","text":""},{"location":"Features/#introduction_2","title":"Introduction","text":"<p>Face restore will try to detect a face or multiple faces in a generated image, then it will do a seperate pass over the face which makes the face have an higher resolution and more detailed.</p>"},{"location":"Features/#models","title":"Models","text":"<p>SD.NEXT has 3 different choices for face restoration:</p> <ul> <li>Codeformer</li> <li>GFPGAN</li> <li>Detailer</li> </ul>"},{"location":"Features/#codeformer","title":"Codeformer","text":"<p>CodeFormer, created by sczhou, is a robust face restoration algorithm designed to work with both old photos and AI-generated faces. The underlying technology of CodeFormer is based on a Transformer-based prediction network, which models global composition and context for code prediction. This allows the model to discover natural faces that closely approximate the target faces, even when the inputs are severely degraded. A controllable feature transformation module is also included, which enables a flexible trade-off between fidelity and quality. More here: Codeformer.</p> <p>The CodeFormer weight parameter:</p> <p>0 = Maximum effect; 1 = Minimum effect.</p>"},{"location":"Features/#gfpgan","title":"GFPGAN","text":"<p>GFPGAN stands for \"Generative Facial Prior Generative Adversarial Network\". It is an artificial intelligence model developed for the purpose of real-world face restoration. The model is designed to repair and enhance faces in photos, particularly useful for restoring old or damaged photographs. GFPGAN leverages generative adversarial networks (GANs), specifically utilizing facial priors encapsulated in a pre-trained face GAN like StyleGAN2, to restore realistic and faithful facial details. More here: GFPGAN.</p>"},{"location":"Features/#detailer","title":"Detailer","text":"<p>Face Hires is a feature that aims to improve the details of faces in generated images. It draws inspiration from the popular Adetailer extension, but simplifies the workflow to a single checkbox, making it easy to enable or disable.</p> <p>Here's what Face Hires does:</p> <ul> <li>Detection: Identifies and locates faces in the image.</li> <li>Cropping and Resizing: Crops each detected face and enlarges it to become a full image.</li> <li>Enhancement: Applies an image-to-image (img2img) process to enhance the face.</li> <li>Restoration: Resizes the enhanced face back to its original size and integrates it back into the original image.</li> </ul> <p>This process addresses the common issue where models fail to perfectly resolve details in images, especially for faces that are not front and center.</p>"},{"location":"Features/#parameters","title":"Parameters","text":"<p>Face hires will use secondary prompt for face restore step if its present. if its not, it will use normal primary prompt.</p> <p>Face hires has number of tunable paramteters in settings in the postprocessing tab.</p> <ul> <li>Minimum confidence: minimum score that each detected face must meet during detection phase.</li> <li>Max faces: maximum number of faces per image it will try to run on.</li> <li>Max face overlap: maximum overlap of when multiple faces are detected before it considers them a single object.</li> <li>Min face size: minimum face size it should attempt to fix (e.g. do not try to fix very small faces in the background).</li> <li>Max face size: maximum face size it should attempt to fix (e.g. why try to fix something if its already front-and-center and takes 90% of image).</li> <li>Face padding: when cropping face, add padding around it. </li> </ul> <p> </p>"},{"location":"Features/#second-pass","title":"Second pass","text":""},{"location":"Features/#introduction_3","title":"Introduction","text":"<p>Second pass means that after an image is finished with it's first pass, it has the function to do another pass, Second pass.</p>"},{"location":"Features/#usage_1","title":"Usage","text":"<p>You can enable the second pass on the refine tab in ther generation settings. </p> <p>You can customize the second pass to your needs: </p> <ul> <li>Upscaler: Here you can choose a upscaler model.</li> <li>Rescale by: Here you can choose how much the resolution of your image gets multiplied. (You can also choose a custom resolution at Width resize and Height resize).</li> <li>Force HiRes: This will force it to execute HiRes fix, that means it will not only multiply the resolution of the image, but also add more detail to it, you can also use HiRes fix without forcing it by selecting a latent upscaling method.</li> <li>Secondary sampler: Here you can choose a different sampler that will be used during HiRes fix, you can also let it use the same one as the first pass by setting it to \"Same as primary\".</li> <li>HiRes steps: How much steps you want your HiRes fix to take.</li> <li>Strength: Strength is basically the amount that HiRes Fix or Refiner is allowed to change.</li> <li>Refiner start: Refiner pass will start when base model is this much complete. (Set bigger than 0 and smaller than 1 to run after full base model run).</li> <li>Refiner steps: How much steps you want your Refiner pass to take.</li> <li>Secondary prompts: You can also add an secondary positive and negative prompt to your HiRes fix, so you can make a base with the first pass and add the details in the second pass.</li> </ul>"},{"location":"Features/#refiner","title":"Refiner","text":"<p>Refiner will only start if you have selected a refiner model in \"Execution &amp; Models\" tab in the settings:</p> <p> </p>"},{"location":"Features/#styles","title":"Styles","text":""},{"location":"Features/#introduction_4","title":"Introduction","text":"<p>Styles are prompt presets you can enable, it saves both headaches and time, because it adds a specific style to your prompt without you having to type it yourself.</p>"},{"location":"Features/#usage_2","title":"Usage","text":"<p>You can select a style under the generation controls:</p>"},{"location":"Features/#adding-your-own-styles","title":"Adding your own styles","text":"<p>You can add your own styles in <code>.\\*Your SD.NEXT directory*\\models\\styles</code></p> <p> </p>"},{"location":"Features/#clip-skip","title":"Clip skip","text":""},{"location":"Features/#introduction_5","title":"Introduction","text":"<p>Clip Skip plays a significant role in stable diffusion models. It is a technique used to improve the performance of image and video compression in stable diffusion algorithms. By allowing the skipping of certain pixels or blocks, Clip Skip reduces the amount of data that needs to be processed, resulting in faster and more efficient compression. This technique also helps to reduce artifacts and enhance the overall quality of compressed images and videos.</p>"},{"location":"Features/#usage_3","title":"Usage","text":"<p>You can enable clip skip in the advanced tab in the image generation settings. The default is 1, but also a very popular value is 2 and lots of models are compatible with it. </p> <p> </p>"},{"location":"Features/#embedding","title":"Embedding","text":""},{"location":"Features/#introduction_6","title":"Introduction","text":"<p>Embedding, also called textual inversion, is an alternative way to control the style of your images in Stable Diffusion. Embedding is the result of textual inversion, a method to define new keywords in a model without modifying it. The method has gained attention because its capable of injecting new styles or objects to a model with as few as 3 -5 sample images.</p>"},{"location":"Features/#usage_4","title":"Usage","text":"<p>You can either access the embedding in the networks menu or you can type it yourself by writing the embedding name without the file extensions. (You can also edit the weight of the embedding by writing <code>(*Embedding name*:*desired weight*)</code>). </p>"},{"location":"Features/#add-embeddings","title":"Add embeddings","text":"<p>You can add your embeddings in <code>.\\*Your SD.NEXT directory*\\models\\embeddings</code></p> <p> </p>"},{"location":"Features/#upscaling","title":"Upscaling","text":""},{"location":"Features/#introduction_7","title":"Introduction","text":"<p>Upscaling is when an image is upscaled using stable diffusion, the algorithm analyzes the image's pixel values to determine the diffusion rate. The rate calculated is then used to expand the pixels in higher resolution, resulting in a sharper and clearer image without compromising its quality.</p>"},{"location":"Features/#usage_5","title":"Usage","text":"<p>In SD.NEXT there are 2 ways to use upscaling, you can either enable it in the second pass(this will not upscale the end results from the second pass, it will only upscale the image or latent from the first pass) or you can use it in the process tab under the upscaling menu in the image generation settings.</p> <p></p> <p></p>"},{"location":"Features/#custom-upscale-models","title":"Custom upscale models","text":"<p>You can also add your own upscale models to SD.NEXT in the directories below:</p> <ul> <li>ESRGAN</li> <li>LDSR</li> <li>SCUNet</li> <li>SwinIR</li> <li>RealESRGAN</li> <li>chaiNNer </li> </ul> <p> You can find these directories in <code>.\\*Your SD.NEXT directory*\\models</code></p>"},{"location":"Features/#most-used","title":"Most used","text":"<p>General:</p> <ul> <li>Ultrasharp 4x</li> <li>RealESRGAN 4x+</li> </ul> <p>Anime:</p> <ul> <li>Animesharp 4x</li> <li>RealESRGAN 4x+ Anime6B</li> </ul> <p> </p>"},{"location":"Features/#samplers","title":"Samplers","text":""},{"location":"Features/#introduction_8","title":"Introduction","text":"<p>To produce an image, Stable Diffusion first generates a completely random image in the latent space. The noise predictor then estimates the noise of the image. The predicted noise is subtracted from the image. This process is repeated a dozen times. In the end, you get a clean image.</p> <p>This denoising process is called sampling because Stable Diffusion generates a new sample image in each step. The method used in sampling is called the sampler or sampling method.</p> <p>For more information you can click on the links below:</p> <ul> <li>Complete Samplers Guide</li> <li>Stable Diffusion Sampler Art</li> </ul> <p>Both guides explain in detail several different samplers and their capabilities along with their advantages and disadvantages and the appropiate amount of steps you should use with each sampler.</p> <p>Below is a screenshot of all the samplers SD.Next provides:</p> <p> </p>"},{"location":"Features/#pag","title":"PAG","text":""},{"location":"Features/#introduction_9","title":"Introduction","text":"<p>PAG or Pertubed Attention Guidance is like a modern/better version of CFG scale, although less universal as it cannot be applied in all circumstances. If it's applied, it will be added to image metadata and you'll see it in the log as <code>StableDiffusionPAGPipeline</code>. </p> <p>You can find more information about PAG by clicking on this.</p>"},{"location":"Features/#usage_6","title":"Usage","text":"<p>You can enable PAG by setting the attention guidance slider above 0. The attention guidance slider is located in the advanced tab of the image generation settings.  </p> <p> </p>"},{"location":"Features/#interrogate","title":"Interrogate","text":""},{"location":"Features/#introduction_10","title":"Introduction","text":"<p>Interrogation, or captioning helps us refine the prompts we use, enabling us to see how the AI system tags and classifies, and what terms it uses. By looking at these we can further refine our images to attain the concept we have in mind, or remove them via negative prompts.</p>"},{"location":"Features/#usage_7","title":"Usage","text":"<p>You can find the interrogate option on the process tab under \"Interrogate image\" or \"Interrogate batch\" if you want to interrogate a batch of images, then select the CLIP Model you want to use to interrogate the image(s). </p> <p> </p>"},{"location":"Features/#vae","title":"VAE","text":""},{"location":"Features/#introduction_11","title":"Introduction","text":"<p>VAE or Variational Auto Encoder encodes an image into a latent space, which is a lower-dimensional representation of the image. The latent space is then decoded into a new image, which is typically of higher quality than the original image.   There are two main types of VAEs that can be used with Stable Diffusion: exponential moving average (EMA) and mean squared error (MSE).  EMA is generally considered to be the better VAE for most applications, as it produces images that are sharper and more realistic. MSE can be used to produce images that are smoother and less noisy, but it may not be as realistic as images generated by EMA.</p>"},{"location":"Features/#usage_8","title":"Usage","text":"<p>You can change the VAE in the settings in \"Execution &amp; Models\".   You can add the VAE's in <code>.\\*Your SD.NEXT directory*\\models\\VAE</code>.</p> <p> </p>"},{"location":"Features/#cfg-scale","title":"CFG scale","text":""},{"location":"Features/#introduction_12","title":"Introduction","text":"<p>the CFG scale (classifier-free guidance scale) or guidance scale is a parameter that controls how much the image generation process follows the text prompt. The higher the value, the more the image sticks to a given text input.   But this does not mean that the value should always be set to maximum, as more guidance means less diversity and quality.</p>"},{"location":"Features/#usage_9","title":"Usage","text":"<p>Simply use the CFG scale slider in the image generation settings. </p> <p> </p>"},{"location":"Features/#live-preview","title":"Live Preview","text":""},{"location":"Features/#introduction_13","title":"Introduction","text":"<p>The live preview feature allows you to see the image before it is fully generated, so in other words you can see the progress of the image while it is generating.</p>"},{"location":"Features/#usage_10","title":"Usage","text":"<p>You can modify the live preview settings to your liking in settings &gt; Live Previews. </p> <p></p> <p>Live preview display period: The amount of steps it has to take before requesting a preview image. Live preview method: The method you want SD.NEXT to use to display preview images.</p>"},{"location":"Features/#methods","title":"Methods","text":""},{"location":"Features/#simple","title":"Simple","text":"<p>Very cheap approximation. Very fast compared to VAE, but produces pictures with 8 times smaller horizontal/vertical resolution and extremely low quality.</p>"},{"location":"Features/#approximate","title":"Approximate","text":"<p>Cheap neural network approximation. Very fast compared to VAE, but produces pictures with 4 times smaller horizontal/vertical resolution and lower quality.</p>"},{"location":"Features/#taesd","title":"TAESD","text":"<p>TAESD is very tiny autoencoder which uses the same \"latent API\" as Stable Diffusion's VAE*. TAESD can decode Stable Diffusion's latents into full-size images at (nearly) zero cost.</p>"},{"location":"Features/#full-vae","title":"Full VAE","text":"<p>Uses the entire VAE to decode the full resolution image as preview during the generation, this is by far the slowest of the other 3 options.</p> <p> </p>"},{"location":"FramePack/","title":"FramePack","text":"<p>Implementation of Lllyasviel FramePack for Tencent HunyuanVideo I2V With some major differences and improvements: - T2V, I2V &amp; FLF2V modes support - Bi-directional and Forward-only (F1) model variants - Resolution and frame-rate scaling: output resolution in any aspect ration and/or resolution - Prompt enhancer: use LLM to enhance your short prompts - Complex actions: modify prompt each section of the video - Video encode: multiple video codecs, raw export, frame export, frame interpolation - LoRA: support for LoRA models - Offloading and quantization: support for offloading and quantization - API: support for use via HTTPRest API calls - Custom model: support for custom model loading  </p> <p>Screenshot:</p> <p></p> <p>Example:</p> <p>Important</p> <p>Video support requires <code>ffmpeg</code> to be installed and available in the <code>PATH</code></p>"},{"location":"FramePack/#modes","title":"Modes","text":"<p>Supports 3 modes of operation:  </p>"},{"location":"FramePack/#t2v-text-to-video","title":"T2V: text-to-video","text":"<ul> <li>Uses only text prompt and generates video from it  </li> <li>Automatically triggered if no init image is provided  </li> </ul>"},{"location":"FramePack/#i2v-image-to-video","title":"I2V: image-to-video","text":"<ul> <li>Uses image as init frame and text prompt to generate video  </li> <li>Automatically triggered if init image is provided and there is no end frame  </li> <li>Init strength controls the strength of the image, similar to img2img denoising strength  </li> <li>Vision strength controls the strength of the vision model that controls video generation  </li> </ul>"},{"location":"FramePack/#flf2v-frame-last-frame-to-video","title":"FLF2V: frame-last-frame-to-video","text":"<ul> <li>Uses images as init frame and end-frame and text prompt to generate video  </li> <li>Automatically triggered if init image and end frame are provided  </li> <li>Init strength controls the strength of the image, similar to img2img denoising strength  </li> <li>End strength controls the strength of the end frame compared to init frame  </li> <li>Vision strength controls the strength of the vision model that controls video generation  </li> <li>Ratio of init and end strengths can be used to skew video towards init or end frame  </li> </ul>"},{"location":"FramePack/#variants","title":"Variants","text":"<p>Both FramePack variants are based on HunyuanVideo model, but with different approach to video generation: - Bi-directional model: default   Runs generation in reverse order and assembles video - Forward-only model: F1   Runs generation in forward order and assembles video  </p>"},{"location":"FramePack/#resolution-scaling","title":"Resolution Scaling","text":"<p>Video model is trained on 640p resolution, but can be used to generate video at different resolutions However, the resolution must be within exact supported aspect rations Given any input image, the model will first find the closest aspect ratio and then scale it to the desired resolution As a result, the output resolution will be the closest supported aspect ratio to the desired resolution  </p> <p>Note</p> <p>Resolution is directly proportional to VRAM usage, so if you have low VRAM, use lower resolution</p> <p>Frame rate can be set to any value and is used to calculate the number of frames to be generated as well as the playback speed of the encoded video  </p>"},{"location":"FramePack/#prompt-enhancer","title":"Prompt Enhancer","text":"<p>Uses VLM to enhance your short prompts, for example you can enter just <code>dancing</code> or <code>jumping</code> as a prompt and it will be expanded to a longer prompt VLM first analyzes the input image (if provided) and then generates a longer prompt based on the short prompt to incorporate both the input image and the short prompt  </p>"},{"location":"FramePack/#complex-actions","title":"Complex Actions","text":"<p>When changing duration or FPS parameters, model will print number of sections that will be generated Each video section can have its own prompt suffix, which can be used to change the prompt over time Prompt suffix is a string that will be added to the end of the prompt Each line of section prompts will be used as a separate prompt suffix For example, if you have 3 sections and 3 lines in the prompt suffix, each section will use a different line of the prompt suffix  </p> <p>Example: - main prompt: astronaut on the moon - section prompts:   - line-1: walking   - line-2: jumping </p> <p>Note that number of lines in sections prompts does not have to match the number of sections If there are less lines than sections, sections will be interpolated to strech duration of the entire video For example, video with 4 sections and 2 lines in the section prompts will result in first line being used for the first 2 sections and second line being used for the last 2 sections  </p> <p>Use of section prompts is optional, but can be used to create more complex videos Section prompts are compatible with Prompt Enhancer and in that case, each combine prompt (base prompt plus per-section prompt) will be enhanced separately  </p>"},{"location":"FramePack/#video-encode","title":"Video Encode","text":"<p>Video location is set in settings -&gt; image paths -&gt; video </p> <p>Video is encoded using selected codec and codec options Default codec is <code>libx264</code>, to see codecs available on your system, use refresh By default, model will not create image files, but can be enabled in video settings  </p> <p>Tip</p> <p>Hardware-accelerated codecs (e.g. <code>hevc_nvenc</code>) will be at the top of the list Use hardware-accelerated codecs whenever possible</p> <p>Warning</p> <p>Video encoding can be very memory intensive depending on codec and number of frames</p>"},{"location":"FramePack/#advanced-video-options","title":"Advanced Video Options","text":"<p>Any specified video options will be sent to <code>ffmpeg</code> as-is For example, default <code>crf:16</code> specifies the quality of the video vs compression rate, lower is better For details, see https://trac.ffmpeg.org/wiki#Encoding </p>"},{"location":"FramePack/#interpolation","title":"Interpolation","text":"<p>Video can optionally have additional interpolated frames added using RIFE interpolation method For example, if you render 10sec 30fps video with 0 interpolated frames, its 300 frames that need to be generated But if you set 3 interpolated frames, video fps and duration do not change, but only 100 frames need to be generated and additional 200 interpolated frames are added in-between generated frames  </p>"},{"location":"FramePack/#cli-video-encode","title":"CLI Video Encode","text":"<p>Video encoding can be skipped by setting codec to <code>none</code> In which case, you may want to save raw video frames as <code>safetensors</code> file and use command line utility to encode video later  </p> <p><code>python encode-video.py</code></p> <p>Allows to: - Export frames from <code>safetensors</code> file as individual images   Those can be used for further processing or to manually create video using <code>ffmpeg</code> from-image-sequence - Encode frames from <code>safetensors</code> file into video using <code>cv2</code> - Encode frames from <code>safetensors</code> file into video using <code>torchvision/ffmpeg</code> </p>"},{"location":"FramePack/#lora","title":"LoRA","text":"<p>Limited support for any HunyuanVideo LoRAs Effects will be limited unless LoRA is trained on FramePack itself Uses standard syntax: <code>&lt;lora:filename:weight&gt;</code> </p> <p>Note</p> <p>There is no networks panel available in FramePack so you have to add LoRA to prompt manually</p>"},{"location":"FramePack/#offloading-and-quantization","title":"Offloading and Quantization","text":"<p>Implementation replaces lllyasviel offloading with SD.Next Balanced offloading Balanced offload will use more resources, but unless you have a low-end GPU, it should also be much faster especially when used together with quantization  </p> <p>Add support for LLM and DiT/Video modules on-the-fly quantization quantization Only available when using native offloading, configure as usual in settings -&gt; quantization </p> <p>Tip</p> <p>Its recommended to enable quantization for <code>Model</code>, <code>TE</code>, <code>LLM</code> modules</p> <p>See docs for for more details on offloading and quantization </p>"},{"location":"FramePack/#api","title":"API","text":"<p>Extension supports API calls: <code>/sdapi/v1/framepack</code> Only required params are base64-encoded init-image and prompt, all other parameters are optional Once video has been generated, you can download it using <code>/file={path-to-file}</code> endpoint For example, see <code>create-video.py</code> </p> <p><code>python create-video.py --help</code></p>"},{"location":"FramePack/#custom-model","title":"Custom Model","text":"<p>You can get current receipe to see which modules would be loaded and change them if desired For example, changing original llama to different one can be done with:  </p> <pre><code>text_encoder: Kijai/llava-llama-3-8b-text-encoder-tokenizer/\ntokenizer: Kijai/llava-llama-3-8b-text-encoder-tokenizer/\n</code></pre>"},{"location":"Gated/","title":"Gated Models","text":""},{"location":"Gated/#huggingface-login","title":"Huggingface Login","text":"<p>Access to some models is gated by vendor and in those cases, you need to request access to model from the vendor. For this you need to have a valid Huggingface account: Login or Sign Up </p> <p>Huggingface login and/or access token is not required for non-gated models  </p>"},{"location":"Gated/#create-token","title":"Create Token","text":"<p>Note: This is a one-time operation as same access token is used for all gated models.</p> <p>Once you are logged in, create access token that an external application such as SD.Next can use to access Huggingface on your behalf:</p> <p>Go to: Huggingface -&gt; Profile -&gt; Settings -&gt; Access Token -&gt; Create new token Or use this link </p> <ul> <li>Token type: READ   Do not use fine-grained to avoid complications   Name is your choice  </li> <li>Create token   Copy the token and store it in a safe place  </li> </ul>"},{"location":"Gated/#add-token-to-sdnext","title":"Add Token to SD.Next","text":"<p>Go to: SD.Next -&gt; System -&gt; Settings -&gt; Huggingface - Paste the token in the Huggingface Token field - Click \"Apply settings\"  </p>"},{"location":"Gated/#requesting-access","title":"Requesting Access","text":"<p>Note: Requesting access must be done on individual per-model case</p> <p>Requesting access can be in the form of simply accepting vendors terms of service or filling a form to get access to the model or requesting access and waiting for approval. In all cases, you need to go to model page on Huggingface and follow instruction.  </p> <p>Examples: FLUX.1, SD3.5</p> <p>Once you have access, you can use the model in SD.Next as usual</p>"},{"location":"Getting-Started/","title":"Getting started","text":"<p>This section describes how to use sdnext with assumption of basic knowledge of stable Diffusion. This section will show you how easy it is to generate images with SDNext with a few clicks. As a user only adjust and click the settings highlighted and noted in red ink</p> <p>On running the <code>./webui.bat</code> the main page will load up which looks like the following below:   </p> <p>The following sections are important and need to be modified by the user for an image to be generated. All red sections selected with red ink are sections the user must adjust and implement.</p> <p>Base Model This is the base model you are going to use to generate an image with. This is also called a checkpoint. From the civitAi website there are hundreds if not thousands of models you can download and use. Note that models are typically very large typically in the range of ~2Gb to 30Gb of space. these models are typically stored in the models/stable-diffusion folder of your sdnext directory.</p> <p>Positive Prompt There are two prompts positive and negative. The positive prompts is where you write and describe the image and picture you wish to generate.</p> <p>Negative Prompt This is the section where you write descriptions and components you DON'T want in your image. For example if your are drawing an anime girl sitting on a desk, typical negative prompts can and could be making sure you have a detailed face or not having several hand and legs. Note that there are also embeddings you can use such as bad hands which will be written and placed here.</p> <p>Width and Height are the image size you wish to work with. For SD1.5 image typically you start with 512x512 or 512x768 images. Using images larger then the training size do not help in image generation and result in poor or horrible images generated. Typically the following image resolutions are used and desired based on the following</p> <p>Sampling Types: The sampler you wish to use that will generate the image itself along with the number of steps the sampler will use. increasing the steps does not improve image quality and only consumes more resources and may potentially degrade image quality as well therefore it is ideal to choose an appropiate number of steps. For example Euler A typically can generate images in 24 steps. Please see the Sampler section for more information on samplers.</p> <p>CFG Scale: How closely do you wish the AI to follow your prompt. Again an appropiate value should be chosen. If the CFG value is too low the AI is given lots of freedom and conversely if the CFG value is very high you restrict the AI and force it to follow your prompt more closely. Note that having too high a CFG value will result in image generation. Typical values for CFG are between 6-9. Any lower and higher typically does not produce any good quality image at all.</p> <p>Seed: The seed is the number. An unique number for every image that is generated. Setting it to -1 you will get a random new image everytime. If you wish to upload or use a pre-existing image say from civitAI, use the process tab and import the settings and you will see the seed value will be the number of the existing generated image.</p>"},{"location":"HiDream/","title":"HiDream I1","text":"<p>HiDream is a new absolutely massive image generative foundation model with 17B parameters  </p> <p></p> <p>HiDream-I1 family consists of 3 variations: - Full - Dev - Fast </p> <p>Difference between variants is recommended number of steps: - full=50, dev=28, fast=16 </p> <p>HiDream-I1 is compatibile with: - FlowMatching Samplers - Remote VAE feature - TAE Live-preview feature  </p> <p>Important</p> <p>Due to size (over 25B params in 58GB), offloading and on-the-fly quantization are pretty much a necessity Running HiDream on &lt;16GB GPU is possible with BnB-NF4 or Quanto-Int4 quantization and default Balanced offload settings Note that you must pick quantization methods that are compatible with your GPU and platform</p> <p>Note</p> <p>Set appropriate offloading setting before loading the model to avoid out-of-memory errors For more information see Offloading Wiki</p> <p>Note</p> <p>Check compatibility of different quantizations with your platform and GPU! For more information see Quantization Wiki</p> <p>Important</p> <p>Use reference models Simply select it from Networks -&gt; Models -&gt; Reference and model will be auto-downloaded on first use</p> <p>Location of downloaded model is: - <code>hugginface</code> folder is used for individual components: transformers, t5 text-encoder and llama llm - <code>diffusers</code> folder is used for the main model Exact location of both folders can be found in Settings -&gt; System Paths</p> <p>Warning</p> <p>Manually downloaded models in either <code>safetensors</code> or <code>gguf</code> formats are currently not supported</p> <p>Important</p> <p>Llama-3.1-8b-instruct LLM model used by HiDream is a gated model! You need to request access from the authors to use it See Gated Wiki for more information</p>"},{"location":"HiDream/#text-encoders","title":"Text Encoders","text":"<p>HiDream utilizes 4 text-encoders: clip-l, clip-g, t5-1.1-xxl, llama-3.1-8b-instruct for total of 8.3B parameters  </p> <p>Custom <code>llama</code> model can be set in: Settings -&gt; Model options -&gt; HiDream </p> <p>Note</p> <p>SD.Next implementation differens from reference as it bumps up default max token length from 128 to 256 Max token length can be further overriden using env variable <code>HIDREAM_MAX_SEQUENCE_LENGTH</code></p>"},{"location":"Hints/","title":"Hints","text":"<p>SD.Next hints system is based on the single JSON file: html/locale_en.json </p> <p>Structure of the file is simple: For each visible UI label, there is a corresponding hint that is displayed when user hovers over the label.  </p> <p>Example: generate button:</p> <pre><code>{ \"id\":\"\", \"label\":\"Generate\", \"localized\":\"\", \"hint\":\"Start processing\" }\n</code></pre> <p>See Localization document for more details since same file is used for langugage localizations as well as providing hints  </p>"},{"location":"Hints/#editing","title":"Editing","text":"<p>By editing the hints file, you can add or modify hints displayed in the UI for each item  </p> <p>Warning</p> <p>Do not change any of the other fields in the file, only the <code>hint</code> field should be modified</p> <p>Important</p> <p>Items that are part of \"missing\" section are auto-created items with default hint only If you want to customize/improve them, move them to the correct section and add your hint, do not leave them in missing section as it will be overwritten on the next update</p>"},{"location":"Hints/#validation","title":"Validation","text":"<p>Warning</p> <p>All hints updates should be validated before submitting</p> <p>Before submitting updates, validate your edits: Save it locally and run <code>python cli/validate-locale.py</code> </p> <p>Optional: If you want to run full validation, start you local SDNext server, open browser console and run <code>analyzeHints()</code> function It will analyze all hints and print detailed report in the console:</p>"},{"location":"Hints/#contribution","title":"Contribution","text":"<p>Best way to submit additions/changes is to create GitHub PR with changes to that file Alternatively, you can use discussions or issue tracker to suggest changes and they will be applied manually Or reach out via Discord server - #Contribute channel </p>"},{"location":"Hotkeys/","title":"Hotkeys","text":"<ul> <li><code>escape</code>: stop/interrupt generate</li> <li><code>ctrl + enter</code>: generate</li> <li><code>ctrl + up/down</code>: change weight of the highlighted section of prompt up/down</li> <li><code>ctrl + n</code> or <code>ctrl + space</code>: show/hide networks panel</li> <li><code>ctrl + s</code> or <code>ctrl + insert</code>: save image</li> <li><code>ctrl + i</code>: reprocess image</li> <li><code>ctrl + d</code>: delete image</li> <li><code>ctrl + m</code>: model selector</li> <li><code>ctrl + e</code>: enqueue (agent scheduler add to queue)</li> <li><code>left/right</code>: cycle through images</li> <li><code>+/-</code>: image viewer zoom in/out</li> </ul>"},{"location":"IPAdapter/","title":"IP-Adapter","text":"<p>The IP-Adapter is a tool designed for style transfer with minimal resource usage. It provides an efficient way to clone faces or apply image transformations. It supports both SD 1.5 and SD-XL models, allowing for quick and cost-effective style transfer processes.</p>"},{"location":"IPAdapter/#key-features","title":"Key Features","text":"<ul> <li>Low Resource Usage: The IP-Adapter is lightweight, with memory requirements under 100MB for SD 1.5 and 700MB for SD-XL, making it an efficient choice for style transfer tasks.</li> <li>Style Transfer: It offers powerful style transfer capabilities, allowing you to clone faces or apply various image styles.</li> <li>Integration with ControlNet: IP-Adapter can be combined with ControlNet for more stable results, especially useful for batch processing or video tasks.</li> </ul>"},{"location":"IPAdapter/#available-models","title":"Available Models","text":"<p>The TenecentAILab IP-Adapter includes 10 models for various image or face cloning needs, enabling flexibility and versatility in different scenarios.</p>"},{"location":"IPAdapter/#examples","title":"Examples","text":""},{"location":"Installation/","title":"Installing SD.Next","text":"<p>Tip</p> <p>These instructions assume that you already have Git and Python installed and available in your user <code>PATH</code> If you do not have both Git and Python installed, follow the instructions in Install Python and Git to set those up first</p>"},{"location":"Installation/#clone-sdnext","title":"Clone SD.Next","text":"<p>Start terminal Launch your preferred system terminal and navigate to the directory where you want to install SD.Next - This should be a directory which your user account has read/write/execute access to  </p> <p>Warning</p> <p>Following scenarios are not recommended and can lead to SDNext not launching properly: Note: this includes all folders leading up-to <code>sdnext</code> folder, not just final folder name! - Installing SDNext as superuser/administrator/root: use user account - Using folder with admin/superuser/root permissions: use folder owned by user account - Using hidden/readonly folders or folders starting with <code>.</code>, e.g.: <code>.sdnext</code> - Using special folders such as Windows or OneDrive</p> <p>Clone SD.Next Clone the repository by running following command in your desired location and then navigate into the cloned directory  </p> <p><code>git clone https://github.com/vladmandic/sdnext &lt;optional directory name&gt;</code> </p> <p>Tip</p> <p>If you already have SD.Next installed and want to update to latest version, see Update guide</p>"},{"location":"Installation/#initial-installation","title":"Initial Installation","text":"<p>Important</p> <p>Decide on appropriate compute backend for your system ahead of time as that will determine which libraries are installed on your system</p> <p>--use-cuda       Use nVidia CUDA backend (autodetected by default) --use-rocm       Use AMD ROCm backend (autodetected by default) --use-ipex       Use Intel OneAPI XPU backend (autodetected by default) --use-openvino   Use Intel OpenVINO backend --use-zluda      Use ZLUDA --use-directml   Use DirectML  </p> <p>Note</p> <p>nVidia CUIDA, AMD ROCm and Intel OneAPI XPU are autodetected when available all other compute backends require explicit selection on first startup For platform specific information, check out WSL | Intel Arc | DirectML | OpenVINO | ONNX &amp; Olive | ZLUDA | AMD ROCm | MacOS | nVidia</p>"},{"location":"Installation/#launch-sdnext","title":"Launch SD.Next","text":"<p>Run the appropriate launcher for your OS to start the web interface: - Windows: <code>webui.bat --debug --use-xxx</code> or <code>.\\webui.ps1 --debug --use-xxx</code> - Linux &amp; Mac: <code>./webui.sh --debug --use-xxx</code> </p> <p>Now wait for few minutes to let the server install all required libraries The server is finished launching when the console shows an entry for \"Startup time\" </p> <p>Tip</p> <ul> <li>For the initial setup and future tech support, it is advisable to include the <code>--debug</code> option which provides more detailed logging information  </li> <li>All command line options can also be set via env variable   For example <code>--debug</code> is same as <code>set SD_DEBUG=true</code></li> </ul> <p>Tip</p> <ul> <li>If you don't want to use built-in <code>venv</code> support and prefer to run SD.Next in your own environment   such as Docker container, Conda environment or any other virtual environment,   you can skip <code>venv</code> create/activate and launch SD.Next directly (command line flags noted above still apply): <code>python launch.py --debug</code></li> </ul> <p>Tip</p> <ul> <li>For improved memory utilization on Linux, see Malloc</li> </ul> <p>Tip</p> <ul> <li>If you want to use <code>dev</code> version instead of main release <code>master</code>, see Update guide</li> </ul>"},{"location":"Installation/#first-time-setup","title":"First-Time Setup","text":"<ul> <li>Start the web interface   Once the web interface starts running, you can access it by opening your web browser and navigating to the address listed in the console next to \"Local URL.\" For most users, this should be <code>http://localhost:7860/</code>   You will see a brief loading screen, then you should be taken to the <code>Text</code> tab  </li> <li>Adjust paths   You may want to adjust these settings in the <code>System</code>:<code>Settings</code> tab:  </li> <li>If you already have models, LoRAs, Embeddings, LyCORIS, etc. set your paths in the <code>System Paths</code> page now  </li> <li>Pay special attention to the <code>Folder with Huggingface models</code> and <code>Folder for Huggingface Cache</code> as they can grow to significant size  </li> <li>You can use <code>Base path</code> to set a common root for all paths  </li> <li>Set your desired look &amp; feel   You can change the theme in the <code>User Interface</code> section  </li> <li>Save your settings   If you changed any settings in the previous step, click <code>Apply settings</code> to save those settings to your config file. This will also apply some defaults from built-in extensions  </li> <li>Restart server   Click <code>Restart server</code> to re-launch the SD.Next server with the updated settings  </li> </ul>"},{"location":"Installation/#install-python-and-git","title":"Install Python and Git","text":"<p>Note</p> <p>SD.Next supports Python versions <code>3.9.x</code> up to <code>3.12.x</code> However, not all compute backends exist on Python <code>3.12</code> as they may be based on older <code>torch</code> versions Recommended version is latest service release of Python <code>3.11.x</code></p>"},{"location":"Installation/#windows","title":"Windows","text":""},{"location":"Installation/#git-for-windows","title":"Git-for-Windows","text":"<ol> <li>Download Git for Windows from the following link: Git for Windows </li> <li>Run the downloaded <code>.exe</code> file and follow the installation wizard  </li> <li>During the installation process, make sure to check the box for    \"Use Git from the Windows Command line and also from 3rd-party-software\" to add Git to your system's PATH  </li> <li>Complete the installation by following the on-screen instructions  </li> </ol>"},{"location":"Installation/#python-for-windows","title":"Python-for-Windows","text":"<ol> <li>Download Python for Windows from the following link: Python for Windows </li> <li>Run the downloaded <code>.exe</code> file and follow the installation wizard  </li> <li>On the \"Customize Python\" screen, make sure to check the box for \"Add Python to PATH\"  </li> <li>Continue the installation by following the prompts  </li> <li>Once the installation is complete, you can open the command prompt and verify that Python is installed    by executing <code>python --version</code> and <code>pip --version</code> to check the Python and Pip versions respectively  </li> </ol>"},{"location":"Installation/#macos","title":"MacOS","text":""},{"location":"Installation/#git-for-macos","title":"Git-for-MacOS","text":"<ol> <li>Download Git for macOS from the following link: Git for macOS </li> <li>Open the downloaded <code>.pkg</code> file and follow the installation instructions  </li> <li>During the installation process, make sure to check the box for \"Install Git Bash\" to have a command-line Git interface  </li> <li>Complete the installation by following the prompts  </li> </ol>"},{"location":"Installation/#python-for-macos","title":"Python-for-MacOS","text":"<p>See these instructions for Python on MacOS (and an explanation why it's unique). </p>"},{"location":"Intel-ARC/","title":"IPEX and Intel GPUs","text":""},{"location":"Intel-ARC/#gpu-support-with-ipex-and-sdnext","title":"GPU Support with IPEX and SD.Next","text":"<ul> <li>Intel ARC Series  </li> <li>Intel Flex Series  </li> <li>Intel Max Series  </li> </ul> <p>Note</p> <p>Iris Xe and older iGPUs are not supported with IPEX, use OpenVINO if you want to use an older iGPU. Having an Intel iGPU can cause conflicts and random errors if you want to use a dedicated Intel GPU with IPEX. Disable your iGPU (if any, e.g. UHD or Iris Xe) in the device manager.</p>"},{"location":"Intel-ARC/#install-guide-for-windows","title":"Install Guide for Windows","text":""},{"location":"Intel-ARC/#preparations","title":"Preparations","text":"<ul> <li>Install Intel ARC Drivers</li> <li>Install Git and Python</li> </ul>"},{"location":"Intel-ARC/#running-sdnext-on-windows","title":"Running SD.Next on Windows","text":"<p>Open the CMD in a folder you want to install SD.Next and install SD.Next from Github with this command: <pre><code>git clone https://github.com/vladmandic/sdnext\n</code></pre></p> <p>Then enter into the sdnext folder: <pre><code>cd sdnext\n</code></pre></p> <p>Then run SD.Next with this command: <pre><code>.\\webui.bat --use-ipex\n</code></pre></p> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"Intel-ARC/#install-guide-for-linux-or-wsl","title":"Install Guide for Linux or WSL","text":"<p>Note</p>"},{"location":"Intel-ARC/#dont-use-linux-kernel-68-or-69-with-linux","title":"Don't use Linux Kernel 6.8 or 6.9 with Linux!","text":"<p>https://github.com/intel/compute-runtime/issues/726 Update your kernel to at least 6.10 or update to the latest available kernel.  </p> <p>*Updating kernel is not necessary for WSL as it is using the Windows GPU drivers instead.</p>"},{"location":"Intel-ARC/#install-guide-for-ubuntu-linux-or-ubuntu-with-wsl","title":"Install Guide for Ubuntu Linux or Ubuntu with WSL","text":"<p>Following Ubuntu instructions are for Ubuntu 24.04. Install the base packages: <pre><code>sudo apt update &amp;&amp; sudo apt install -y software-properties-common build-essential ca-certificates wget gpg git\n</code></pre></p> <p>Add the package lists for Intel Level Zero Drivers:</p> <pre><code>wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | sudo gpg --yes --dearmor --output /usr/share/keyrings/intel-graphics.gpg\necho \"deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu noble client\" | sudo tee /etc/apt/sources.list.d/intel-gpu-noble.list\nsudo apt update\n</code></pre> <p>Install the Intel Level Zero Drivers and the packages needed for PyTorch:</p> <pre><code>sudo apt install -y intel-opencl-icd libze-intel-gpu1 libze1 libgl1 libglib2.0-0 libgomp1\n</code></pre>"},{"location":"Intel-ARC/#install-guide-for-arch-linux","title":"Install Guide for Arch Linux","text":"<p>Install the necessary packages for Arch Linux:</p> <pre><code>sudo pacman -S intel-compute-runtime level-zero-headers level-zero-loader base-devel git python-pip python-virtualenv\n</code></pre> <p>Install Python 3.12 (or anything between 3.10 and 3.13): <pre><code>git clone https://aur.archlinux.org/python312.git\ncd python312\nmakepkg -si\ncd ..\nexport PYTHON=python3.12\n</code></pre></p>"},{"location":"Intel-ARC/#running-sdnext-on-linux","title":"Running SD.Next on Linux","text":"<p>Open the terminal in a folder you want to install SD.Next and install SD.Next from Github with this command: <pre><code>git clone https://github.com/vladmandic/sdnext\n</code></pre></p> <p>Then enter into the sdnext folder: <pre><code>cd sdnext\n</code></pre></p> <p>Then run SD.Next with this command: <pre><code>./webui.sh --use-ipex\n</code></pre></p> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"Intel-ARC/#running-sdnext-with-docker","title":"Running SD.Next with Docker","text":"<p>Checkout the Docker wiki if you want to build a custom Docker image.  </p> <p>Using Docker with a prebuilt image:  </p> <pre><code>export SDNEXT_DOCKER_ROOT_FOLDER=~/sdnext\nsudo docker run -it \\\n  --name sdnext-ipex \\\n  --device /dev/dri \\\n  -p 7860:7860 \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/app:/app \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/python:/mnt/python \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/data:/mnt/data \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/models:/mnt/models \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/huggingface:/root/.cache/huggingface \\\n  disty0/sdnext-ipex:latest\n</code></pre> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet. Resulting docker image will use 1.1 GB disk space (uncompressed) for the docker image and 8 GB for the venv.</p>"},{"location":"Intel-ARC/#environment-variables","title":"Environment Variables","text":"<ul> <li> <p><code>IPEX_SDPA_SLICE_TRIGGER_RATE</code>: Specify when dynamic attention slicing for Scaled Dot Product Attention should get triggered for Intel ARC. This environment variable allows you to set the trigger rate in gigabytes (GB). The default is <code>1</code>.</p> </li> <li> <p><code>IPEX_ATTENTION_SLICE_RATE</code>: Specify the dynamic attention slicing rate for 32 bit GPUs. This environment variable allows you to set the slicing rate in gigabytes (GB). The default is <code>0.5</code>.</p> </li> <li> <p><code>IPEX_FORCE_ATTENTION_SLICE</code>: Specify to enable or disable Dynamic Attention. The default is <code>1</code>.</p> </li> <li><code>1</code> will force enable dynamic attention slicing even if the GPU supports 64 bit.</li> <li><code>-1</code> will force disable dynamic attention slicing even if the GPU doesn't support 64 bit.</li> <li> <p><code>0</code> will automatically enable or disable dynamic attention based on the GPU.</p> </li> <li> <p><code>IPEXRUN</code>: Specify to launch the webui with ipexrun. Set it to <code>True</code> to use ipexrun. The default is unset.</p> </li> </ul>"},{"location":"LTX/","title":"LTXVideo","text":"<p>Note</p> <p>This docs covers the optimized LTXVideo integration with SD.Next available in Video-&gt;LTX tab Other LTXVideo models are available under Generic integration in Video-&gt;Generic tab</p> <p>Optimized LTXVideo support is based on LTXVideo 0.9.7 13B with size of 46.5GB Model will be auto-downloaded on first use  </p> <p>Warning</p> <p>Due to model size, quantization and offloading are highly recommended See docs for for more details on offloading and quantization</p>"},{"location":"LTX/#parameters","title":"Parameters","text":"<p>The model works best on resolutions under 1280x720 and number of frames below 257  </p>"},{"location":"LTX/#conditions","title":"Conditions","text":"<p>The model supports default text-to-video workflow, but can optionally use image-to-video and video-to-video conditioning In video-to-video workflow, you can set maximum number of frames and option to skip every n-th frame from input video  </p>"},{"location":"LTX/#lora","title":"LoRA","text":"<p>Support includes official LTXVideo LoRAs that can be downloaded from HuggingFace This includes LoRAs that can be used to change behavior of conditioning For example, you can load Canny/Pose/Depth or LoRA and then use pre-processed image or video as conditioning input  </p> <p>Also provided in official repo is Distilled LoRA which can be used to reduce required step count and improve quality of generated video  </p> <p>Third party LoRAs should work as well, but are not tested  </p>"},{"location":"LTX/#video-encode","title":"Video Encode","text":"<p>Important</p> <p>Video support requires <code>ffmpeg</code> to be installed and available in the <code>PATH</code></p> <p>Video location is set in settings -&gt; image paths -&gt; video </p> <p>Video is encoded using selected codec and codec options Default codec is <code>libx264</code>, to see codecs available on your system, use refresh By default, model will not create image files, but can be enabled in video settings  </p> <p>Tip</p> <p>Hardware-accelerated codecs (e.g. <code>hevc_nvenc</code>) will be at the top of the list Use hardware-accelerated codecs whenever possible</p> <p>Warning</p> <p>Video encoding can be very memory intensive depending on codec and number of frames</p>"},{"location":"LTX/#advanced-video-options","title":"Advanced Video Options","text":"<p>Any specified video options will be sent to <code>ffmpeg</code> as-is For example, default <code>crf:16</code> specifies the quality of the video vs compression rate, lower is better For details, see https://trac.ffmpeg.org/wiki#Encoding </p>"},{"location":"LTX/#interpolation","title":"Interpolation","text":"<p>Video can optionally have additional interpolated frames added using RIFE interpolation method For example, if you render 10sec 30fps video with 0 interpolated frames, its 300 frames that need to be generated But if you set 3 interpolated frames, video fps and duration do not change, but only 100 frames need to be generated and additional 200 interpolated frames are added in-between generated frames  </p>"},{"location":"LoRA/","title":"LoRA","text":""},{"location":"LoRA/#what-is-lora","title":"What is LoRA?","text":"<p>LoRA, short for Low-Rank Adaptation, is a method used in Generative AI models to fine-tune the model with specific styles or concepts while keeping the process efficient and lightweight.</p> <p>Here\u2019s how it works in simple terms: - The Problem:   Fine-tuning a huge model like Stable Diffusion to recognize or replicate new styles or concepts (e.g., making it draw in the style of a specific artist or recognize unique objects) usually requires a lot of computational power and storage.</p> <p>The LoRA Solution: - Instead of tweaking all the internal parameters of the Generative AI model, LoRA focuses only on a small subset of them. Think of it as adding a \"style filter\" to the model that can be applied or removed as needed.   It reduces the complexity by breaking down large changes into smaller, simpler steps.   These smaller steps don\u2019t interfere with the original model, meaning you don\u2019t lose the model\u2019s core abilities.  </p> <p>Why it\u2019s Cool: - Efficient: It uses way less memory and is faster than traditional fine-tuning methods. - Flexible: You can train multiple LoRA \"filters\" for different styles or concepts and swap them in and out without modifying the base model. - Compatible: LoRA modules can be shared or reused easily, so artists and developers can collaborate or try out others\u2019 custom styles.</p> <p>Example Use Case - Say you want to teach Generative AI models to draw in the style of a fictional artist.   You can train a LoRA on a handful of sample images in that style.   Once trained, the LoRA module acts like a plug-in\u2014you just load it into Generative AI models, and the model starts generating images in that style!</p> <p>In short, LoRA makes it easy to teach models new tricks without overwhelming your computer or altering the original model. It\u2019s a user-friendly way to get customized results!  </p>"},{"location":"LoRA/#lora-types","title":"LoRA Types","text":"<p>There are many LoRA types, here are some of the most common ones: LoRA, DoRA, LoCon, HaDa, gLoRA, LoKR, LyCoris They vary in: - Which model components are being trained. Typically UNET, but can be TE as well - Which layers of the model are being trained. Each LoRA type trains different layers of the model - Math algorithm to extrach LoRA weights for the specific trained layers</p> <p>Warning</p> <p>LoRA must always match base model used for its training For example, you cannot use SD1.5 LoRA with SD-XL model</p> <p>Warning</p> <p>SD.Next attempts to automatically detect and apply the correct LoRA type. However, new LoRA types are popping up all the time If you find LoRA that is not compatible, please report it so we can add support for it.</p>"},{"location":"LoRA/#how-to-use","title":"How to use?","text":"<ul> <li>Using UI: go to the networks tab and go to the lora's and select the lora you want and it will be added to the prompt.</li> <li>Manually: you can also add the lora manually by adding <code>&lt;lora:lora_name:strength&gt;</code> to the prompt and then selecting the lora you want to use.</li> </ul>"},{"location":"LoRA/#trigger-words","title":"Trigger words","text":"<p>Some (not all) LoRAs associate specific words during training so same words can be used to trigger specific behavior from the LoRA. SD.Next displays these trigger words in the UI -&gt; Networks -&gt; LoRA, but they can also be used manually in the prompt.  </p> <p>You can combine any number of LoRAs in a single prompt to get the desired output.  </p> <p>Tip</p> <p>If you want to automatically apply trigger words/tags to prompt, you can use <code>auto-apply</code> feature in \"Settings -&gt; Networks\"</p> <p>Tip</p> <p>You can change the strength of the lora by changing the number <code>&lt;lora:name:x.x&gt;</code> to the desired number</p> <p>Tip</p> <p>If you're combining multiple LoRAs, you can also \"export\" that as a single lora via \"Models -&gt; Extract LoRA\"</p>"},{"location":"LoRA/#advanced","title":"Advanced","text":""},{"location":"LoRA/#component-selection","title":"Component selection","text":"<p>By default, LoRA is applied to all model components it was trained on. However, you can also specify which component to apply LoRA to by adding <code>:module=xxx</code> to the LoRA tag. </p> <p>Example:</p> <p><code>&lt;lora:test_lora:1.0:module=unet&gt;</code></p> <p>would apply LoRA only on unet regardless of LoRA content.  </p> <p>This is particularly useful when you have multiple LoRAs and you want to apply them on different parts of the model.  </p> <p>Example:</p> <p><code>&lt;lora:firstlora:1.0:low&gt;</code> and <code>&lt;lora:secondlora:1.0:high&gt;</code> </p> <p>note: <code>low</code> is shorthand for <code>module=transformer_2</code> and <code>high</code> is shortcut for <code>module=transformer</code></p>"},{"location":"LoRA/#component-weights","title":"Component weights","text":"<p>Typically <code>:strength</code> is applied uniformly for all components of the LoRA. However, you can also specify individual component weights by adding <code>:comp=x.x</code> to the LoRA tag.  </p> <p>Example:</p> <p><code>&lt;lora:test_lora:te=0.5:unet=1.5&gt;</code> </p>"},{"location":"LoRA/#block-weights","title":"Block weights","text":"<p>Instead of using simple <code>:strength</code>, you can specify individual block weights for LoRA by adding <code>:in=x.x:mid=y.y:out=z.z</code> to the LoRA tag. Example:</p> <p><code>&lt;lora:test_lora:1.0:in=0:mid=1:out=0&gt;</code> </p>"},{"location":"LoRA/#stepwise-weights","title":"Stepwise weights","text":"<p>LoRA can also be applied will full per-step control by adding step-specific instuctions to the LoRA tag. Example:</p> <p><code>&lt;lora:test_lora:te=0.1@1,0.6@6&gt;</code> </p> <p>Would mean apply LoRA to text-encoder with strength 0.1 on step 1 and then switch to strength 0.6 on step 6.  </p>"},{"location":"LoRA/#troubleshooting","title":"Troubleshooting","text":"<p>For any LoRA related issues, please follow the below procedure: - set environment variable <code>SD_LORA_DEBUG=true</code> - start SD.Next as usual and run it until problem occurs - create GitHub issue - upload full <code>sdnext.log</code> as well as any console exception messages  </p>"},{"location":"LoRA/#lora-loader","title":"LoRA Loader","text":"<p>SD.Next has multiple ways that address how LoRA is loaded and applied, all available in \"Settings -&gt; Networks\": You must pick only one of the below methods: - LoRA fuse directly to model: enabled (default)   - Apply LoRA first time its needed without maintaining model weights as backup   - Unapply works by subtracting LoRA from model weights   - This can result in numerical instabilities and/or model degradation when switching LoRAs     if LoRA numerical range is not normalized or badly trained,     but saves 2x memory compared to older load/apply/unapply methods - LoRA load using legacy method: disabled   - Apply LoRA first time its needed and create model backup weights in system memory   - Unapply works by restoring model weights   - Most stable method, but can use up to 2x sysem memory - LoRA load using legacy method: enabled   - Apply LoRA on-the-fly during model execution and create model backup weights in system memory   - Unapply works by restoring model weights   - This was default method prior to 12/2024 release - LoRA load using Diffusers method: enabled   - Do not use SD.Next native LoRA processing and rely on underlying <code>diffusers</code> library for all LoRA actions   - Does not support all LoRA types  </p> <p>Tip</p> <p>If you see visual model degradation after unapplying LoRA, you can restore model weights using \"System -&gt; Reload Model\"</p>"},{"location":"Loader/","title":"Custom Model Loader","text":"<ul> <li>in Models -&gt; Loader</li> <li>Can be used to load any known diffusion model with default or custom model components</li> </ul>"},{"location":"Loader/#model-type","title":"Model Type","text":"<ul> <li>Can be any currently supported model type  </li> <li>Autodetect can be used to attempt loading repo for any model   e.g. there is a new model on huggingface that is not yet added to predefined list - well, try it out ;)</li> <li>Current can be used to replace selected components without reloading model itself  </li> </ul>"},{"location":"Loader/#model-repo","title":"Model Repo","text":"<p>Valid model repo on huggingface is required to access base model config - Default model repo is provided for common models - Model repo can be overriden to any valid repo on huggingface</p>"},{"location":"Loader/#loadable-components","title":"Loadable Components","text":"<p>Note</p> <p>Any model component that is marked as loadable can be overriden If neither local or remote values are set, default from model repo will be used Any loadable model component with set value will be loaded from that value</p>"},{"location":"Loader/#local-vs-remote","title":"Local vs Remote","text":"<ul> <li>Can use local or remote models  </li> <li>Local model can be any valid path to safetensors file or folder with hf-style structure  </li> <li>Remote model can be any valid huggingface repo, with or without subfolder or full url to file hosted on huggingface  </li> </ul>"},{"location":"Loader/#dtype","title":"Dtype","text":"<ul> <li>Can be used to override torch compute dtype when loading each individual component</li> </ul>"},{"location":"Loader/#quant","title":"Quant","text":"<ul> <li>Can be used to enable quantization when loading each individual component</li> <li>Quant settings will be used from settings -&gt; quantization </li> </ul>"},{"location":"Loader/#receipe","title":"Receipe","text":"<p>Model receipe can be saved to JSON file for later use</p> <p>[!WARNING] Load/Save receipe is not yet implemented</p>"},{"location":"Locale/","title":"Localization","text":"<p>SD.Next localization system is based on the single JSON file: html/locale_en.json </p> <p>Structure of the file is simple: For each visible UI label, there is a corresponding hint that is displayed when user hovers over the label.  </p> <p>Example: generate button: <pre><code>{\"id\":\"\",\"label\":\"Generate\",\"localized\":\"\",\"hint\":\"Start processing\"}\n</code></pre></p> <p>Tip</p> <p>See Hints document for more details</p> <p>This file which is then auto-translated using LLM model to other languages and saved as separate files in a format <code>hmtl/locale_{locale}.json</code> Those files should not be edited directly as they are auto-generated and will be overwritten on the next update, but if you have suggestions for improvements, please contact us  </p> <p>Note</p> <p>Translation is performed using <code>cli/localize.js</code> script</p>"},{"location":"Locale/#editing-localization","title":"Editing Localization","text":"<p>In case of bad ML translation or you just want to provide a better alternernative, you can create/edit <code>hmtl/override_{locale}.json</code> which only needs to include specific items you want to override  </p>"},{"location":"Locale/#supported-languages","title":"Supported Languages","text":"<ul> <li><code>en</code>: English</li> <li><code>de</code>: German</li> <li><code>es</code>: Spanish</li> <li><code>fr</code>: French</li> <li><code>it</code>: Italian</li> <li><code>pt</code>: Portuguese</li> <li><code>hr</code>: Croatian</li> <li><code>zh</code>: Chinese</li> <li><code>ja</code>: Japanese</li> <li><code>ko</code>: Korean</li> <li><code>ru</code>: Russian</li> </ul> <p>If you want to add additional language, you can create a new file with the same structure and submit it as a PR, or contact us.</p>"},{"location":"Locale/#selecting-locale","title":"Selecting Locale","text":"<p>Default locale is auto selected based on browser's language settings,  </p> <p>You can manually select it in settings -&gt; user interface -&gt; language Or by using CLI parameter <code>--locale</code>, for example  </p> <p><code>./webui.sh --debug --locale hr</code> </p> <p>For testing purposes, quick-switch is also available by clicking near the top-right corner of the page  </p>"},{"location":"MacOS-Python/","title":"MacOS and Python","text":"<p>TL;DR: Installation Instructions</p> <p>The MacOS operating system requires Python, so it is installed by default.  However, if you are going to start using Python on your own, it is likely that you'll want to install new packages, perform package updates, and so on -- which can be a problem for MacOS -- it's important to let the operating system handle its own Python.</p> <p>Most sources online will tell you to use Homebrew as a package manager for MacOS, so it is a natural conclusion to consider using Homebrew for Python -- and it's common to discover that you already have Python installed through Homebrew as a dependency for something else.  However, this isn't the correct solution either -- it's really the same problem as the MacOS problem, but with a coat of paint.</p> <p>Homebrew's Python is there to support other packages.  Importantly, Homebrew deletes old versions of packages after 30 days.  So, if you are using, say, python 3.12 for random scripting and other tasks, but need version 3.10 for, say, SD.Next, Homebrew will delete 3.10, since it doesn't know that you need it for something (since you didn't install SD.Next via Homebrew).  </p> <p>The solution is to use another way to manage the Python version(s) that you use on your own.  I use asdf, which has a Python plugin, but there are others if you prefer something else.</p> <p>Sources / Further Reading:  - https://justinmayer.com/posts/homebrew-python-is-not-for-you/ - https://hackercodex.com/guide/python-development-environment-on-mac-osx/ - https://github.com/asdf-community/asdf-python - https://asdf-vm.com/ - https://docs.brew.sh/Installation</p>"},{"location":"MacOS-Python/#installation-instructions","title":"Installation Instructions","text":"<ol> <li>If you haven't got Homebrew installed already:</li> </ol> <pre><code>mkdir homebrew &amp;&amp; curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip-components 1 -C homebrew\n\neval \"$(homebrew/bin/brew shellenv)\"\nbrew update --force --quiet\nchmod -R go-w \"$(brew --prefix)/share/zsh\"\n</code></pre> <ol> <li>Install asdf and python build dependencies:</li> </ol> <pre><code>brew install asdf openssl readline sqlite3 xz zlib\n</code></pre> <ol> <li>Add asdf to <code>.zshrc</code> to use it immediately and persistently:</li> </ol> <pre><code>. $(brew --prefix asdf)/asdf.sh\necho -e \"\\n. $(brew --prefix asdf)/asdf.sh\" &gt;&gt; ~/.zshrc\n</code></pre> <ol> <li>Add the python asdf plugin:</li> </ol> <pre><code>asdf plugin add python\n\n# for SD.Next\nasdf install python 3.10.14\n\n# you may want the latest version too; take note of which version is installed\nasdf install python latest\n</code></pre> <ol> <li>Set the default global version of python:</li> </ol> <p>Since you will always want 3.10 for SD.Next, you will want to always specifically use that version. You will probably want to use the command <code>python</code> in most contexts, and <code>python3.10</code> for version-specific uses.</p> <pre><code>asdf global python 3.12.2 \n# or whatever version you installed\n</code></pre> <ol> <li>Run SD.Next using python3.10:</li> </ol> <pre><code>export PYTHON=$(which python3.10)\ncd /path/to/SD.Next\n./webui.sh --debug\n</code></pre>"},{"location":"Malloc/","title":"Memory Allocator","text":"<p>Combination of OS default memory allocator <code>malloc</code> with Python's default memory allocator is pessimistic when it comes to system memory garbage collection and it will sometimes hold on to allocated memory longer than necessary even if GC is triggered explicitly  This appears to user as a memory leak as process memory usage grows over time This is especially noticeable when frequently loading/unloading large objects such as models or LoRAs  </p> <p>Note</p> <p>This applies to system memory only and has no impact on GPU memory management</p>"},{"location":"Malloc/#linux","title":"Linux","text":"<p>Tip</p> <p>For Linux deployments you can switch out memory allocator to <code>tcmalloc</code>, <code>jemalloc</code> or <code>mimalloc</code> which are more efficient and have better memory management</p>"},{"location":"Malloc/#tcmalloc","title":"tcmalloc","text":"<pre><code>sudo apt install google-perftools  \nsudo ldconfig  \nexport LD_PRELOAD=libtcmalloc.so.4  \n./webui.sh --debug\n</code></pre>"},{"location":"Malloc/#jemalloc","title":"jemalloc","text":"<pre><code>sudo apt install libjemalloc2\nsudo ldconfig  \nexport LD_PRELOAD=libjemalloc.so.2  \n./webui.sh --debug\n</code></pre>"},{"location":"Malloc/#mimalloc","title":"mimalloc","text":"<pre><code>sudo apt install libmimalloc2.0\nsudo ldconfig  \nexport LD_PRELOAD=libmimalloc.so.2  \n./webui.sh --debug\n</code></pre>"},{"location":"Malloc/#windows","title":"Windows","text":"<p>Tip</p> <p>For Windows deployments you can switch out memory allocator to <code>mimalloc</code> which is more efficient and has better memory management</p>"},{"location":"Malloc/#mimalloc_1","title":"mimalloc","text":"<p><code>mimalloc</code> for windows requires custom compiled library Full instructions can be found here: https://microsoft.github.io/mimalloc/index.html </p>"},{"location":"Malloc/#compare","title":"Compare","text":"<p>Using Ubuntu 24.04 and after executing 10 batches of 1024px images using SDXL model  </p> manager reserved memory note malloc 9345 MB baseline tcmaloc 8423 MB very performant and stable over long runs jemalloc 5468 MB best memory savings mimalloc 6132 MB new contender <p>Note</p> <p>Results will vary based on system and usage pattern What matters is that memory allocator is can intelligently free up memory that is marked as available by SD.Next over time while maintaining low memory fragmentation and allowing low-latency allocations For example, when unloading LoRAs or switching models</p>"},{"location":"Model-Support/","title":"Model support","text":"<p>Additional models will be added as they become available and there is public interest in them See models overview for details on each model, including their architecture, complexity and other info  </p> <ul> <li>RunwayML Stable Diffusion 1.x and 2.x (all variants) </li> <li>StabilityAI Stable Diffusion XL, StabilityAI Stable Diffusion 3.0 Medium, StabilityAI Stable Diffusion 3.5 Medium, Large, Large Turbo  </li> <li>StabilityAI Stable Video Diffusion Base, XT 1.0, XT 1.1  </li> <li>StabilityAI Stable Cascade Full and Lite  </li> <li>Black Forest Labs FLUX.1 Dev and Schnell  </li> <li>Black Forest Labs FLUX.1 Fill and Depth  </li> <li>Black Forest Labs FLUX.1 Kontext-Dev  </li> <li>Black Forest Labs FLUX.1 Krea-Dev  </li> <li>lodestones Chroma Standard, Detail Calibrated and Flash  </li> <li>FreePik F-Lite Standard, Texture and 7B   </li> <li>NVLabs Sana 1.0 and 1.5  </li> <li>nVidia Cosmos-Predict2 T2I 2B and 14B  </li> <li>AuraFlow 0.3 and 0.2  </li> <li>AlphaVLLM Lumina-Next-SFT </li> <li>AlphaVLLM Lumina 2 </li> <li>Playground AI v1, v2 256, v2 512, v2 1024 and latest v2.5</li> <li>Tencent HunyuanDiT 1.2 and 1.1  </li> <li>OmniGen </li> <li>Meissonic </li> <li>Kwai Kolors </li> <li>Ostris Flex.1-Alpha </li> <li>CogView 3+ </li> <li>LCM: Latent Consistency Models </li> <li>aMUSEd 256 and 512  </li> <li>Segmind Vega, Segmind SSD-1B, Segmind SegMoE SD and SD-XL, Segmind SD Distilled (all variants) </li> <li>Kandinsky 2.1, 2.2, 3.0  </li> <li>PixArt-\u03b1 XL 2 Medium and Large</li> <li>PixArt-\u03a3 </li> <li>Warp Wuerstchen </li> <li>Tsinghua UniDiffusion </li> <li>DeepFloyd IF Medium and Large  </li> <li>ModelScope T2V </li> <li>BLIP-Diffusion </li> <li>KOALA 700M </li> <li>VGen </li> <li>SDXS </li> <li>Hyper-SD </li> <li>HiDream-I1 Full, Dev, Fast </li> <li>Wan 2.1 1.3B and 14B T2V models for T2I workflows  </li> <li>Wan 2.2 5B and A14B T2V models for T2I workflows  </li> <li>Bria 3.2 </li> <li>Qwen-Image </li> <li>Qwen-Image-Edit </li> </ul>"},{"location":"Model-Support/#support-notes","title":"Support Notes","text":"<ul> <li>HunyuanDit models requires explicit <code>float16</code> precision set in settings -&gt; compute settings -&gt; model precision -&gt; fp16 </li> <li>aMUSEd models require explicit noise generator device to be unset set in settings -&gt; compute settings -&gt; generator device -&gt; unset </li> </ul>"},{"location":"Models-Tab/","title":"Models Tab","text":"<p>Current | Convert | Merge | Validate | Huggingface | CivitAI | Update | Extract LoRa</p>"},{"location":"Models-Tab/#current","title":"Current","text":""},{"location":"Models-Tab/#analyze-current-model","title":"Analyze current model","text":"<p>Analyze the current model to understand its characteristics, training data and details, so you can understand the model on a more technological level.</p>"},{"location":"Models-Tab/#convert","title":"Convert","text":""},{"location":"Models-Tab/#convert-current-model","title":"Convert current model","text":"<p>Convert the current model to make it have a different name, a different precision, a different format, make it pruned or non pruned and etc.</p>"},{"location":"Models-Tab/#merge","title":"Merge","text":""},{"location":"Models-Tab/#simple-merge","title":"Simple merge","text":"<p>Simple merge allows you to merge 2 models with each other in a non advanced way, so there are basic options are available.  </p> <p>Alpha Ratio: The alpha ratio standard value is 0.5 that means the merged model will be 50% primary model and 50% secondary model. 0.2 would be 80% primary and 20% secondary model.  </p> <p>Overwrite model: If you check this the merged model will overwrite the primary model.  </p> <p>Save Metadata: Saves the metadata of both the primary and secondary model into merged one.  </p> <p>Weights clip: Forces merged weights to be no heavier than the original model, preventing burn in and overly saturated models.  </p> <p>ReBasin: Performs multiple merges with permutations in order to keep more features from both models.  </p> <p>Model precision: lets you choose what precision you want the merged model to have.  </p> <p>Merge device: Which device to use for the merge.  </p> <p>Replace VAE: Allows you to replace the VAE of the merged model with your preferred VAE.</p>"},{"location":"Models-Tab/#preset-block-merge","title":"Preset Block Merge","text":"<p>Similar to the simple merge tab, only here you can check the SDXL checkbox if your models are SDXL models and no need to change the alpha ratio by yourself, instead you can select one of the many presets available.</p>"},{"location":"Models-Tab/#validate","title":"Validate","text":""},{"location":"Models-Tab/#list-model-details","title":"List model details","text":"<p>Lists all the details(not as much as in current) of all the checkpoint models.</p>"},{"location":"Models-Tab/#calculate-hash-for-all-models","title":"Calculate hash for all models","text":"<p>Calculates hash for each checkpoint model. The hash makes it easier to identify the model or model used(even if its renamed).</p>"},{"location":"Models-Tab/#huggingface","title":"Huggingface","text":"<p>Search models on Huggingface and download them into SD.NEXT with this sub tab.</p>"},{"location":"Models-Tab/#civitai","title":"CivitAI","text":""},{"location":"Models-Tab/#fetch-information","title":"Fetch information","text":"<p>Fetches preview and metadata information for all models with missing information. Models with existing previews and information are not updated.</p>"},{"location":"Models-Tab/#search-for-models","title":"Search for models","text":"<p>Allows you to search for models on CivitAI. Just select the type of model you want, a keyword and optionally some tags. After that click on search and you results will show below.</p>"},{"location":"Models-Tab/#download-model","title":"Download model","text":"<p>To download a model you need to click on the desired model in searched models then the model version and lastly the variant.</p>"},{"location":"Models-Tab/#update","title":"Update","text":"<p>Checks for updates for all checkpoint models.</p>"},{"location":"Models-Tab/#extract-lora","title":"Extract LoRa","text":"<p>Allows you to tweak a lora to the preferred strength.</p>"},{"location":"Models/","title":"Models","text":"<p>List of popular text-to-image generative models with their respective parameters and architecture overview  </p> Publisher Model Version Size Diffusion Architecture Model Params Text Encoder(s) TE Params Auto Encoder License Release date StabilityAI Stable Diffusion 1.5 2.28GB UNet 0.86B CLiP ViT-L 0.12B VAE OpenRAIL 2022 October StabilityAI Stable Diffusion 2.1 2.58GB UNet 0.86B CLiP ViT-H 0.34B VAE OpenRAIL 2022 December StabilityAI Stable Diffusion XL 6.94GB UNet 2.56B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE OpenRAIL 2023 July StabilityAI Stable Diffusion 3.0 Medium 15.14GB MMDiT 2.0B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE Proprietary 2024 June StabilityAI Stable Diffusion 3.5 Medium 15.89GB MMDiT 2.25B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE Proprietary 2024 October StabilityAI Stable Diffusion 3.5 Large 26.98GB MMDiT 8.05B CLiP ViT-L + ViT+G + T5-XXL 0.12B + 0.69B + 4.76B 16ch VAE Proprietary 2024 October StabilityAI Stable Cascade Medium 11.82GB Multi-stage UNet 1.56B + 3.6B CLiP ViT-G 0.69B 42x VQE Proprietary 2024 February StabilityAI Stable Cascade Lite 4.97GB Multi-stage UNet 0.7B + 1.0B CLiP ViT-G 0.69B 42x VQE Proprietary 2024 February Black Forest Labs Flux 1 Schnell 32.93GB MMDiT 11.9B CLiP ViT-L + T5-XXL 0.12B + 4.76B 16ch VAE Apache 2.0 2024 August Black Forest Labs Flux 1 Dev 32.93GB MMDiT 11.9B CLiP ViT-L + T5-XXL 0.12B + 4.76B 16ch VAE Proprietary 2024 August Black Forest Labs Flux 1 Kontext-Dev 32.93GB MMDiT 11.9B CLiP ViT-L + T5-XXL 0.12B + 4.76B 16ch VAE Proprietary 2025 June Black Forest Labs Flux 1 Krea-Dev 32.93GB MMDiT 11.9B CLiP ViT-L + T5-XXL 0.12B + 4.76B 16ch VAE Proprietary 2025 July lodestones Chroma 48 26.84GB MMDiT 8.9B CLiP ViT-L + T5-XXL 0.12B + 4.76B 16ch VAE Apache 2.0 2025 July Ostris Flex 1 Alpha 25.65GB MMDiT 8.16B CLiP ViT-L + T5-XXL 0.12B + 2.95B 16ch VAE Apache 2.0 2025 January Ostris Flex 2 Preview 25.65GB MMDiT 8.16B CLiP ViT-L + T5-XXL 0.12B + 2.95B 16ch VAE Apache 2.0 2025 April FreePik F-Lite 19.81GB MMDiT 9.8B T5-XXL 2.95B 16ch VAE OpenRAIL 2025 May FreePik F-Lite Texture 19.81GB MMDiT 9.8B T5-XXL 2.95B 16ch VAE OpenRAIL 2025 May FreePik F-Lite 7B 13.89GB MMDiT 7B T5-XXL 2.95B 16ch VAE OpenRAIL 2025 May NVLabs Sana 1.5 1.6B 9.49GB MMDiT 1.60B Gemma2 2.61B DC-AE Proprietary 2025 March NVLabs Sana 1.5 4.8B 15.58GB MMDiT 4.72B Gemma2 2.61B DC-AE Proprietary 2025 March NVLabs Sana 1.0 1600M 12.63GB MMDiT 1.60B Gemma2 2.61B DC-AE Proprietary 2024 November NVLabs Sana 1.0 600M 7.51GB MMDiT 0.59B Gemma2 2.61B DC-AE Proprietary 2024 November nVidia Cosmos-Predict2 T2I 2B 13.32GB MMDiT 1.96B T5-XXL 4.86 WAN-VAE Proprietary 2025 June nVidia Cosmos-Predict2 T2I 14B 37.36GB MMDiT 14.26B T5-XXL 4.86 WAN-VAE Proprietary 2025 June FAL AuraFlow 0.2 31.90GB MMDiT 6.8B UMT5 12.1B VAE Apache 2.0 2024 July FAL AuraFlow 0.3 31.90GB MMDiT 6.8B UMT5 12.1B VAE Apache 2.0 2024 August AlphaVLLM Lumina Next SFT 8.67GB DiT 1.7B Gemma 2.5B VAE Apache 2.0 2024 June AlphaVLLM Lumina 2 20.75GB DiT 2.61B Gemma-2 2.61B 16ch VAE Apache 2.0 2025 January PixArt Alpha XL 2 21.3GB DiT 0.61B T5-XXL 4.76B VAE OpenRAIL 2023 November PixArt Sigma XL 2 21.3GB DiT 0.61B T5-XXL 4.76B VAE OpenRAIL 2024 April Segmind SSD-1B 8.72GB UNet 1.33B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Apache 2.0 2023 October Segmind Vega 6.43GB UNet 0.75B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Apache 2.0 2023 November Segmind Tiny 1.03GB UNet 0.32B CLiP ViT-L 0.12B VAE OpenRAIL 2023 July Thu-ML UniDiffuser v1 5.37GB U-ViT 0.95B CLiP ViT-L + CLiP ViT-B 0.12B + 0.16B VAE AGPL 3 2023 May Kwai Kolors 17.40GB UNnet 2.58B ChatGLM 6.24B VAE Apache 2.0 2024 July PlaygroundAI Playground 1.0 4.95GB UNet 0.86B CLiP ViT-L 0.12B VAE ? 2023 December PlaygroundAI Playground 2.x 13.35GB UNet 2.56B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Proprietary 2023 December Tencent HunyuanDiT 1.2 14.09GB DiT 1.5B BERT + T5-XL 3.52B + 1.67B VAE Proprietary 2024 May Warp AI Wuerstchen 12.16GB Multi-stage UNet 1.0B + 1.05B CLiP ViT-L + ViT+G 0.12B + 0.69B 42x VQE MIT 2023 August Kandinsky Kandinsky 2.1 5.15GB Unet 1.25B CLiP ViT-G 0.69B VQ Apache 2.0 2023 April Kandinsky Kandinsky 2.2 5.15GB Unet 1.25B CLiP ViT-G 0.69B VQ Apache 2.0 2023 July Kandinsky Kandinsky 3.0 27.72GB Unet 3.05B T5-XXXL 8.72B VQ Apache 2.0 2023 November Thudm CogView 3 Plus 24.96GB DiT 2.85B T5-XXL 4.76B VAE Apache 2.0 2024 October Thudm CogView 4 30.39GB DiT 6.37B GLM-4 9.40B VAE Apache 2.0 2025 March IDKiro SDXS 2.05GB UNet 0.32B CLiP ViT-L 0.12B VAE OpenRAIL 2024 March Open-MUSE aMUSEd 256 3.41GB ViT 0.60B CLiP ViT-L 0.12B VQ OpenRAIL 2023 December Koala Koala 700M 6.58GB UNet 0.78B CLiP ViT-L + ViT+G 0.12B + 0.69B VAE Proprietary 2024 January Thu-ML UniDiffuser v1 5.37GB U-ViT 0.95B CLiP ViT-L + CLiP ViT-B 0.12B + 0.16B VAE aGPL v3 2023 May Salesforce BLIP-Diffusion 7.23GB UNet 0.86B CLiP ViT-L + BLiP-2 0.12B + 0.49B VAE BSD 3 2023 July DeepFloyd IF M 12.79GB Multi-stage UNet 0.37B + 0.46B T5-XXL 4.76B Pixel Proprietary 2023 April DeepFloyd IF L 15.48GB Multi-stage UNet 0.61B + 0.93B T5-XXL 4.76B Pixel Proprietary 2023 April MeissonFlow Meissonic 3.64GB DiT 1.18B CLiP ViT-H 0.35B VQ Apache 2.0 2024 October VectorSpaceLab OmniGen v1 15.47GB Transformer 3.76B Phi-3 0 VAE MIT 2024 October VectorSpaceLab OmniGen v2 30.50GB Transformer 3.97B Qwen-VL-2.5 3.75B VAE Apache 2.0 2025 June HiDream-AI HiDream I1 Fast/Dev/Full 42.71 GB + 15.69 MMDiT 17.10B CLiP ViT-L + ViT+G + T5-XXL + LLama-3.1-8B 0.12B + 0.69B + 2.95B + 4.54B 16ch VAE MIT 2025 April Wan-AI WAN 2.1 1.3B 27.72GB MMDiT 1.42xB UMT5-XXL 5.68B 16ch VAE Apache 2.0 2025 February Wan-AI WAN 2.1 14B 78.52GB MMDiT B UMT5-XXL 14.28B 16ch VAE Apache 2.0 2025 February Bria Bria 3.2 18.66GB MMDiT 3.78B T5-XXL 4.76B 16ch VAE Proprietary 2025 June Alibaba Qwen-Image 56.10GB MMDiT 20.43B QWen-2.5 8.29B Apache 2.0 2025 August"},{"location":"Models/#notes","title":"Notes","text":"<ul> <li>Created using SD.Next built-in model analyzer  </li> <li>Number of parameters is proportional to model complexity and ability to learn   Quality of generated images is also influenced by training data and duration of training  </li> <li>Size refers to original model variant in 16bit precision where available   Quantized variations may be smaller  </li> <li>Distilled variants are not included as typical goal-distilling does not change underlying model params   e.g. Turbo/LCM/Hyper/Lightning/etc. or even Dev/Schnell  </li> </ul>"},{"location":"Networks-Search/","title":"Networks Search","text":"<p>Search input for the extra networks accepts additional search syntax. This allows to do more complicated searches, in addition to the searches which were previously available.</p>"},{"location":"Networks-Search/#search-syntax","title":"Search Syntax","text":"<p>In a normal search, there are 3 special symbols which can be used to modify the search:</p> <ul> <li><code>|</code> - Or, it allows to search for one of multiple terms at once (or exclude multiple terms, if prefixed with <code>-</code>)</li> <li><code>-</code> - Exclude, terms or phrases prefixed with this symbol will be excluded from the search</li> <li><code>&amp;</code> - And, require all terms to be present (or not present, if prefixed with <code>-</code>) in the search results</li> </ul> <p>These symbols are parsed in the order of <code>|</code>, <code>&amp;</code>, <code>-</code>. This means that <code>a|b&amp;c</code> is equivalent to \"a or [b and c]\" and <code>a&amp;b|c</code> is equivalent to \"[a and b] or c\". It is not possible to modify the order of operations with parentheses or other symbols.</p> <p>Whitespace (this means spaces, tabs, newlines, etc.) is trimmed from individual terms. This means that <code>a |    b</code> is equivalent to <code>a|b</code>. Also <code>a|-b</code> is equivalent to <code>a| -   b</code>.</p> <p>Search is case-insensitive, this means that <code>a</code> is equivalent to <code>A</code>.</p> <p>However this concept might be clearer with a few examples based on a list of networks.</p>"},{"location":"Networks-Search/#examples","title":"Examples","text":"<p>Assume the following folder structure for your networks:</p> <pre><code>\u251c\u2500\u2500 SDXL/\n\u2502   \u251c\u2500\u2500 Clothing/\n\u2502   \u2502   \u251c\u2500\u2500 Elegant Gown.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Vintage Suit.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Summer Dress.safetensor\n\u2502   \u2502   \u2514\u2500\u2500 Sports Attire.safetensor\n\u2502   \u251c\u2500\u2500 Concept/\n\u2502   \u2502   \u251c\u2500\u2500 Futuristic Cityscape.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Underwater World.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 Space Exploration.safetensor\n\u2502   \u2502   \u2514\u2500\u2500 Cyberpunk Street.safetensor\n\u2502   \u2514\u2500\u2500 Person/\n\u2502       \u251c\u2500\u2500 Celebrity Portrait.safetensor\n\u2502       \u251c\u2500\u2500 Historical Figure.safetensor\n\u2502       \u251c\u2500\u2500 Fictional Character.safetensor\n\u2502       \u2514\u2500\u2500 Movie Star.safetensor\n\u2514\u2500\u2500 SD1.5/\n    \u251c\u2500\u2500 Clothing/\n    \u2502   \u251c\u2500\u2500 Casual Wear.safetensor\n    \u2502   \u251c\u2500\u2500 Casual Winter-Wear.safetensor\n    \u2502   \u251c\u2500\u2500 Winter Coat.safetensor\n    \u2502   \u251c\u2500\u2500 Evening Gown.safetensor\n    \u2502   \u2514\u2500\u2500 Vintage Suit.safetensor\n    \u251c\u2500\u2500 Concept/\n    \u2502   \u251c\u2500\u2500 Alien Landscape.safetensor\n    \u2502   \u251c\u2500\u2500 Medieval Castle.safetensor\n    \u2502   \u251c\u2500\u2500 Cyberpunk Street.safetensor\n    \u2502   \u2514\u2500\u2500 Futuristic Cityscape.safetensor\n    \u2514\u2500\u2500 Person/\n        \u251c\u2500\u2500 Scientist Portrait.safetensor\n        \u251c\u2500\u2500 Movie Star.safetensor\n        \u251c\u2500\u2500 Mythological Hero.safetensor\n        \u2514\u2500\u2500 Historical Figure.safetensor\n</code></pre> <p>Below are some example queries, which are explained in detail afterwards.</p> <ul> <li><code>Vintage Suit</code> - Search for everything which contains the phrase <code>Vintage Suit</code>, meaning in this case <code>Vintage Suit.safetensor</code> in <code>SDXL</code> and <code>Vintage Suit.safetensor</code> in <code>SD1.5</code>.</li> <li><code>clothing</code> - Search for everything which contains the word <code>clothing</code>, meaning in this case all networks in the <code>Clothing</code> folders.</li> <li><code>clothing &amp; SDXL</code> - Search for everything which contains the word <code>clothing</code> and the word <code>SDXL</code>, meaning in this case all networks in the <code>Clothing</code> folder of the <code>SDXL</code> folder.</li> <li><code>-SD1.5</code> - Search for everything which does not contain the word <code>SD1.5</code>, meaning in this case all networks in the <code>SDXL</code> folder.</li> <li><code>gown | dress</code> - Search for everything which contains the word <code>gown</code> or the word <code>dress</code>, meaning in this case <code>Evening Gown.safetensor</code> in <code>SD1.5</code> and <code>Elegant Gown.safetensor, Summer Dress.safetensor</code> in <code>SDXL</code>.</li> <li><code>SDXL &amp; gown | SDXL &amp; dress</code> - Search for everything which contains the word <code>gown</code> or the word <code>dress</code> and the word <code>SDXL</code>, meaning in this just case <code>Elegant Gown.safetensor, Summer Dress.safetensor</code> in <code>SDXL</code>.</li> <li><code>clothing | - Casual Wear</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain the phrase <code>Casual Wear</code>, meaning in this case <code>Casual Winter-Wear.safetensor, Elegant Gown.safetensor, Vintage Suit.safetensor, Summer Dress.safetensor, Sports Attire.safetensor</code> in <code>SDXL</code> and <code>Evening Gown.safetensor, Vintage Suit.safetensor, Winter Coat</code> in <code>SD1.5</code>.</li> <li><code>clothing | - Casual &amp; Wear</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain both the word <code>casual</code> and the word <code>wear</code>, meaning in this case <code>Elegant Gown.safetensor, Vintage Suit.safetensor, Summer Dress.safetensor, Sports Attire.safetensor</code> in <code>SDXL</code> and <code>Evening Gown.safetensor, Vintage Suit.safetensor, Winter Coat.safetensor</code> in <code>SD1.5</code> (Note that <code>Casual Winter-Wear.safetensor</code> is excluded from the search results).</li> <li><code>clothing | -black | - gray | -white</code> - Search for everything which contains the word <code>clothing</code>, as long as it does not contain the words <code>black</code>, <code>gray</code> or <code>white</code>.</li> <li><code>SDXL | -Turbo | -LCM</code> - Search for everything which contains the word SDXL, as long as it does not contain the words <code>Turbo</code> or <code>LCM</code>. Can be used to filter different types of networks.</li> <li><code>SDXL &amp; Turbo | SDXL &amp; LCM</code> - The inverse of the previous search. Search for everything which contains the word <code>Turbo</code> or <code>LCM</code> and the word <code>SDXL</code>.</li> </ul>"},{"location":"Networks-Search/#advanced-usage","title":"Advanced usage","text":"<p>It is also possible to do a RegEx search instead of a normal search. To do so, the search term has to be prefixed with <code>r#</code>. Everything after that will be interpreted as a RegEx search.</p> <p>This means that <code>r#.*</code> will match everything, <code>r#^$</code> will match nothing and <code>r#a</code> will match everything which contains the letter <code>a</code>. Please watch out that valid RegEx syntax is used, otherwise the search will fail. In addition to that, the RegEx search is also slower than a normal search, so it is recommended to only use it if necessary. RegEx searches are case-insensitive too.</p> <p>One more thing to watch out for, is file paths. For the normal search the Windows <code>\\</code> is replaced with a <code>/</code> to make it easier to search for file paths. This is not the case for the RegEx search. So you could use for example <code>[\\/\\\\]</code> to match both <code>\\</code> and <code>/</code> in a RegEx search.</p> <p>Considering the RegEx search, the field which is searched is structured in the following way:</p> <pre><code>`filename: ${filename}|name: ${name}|tags: ${tags}`\n</code></pre> <p>As of writing this, the fields represent the following:</p> <ul> <li><code>name</code> - The relative path to the network, not including filetype (also shows when hovering the network card for a longer time)</li> <li><code>filename</code> - The absolute path to the network. (This does not resolve symlinks, so the path shown is the path to the symlink if one is used, not the path to the file the symlink points to)</li> <li><code>tags</code> - The tags the network has, separated by <code>|</code>. Not all networks necessarily have tags, and they can be either keywords used when prompting, or the keyword to trigger the network specifically.</li> </ul> <p>This allows to exclude (or include) certain strings only in specific fields. For example a negative lookahead can be used to exclude certain tags from the search, while still searching for them in the title.</p> <p>An example RegEx would be the following for this use case:</p> <pre><code>r#^(?!.*\\|tags:.*(1girl|woman|girl)).*$\n</code></pre> <p>This search excludes every network which has tags which contain either <code>1girl</code>, <code>woman</code> or <code>girl</code>.</p> <p>Another usecase would be searching in just the filename, but not in the entire path. For example requiring the network to contain <code>SDXL</code>, with it not mattering whether it is in a folder called <code>SDXL</code> or not:</p> <pre><code>r#^.*\\|name:[^|]*([^\\/\\\\]*SDXL[^\\/\\\\]*)$(?&lt;![\\/\\\\]).*$\n</code></pre> <p>As this is already advanced usage, the assumption is made that the user knows how to use RegEx, so no further examples or explanations of the provided RegEx are given here. Tools like https://regex101.com/ can be used to create, test, understand and debug a regular expression.</p>"},{"location":"Networks/","title":"Networks","text":"<p>The Networks button under Generation Controls opens a convenient side menu, providing shortcuts to your: - Models - LoRas: see LoRA for more information - Styles: see Styles for more information - Embeddings - VAEs - Latent history</p>"},{"location":"Networks/#reference-models","title":"Reference models","text":"<p>In the models tab, you can find Reference models section This is the list of predefined models that can be immediately selected and used Once reference model is selected, it will be automatically downloaded and saved in the models folder for future use  </p> <p>Tip</p> <p>Reference models is recommended way to start with any base model</p> <p>Reference models section can be hidden in \"Settings -&gt; Networks\" </p>"},{"location":"Networks/#latent-history","title":"Latent history","text":"<p>Near the completion of each generate workflow, its latents (raw data from model before final decoding) are added to latent history. Why? So they can be reused and reprocessed at will.  </p> <p>For example, text + refine + detailer will generate 3 entries in latent history, one for each step. You can then reprocess any of these steps with different settings or even different models.</p> <p>You can control the length of maintained latent history in \"Settings -&gt; Execution\"</p>"},{"location":"NudeNet/","title":"NudeNet","text":""},{"location":"NudeNet/#censorship-with-style-tm","title":"Censorship... with style! \u2122","text":""},{"location":"NudeNet/#features","title":"Features","text":""},{"location":"NudeNet/#main-nudenet","title":"Main NudeNet","text":"<ul> <li>Detect geneder and any body part   e.g. female face, belly, feet</li> <li>Exposed or unexposed:   e.g. breast or breast-bare</li> <li>Add information to image metadata   e.g. \"NudeNet: female-face:0.86; belly:0.54\", NSFW: True </li> <li>Censor as desired (or not):</li> <li>blur (adjust block size for effect)</li> <li>pixelate (adjust block size for effect)</li> <li>cover with pasty (overlay image) :) note: RGBA image is recommended for overlays </li> <li>Use as extension from UI or via CLI <code>python nudenet.py --help</code> </li> <li>Adjustable sensitivity</li> <li>Can be used for txt2img, img2img or process </li> <li>FAST!   Uses <code>CV2</code> and <code>ONNX</code> backend and typically executes in &lt;0.1sec  </li> </ul>"},{"location":"NudeNet/#language-check","title":"Language Check","text":"<p>If Check language is enabled, NudeNet will use <code>facebook/fasttext-language-identification</code> to check if prompt is in allowed language and using allowed alphabet. For example:  <pre><code>LangDetect: ['eng_latn:0.83']\n</code></pre></p> <p>Means that language is English and uses Latin alphabet with 83% confidence.</p>"},{"location":"NudeNet/#image-guard","title":"Image-Guard","text":"<p>If Policy check is enabled, NudeNet will use <code>LlavaGuard</code> VLM to check if image violates any policies and reasons why. For example: <pre><code>'LlavaGuard': {\n  'rating': 'Unsafe',\n  'category': 'O3: Sexual Content',\n  'rationale': 'The image contains a woman with her body covered in paint splatters, which is considered sexually explicit content. This type of content is not allowed on our platform as it violates our policy against sexually explicit content. The image is not educational in nature and does not provide information on sexuality or sexual education. It is simply a depiction of a woman in a provocative pose, which is not appropriate for our platform.'\n}\n</code></pre></p>"},{"location":"NudeNet/#banned-words","title":"Banned Words","text":"<p>If Banned words check is enabled, NudeNet will use <code>regex</code> to check prompt variations if they contain any banned words or expressions. For example: <pre><code>BannedWords: ['naked']\n</code></pre></p>"},{"location":"NudeNet/#settings","title":"Settings","text":"<p>Should be self-explanatory...</p> <p></p>"},{"location":"NudeNet/#examples","title":"Examples","text":""},{"location":"Nunchaku/","title":"Nunchaku","text":"<p>Nunchaku is a high-performance inference engine from MIT-Han-Lab optimized for 4-bit neural networks Nunchaku uses novel 4-bit SVDQuant quantization via DeepCompressor</p> <p>Nunchaku can speed up inference by 2-5x compared to standard 16-bit or 8-bit models!</p> <p>Important</p> <p>Nunchaku is supported only on CUDA platforms using Turing/Ampere/Ada/Blackwell GPUs Nunchaku requires Python 3.11 or 3.12</p> <p>Note</p> <p>On Blackweel GPUs, Nunchaku will default to <code>FP4</code> methods while on other GPU generations it will use <code>INT4</code> methods</p>"},{"location":"Nunchaku/#install","title":"Install","text":"<p>SD.Next will attempt to auto-install pre-built wheels when possible, but if you encounter issues, see Manual build section  </p>"},{"location":"Nunchaku/#configure","title":"Configure","text":"<p>To enable Nunchaku support, set appropriate quantization options in Settings -&gt; Quantization - Enable for modules:   Enable or disable Nunchaku for specific modules   Currently supports Transformers and TE - Nunchaku attention:   Overrides current attention module with Nunchaku's custom fp16 attention mechanism - Nunchaku offloading:   Overrides current offloading method with Nunchaku's custom offloading method  </p>"},{"location":"Nunchaku/#support","title":"Support","text":"<p>At the moment, Nunchaku supports following models: - FLUX.1 both Dev and Schnell variants - SANA 1.0-1600M variant - Qwen Image original and lightning variants - T5 XXL variant text-encoder   as used by SD35, FLUX.1, HiDream models  </p> <p>Important</p> <p>SD.Next will auto-download Nunchaku's prequantized modules as needed on first access Nunchaku replaces model's DiT module with a custom pre-quantized one, Any model fine-tune will be ignored</p> <p>Note</p> <p>For FLUX.1 Nunchaku supports multiple base modeles: - Black-Forrest FLUX.1 Dev and Schnell - Shuttle Jaguar finetune</p>"},{"location":"Nunchaku/#notes","title":"Notes","text":"<p>Warning</p> <p>Nunchaku is EXPERIMENTAL and many normal features are not supported yet</p> <p>Nunchaku is compatible with some advanced features like: - LoRA loading   however, Nunchaku uses custom LoRA loader so not all LoRAs may be supported - Para-attention first-block-cache   enable in Settings -&gt; Pipeline modifiers </p> <p>Unsupported and/or known limitations: - Batch size - Model unload causes memory leaks  </p>"},{"location":"Nunchaku/#manual-build","title":"Manual build","text":""},{"location":"Nunchaku/#install-cuda","title":"Install CUDA","text":"<p>Warning</p> <p>Requires <code>CUDA</code> dev installation with <code>NVCC</code></p> <p>URL: https://developer.nvidia.com/cuda-12-6-3-download-archive</p>"},{"location":"Nunchaku/#install-docs","title":"Install docs","text":"<p>URL: https://github.com/mit-han-lab/nunchaku/blob/main/README.md#build-from-source</p>"},{"location":"Nunchaku/#quick-steps","title":"Quick-steps","text":"<p>Note</p> <p>Build process will take a while, so be patient</p> <pre><code>cd sdnext\nsource venv/bin/activate\ncd ..\ngit clone https://github.com/mit-han-lab/nunchaku\ncd nunchaku\ngit submodule init\ngit submodule update\npip install torch torchvision torchaudio ninja wheel sentencepiece protobuf\npython setup.py develop\n</code></pre> <pre><code>Found nvcc version: 12.6.85\nDetected SM targets: ['89']\nrunning develop\n...\nAdding nunchaku 0.2.0+torch2.6 to easy-install.pth file\nInstalled /home/vlado/branches/nunchaku\nProcessing dependencies for nunchaku==0.2.0+torch2.6\n...\nFinished processing dependencies for nunchaku==0.2.0+torch2.6\n</code></pre> <p>python <pre><code>&gt;&gt;&gt; import sys\n&gt;&gt;&gt; import platform\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; sys.version_info\nsys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n&gt;&gt;&gt; platform.system()\n'Linux'\n&gt;&gt;&gt; torch.__version__\n'2.6.0+cu126'\n&gt;&gt;&gt; torch.version.cuda\n'12.6'\n&gt;&gt;&gt; torch.cuda.get_device_name(0)\n'NVIDIA GeForce RTX 4090'\n&gt;&gt;&gt; import nunchaku\n&gt;&gt;&gt; nunchaku.__path__\n['/home/vlado/dev/nunchaku/nunchaku']\n</code></pre></p>"},{"location":"ONNX-Runtime/","title":"ONNX Runtime","text":"<p>SD.Next includes support for ONNX Runtime.</p>"},{"location":"ONNX-Runtime/#how-to","title":"How to","text":"<p>Currently, we can't use <code>--use-directml</code> because there's no release of <code>torch-directml</code> built with latest PyTorch. (this does not mean that you can't use DmlExecutionProvider) Change <code>Diffusers pipeline</code> to <code>ONNX Stable Diffusion</code> on the <code>System</code> tab  </p>"},{"location":"ONNX-Runtime/#performance","title":"Performance","text":"<p>The performance depends on the execution provider.</p>"},{"location":"ONNX-Runtime/#execution-providers","title":"Execution Providers","text":"<p>Currently, <code>CUDAExecutionProvider</code> and <code>DmlExecutionProvider</code> are supported.</p> ONNX Olive GPU CPU CPUExecutionProvider \u2705 \u274c \u274c \u2705 DmlExecutionProvider \u2705 \u2705 \u2705 \u274c CUDAExecutionProvider \u2705 \u2705 \u2705 \u274c ROCMExecutionProvider \u2705 \ud83d\udea7 \u2705 \u274c OpenVINOExecutionProvider \u2705 \u2705 \u2705 \u2705"},{"location":"ONNX-Runtime/#cpuexecutionprovider","title":"CPUExecutionProvider","text":"<p>Not recommended.</p> <p>Enabled by default.</p>"},{"location":"ONNX-Runtime/#dmlexecutionprovider","title":"DmlExecutionProvider","text":"<p>You can select <code>DmlExecutionProvider</code> by installing <code>onnxruntime-directml</code>.</p> <p>DirectX 12 API is required. (Windows or WSL)</p>"},{"location":"ONNX-Runtime/#cudaexecutionprovider","title":"CUDAExecutionProvider","text":"<p>You can select <code>CUDAExecutionProvider</code> by installing <code>onnxruntime-gpu</code>. (may have been automatically installed)</p>"},{"location":"ONNX-Runtime/#rocmexecutionprovider","title":"\ud83d\udea7 ROCMExecutionProvider","text":"<p>Olive for ROCm is working in progress.</p>"},{"location":"ONNX-Runtime/#openvinoexecutionprovider","title":"\ud83d\udea7 OpenVINOExecutionProvider","text":"<p>Under development.</p>"},{"location":"ONNX-Runtime/#supported","title":"Supported","text":"<ul> <li>Models from huggingface</li> <li>Hires and second pass (without sdxl refiner)</li> <li>.safetensors VAE</li> </ul>"},{"location":"ONNX-Runtime/#known-issues","title":"Known issues","text":"<ul> <li>SD Inpaint may not work.</li> <li>SD Upscale pipeline is not tested.</li> <li>SDXL Refiner does not work. (due to onnxruntime's issue)</li> </ul>"},{"location":"ONNX-Runtime/#faq","title":"FAQ","text":""},{"location":"ONNX-Runtime/#im-getting-onnxstablediffusionpipeline__init__-missing-4-required-positional-arguments-vae_encoder-vae_decoder-text_encoder-and-unet","title":"I'm getting <code>OnnxStableDiffusionPipeline.__init__() missing 4 required positional arguments: 'vae_encoder', 'vae_decoder', 'text_encoder', and 'unet'</code>.","text":"<p>It's due to the broken model cache which was previously generated by failed conversion or Olive run. Find one in <code>models/ONNX/cache</code> and remove it. You can also use <code>ONNX</code> tab on UI. (You should enable it on settings to make it show up)</p>"},{"location":"ONNX-Runtime/#olive","title":"Olive","text":"<p>Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation. (from pypi)</p>"},{"location":"ONNX-Runtime/#how-to_1","title":"How to","text":"<p>As Olive optimizes the models in ONNX format, you should set up ONNX Runtime first.</p> <ol> <li>Go to <code>System</code> tab \u2192 <code>Compute Settings</code>.</li> <li>Select <code>Model</code>, <code>Text Encoder</code> and <code>VAE</code> in <code>Compile Model</code>.</li> <li>Set <code>Model compile backend</code> to <code>olive-ai</code>.</li> </ol> <p>Olive-specific settings are under <code>Olive</code> in <code>Compute Settings</code>.</p>"},{"location":"ONNX-Runtime/#how-to-switch-to-olive-from-torch-directml","title":"How to switch to Olive from torch-directml","text":"<p>Run these commands using PowerShell.</p> <pre><code>.\\venv\\Scripts\\activate\npip uninstall torch-directml\npip install torch torchvision --upgrade\npip install onnxruntime-directml\n.\\webui.bat\n</code></pre>"},{"location":"ONNX-Runtime/#from-checkpoint","title":"From checkpoint","text":"<p>Model optimization occurs automatically before generation.</p> <p>Target models can be .safetensors, .ckpt, Diffusers pretrained model and the optimization progress takes time depending on your system and execution provider.</p> <p>The optimized models are automatically cached and used later to create images of the same size (height and width).</p>"},{"location":"ONNX-Runtime/#from-huggingface","title":"From Huggingface","text":"<p>If your system memory is not enough to optimize model or you don't want to waste your time to optimize the model yourself, you can download optimized model from Huggingface.</p> <p>Go to <code>Models</code> \u2192 <code>Huggingface</code> tab and download optimized model.</p>"},{"location":"ONNX-Runtime/#advanced-usage","title":"Advanced Usage","text":""},{"location":"ONNX-Runtime/#customize-olive-workflow","title":"Customize Olive workflow","text":"<p>TBA</p>"},{"location":"ONNX-Runtime/#performance_1","title":"Performance","text":"Property Value Prompt a castle, best quality Negative Prompt worst quality Sampler Euler Sampling Steps 20 Device RX 7900 XTX 24GB Version olive-ai(0.4.0) onnxruntime-directml(1.16.3) ROCm(5.6) torch(olive: 2.1.2, rocm: 2.1.0) Model runwayml/stable-diffusion-v1-5 (ROCm), lshqqytiger/stable-diffusion-v1-5-olive (Olive) Precision fp16 Token Merging Olive(0, not supported) ROCm(0.5) Olive with DmlExecutionProvider ROCm"},{"location":"ONNX-Runtime/#pros-and-cons","title":"Pros and Cons","text":""},{"location":"ONNX-Runtime/#pros","title":"Pros","text":"<ul> <li>The generation is faster.</li> <li>Uses less graphics memory.</li> </ul>"},{"location":"ONNX-Runtime/#cons","title":"Cons","text":"<ul> <li>Optimization is required for every models and image sizes.</li> <li>Some features are unavailable.</li> </ul>"},{"location":"ONNX-Runtime/#faq_1","title":"FAQ","text":""},{"location":"ONNX-Runtime/#my-execution-provider-does-not-show-up-in-my-settings","title":"My execution provider does not show up in my settings","text":"<p>After activating python venv, run this command and try again:</p> <pre><code>(venv) $ pip uninstall onnxruntime onnxruntime-... -y\n</code></pre>"},{"location":"Offload/","title":"Offload","text":"<p>Offload is a method of moving model or parts of the model between the GPU memory (VRAM) and system memory (RAM) in order to reduce the memory footprint of the model and allow it to run on GPUs with lower VRAM.</p>"},{"location":"Offload/#offload-mode","title":"Offload Mode","text":"<p>Tip</p> <p>Offload mode is set by the Settings -&gt; Models &amp; Loading -&gt; Model offload mode</p>"},{"location":"Offload/#balanced","title":"Balanced","text":"<p>Balanced offload works differently than all other offloading methods as it performs offloading only when the VRAM usage exceeds the user-specified threshold.</p> <ul> <li>Recommended for compatible high VRAM GPUs  </li> <li>Faster but requires compatible platform and sufficient VRAM  </li> <li>Balanced offload moves parts of the model depending on the user-specified threshold allowing to control how much VRAM is to be used  </li> <li>High threshold will set the maximum memory usage allowed for the model weights of a single model component  </li> <li>Low threshold will decide when to offload unused models back to RAM    If the VRAM usage is higher than the low threshold, it will offload, otherwise it will do nothing  </li> <li>Configure threshold in Settings -&gt; Models &amp; Loading -&gt; Balanced offload GPU high / low watermark</li> </ul> <p>Balanced offloading default behavior is based on detected GPU memory: - default: offload=balanced gpu-min=0.2 gpu-max=0.6 gc-threshold=0.7 - &lt;= 4gb/lowvram: offload=sequential quantization=cpu vae-tiling=on gc-threshold=0.0 - &lt;= 12gb/medvram: offload=balanced gpu-min=0.0 vae-tiling=on - &gt;= 24gb/highvram: offload=balanced gpu-max=0.8 never=clip-l,clip-g,vae  </p> <p>Warning</p> <p>Not compatible with Optimum.Quanto <code>qint</code> quantization</p>"},{"location":"Offload/#sequential","title":"Sequential","text":"<p>Works on layer-by-layer basis of each model component that is marked as offload-compatible  </p> <ul> <li>Recommended for low VRAM GPUs</li> <li>Much slower but allows to run large models such as FLUX even on GPUs with 2-4GB VRAM  </li> </ul> <p>Warning</p> <p>Not compatible with Quanto <code>qint</code> or BitsAndBytes <code>nf4</code> quantization</p> <p>Note</p> <p>Use of <code>--lowvram</code> automatically triggers use of sequenential offload</p>"},{"location":"Offload/#model","title":"Model","text":"<p>Works on model component level by offloading components that are marked as offload-compatible For example, VAE, text-encoder, etc.</p> <ul> <li>Recommended for medium when balanced offload is not compatible  </li> <li>Higher compatibility than either balanced and sequential, but lesser savings  </li> </ul> <p>Limitations: N/A</p>"},{"location":"Offload/#performance-notes","title":"Performance Notes","text":"<ul> <li>Tested using SDXL with 2 large LoRA models  </li> <li>Sequential offload is default for GPUs with 4GB or less</li> <li>Balanced offload is default for GPUs with more than 4GB   Balanced offload is slower than no offload, but allows using large models such as SD35 and FLUX.1 out-of-the-box</li> <li>Balanced offload set to default values  </li> <li>LoRA overhead is measured in sec for first and subsequent iterations  </li> <li>LoRA mode=backup can use up to 2x system memory   Using backup can be prohibitive on large models such as SD35 or FLUX.1</li> </ul> Offload mode LoRA type LoRA mode LoRA overhead End-to-end it/s Note none none N/A N/A 6.7 fastest inference balanced none N/A N/A 4.5 default without LoRA sequential none N/A N/A 0.6 lowvram none native backup 1.8 / 0.0 6.0 balanced native backup 1.3 / 0.0 2.8 sequential native backup 5.8 / 0.0 0.5 none native fuse 1.3 / 1.3 4.8 balanced native fuse 2.8 / 2.5 3.1 default with LoRA sequential native fuse 8.8 / 7.7 0.4 none diffusers default 2.9 / 2.9 3.8 balanced diffusers default 2.2 / 2.2 2.1 sequential diffusers default 4.6 / 4.6 0.3 none diffusers fuse 5.7 / 5.7 2.0 balanced diffusers fuse N/A did not complete sequential diffusers fuse N/A did not complete"},{"location":"OpenVINO/","title":"OpenVINO","text":"<p>OpenVINO is an open-source toolkit for optimizing and deploying deep learning models.  Compiles models for your hardware.  Supports Linux and Windows  Supports CPU / GPU / iGPU / NPU  Supports AMD GPUs on Windows with FP16 support.  Supports INTEL dGPUs and iGPUs.  Supports NVIDIA GPUs.  Supports CPUs with BF16 and INT8 support.  Supports multiple devices at the same time using Hetero Device.  </p> <p>It is basically a TensorRT / Olive competitor that works with any hardware.  </p>"},{"location":"OpenVINO/#installation","title":"Installation","text":""},{"location":"OpenVINO/#preparations","title":"Preparations","text":"<ul> <li>Install the drivers for your device</li> <li>Install Git and Python</li> </ul> <p>Note</p> <p>Do not mix OpenVINO with your old install. Treat OpenVINO as a seperate backend.</p>"},{"location":"OpenVINO/#running-sdnext-with-openvino","title":"Running SD.Next with OpenVINO","text":"<p>Open CMD / Terminal in a folder you want to install SD.Next and install SD.Next from Github with this command:  </p> <pre><code>git clone https://github.com/vladmandic/sdnext\n</code></pre> <p>Then enter into the sdnext folder:</p> <pre><code>cd sdnext\n</code></pre> <p>Then start WebUI with this command:</p> <p>Windows:</p> <pre><code>.\\webui.bat --use-openvino\n</code></pre> <p>Linux:</p> <pre><code>./webui.sh --use-openvino\n</code></pre> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet.</p>"},{"location":"OpenVINO/#running-sdnext-with-docker","title":"Running SD.Next with Docker","text":"<p>Checkout the Docker wiki if you want to build a custom Docker image.  </p> <p>Using Docker with a prebuilt image:  </p> <pre><code>export SDNEXT_DOCKER_ROOT_FOLDER=~/sdnext\nsudo docker run -it \\\n  --name sdnext-openvino \\\n  --device /dev/dri \\\n  -p 7860:7860 \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/app:/app \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/python:/mnt/python \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/data:/mnt/data \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/models:/mnt/models \\\n  -v $SDNEXT_DOCKER_ROOT_FOLDER/huggingface:/root/.cache/huggingface \\\n  disty0/sdnext-openvino:latest\n</code></pre> <p>Note</p> <p>It will install the necessary libraries at the first run so it will take a while depending on your internet. Resulting docker image will use 1.1 GB disk space (uncompressed) for the docker image and 2.5 GB for the venv.</p>"},{"location":"OpenVINO/#more-info","title":"More Info","text":""},{"location":"OpenVINO/#limitations","title":"Limitations","text":"<ul> <li>Same limitations with TensorRT / Olive applies here too.  </li> <li>Compilation takes a few minutes and using LoRas will trigger recompilation.   </li> <li>Attention Slicing and HyperTile will not work.  </li> </ul>"},{"location":"OpenVINO/#quantization","title":"Quantization","text":"<p>Quantization enables 8 bit support without autocast. Enable <code>OpenVINO Quantize Models with NNCF</code> option in Compute Settings to use it.  </p> <p>Note</p> <p>Quantization has noticeable quality impact and generally not recommended.</p>"},{"location":"OpenVINO/#model-compression","title":"Model Compression","text":"<p>Enable <code>Compress Model weights with NNCF</code> option in Compute Settings to use it. Select a 4 bit mode from <code>OpenVINO compress mode for NNCF</code> to use 4 bit. For GPUs; select both CPU and GPU from the device selection if you want to use GPU with Model Compression.  </p> <p>Note</p> <p>VAE will still be compressed to INT8 if you use a 4 bit mode.</p>"},{"location":"OpenVINO/#custom-devices","title":"Custom Devices","text":"<p>Use the <code>OpenVINO devices to use</code> option in <code>Compute Settings</code> if you want to specify a device. Selecting multiple devices will use multiple devices as a single <code>HETERO</code> device.  </p> <p>Using <code>--device-id</code> cli argument with the WebUI will use a GPU with the specified Device ID. Using <code>--use-cpu openvino</code> cli argument with the WebUI will use the CPU.  </p> <p>If no device is specified, then the default device that will be used with OpenVINO will be auto detected with the following priority: <code>GPU</code> -&gt; <code>GPU.1</code> -&gt; <code>GPU.0</code> -&gt; Last device in the OpenVINO available devices list (<code>CPU</code>)</p>"},{"location":"OpenVINO/#model-caching","title":"Model Caching","text":"<p>OpenVINO can save compiled models to cache folder so you won't have to compile them again. <code>OpenVINO disable model caching</code> option in Compute Settings will disable caching. Disable this option to enable caching. <code>Directory for OpenVINO cache</code> option in System Paths will set a new location for saving OpenVINO caches.  </p>"},{"location":"Outpaint/","title":"Outpainting","text":"<p>Following guide is based on Control tab functionality  </p> <p>Note</p> <p>Screenshots are using Standard UI, but functionality using Modern UI is the same</p>"},{"location":"Outpaint/#how-does-it-work","title":"How does it work?","text":"<p>Outpaint is combination of expand image + use original image as mask area + trigger inpaint Result is that newly expanded area is now outpainted while original image remains unchanged  </p>"},{"location":"Outpaint/#select-what-to-outpaint","title":"Select what to outpaint","text":"<p>There are several ways to use outpainting:  </p> <ul> <li>If you know exactly by how much you want to outpaint,   Easiest is to simply specify outpaint size using resizing method </li> <li>If you want to experiment interactively,   You can zoom in/out and/or move input image using interactive method Note: this method is very powerful, but can be finicky due to Gradio image control usability limitations  </li> </ul> <p>Note</p> <p>Settings noted at the bottom apply to both methods</p>"},{"location":"Outpaint/#resize-outpaint","title":"Resize Outpaint","text":"<ul> <li>Select Control tab  </li> <li>Upload image Note: Status bar will display image details:   <p>Control input | Image | Size 768x768 | Mode RGB  </p> </li> <li>In Size settings   Select Before -&gt; Mode -&gt; Outpaint   Select desired size Note: Size can be either fixed resolution or scale-by  </li> </ul> <p>Note</p> <p>Resize outpaint mode is different than normal resize since it does not resize input image directly Instead it creates new image and places original image in the center of newly created larger image with remaining areas marked for outpainting</p>"},{"location":"Outpaint/#image-vs-inpaint-method","title":"Image vs Inpaint Method","text":"<p>With resize outpaint, you again have two options:</p> <ul> <li>By uploading input image to Select tab   You will be using Image-to-Image outpaint mode   Which will perform seamless integration of outpainted area,   but will also allow modifications input content as well Note: when using this mode, you must use prompt that includes description of the original image   to allow it to be blended and avoid replacing it completely  </li> <li>By uploading input image to Outpaint tab   You will be using Inpaint-with-Mask outpaint mode   Which will maintain input image,   but will make it more difficult to blend outpainted area Note: when using this mode, experiment with mask blur/errode/dilate values  </li> </ul> <p>Example: </p>"},{"location":"Outpaint/#interactive-outpaint","title":"Interactive Outpaint","text":"<ul> <li>Select Control tab  </li> <li>In Input panel, select Outpaint tab   Interactive outpaint is only available when using outpaint tab  </li> <li>Upload image Note: Status bar will display image details:   <p>Control input | Image | Size 768x768 | Mode RGB  </p> </li> <li>Click on edit button in the top-right corner of the image  </li> <li>Using mouse resize edges of the image   Confirm resize with the left click Note: Status bar will display updated image details:   <p>Control input | Image | Size 898x768 | Mode RGB  </p> </li> <li>Use mouse scroll to zoom out and left-click drag to position image    Confirm with left click Note: Status bar will display updated image details:   <p>Control input | Image | Size 929x929 | Mode RGB  </p> </li> </ul> <p>Note</p> <p>Instead of uploading image to outpaint tab, you can use send to feature to transfer generated image from Text/Image/Control tabs</p> <p>Tip</p> <p>Note that after confirming expand/zoom/reposition with the mouse click image may look the same so pay attention to the status bar! This is the limitation of current Gradio and we're looking at possible replacements for native canvas</p> <p>Example </p>"},{"location":"Outpaint/#settings","title":"Settings","text":""},{"location":"Outpaint/#strength","title":"Strength","text":"<p>In Input settings set denoising strength to a high value (at least 0.55 or higher) so outpaint can freely create new content  </p> <p></p> <p>Warning</p> <p>using low denoising strength will result in expanded image, but not actually outpainted </p>"},{"location":"Outpaint/#mask","title":"Mask","text":"<p>In Mask settings you can control amount of edge blur/erode/dilate between original image and outpained section  </p> <p>Tip</p> <p>Start with 0 for blur/erode/dilate and increase as needed</p> <p></p> <p>By allowing larger overlap between original image and outpainted region, outpaint will be more context-aware since it can \"fit\" in the image better, but it will also change edges of original image  </p>"},{"location":"Parameters/","title":"SDnext Parameters Guide","text":"<p>This guide documents generation parameters available in SDnext. This guide does not cover mostly static system settings Parameters listed can be used in the <code>extra</code> field of styles Parameters from the \"Settings\" sections (like advanced sampler or postprocessing options) can be used in the <code>override_settings</code> dictionary for per-generation changes  </p>"},{"location":"Parameters/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Parameter Terminology</li> <li>Base Parameters</li> <li>Sampler Parameters</li> <li>Guidance Parameters</li> <li>Text Encoder Parameters</li> <li>Style Parameters</li> <li>VAE Parameters</li> <li>Enhancement Parameters</li> <li>Face Restoration Parameters</li> <li>Detailer Parameters</li> <li>HDR Correction Parameters</li> <li>Image-to-Image Parameters</li> <li>High-Resolution (HiRes) Parameters</li> <li>Inpainting Parameters</li> <li>Refiner Parameters</li> <li>Postprocessing Parameters</li> <li>Save Options</li> <li>Script Parameters</li> <li>Override Parameters</li> <li>Video Processing Parameters</li> <li>CLiP Skip</li> <li>Resize modes</li> <li>Context aware scaling</li> <li>Notes</li> </ul>"},{"location":"Parameters/#understanding-parameter-terminology","title":"Understanding Parameter Terminology","text":"<p>This section helps you understand the different terms used throughout the SDnext Processing Parameters Guide. Here\u2019s a breakdown:</p> Type Use For Example UI Label <code>key</code> Identifier name for the parameter, before the colon <code>prompt</code>, <code>seed</code>, <code>width</code> <code>str</code> Text or labels <code>\"portrait, masterpiece\"</code> <code>int</code> Whole numbers <code>20</code>, <code>512</code> <code>float</code> Numbers with decimals <code>0.5</code>, <code>7.5</code> <code>bool</code> On/Off toggles <code>True</code>, <code>False</code> <code>list</code> Multiple values <code>[\"style1\", \"style2\"]</code> <code>Dict</code> Key-value settings <code>{key: value}</code> <code>Any</code> Flexible input types, context dependent <code>image.png</code>, <code>#FFAA00</code>, <code>True</code>"},{"location":"Parameters/#base-parameters","title":"Base Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>sd_model_checkpoint</code> <code>str</code> Varies <code>Values: List of available checkpoint models.</code> The primary (base) model used for image generation. Base model <code>sd_unet</code> <code>str</code> <code>\"Default\"</code> <code>Values: List of available UNET models.</code> Specifies a custom UNET model to be used instead of the one packaged with the base model. unet model <code>prompt</code> <code>str</code> <code>\"\"</code> <code>Syntax: prompt: your text here</code> The positive text prompt describing what you want to generate. This is the main instruction that tells the AI what image to create. Prompt <code>negative_prompt</code> <code>str</code> <code>\"\"</code> <code>Syntax: negative_prompt: bad quality, blurry, extra fingers</code> Text describing what you want to avoid in the generation. Common negatives include unwanted elements, quality issues, or artistic styles you want to exclude. Negative prompt <code>seed</code> <code>int</code> <code>-1</code> <code>Values: -1 (random), or any positive integer</code> <code>Syntax: seed: 12345</code> Random seed for reproducible results. Using the same seed with identical settings will generate the same image. -1 generates a random seed each time. Seed <code>subseed</code> <code>int</code> <code>-1</code> <code>Values: -1 (random), or any positive integer</code> <code>Syntax: subseed: 67890</code> Secondary seed for additional variation control. Works together with subseed_strength to add controlled randomness to your generation. Variation <code>subseed_strength</code> <code>float</code> <code>0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: subseed_strength: 0.15</code> How much the subseed affects the generation (0 = no effect, 1 = maximum effect). Useful for creating slight variations of the image with the same seed. Variation strength <code>seed_resize_from_h</code> <code>int</code> <code>-1</code> <code>Values: -1 (disabled), or positive integer</code> <code>Syntax: seed_resize_from_h: 512</code> Original height for seed resizing. This maintains noise patterns when changing resolution, useful when upscaling while keeping similar composition. Resize seed from height <code>seed_resize_from_w</code> <code>int</code> <code>-1</code> <code>Values: -1 (disabled), or positive integer</code> <code>Syntax: seed_resize_from_w: 512</code> Original width for seed resizing. Works with seed_resize_from_h to maintain consistency across resolution changes. Resize seed from width <code>batch_size</code> <code>int</code> <code>1</code> <code>Values: 1 or higher</code> <code>Syntax: batch_size: 4</code> Number of images to generate simultaneously. Higher values use more GPU memory but are more efficient than generating images one by one. Batch size <code>n_iter</code> <code>int</code> <code>1</code> <code>Values: 1 or higher</code> <code>Syntax: n_iter: 5</code> Number of generation iterations (batches) to run. If batch_size is 4 and n_iter is 5, you'll get 20 images total. Batch count <code>width</code> <code>int</code> <code>1024</code> <code>Values: Multiples of 8</code> <code>Syntax: width: 1280</code> Image width in pixels. Must be divisible by 8. For best results use resolutions recommended for the model you're using. Width <code>height</code> <code>int</code> <code>1024</code> <code>Values: Multiples of 8</code> <code>Syntax: height: 768</code> Image height in pixels. Must be divisible by 8. Consider aspect ratio for best results. Height"},{"location":"Parameters/#vae-parameters","title":"VAE Parameters","text":"<p>Controls the Variational Autoencoder (VAE) used for encoding and decoding images to and from latent space.</p> Parameter Type Default Details / Syntax Description UI Label <code>sd_vae</code> <code>str</code> <code>\"Automatic\"</code> <code>Values: \"Automatic\", \"None\", or a list of available VAE models.</code> Selects the VAE model. \"Automatic\" will try to find a matching VAE for the main model. VAE model <code>tiling</code> <code>bool</code> <code>False</code> <code>Syntax: tiling: True</code> Enable tiled VAE decoding for seamless textures. Perfect for creating repeating patterns or textures that tile without visible seams. Tiling <code>vae_type</code> <code>str</code> <code>\"Full\"</code> <code>Values: \"Full\", \"Tiny\", \"Remote\"</code> <code>Syntax: vae_type: Tiny</code> What type of VAE will be used for VAE Decode step at the end of generation. \"Full\" uses full VAE provided for the model, \"Tiny\" uses TAESD, a smaller VAE version that provides memory savings but lower quality, \"Remote\" utilises Huggingface remote VAE service to decode latent images on Huggingface servers instead of locally."},{"location":"Parameters/#sampler-parameters","title":"Sampler Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>sampler_name</code> <code>str</code> <code>None</code> <code>Values: \"Euler\", \"Euler a\", \"DPM++ 2M\", etc.</code> <code>Syntax: sampler_name: DPM++ 2M Karras</code> Algorithm used for image generation. Different samplers can produce different styles and qualities. Sampling method <code>steps</code> <code>int</code> <code>20</code> <code>Range: 1 to 150 (typical)</code> <code>Syntax: steps: 30</code> Number of denoising steps. More steps generally mean better quality but slower generation. 20-50 is typical, with diminishing returns above 50. Settings may vary depending on sampler chosen. Steps <code>eta</code> <code>float</code> <code>None</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: eta: 0.67</code> Noise multiplier for samplers that support it (like DDIM). Adds randomness to deterministic samplers. 0 = fully deterministic, 1 = maximum noise. noise multiplier (eta) <code>eta_noise_seed_delta</code> <code>int</code> <code>0</code> <code>Syntax: eta_noise_seed_delta: 31337</code> An additional seed that influences the noise (eta) in ancestral samplers, allowing for variations without changing the main seed. Essentially adds 31337 to your seed. Only useful when used with originally leaked NovelAI models to replicate images. noise seed delta (eta) <code>scheduler_eta</code> <code>float</code> <code>1.0</code> <code>Range: 0.0 to 1.0</code> Noise multiplier (eta) for ancestral samplers. <code>0.0</code> is deterministic, <code>1.0</code> is fully stochastic. noise multiplier (eta) <code>schedulers_solver_order</code> <code>int</code> <code>0</code> <code>Range: 0 to 5</code> The order of the solver for certain schedulers (e.g., DPM). Higher orders can be more accurate but slower. sampler order <code>schedulers_use_loworder</code> <code>bool</code> <code>True</code> <code>Values: True, False</code> For some schedulers, uses a simpler, faster solver for the final steps of generation, also generally stabilising the output. use simplified solvers in final steps <code>schedulers_prediction_type</code> <code>str</code> <code>\"default\"</code> <code>Values: 'default', 'epsilon', 'sample', 'v_prediction'</code> Overrides the model's configured prediction type (what the model is trained to predict at each step). Frequently used with velocity prediction (V-Pred) models. prediction method <code>schedulers_sigma</code> <code>str</code> <code>\"default\"</code> <code>Values: 'default', 'karras', 'exponential', 'polyexponential'</code> The algorithm used to calculate the noise schedule (sigmas). 'karras' is a popular choice. sigma method <code>schedulers_beta_schedule</code> <code>str</code> <code>\"default\"</code> <code>Values: 'default', 'linear', 'scaled_linear', ...</code> Defines how the noise level changes over time (the beta schedule). beta schedule <code>schedulers_use_thresholding</code> <code>bool</code> <code>False</code> <code>Values: True, False</code> Enables dynamic thresholding to prevent oversaturation and artifacts, especially at high CFG scales. use dynamic thresholding <code>schedulers_timestep_spacing</code> <code>str</code> <code>\"default\"</code> <code>Values: 'default', 'linspace', 'leading', 'trailing'</code> How the timesteps are distributed across the generation process. timestep spacing <code>schedulers_timesteps</code> <code>str</code> <code>''</code> <code>Syntax: \"1000, 999, ...\"</code> A comma-separated list of exact timesteps to use, overriding all other timestep settings. timesteps override <code>schedulers_rescale_betas</code> <code>bool</code> <code>False</code> <code>Values: True, False</code> Also known as Zero terminal SNR (ZSNR). Rescales the beta schedule to have a zero signal-to-noise ratio at the end, which can improve sample quality. Frequently used with velocity prediction (V-Pred) models. rescale betas with zero terminal snr <code>schedulers_beta_start</code> <code>float</code> <code>0.0</code> <code>Range: 0.0 to 1.0</code> The starting value for a custom beta schedule. beta start <code>schedulers_beta_end</code> <code>float</code> <code>0.0</code> <code>Range: 0.0 to 1.0</code> The ending value for a custom beta schedule. beta end <code>schedulers_timesteps_range</code> <code>int</code> <code>1000</code> <code>Range: 250 to 4000</code> The total number of discrete timesteps in the full diffusion process from which the sampler will draw. timesteps range <code>schedulers_shift</code> <code>float</code> <code>3.0</code> <code>Range: 0.1 to 10.0</code> Sampler shift value for samplers. Controls the timing of noise removal during sampling - positive values do more denoising early (better details), negative values do more denoising late (better structure), zero is the default schedule. sampler shift <code>schedulers_dynamic_shift</code> <code>bool</code> <code>False</code> <code>Values: True, False</code> Enables dynamic sampler shifting, applying timestep shifting on-the-fly based on the image resolution. sampler dynamic shift <code>schedulers_sigma_adjust</code> <code>float</code> <code>1.0</code> <code>Range: 0.5 to 1.5</code> Sigma adjustment value for samplers. <code>schedulers_sigma_adjust_min</code> <code>float</code> <code>0.2</code> <code>Range: 0.0 to 1.0</code> Sigma adjustment start value for samplers. <code>schedulers_sigma_adjust_max</code> <code>float</code> <code>0.8</code> <code>Range: 0.0 to 1.0</code> Sigma adjustment end value for samplers. <code>uni_pc_variant</code> <code>str</code> <code>\"bh2\"</code> <code>Values: \"bh1\", \"bh2\", \"vary_coeff\"</code> Specifies the variant of the UniPC sampler to use. <code>uni_pc_skip_type</code> <code>str</code> <code>\"time_uniform\"</code> <code>Values: \"time_uniform\", \"time_quadratic\", \"logSNR\"</code> The skip type for the UniPC sampler, affecting how it moves through timesteps."},{"location":"Parameters/#guidance-parameters","title":"Guidance Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>cfg_scale</code> <code>float</code> <code>6.0</code> <code>Range: 1.0 to 30.0</code> <code>Syntax: cfg_scale: 7.5</code> Classifier-Free Guidance scale. Higher values follow your prompt more closely but can cause oversaturation. 4-12 is typical for most uses. guidance scale <code>cfg_end</code> <code>float</code> <code>1.0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: cfg_end: 0.7</code> When to stop applying CFG during generation (1.0 = full generation). Lower values can produce more creative results by reducing guidance in later steps. Guidance End <code>diffusers_guidance_rescale</code> <code>float</code> <code>0.0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: diffusers_guidance_rescale: 0.3</code> Rescale guidance for diffusers to prevent oversaturation when using high CFG values. Helps maintain color balance. Rescale guidance <code>pag_scale</code> <code>float</code> <code>0.0</code> <code>Range: 0.0 to 5.0</code> <code>Syntax: pag_scale: 2.0</code> Perturbed Attention Guidance scale for enhanced details. Improves fine details without affecting overall composition. Attention guidance <code>pag_adaptive</code> <code>float</code> <code>0.5</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: pag_adaptive: 0.8</code> Adaptive strength for PAG. Controls how much the guidance adapts to different image regions. Adaptive scaling"},{"location":"Parameters/#text-encoder-parameters","title":"Text Encoder Parameters","text":"<p>Controls the models and methods used to interpret the text prompt.</p> Parameter Type Default Details / Syntax Description UI Label <code>clip_skip</code> <code>int</code> <code>1</code> <code>Range: 1 to 12</code> <code>Syntax: clip_skip: 2</code> Skip last N layers of CLIP model. Only affects models which use CLIP as their Text Encoder. Values 1 or 2 used with SD1.5, should be left at default 1 with SDXL and other models. More here: CLiP skip in detail Clip skip <code>clip_skip_enabled</code> <code>bool</code> <code>False</code> <code>Values: True, False</code> Enables the <code>clip_skip</code> parameter, which skips final layers of the CLIP model. <code>sd_text_encoder</code> <code>str</code> <code>'Default'</code> <code>Values: List of available Text Encoder models.</code> Specifies a custom Text Encoder model instead of the one packaged with the base model. text encoder model <code>prompt_attention</code> <code>str</code> <code>\"native\"</code> <code>Values: \"native\", \"compel\", \"xhinker\", \"a1111\", \"fixed\"</code> Selects the parser for handling prompt weighting and attention syntax. prompt processor"},{"location":"Parameters/#style-parameters","title":"Style Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>styles</code> <code>List[str]</code> <code>[]</code> <code>Syntax: styles: [photorealistic, dramatic lighting]</code> List of style names to apply to the prompt. These are predefined styles that modify both prompts and parameters. Styles"},{"location":"Parameters/#enhancement-parameters","title":"Enhancement Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>hidiffusion</code> <code>bool</code> <code>False</code> <code>Syntax: hidiffusion: True</code> Enable HiDiffusion for improved high-resolution generation. Helps maintain quality and coherence at resolutions higher than the model is capable of natively. HiDiffusion <code>do_not_reload_embeddings</code> <code>bool</code> <code>False</code> <code>Syntax: do_not_reload_embeddings: True</code> Skip reloading embeddings (performance optimization). Useful when running multiple generations with the same embeddings. <code>enhance_prompt</code> <code>bool</code> <code>False</code> <code>Syntax: enhance_prompt: True</code> Automatically enhance/expand the prompt using AI. Adds more descriptive details to simple prompts for better results. enhance prompt"},{"location":"Parameters/#face-restoration-parameters","title":"Face Restoration Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>restore_faces</code> <code>bool</code> <code>False,</code> <code>Values: False, codeformer, gfpgan</code> <code>Syntax: restore_faces: True</code> Apply face restoration post-processing using Codeformer or GFPGAN neural network. Automatically detects and enhances faces in the image for better facial details. Not the same as detailer, generally inferior, legacy option. <code>face_restoration_model</code> <code>str</code> <code>\"None\"</code> <code>Values: 'None', 'CodeFormer', 'GFPGAN', etc.</code> Selects the model to use for the face restoration post-processing step. <code>code_former_weight</code> <code>float</code> <code>0.2</code> <code>Range: 0.0 to 1.0</code> Blending strength for CodeFormer. <code>0</code> shows the original, <code>1</code> shows the fully restored face. CodeFormer weight parameter"},{"location":"Parameters/#detailer-parameters","title":"Detailer Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>detailer_enabled</code> <code>bool</code> <code>False</code> <code>Syntax: detailer_enabled: True</code> Enable automatic detail enhancement pass. Runs after first and second pass, uses detection models for automatic detection and masking. Supports custom YOLO detection models for detection of various objects on the image. Enable detailer pass <code>detailer_prompt</code> <code>str</code> <code>''</code> <code>Syntax: detailer_prompt: highly detailed, sharp focus</code> Specific prompt for detail enhancement. If used, overrides main positive prompt during detail pass for targeted improvements. Detailer prompt <code>detailer_negative</code> <code>str</code> <code>''</code> <code>Syntax: detailer_negative: blurry, low quality</code> Negative prompt for detail enhancement. If used, overrides main negative prompt during detail pass. Detailer negative prompt <code>detailer_steps</code> <code>int</code> <code>10</code> <code>Range: 1 to 50</code> <code>Syntax: detailer_steps: 15</code> Number of steps for detail enhancement. Can be set lower than main steps since it's refining existing details. Detailer steps <code>detailer_strength</code> <code>float</code> <code>0.3</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: detailer_strength: 0.5</code> Strength of detail enhancement. Higher values change the image more dramatically during detailing pass. Detailer strength <code>detailer_model</code> <code>str</code> <code>\"Detailer\"</code> <code>Values: List of detailer names</code> Selects the primary detailer model. Detailer models <code>detailer_classes</code> <code>str</code> <code>\"\"</code> <code>Syntax: \"class1, class2\"</code> A comma-separated list of object classes to detect and detail (e.g., \"face, hand\"). Detailer classes <code>detailer_conf</code> <code>float</code> <code>0.6</code> <code>Range: 0.0 to 1.0</code> The minimum confidence score for an object detection to be considered valid for detailing. Min confidence <code>detailer_max</code> <code>int</code> <code>2</code> <code>Range: 1 to 10</code> The maximum number of detected objects to process with the detailer. Max detected <code>detailer_iou</code> <code>float</code> <code>0.5</code> <code>Range: 0.0 to 1.0</code> Intersection over Union (IoU) threshold to filter out overlapping detections. Max overlap <code>detailer_sigma_adjust</code> <code>float</code> <code>1.0</code> <code>Range: 0.0 to 1.0</code> Adjusts the sigma value for the detailer's sampler. <code>detailer_sigma_adjust_max</code> <code>float</code> <code>1.0</code> <code>Range: 0.0 to 1.0</code> The end sigma adjustment value for the detailer's sampler. <code>detailer_min_size</code> <code>float</code> <code>0.0</code> <code>Range: 0.1 to 1.0</code> The minimum relative size an object must have to be detailed. Min size <code>detailer_max_size</code> <code>float</code> <code>1.0</code> <code>Range: 0.1 to 1.0</code> The maximum relative size an object can have to be detailed. Max size <code>detailer_padding</code> <code>int</code> <code>20</code> <code>Range: 0 to 100</code> Extra padding (in pixels) to add around the detected object's mask before inpainting. Edge padding <code>detailer_blur</code> <code>int</code> <code>10</code> <code>Range: 0 to 100</code> The blur radius (in pixels) for the edge of the detailer's mask to create a smoother blend. Edge blur <code>detailer_models</code> <code>List[str]</code> <code>['face-yolo8n']</code> <code>Values: List of YOLO models</code> The specific YOLO detection model or models to be used by the detailer. Detailer models"},{"location":"Parameters/#hdr-correction-parameters","title":"HDR Correction Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>hdr_mode</code> <code>int</code> <code>0</code> <code>Values: 0 (disabled), 1, 2, 3</code> <code>Syntax: hdr_mode: 1</code> HDR processing mode. Each mode applies different tone mapping algorithms for enhanced dynamic range. correction mode <code>hdr_brightness</code> <code>float</code> <code>0</code> <code>Range: -1.0 to 1.0</code> <code>Syntax: hdr_brightness: 0.2</code> Brightness adjustment. Positive values brighten the image, negative values darken it. brightness <code>hdr_color</code> <code>float</code> <code>0</code> <code>Range: -1.0 to 1.0</code> <code>Syntax: hdr_color: 0.3</code> Color/saturation adjustment. Positive values increase color vibrancy, negative values reduce it. color <code>hdr_sharpen</code> <code>float</code> <code>0</code> <code>Range: 0.0 to 2.0</code> <code>Syntax: hdr_sharpen: 0.5</code> Sharpening strength. Enhances edge details and overall image crispness. sharpen <code>hdr_clamp</code> <code>bool</code> <code>False</code> <code>Syntax: hdr_clamp: True</code> Clamp HDR values to prevent overflow. Prevents extreme bright or dark areas from clipping. HDR Clamp <code>hdr_boundary</code> <code>float</code> <code>4.0</code> <code>Syntax: hdr_boundary: 3.5</code> HDR boundary threshold. Controls the range of HDR effect application. hdr range <code>hdr_threshold</code> <code>float</code> <code>0.95</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: hdr_threshold: 0.9</code> HDR activation threshold. Determines which brightness levels trigger HDR processing. threshold <code>hdr_maximize</code> <code>bool</code> <code>False</code> <code>Syntax: hdr_maximize: True</code> Maximize HDR effect. Applies the strongest possible HDR enhancement. HDR Maximize <code>hdr_max_center</code> <code>float</code> <code>0.6</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: hdr_max_center: 0.5</code> Center point for HDR maximization. Defines the brightness level that receives maximum enhancement. <code>hdr_max_boundary</code> <code>float</code> <code>1.0</code> <code>Syntax: hdr_max_boundary: 0.8</code> Boundary for HDR maximization. Sets the range around the center point for maximum effect. <code>hdr_color_picker</code> <code>str</code> <code>None</code> <code>Syntax: hdr_color_picker: #FF5500</code> Color reference for HDR tinting. Applies a color cast to the HDR effect using hex color codes. <code>hdr_tint_ratio</code> <code>float</code> <code>0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: hdr_tint_ratio: 0.2</code> Strength of HDR color tinting. Controls how much the selected color affects the image."},{"location":"Parameters/#image-to-image-parameters","title":"Image-to-Image Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>init_images</code> <code>list</code> <code>None</code> <code>Syntax: init_images: [image1.png]</code> List of input images for img2img/inpainting. These serve as the starting point for transformation. optional init image or video <code>resize_mode</code> <code>int</code> <code>0</code> <code>Values: 0 (Just resize), 1 (Crop and resize), ...</code> <code>Syntax: resize_mode: 1</code> How to handle size mismatch between input and output. Choose based on whether you want to preserve aspect ratio or fill the canvas. See more: Resize modes resize mode <code>resize_name</code> <code>str</code> <code>'None'</code> <code>Values: Upscaler names (\"Lanczos\", \"ESRGAN_4x\")</code> <code>Syntax: resize_name: ESRGAN_4x</code> Upscaler to use for resizing. Different upscalers excel at different content types. Upscaler <code>resize_context</code> <code>str</code> <code>'None'</code> <code>Values 'None, Add with forward\", \"Remove with forward\", \"Add with backward\", \"Remove with backward</code> <code>Syntax: resize_context: Add with forward</code> Advanced image resizing technique that intelligently adds or removes content from images while preserving important visual elements. See more: Context-aware scaling context <code>denoising_strength</code> <code>float</code> <code>0.3</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: denoising_strength: 0.75</code> How much to change the input image. 0 = no change, 1 = complete reimagining. 0.3-0.7 is typical for variations. Denoising strength <code>scale_by</code> <code>float</code> <code>1.0</code> <code>Range: 0.1 to 8.0</code> <code>Syntax: scale_by: 2.0</code> Scale factor for input image. 2.0 doubles the size, 0.5 halves it. Scale by <code>img2img_background_color</code> <code>str</code> <code>\"#ffffff\"</code> <code>Syntax: \"#RRGGBB\"</code> Sets the background color used to fill transparent areas of an input image. resize background color <code>image_cfg_scale</code> <code>float</code> <code>None</code> <code>Range: 0.0 to 3.0</code> <code>Syntax: image_cfg_scale: 1.5</code> Image conditioning strength (for ControlNet/IP-Adapter). Controls how closely to follow the input image structure. guidance scale"},{"location":"Parameters/#high-resolution-hires-parameters","title":"High-Resolution (HiRes) Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>enable_hr</code> <code>bool</code> <code>False</code> <code>Syntax: enable_hr: True</code> Enable high-resolution second pass. Generates at lower resolution first, then upscales and optionally refines. Enable refine pass <code>hr_sampler_name</code> <code>str</code> <code>None</code> <code>Values: Same as sampler_name</code> <code>Syntax: hr_sampler_name: UniPC</code> Sampler for high-resolution pass. Can be different from primary sampler for specialized effects during upscaling when using HiRes. Refine sampler <code>hr_scale</code> <code>float</code> <code>2.0</code> <code>Range: 1.0 to 4.0</code> <code>Syntax: hr_scale: 2.5</code> Upscale factor for hires pass. 2.0 means the final image will be twice the width and height of the initial generation. scale <code>hr_upscaler</code> <code>str</code> <code>None</code> <code>Values: \"None\", \"Lanczos\", \"ESRGAN_4x\", etc.</code> <code>Syntax: hr_upscaler: R-ESRGAN 4x+</code> Upscaler algorithm for hires pass. Each has strengths for different image types and styles. Can be combined with HiRes to upscale the image and then refine it. Upscaler <code>hr_second_pass_steps</code> <code>int</code> <code>0</code> <code>Values: 0 (use primary steps), or 1-150</code> <code>Syntax: hr_second_pass_steps: 20</code> Number of steps for hires pass. Often fewer steps are needed since it's refining an existing image. Hires steps <code>hr_resize_x</code> <code>int</code> <code>0</code> <code>Values: 0 (use scale), or target width</code> <code>Syntax: hr_resize_x: 2048</code> Target width for hires (overrides scale). Use for exact output dimensions rather than relative scaling. Resize width <code>hr_resize_y</code> <code>int</code> <code>0</code> <code>Values: 0 (use scale), or target height</code> <code>Syntax: hr_resize_y: 1536</code> Target height for hires (overrides scale). Pairs with hr_resize_x for exact dimensions. Resize height <code>hr_denoising_strength</code> <code>float</code> <code>0.0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: hr_denoising_strength: 0.4</code> Denoising strength for hires pass. Lower values preserve more detail from upscale, higher values allow more creative changes. Denoising strength <code>firstphase_width</code> <code>int</code> <code>0</code> <code>Values: 0 (disabled), or initial width</code> <code>Syntax: firstphase_width: 512</code> Override initial generation width. Useful for controlling the first pass resolution independently. <code>firstphase_height</code> <code>int</code> <code>0</code> <code>Values: 0 (disabled), or initial height</code> <code>Syntax: firstphase_height: 512</code> Override initial generation height. Works with firstphase_width for custom initial dimensions."},{"location":"Parameters/#inpainting-parameters","title":"Inpainting Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>mask</code> <code>Any</code> <code>None</code> <code>Syntax: mask: mask_image.png</code> Mask image for inpainting. White areas will be regenerated, black areas will be preserved. Mask <code>mask_blur</code> <code>int</code> <code>4</code> <code>Range: 0 to 64</code> <code>Syntax: mask_blur: 8</code> Blur radius for mask edges. Higher values create smoother transitions between inpainted and original areas. Mask blur <code>inpaint_full_res</code> <code>bool</code> <code>False</code> <code>Syntax: inpaint_full_res: True</code> Inpaint at full resolution of masked area only. Useful for detailed work on small areas without processing the entire image. inpaint masked only <code>inpaint_full_res_padding</code> <code>int</code> <code>0</code> <code>Range: 0 to 256</code> <code>Syntax: inpaint_full_res_padding: 32</code> Padding around masked area for full res inpainting. Helps blend the inpainted area with surroundings. item padding <code>inpainting_mask_invert</code> <code>int</code> <code>0</code> <code>Values: 0 (normal), 1 (inverted)</code> <code>Syntax: inpainting_mask_invert: 1</code> Invert the mask (swap inpaint/keep areas). Useful when your mask highlights what to keep rather than what to change. invert mask <code>img2img_color_correction</code> <code>bool</code> <code>False</code> <code>Values: True, False</code> When enabled, applies color correction to the inpainted area to better match the original image. apply color correction <code>mask_apply_overlay</code> <code>bool</code> <code>True</code> <code>Values: True, False</code> Renders the inpaint mask as a transparent overlay in the UI for easier visualization. apply mask as overlay"},{"location":"Parameters/#refiner-parameters","title":"Refiner Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>sd_model_refiner</code> <code>str</code> <code>'None'</code> <code>Values: 'None' or a list of available checkpoint models.</code> The refiner model used for the final steps of generation to add details. Typically used with SDXL models. Refiner model <code>refiner_steps</code> <code>int</code> <code>5</code> <code>Range: 0 to 50</code> <code>Syntax: refiner_steps: 10</code> Number of steps for refiner model. Refiner models add final details and polish to images. Refiner steps <code>refiner_start</code> <code>float</code> <code>0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: refiner_start: 0.8</code> When to switch to refiner (0.8 = at 80% of steps). Earlier switching gives refiner more influence over the final image. Refiner start <code>refiner_prompt</code> <code>str</code> <code>''</code> <code>Syntax: refiner_prompt: masterpiece, best quality</code> Override main positive prompt for refiner. Allows specialized prompting for the refinement stage. Refine Prompt <code>refiner_negative</code> <code>str</code> <code>''</code> <code>Syntax: refiner_negative: low quality, blurry</code> Override main negative prompt for refiner. Refine negative prompt <code>hr_refiner_start</code> <code>float</code> <code>0</code> <code>Range: 0.0 to 1.0</code> <code>Syntax: hr_refiner_start: 0.85</code> When to switch to refiner in hires pass. Can be different from primary refiner_start for specialized workflows. Refiner start"},{"location":"Parameters/#postprocessing-parameters","title":"Postprocessing Parameters","text":"<p>These settings control postprocessing operations that run after the main image generation is complete.</p> Parameter Type Default Details / Syntax Description UI Label <code>postprocessing_enable_in_main_ui</code> <code>List[str]</code> <code>[]</code> <code>Values: List of script names</code> Selects which postprocessing scripts (like upscalers or face restoration) to run after generation. <code>postprocessing_operation_order</code> <code>List[str]</code> <code>[]</code> <code>Values: List of script names</code> Defines the execution order of the selected postprocessing scripts. postprocessing operation order"},{"location":"Parameters/#save-options","title":"Save Options","text":"Parameter Type Default Details / Syntax Description UI Label <code>outpath_samples</code> <code>str</code> <code>None</code> <code>Syntax: outpath_samples: outputs/my_project/</code> Custom output directory for images. Helps organize outputs for different projects or styles. Images folder <code>outpath_grids</code> <code>str</code> <code>None</code> <code>Syntax: outpath_grids: outputs/grids/</code> Custom output directory for grids. Separate location for batch result grids. Grids folder <code>do_not_save_samples</code> <code>bool</code> <code>False</code> <code>Syntax: do_not_save_samples: True</code> Skip saving individual images. Useful when only the grid matters or for preview runs. <code>do_not_save_grid</code> <code>bool</code> <code>False</code> <code>Syntax: do_not_save_grid: True</code> Skip saving image grid. Useful when generating single images or when individual files are preferred."},{"location":"Parameters/#script-parameters","title":"Script Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>script_args</code> <code>list</code> <code>[]</code> <code>Syntax: script_args: [arg1, arg2, arg3]</code> Arguments passed to active scripts/extensions. Format depends on the specific script being used."},{"location":"Parameters/#override-parameters","title":"Override Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>override_settings</code> <code>Dict[str, Any]</code> <code>{}</code> <code>Syntax: override_settings: {CLIP_stop_at_last_layers: 2}</code> Temporarily override global settings for this generation. Settings revert after generation completes. Override settings <code>override_settings_restore_afterwards</code> <code>bool</code> <code>True</code> <code>Syntax: override_settings_restore_afterwards: False</code> Restore original settings after generation. Set to False to make overrides permanent."},{"location":"Parameters/#video-processing-parameters","title":"Video Processing Parameters","text":"Parameter Type Default Details / Syntax Description UI Label <code>prompt_template</code> <code>str</code> <code>None</code> <code>Syntax: prompt_template: frame {frame_num}: {base_prompt}</code> Template for animated prompts across frames. Allows dynamic prompt changes throughout video generation. <code>frames</code> <code>int</code> <code>1</code> <code>Values: 1 or higher</code> <code>Syntax: frames: 30</code> Number of video frames to generate. More frames = longer video but longer generation time. frames <code>scheduler_shift</code> <code>float</code> <code>0.0</code> <code>Syntax: scheduler_shift: 0.5</code> Shift scheduler timing for animation effects. Creates motion and transformation effects between frames. sampler shift <code>vae_tile_frames</code> <code>int</code> <code>0</code> <code>Syntax: vae_tile_frames: 8</code> Number of frames to tile in VAE for memory efficiency. Helps process longer videos on limited GPU memory. tile frames"},{"location":"Parameters/#resize-modes","title":"Resize modes","text":"<p>TODO</p>"},{"location":"Parameters/#context-aware-scaling","title":"Context aware scaling","text":"<p>TODO</p>"},{"location":"Parameters/#notes","title":"Notes","text":"<ol> <li>Memory Considerations: Higher batch_size, steps, and resolution require more GPU memory</li> <li>Quality vs Speed: More steps generally improve quality but increase generation time</li> <li>Compatibility: Some parameters may not work with all models or samplers</li> <li>Defaults: Default values are optimized for SDXL models at 1024x1024 resolution</li> </ol>"},{"location":"Performance-Tuning/","title":"Performance Tuning","text":""},{"location":"Performance-Tuning/#introduction","title":"Introduction","text":"<p>Hi folks, it's your (moderately) friendly neighborhood Aptronym here!</p> <p>People are always asking me how to get the most it/s out of their GPUs, now... this is a complicated subject due to the current GPU landscape, the wide range of GPUs that can do stable diffusion just within each manufacturer, going back years (nvidia 1080s, 1050s, or RX500 series), as well as varying and often limited amounts of VRAM, possibly inside of a laptop.</p> <p>That's not even counting the complicated selection of inference platforms SDNext has available, everything from plain everyday CUDA and ROCm to Onnxruntime/Olive, and now ZLUDA.</p> <p>I can't promise you'll be able to use all of these options, I can't promise they won't crash your instance (or be buggy for that matter), that's going to vary wildly from GPU, OS, RAM, VRAM, current chosen inference platform, and backend, and some of these options will not work together, or at all, on some platforms. You will have to test that yourselves, but we are hoping to build a matrix of sorts showing what is available and what works with what, but that's going to take some user testing and feedback.</p> <p>If you help us by providing feedback (issues w/logs, screenshots, etc.), which we never have enough of, we will do our best to correct what we can and ensure that the experience of using SDNext is as optimized as it can be. Some limitations we won't be able to overcome purely do to things beyond our control, such as conflicts caused by the nature of the inference platform.</p> <p>Just as an added ray of sunshine to give you hope, there are plans in motion to create a more self-optimizing system that will not only configure itself for optimal performance based on your setup, but also react to what you're doing, what model you have loaded (SD15 v. SDXL is a huge difference), and specified preferences, so that it is always as performant as possible while sacrificing minimal quality loss. We would welcome any assistance with this, recently-laid-off-due-to-AI developers (Curse you Devin!) or any developers, or people with coding experience, at all.</p>"},{"location":"Performance-Tuning/#compute-settings","title":"Compute Settings","text":"<p>Note: Changing any of the settings on this page will require you to, at the least, unload the model and then reload it (after hitting apply!), as these settings are applied on model load, not in realtime.</p> <p>Generally speaking, for most GPUs our user-base has (mostly Nvidia on Windows judging by discord roles, so using CUDA), you are going to want the settings below (BF16 is possible too if using 30xx+).  </p> <p>Good settings:  </p> <p> </p> <p>Bad settings:  </p> <p> </p> <p>In general, using any of these selected \"bad\" settings is considered a bad thing, you only want to use any of these if necessary to make your card work with SD. They're slower and use up a lot more memory. Unless you're on OpenVINO, things show up as fp32 there anyway due to how it works. Leave that alone</p> <p>That being said, if you are having strange issues with squares and whatnot and you're on something other than a newer Nvidia GPU (2000s and up?), you may wish to try these settings. Upcast sampling is the better version of --no-half, use it if at all possible. Try these one by one, unload the model, reload the model, then test generate, hopefully you will find one, or a combination that works.  </p> <p></p>"},{"location":"Performance-Tuning/#model-compile","title":"Model Compile","text":"<p>To use any of the model compile options, you must select via the checkboxes, at least one of these: Model, VAE, Text Encoder, Upscaler. It's probably pretty pointless to compile the text encoder, but Model and VAE will net you a large boost in speed. If you use upscalers a lot, by all means, select that too.</p>"},{"location":"Performance-Tuning/#stable-fast","title":"Stable-fast","text":"<p>Stable-fast is one of the model compiling options, and if you can use it with your setup (Nvidia GPUs, maybe Zluda), you should, as it's a big speed-up. First you will need to open a terminal, activate the venv for sdnext, and from the root sdnext folder (typically automatic), you will type <code>python cli\\install-sf.py</code> and stand back while it hopefully works its magic, acquiring, or potentially compiling then installing the most recent version of stable-fast.  </p> <p>Note that you can do the install of stable-fast while SDNext is already running in another terminal, it will attempt to load the library when you select it, so there is no need to shut down or restart.</p>"},{"location":"Performance-Tuning/#onediff","title":"OneDiff","text":"<p>Linux (and perhaps Mac) users may also use <code>OneDiff</code>, which should be better/faster/stronger than <code>Stable-Fast</code>, though you will need to manually execute a <code>pip install -U onediff</code> from a venv console to install the necessary libraries.  </p> <p>NOTE: Do NOT compile the Text Encoder with OneDiff, it makes things slower.  </p> <p>If you want to thank anyone for OneDiff support, hit up @aifartist on our Discord.  </p>"},{"location":"Performance-Tuning/#inference-options","title":"Inference options","text":"<p>I'm skipping ahead here a bit since I want to get to the heart of the matter and expand later.</p> <p>These will be some of the easiest things you can do.</p>"},{"location":"Performance-Tuning/#token-merging-tome","title":"Token Merging (ToMe)","text":"<p>Sadly ToMe does not work at the same time as Hypertile. It will be disabled if Hypertile is enabled because Hypertile is faster. If hypertile works for you right now, don't even bother touching this.</p> <p>Token merging, aka ToMe, has been around for quite a while and still provides a performance gain if you desire to use it. In short it merges tokens, saving memory and speeding up generations. You can easily use it at 0.3-0.4, performance goes up as the number does, going up higher is up to you but you can always do an xyz and test to see.</p> <p>Default settings: </p> <p> </p> <p>Suggested settings: </p> <p> </p> <p>Honestly, 0.5 and up is the real performance gain, but you test and decide yourself. </p> <p>Bear in mind that it does have a quality impact on your image, greater the higher the setting, and will make perfect reproduction from the same seed and prompt impossible afaik.</p> <p>There is a new implementation along the same lines as ToMe, called ToDo, but works far better. That's in our queue for the near future!</p>"},{"location":"Performance-Tuning/#hypertile","title":"Hypertile","text":"<p>Overrides and is incompatible with ToMe, also can cause issues with some platforms, so if you get errors after turning this on, that might be why. Using Hypertile VAE might also cause issues, so try on and off. Hypertile is a much preferable option to <code>Token Merging</code> at the moment.</p> <p>As long as you enable <code>Hypertile UNet</code> you're good to go, the default settings should suffice, as 0 is auto-adjusting the tile size, it adapts to be half the size of your shortest image side.  </p> <p>Just don't even mess with <code>Hypertile VAE</code>, tends to cause more issues than it could possibly be worth, which isn't much.</p> <p>You may of course screw with the <code>swap size</code> and <code>UNet depth</code> as you like, I did test them briefly but saw some little benefit.</p> <p>Default settings: </p> <p></p> <p>Suggested settings: </p> <p></p> <p>Also has a quality impact, but I've never measured it personally</p>"},{"location":"Performance-Tuning/#other-settings","title":"Other settings","text":"<p>Parallel process images in batch is intended for img2img batch mode. If you set batch size=n, typically it generates n images for each input, with this setting, it will generate 1 image for each input, but process n in parallel.  </p> <p></p>"},{"location":"Process/","title":"Process","text":"<p>This guide covers the process tab in the app. The process tab is where you can upload or otherwise provide your image(s) and apply the functions to them.</p> <p>Tabs:</p> <ul> <li>Process Image </li> <li>Process Batch </li> <li>Process Folder </li> </ul> <p>Functions:</p> <ul> <li>Upscale </li> <li>Video </li> <li>GFPGAN </li> <li>CodeFormer </li> <li>Remove background</li> </ul>"},{"location":"Process/#tabs","title":"Tabs","text":""},{"location":"Process/#process-image","title":"Process Image","text":"<p>Upload your image so you can apply the functions explained below.</p>"},{"location":"Process/#process-batch","title":"Process Batch","text":"<p>Upload set of images so you can apply the functions explained below.</p>"},{"location":"Process/#process-folder","title":"Process Folder","text":"<p>Type in the location of your input directory and output directory so you can apply the functions explained below.</p>"},{"location":"Process/#functions","title":"Functions","text":""},{"location":"Process/#upscale","title":"Upscale","text":"<p>Upscale your image with the chosen model.</p>"},{"location":"Process/#video","title":"Video","text":"<p>Processes a bath or folder of images into a video, gif or png.</p>"},{"location":"Process/#gpfgan","title":"GPFGAN","text":"<p>Uses the GPFGAN model on your image and applies face restore that enhances facial details.</p>"},{"location":"Process/#codeformer","title":"CodeFormer","text":"<p>Use the CodeFormer model on your image and applies face restore that enhances facial details.</p>"},{"location":"Process/#remove-background","title":"Remove background","text":"<p>Uses a model to remove the background of your main subject in your image, experiment with the settings for desired result.</p>"},{"location":"Profiling/","title":"Profiling","text":"<p>SD.Next has built-in support for both Python and Torch profiling  </p> <p>Profiling can be started for the entire session using <code>--profile</code> command line flag Or it can be started/stopped on-demand using UI -&gt; System -&gt; Start/Stop profiling </p> <p>When profiling is enabled, analysis is performed  </p> <ul> <li>As a last step of server startup to see performance and bottlenecks of the startup process  </li> <li>As the last step of any generate workflow  </li> </ul> <p>Warning</p> <p>Collecting profile information may take significant resources and time</p>"},{"location":"Profiling/#saving","title":"Saving","text":"<p>Full <code>torch</code> profiling dump can be saved for analysis using external tools by setting env variable to location where to save profiling info:</p> <p><code>SD_PROFILE_FOLDER=/tmp/profile</code></p> <p>Each profiling run will create a JSON file in the specified folder File can be loaded for further analysis in tools such as:  </p> <ul> <li>&gt;</li> <li>https://ui.perfetto.dev/</li> </ul> <p>Warning</p> <p>Each profile trace file is over 100MB in size</p>"},{"location":"Profiling/#advanced","title":"Advanced","text":"<p>Profiling details can be further increased by setting additional environment variables:</p> <ul> <li><code>SD_PROFILE_STACK=true</code>: enable torch stack information</li> <li><code>SD_PROFILE_FLOPS=true</code>: enable torch flops calculations</li> <li><code>SD_PROFILE_SHAPES=true</code>: group torch profile information per each shape</li> </ul>"},{"location":"Prompt-Enhance/","title":"Prompt Enhance","text":"<p>Note</p> <p>Different model types have different preferences on how to prompt them. For details, see Prompting model specific tips.</p> <p>SD.Next includes built-in prompt enhancer that uses LLM to enhance your prompts:</p> <ul> <li>Can be used to manually or automatically enhance prompts   Automatic enhancement is done during normal generation without user intervention  </li> <li>Built-in presets for: Gemma-3, Qwen-2.5, Phi-4, Llama-3.2, SmolLM2, Dolphin-3 </li> <li>Support for custom system prompt  </li> <li>Support for custom models  </li> <li>Load any models hosted on huggingface </li> <li>Supports models in <code>huggingface</code> format</li> <li>Supports models in <code>gguf</code> format  </li> <li>Models are auto-downloaded on first use  </li> <li>Support quantization and offloading  </li> <li>Advanced options: <code>max tokens, sampling, temperature, repetition penalty</code></li> </ul> <p>Warning</p> <p>If SD.Next detected censored output, it will print warning in the log file and return original prompt</p> <p>Note</p> <p>Any model hosted on huggingface in original format should work as long as it implements standard <code>transformers.AutoModelForCausalLM</code> interface</p> <p>Note</p> <p>Not all model architecture are supported for <code>gguf</code> format Typically <code>gguf</code> support is added slightly later than <code>transformers</code> support</p> <p>Tip</p> <p>Debug logging can be enabled using <code>SD_LLM_DEBUG=true</code> env variable</p>"},{"location":"Prompt-Enhance/#custom-models","title":"Custom models","text":"<p>Can be used to define any model that is not included in predefined list</p>"},{"location":"Prompt-Enhance/#example-standard-huggingface-model","title":"Example: standard huggingface model","text":"<ul> <li>Model repo: <code>nidum/Nidum-Gemma-3-4B-it-Uncensored</code></li> </ul>"},{"location":"Prompt-Enhance/#example-gguf-model-hosted-on-huggingface","title":"Example: gguf model hosted on huggingface","text":"<ul> <li>Model repo: <code>meta-llama/Llama-3.2-1B-Instruct</code>   Link to original model repo on huggingface, required so that SD.Next can download components not present in <code>gguf</code> file such as tokenizer  </li> <li>Model GGUF: <code>mradermacher/Llama-3.2-1B-Instruct-Uncensored-i1-GGUF</code>   Link to repo on huggingface that is hosting the <code>gguf</code> file(s)  </li> <li>Model type: <code>llama</code>   Model type, required for SD.Next to know how to load the model  </li> <li>Model name: <code>Llama-3.2-1B-Instruct-Uncensored.i1-Q4_0.gguf</code>   Name of the <code>gguf</code> file inside gguf repo  </li> </ul> <p>Supported GGUF model types: llama, mistral, qwen2, qwen2moe, falcon, tokenizer, phi3, bloom, t5, stablelm, gpt2, starcoder2, mamba, nemotron, gemma2 Supported Transformer model types is a superset of GGUF model types and includes model types such as latest gemma3 In case of unsupported model type, SD.Next will print currently supported model types in the log file  </p>"},{"location":"Prompting/","title":"Prompting with different models","text":"<p>This is not in-depth technical article, but instead intended to demystify the process of prompting with different models - and importantly - why.</p> <p>Basically, effectiveness prompting depends on:</p> <ul> <li>Text encoder used in the model</li> <li>Dataset used to train the model</li> <li>Structure prompt and negative prompt</li> </ul> <p>Note</p> <p>Different models have different prompting best practices When in doubt, refer to model specific notes on CivitiAI website</p> <p>For example: standard SD15/SDXL models are prompted very differently vs Pony-derived models vs SD35 or Flux.1 derived models - it all comes down to which text enocder is used and how it was trained.  </p>"},{"location":"Prompting/#prompt-parser","title":"Prompt Parser","text":"<p>SDNext supports multiple prompt parsers:</p> <ul> <li>Native: Default  </li> <li>A1111: Included with compatibility with A1111 original implementation  </li> <li>Compel: Use compel-style attention syntax, e.g. <code>dog++</code> or <code>dog--</code> </li> <li>xHinker: Alternative engine compatible with <code>T5</code> text-encoder  </li> </ul>"},{"location":"Prompting/#native-prompt-parser","title":"Native Prompt Parser","text":""},{"location":"Prompting/#prompt-attention","title":"Prompt Attention","text":"<ul> <li><code>(x)</code>: emphasis. Multiplies the attention to x by 1.1, equivalent to <code>(x:1.1)</code></li> <li><code>[x]</code>: de-emphasis, divides the attention to x by 1.1, approximate to <code>(x:0.91)</code></li> <li><code>(x:number)</code>: Multiply the attention by number, either higher or lower than <code>1</code></li> </ul> <p>Note</p> <p>Multiple attentions are multiplied, not added: <code>((a dog:1.5) with a (bone:1.5)1.5)</code> is the same as (a dog:3.375) (with a bone:2.25)</p>"},{"location":"Prompting/#other-prompt-syntax","title":"Other Prompt Syntax","text":"<ul> <li><code>\\(x\\)</code>: Escapes the parentheses, this is how you'd use parenthesis without it causing the parser to add emphasis</li> </ul>"},{"location":"Prompting/#prompt-scheduling","title":"Prompt Scheduling","text":"<ul> <li><code>[x:x:number]</code>: Uses the first x until number steps have finished, then uses the second x  </li> </ul> <p>Note</p> <p>number can be <code>int</code> in which case it's considered exact step count, or <code>float</code> in which case it's considered percentage of total steps</p>"},{"location":"Prompting/#components","title":"Components","text":""},{"location":"Prompting/#text-encoder","title":"Text encoder","text":"<ul> <li>SD15 started with: CLIP-ViT/L</li> <li>SDXL adds second encoder: OpenCLIP-ViT/G</li> <li>SD3 adds third (optional) encoder: T5 Version 1.1</li> </ul> <p>And other models are following similar approach - replace early simple encoders with only basic understanding of English language with more advanced models. For example: PixArt-\u03a3 and Tencent HunyuanDiT</p> <p>Unfortunately, in StabilityAI models, text encoder results are concatenated one after each other so oldest <code>CLIP-ViT/L</code> still has biggest impact.</p>"},{"location":"Prompting/#dataset","title":"Dataset","text":"<p>Note</p> <p>Nearly all modern models are trained on subset of laion-5b dataset Later fine-tuning introduces additional data (e.g. Pony uses heavily tagged dataset)</p>"},{"location":"Prompting/#prompting-tips","title":"Prompting tips","text":""},{"location":"Prompting/#prompt-engineering","title":"Prompt Engineering","text":"<p>Know your model: different models were trained on different datasets, some may understand terms other models don't  </p> <p>Main groups</p> <ul> <li>Mediums: best starting a prompt with it after specifying artist   Examples: painting, photograph, drawing, sketch</li> <li>Flavors: best left as separate token at the end of the prompt   Examples: ray tracing, fine art, black and white, pixiv, artstation</li> <li>Movements: best added to prompt with as keyword   Examples: pop art, photorealism </li> <li>Artists: best starting a prompt with it   Examples: greg rutkowski, artgerm, dc comics, picasso </li> </ul> <p>Modifiers</p> <ul> <li>Feel: best near the end   Examples: beautiful, sharp focus, 4k, hdr, high detailed, canon 5d</li> <li>Composition: best at front, but only use if results don't fit   Examples: 1man, 1woman</li> </ul> <p>Negative Prompt</p> <ul> <li>Any keyword can be specified in a negative prompt as well   Examples: watermark</li> </ul> <p>Advanced Prompt Modifiers </p> <p>Hints</p> <ul> <li>Use either artists or movements   Using both may result in one overpowering the other, or in unexpected outcome  </li> <li>Select medium that fits artist   It helps model a lot to know which medium to use when styling  </li> <li>Add action after subject   Examples: portrait, standing, sitting</li> <li>Moving things to the front of prompt may increase its emphasis   Example: cartoon drawing of a woman as pixar vs pixar drawing of a woman</li> <li>Use both subject and scene keywords   Example: woman on a beach</li> </ul> <p>Example</p> <p>(composition) (artist) (medium) (subject) (action) (scene) (movement) (flavor) (feel) 1woman greg rutkowski painting of a woman happy front portrait on a beach as photorealism, sharp focus, artstation</p>"},{"location":"Prompting/#negative-prompt","title":"Negative prompt","text":"<p>When you add negative prompt, what happens is that its basically appended to prompt just using negative weights. This makes model \"steer away\" from terms in negative prompt, but to do so it first has to INTRODUCE them to the context.</p> <p>So by adding negative prompt, you're:</p> <ul> <li>Limiting the freedom of the model   Quite commonly this means that all faces will look similar without variance, etc.</li> <li>Making the model more prone to hallucinations   If you're trying to steer away from something that doesn't exist in the first place, it might introduce opposite of what you're trying to do.</li> </ul> <p>All-in-all, negative prompts are useful to steer model away from certain concepts, but should not be used as a \"general purpose long negative prompt\"</p>"},{"location":"Prompting/#prompt-attention_1","title":"Prompt attention","text":"<p>First, nothing wrong with wanting to add extra attention to certain parts of the prompt. But it should be used sparingly and only when needed and keep in mind that overall prompt should be balanced.</p> <p>E.g, If your prompt has 10 words and you're raising attention to 5 of them, you're basically telling the model average weight of the prompt is massive. Common result? Overbaked images.</p> <p>Prompt balance doesn't have to be perfect, but any prompt that has more than few of words with light attention modifiers is a red flag.  </p> <p>Also, keep in mind that adding extremely strong attention modifiers such as <code>(((xyz)))</code> or <code>(xyz:1.5)</code> will make model completely loose concept of prompt as a whole.</p>"},{"location":"Prompting/#model-specific-tips","title":"Model Specific Tips","text":""},{"location":"Prompting/#sd15","title":"SD15","text":"<p>Dataset:</p> <ul> <li>Used small laion-5b subset with preexisting captions</li> <li>No major effort was put into processing or captioning other than what's in the subset</li> </ul> <p>Result?</p> <ul> <li>Model that's has basic understanding of the world, but to use it effectively you need to \"hunt\" for the right keywords  </li> <li>Models are easily fine-tuned as nothing-forced-nothing-removed approach was used during training  </li> <li>How to prompt? Old-school prompting which put heavy emphasis on keywords and attention modifiers  </li> </ul>"},{"location":"Prompting/#sd21","title":"SD21","text":"<p>Dataset:</p> <ul> <li>Trained on same dataset as SD15 and then fine-tuned on extended dataset with larger resolution  </li> <li>It was also censored in the final parts of training which introduced heavy bias  </li> </ul> <p>Result?</p> <ul> <li>Its almost like model was \"lobotomized\".   E.g. For concept of \"topless\", its not like it just doesn't have sample data for it, it was \"burned out\" of the model.   So to add concepts that model did not understand it takes massive effort, almost close to retraining. Fail.</li> </ul>"},{"location":"Prompting/#sdxl","title":"SDXL","text":"<p>Dataset:</p> <ul> <li>Used larger subset with extended captions and also diverse resolutions  </li> <li>Instead of censoring in the final stages, they simply pruned dataset used for training.</li> </ul> <p>Result?</p> <ul> <li>Model that knows-what-it-knows and the rest can be added with fine-tuning.</li> <li>E.g. It knows what \"topless\" means, just doesn't have enough examples to develop it fully.  </li> <li> <p>How to prompt? Extended captions and second text encoder mean that model can be prompted in a more natural way and extended use of keywords and attention-modifiers should be avoided.</p> </li> <li> <p>Extra note: PonyXL was extensively trained on heavily tagged dataset without natural language captions and as such it needs to be prompted differently - using tags and keywords instead of natural language.</p> </li> </ul>"},{"location":"Prompting/#sd3","title":"SD3","text":"<ul> <li>Used even larger subset with even more diverse resolutions, but dataset itself was processed differently:</li> <li>Censored not only by removing images from dataset, but also by modifying them by censoring parts of the image</li> <li>Captioned extensively using LLM. Unfortunately, not ON-TOP of existing captions so it can augment them, but instead it REPLACED them - thus keywords already existing in dataset are not trained for at all.</li> </ul> <p>Result?</p> <ul> <li>Model that thinks-it-knows-everything and now it's up to you to prove it wrong.   E.g., it knows what a topless person looks like, and its \"certain\" that nipples should be presented as blank.   Which means it would like take a massive effort to retrain what it learned in the wrong way.   I hope to be proven wrong, but this looks like a fail.</li> <li>How to prompt? Use of long LLM-generated captions means that model should be prompted using very descriptive language and completely stop using using styles, keywords and attention-modifiers. And since LLM generated captions do not include styles as we know them, we need to replace them with detailed descriptions - it's almost like we need to think like LLM to prompt it - how would LLM describe the image I'm trying to create?</li> </ul>"},{"location":"Quantization/","title":"Quantization","text":"<p>Quantization is a process of:</p> <ul> <li>Storage-optimization   reducing the memory footprint of the model by reducing the precision of parameters in a model  </li> <li>Compute-optimization   speed up the inference process by providing optimized kernels for native execution in quantized precision  </li> </ul> <p>For storage-only quantization, the model is quantized to lower precision but the operations are still performed in the original precision which means that each operation needs to be upcasted to the original precision before execution resulting in a performance overhead  </p> <p>Important</p> <p>Quantization considerations</p> <p>Before deciding which quantization method to use, you need to consider the following:</p> <ul> <li>Compatibility with your platform   Some quantization methods are not available on all platforms, see below for details  </li> <li>Performance benefits   Some quantization methods may provide significant performance benefits on certain platforms  </li> <li>Quality trade-offs   Some quantization methods may result in a loss of quality  </li> </ul>"},{"location":"Quantization/#using-quantized-models","title":"Using Quantized Models","text":"<p>Quantization can be done in multiple ways:  </p> <ul> <li>On-the-fly by quantizing on-the-fly during model load   Available by selecting settings -&gt; quantization for some quantization types   Sometimes referred to as <code>pre</code> mode This is recommended for most users!</li> <li>By quantizing immediately after model load   Available by selecting settings -&gt; quantization for all quantization types   Sometimes referred to as <code>post</code> mode  </li> <li>By simply loading a pre-quantized model   Quantization type will be auto-determined at the start of the load  </li> <li>During model training itself   Out-of-scope for this document  </li> </ul>"},{"location":"Quantization/#on-the-fly-quantization","title":"On-the-Fly Quantization","text":"<p>On-the-fly quantization is available for <code>SDNQ</code>, <code>BitsAndBytes</code>, <code>Optimum.Quanto</code>, <code>TorchAO</code> and <code>Layerwise</code> quantization methods and can be configured in Settings -&gt; Quantization Settings </p> <p>You can specify quantization for each model component:</p> <ul> <li>Model   Applies to the UNet / Transformer part of the Diffusion Models  </li> <li>TE   Applies to text-encoders  </li> <li>LLM   Applies to VLM models during captioning and interrogate and prompt enhance features  </li> <li>Control   Applies to ControlNets    </li> <li>VAE   Applies to VAE, quantization of VAE module is not recommended  </li> </ul> <p>You can mix-and-match quantization types for each model component For example, you can use <code>BitsAndBytes</code> for the transformer and <code>Optimum.Quanto</code> for the llm</p>"},{"location":"Quantization/#quantization-engines","title":"Quantization Engines","text":"<p>SD.Next supports multiple quantization engines, each with multiple quantization schemes:</p> <ul> <li><code>SDNQ</code> 15 int-based and 4 float8-based quantization schemes  </li> <li><code>BitsAndBytes</code> 3 float-based quantization schemes  </li> <li><code>Optimium.Quanto</code> 3 int-based and 2 float-based quantization schemes  </li> <li><code>TorchAO</code>: 4 int-based and 3 float-based quantization schemes  </li> <li><code>Layerwise</code> 2 float8-based quantization schemes  </li> <li><code>GGUF</code> with pre-quantized weights  </li> </ul> <p>Important</p> <p>Not all quantization engines are available on all platforms, see notes below for details! Using any quantization engine for the first time may result in failure as required libraries are downloaded and installed Restart SD.Next and try again if you encounter any issues</p> <p>Tip</p> <p>If you're on Windows with a compatible GPU, you may try WSL2 for broader feature compatibiliy See WSL Wiki for more details</p>"},{"location":"Quantization/#sdnq","title":"SDNQ","text":"<p>SD.Next Quantization provides full cross-platform quantization for any device and SDNQ is the most versatile choice  </p> <p>Note</p> <p>Advantage of SDNQ is that it does work on any platform with good performance!</p> <ul> <li>broad platform and GPU support  </li> <li>balanced offload and Lora support  </li> <li>quantized matmul support  </li> <li>on-the-fly fast quantization support  </li> <li>enable in Settings -&gt; Quantization Settings -&gt; SDNQ </li> <li>see SDNQ Wiki for more details  </li> </ul>"},{"location":"Quantization/#bitsandbytes","title":"BitsAndBytes","text":"<p>Typical models pre-quantized with <code>bitsandbytes</code> would have look like <code>*nf4.safetensors</code> or <code>*fp8.safetensors</code></p> <p>Note</p> <p>BnB is allows for usage of balanced offload as well as fast quantization on-the-fly during load but it is not available on all platforms.</p> <p>Limitations:</p> <ul> <li>default <code>bitsandbytes</code> package only supports nVidia GPUs   some quantization types require newer GPU with supported CUDA ops: e.g. nVidia Turing GPUs or newer  </li> <li><code>bitsandbytes</code> relies on <code>triton</code> packages which are not available on windows unless manually compiled/installed   without them, performance is significantly reduced  </li> <li>for nVidia: automatically installed as needed  </li> <li>for AMD/ROCm: link </li> <li>for Intel/IPEX: link </li> </ul>"},{"location":"Quantization/#optimum-quanto","title":"Optimum-Quanto","text":"<p>Typical models pre-quantized with <code>optimum.quanto</code> would have look like <code>*qint.safetensors</code>.</p> <p>Note</p> <p>OQ is efficient with its qint8/qint4 quantization types, but it cannot be used with broad offloading methods</p> <p>Limitations:</p> <ul> <li>requires <code>torch&gt;=2.4.0</code>   if you're running older torch, you can try upgrading it or running sdnext with <code>--reinstall</code> flag  </li> <li>not compatible with balanced offload  </li> </ul>"},{"location":"Quantization/#torchao","title":"TorchAO","text":"<p>TorchAO is available for quantization on-the-fly during model load as well as post-load quantization Limitations: - Requires <code>torch&gt;=2.5.0</code> - int4 based quantization cannot be used with any offload method  </p>"},{"location":"Quantization/#gguf","title":"GGUF","text":"<p>GGUF is a binary file format used to package pre-quantized models.  </p> <p>GGUF is originally desiged by <code>llama.cpp</code> project and intended to be used with its GGML execution runtime. However, without GGML, GGUF provides storage-only quantization which means that every operation needs to be upcast to current device precision before execution (typically FP16 or BF16) which comes with a significant performance overhead.</p> <p>Warning</p> <p>Right now, all popular T2I inference UIs (SD.Next, Forge, ComfyUI, InvokeAI etc.) are using GGUF as storage-only and as such usage of GGUF is not recommended!</p> <ul> <li><code>gguf</code> supports wide range of quantization types and is not platform or GPU dependent  </li> <li><code>gguf</code> does not provide native GPU kernels which means that <code>gguf</code> is purely a storage optimization </li> <li><code>gguf</code> reduces model size and memory usage, but it does slow down model inference since all quantized weights are de-quantized on-the-fly  </li> </ul> <p>Limitations:</p> <ul> <li><code>gguf</code> is not compatible with model offloading as it would trigger de-quantization  </li> <li>note: only supported component in <code>gguf</code> binary format is UNET/Transformer   you cannot load all-in-one single-file GGUF model</li> </ul>"},{"location":"Quantization/#benchmarks","title":"Benchmarks","text":"<p>Comparing performance of different quantization methods on the FLUX.1-Dev model This is not a comprehensive benchmark, but rather a quick overview of the performance of different quantization methods  </p>"},{"location":"Quantization/#environment","title":"Environment","text":"<p>model==FLUX.1-Dev gpu==nVidia RTX4090 torch==2.7.1 dtype==auto attention==sdpa offload==balanced sampler==FlowMatchEulerDiscreteScheduler resolution=1024x1024 steps==20 batch==1  </p> Engine DType It/s Note Torch Float16 0.68 Torch BFloat16 0.68 Default BnB NF4 1.48 BnB FP8 1.42 SDNQ INT8 1.33 SDNQ INT8 1.44 Dequantize using torch.compile SDNQ INT8 2.08 Dequantize &amp; MatMul SDNQ FP8 E4M 1.33 SDNQ FP8 E4M 1.44 Dequantize using torch.compile SDNQ FP8 E4M 1.88 Dequantize &amp; MatMul SDNQ INT7 1.10 SDNQ INT7 1.37 Dequantize using torch.compile SDNQ INT7 1.96 Dequantize &amp; MatMul SDNQ INT6 1.16 SDNQ INT6 1.42 Dequantize using torch.compile SDNQ INT6 2.04 Dequantize &amp; MatMul SDNQ INT5 1.14 SDNQ INT5 1.42 Dequantize using torch.compile SDNQ INT5 2.04 Dequantize &amp; MatMul SDNQ UINT4 1.28 SDNQ UINT4 1.48 Dequantize using torch.compile SDNQ UINT3 1.20 SDNQ UINT3 1.42 Dequantize using torch.compile Quanto INT8 1.23 offload=model TorchAO INT8 0.65 Weights-only TorchAO INT8 1.20 Weights-only &amp; offload=model Layerwise FP8 E4M 1.37 SVDQuant INT4 4.75 Nunchaku pre-compiled SVDQuant INT4 5.65 Nunchaku pre-compiled + attention"},{"location":"Quantization/#recommendations","title":"Recommendations","text":"<p><code>SDNQ</code> is the best fully cross-platform option and brings good memory savings with optional compute optimizations using Triton  </p>"},{"location":"Quantization/#other-options","title":"Other Options","text":"<p>For CUDA environments <code>BitsAndBytes</code> is another versatile option and <code>nf4</code> is an efficient quantization type from both memory and compute perspective  </p> <p>For ROCm, ZLUDA or IPEX environments <code>Optimum.Quanto</code> is another option if you can fit the model into VRAM either using Model offload or without using any offload as its not compatible with the default Balanced offload </p>"},{"location":"Quantization/#errors","title":"Errors","text":"<p>Caution</p> <p>Using incompatible configurations will result in errors during model load:</p> <ul> <li>BitsAndBytes nf4 quantization is not compatible with sequential offload   <p>Error: <code>Blockwise quantization only supports 16/32-bit floats</code> </p> </li> <li>Quanto qint quantization is not compatible with balanced offload   <p>Error: <code>QBytesTensor.__new__() missing 5 required positional arguments</code> </p> </li> <li>Quanto qint quantization is not compatible with sequential offload   <p>Error: <code>Expected all tensors to be on the same device</code> </p> </li> </ul>"},{"location":"Quantization/#triton","title":"Triton","text":"<p>Many quantization schemes rely on Triton compiler for Torch which is not available on all platforms If your installation fails, you can try building <code>triton</code> from sources or find pre-build binary wheels</p>"},{"location":"Quantization/#triton-for-windows","title":"Triton for Windows","text":"<p>A Triton fork is available for Windows and can be installed by running the following PowerShell script from your SD.Next installation folder:</p> <p>install-triton.ps1</p> <pre><code>$ErrorActionPreference = \"Stop\"\n\n$VENV_DIR = if ($env:VENV_DIR) { $env:VENV_DIR } else { Resolve-Path \"venv\" }\n$PYTHON = \"$VENV_DIR\\Scripts\\python\"\n$PIP = \"$VENV_DIR\\Scripts\\pip\"\n\n$sys_ver = &amp; $PYTHON -VV\n$sys_ver_major, $sys_ver_minor = $sys_ver.Split(\" \")[1].Split(\".\")[0, 1]\n$filename = \"triton-3.2.0-cp$sys_ver_major$sys_ver_minor-cp$sys_ver_major$sys_ver_minor-win_amd64.whl\"\n$url = \"https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/$filename\"\n\nInvoke-WebRequest $url -OutFile $filename\n&amp; $PIP install $filename\nRemove-Item $filename\n</code></pre>"},{"location":"Reprocess/","title":"Reprocess","text":"<p>Reprocess is a new top-level option that allows to repeat the processing of a given image using different parameters.</p>"},{"location":"Reprocess/#use-cases","title":"Use cases","text":"<p>Below are several examples where reprocess can be useful:</p> <ul> <li>You can generate an image at lower quality   such as using lower step count or using fast vae instead of full   and if you like the image, bump up step count and switch vae and reprocess it  </li> <li>Or process as usual and if you like the results   enable detailer, hires or refine workflows and reprocess  </li> <li>Or if you don't like what hires did because you used too high of denoise strength   you can go back and reprocess original image before hires, but now with different strength  </li> </ul>"},{"location":"Reprocess/#how-does-it-work","title":"How does it work","text":"<p>Reprocess works on latent level, meaning that any workflow that runs in latent space will add its latents to reprocess history For example, each of the following operations will add to reprocess history:  </p> <ul> <li>Generate</li> <li>HiRes</li> <li>Detailer</li> <li>Refine</li> </ul> <p>Tip</p> <p>Reprocess by default will take last known latent However, you can see entire history of latents in Networks -&gt; History and pick one that you want to reprocess Length of maintained history is configuratble in Settings -&gt; Networks</p>"},{"location":"SD-Pipeline-How-it-Works/","title":"Stable Diffusion Pipeline","text":"<p>This is probably the best end-to-end semi-technical article: https://stable-diffusion-art.com/how-stable-diffusion-work/</p> <p>And a detailed look at diffusion process: https://towardsdatascience.com/understanding-diffusion-probabilistic-models-dpms-1940329d6048</p> <p>But this is a short look at the pipeline:</p> <ol> <li>Encoder / Conditioning    Text (via tokenizer) or image (via vision model) to semantic map    (e.g CLiP text encoder)  </li> <li>Sampler    Generate noise which is starting point to map to content    (e.g. k_lms)  </li> <li>Diffuser    Create vector content based on resolved noise + semantic map    (e.g. actual stable diffusion checkpoint)  </li> <li>Autoencoder    Maps between latent and pixel space (actually creates images from vectors)    (e.g. typically some image-database trained GAN)  </li> <li>Denoising    Get meaningful images from pixel signatures    Basically, blends what autoencoder inserted using information from diffuser    (e.g. U-NET)</li> <li>Loop and repeat    From step#3 with cross-attention to blend results  </li> <li>Run additional models as needed  </li> <li>Upscale (e.g. ESRGAN)  </li> <li>Resore Face (e.g. GFPGAN or CodeFormer)  </li> </ol>"},{"location":"SD-Training-Methods/","title":"StableDiffusion Training Methods","text":""},{"location":"SD-Training-Methods/#fine-tuning","title":"fine-tuning","text":"<ul> <li>retrains parts of the hypernetwork with new data thus modifying original weights   requires large and precisely labelled dataset</li> <li>size is same as original model size, ~2-7gb</li> <li>verdict: prohibitive due to large dataset and effort required</li> </ul>"},{"location":"SD-Training-Methods/#model-merge","title":"model merge","text":"<ul> <li>combines weights from multiple models according to specified rules</li> <li>verdict: highly desired to create pre-set models for specific use-case</li> </ul>"},{"location":"SD-Training-Methods/#textual-inversion","title":"textual inversion","text":"<ul> <li>assign vector to a new concept with originally one vector per embedding, hacks to enable multi-vector embeddings   works by expanding vocabulary of a model, but majority of learned content is actually assembled from existing concepts   can be considered as a formula on which already learned weights should be combined to achieve learned concept  </li> <li>size 768/1024b per vector</li> <li>verdict: best currently viable short-term training solution</li> </ul>"},{"location":"SD-Training-Methods/#aesthetic-gradient","title":"aesthetic gradient","text":"<ul> <li>uses low-precision trained embeddings to steer clip using classifier guidance   training is very cheap, but classifier guidance sloes down image generation   result is basic transfer of style from learned image to generated image  </li> <li>size is same as embedding</li> <li>origin: independent work</li> <li>verdict: inconsistent results with minimal value</li> </ul>"},{"location":"SD-Training-Methods/#custom-diffusion","title":"custom diffusion","text":"<ul> <li>fine-tuning specific model matrices with textual inversion   similar speed and memory requirements to embedding training and supposedly gives better results in less steps</li> <li>size ~50mb</li> <li>origin: cmu</li> <li>verdict: possibly promising, requires further investigation, surprisingly low chatter on this topic</li> </ul>"},{"location":"SD-Training-Methods/#hypernetwork","title":"hypernetwork","text":"<ul> <li>similar to model fine-tuning, but adds small a small neural network that on-the-fly modifies weights of the last two layers of the main model   works like adaptive head that steers model in a learned direction so primary use-case is style transfer, not concept transfer</li> <li>size is limited to learned layers, ~100-200mb</li> <li>origin: leaked from novel.ai</li> <li>verdict: lower priority as concept transfer is more important than style transfer</li> </ul>"},{"location":"SD-Training-Methods/#null-text-inversion","title":"null-text inversion","text":"<ul> <li>similar concept to textual inversion, but trains unconditional embedding that is used for classifier free guidance instead of text embedding   resulting embedding is apparently more detailed than standard textual embedding</li> <li>size is larger but comparable to textual inversion</li> <li>origin: google</li> <li>verdict: possibly promising, requires further investigation, but no working prototype as of yet</li> </ul>"},{"location":"SD-Training-Methods/#clip-inversion","title":"clip inversion","text":"<ul> <li>similar concept to textual inversion, but uses clip embedding instead of text embedding  </li> <li>size is same as textual inversion</li> <li>origin: google</li> <li>verdict: prohibitive due to requirement of specially fine-tuned model as a starting point</li> </ul>"},{"location":"SD-Training-Methods/#dream-artist","title":"dream artist","text":"<ul> <li>variation on ti training where both positive and negative embeddings are created</li> <li>size is same as textual inversion</li> <li>origin: independent work</li> <li>verdict: skip for now as solution does not appear to be sufficiently maintained</li> </ul>"},{"location":"SD-Training-Methods/#dreambooth","title":"dreambooth","text":"<ul> <li>similar to model fine-tuning except it adds information on top of model instead of forgetting/overwriting existing concepts  </li> <li>size is equal to original model size, ~2-7gb</li> <li>origin: google, but heavily modified by independent work</li> <li>verdict: prohibitive due to resulting size and requirement to load full model on-demand</li> </ul>"},{"location":"SD-Training-Methods/#lora","title":"lora","text":"<ul> <li>\"low-rank adaptation of large language models\"   injects trainable layers to steer cross attention layers   very flexible, but memory intensive so limited training opportunities on normal gpu   multiple incompatible implementations: should choose which implementation to use  </li> <li>size varies from ~5mb to full-model size, average ~150-300mb</li> <li>origin: microsoft</li> </ul>"},{"location":"SD-XL/","title":"StableDiffusion-XL","text":""},{"location":"SD-XL/#downloading-sd-xl","title":"Downloading SD-XL","text":"<p>You can simply download these two files from Huggingface and place them into your normal checkpoint directory, though we recommend a subfolder.</p> <ul> <li>SD-XL Base</li> <li>SD-XL Refiner</li> </ul>"},{"location":"SD-XL/#setup-for-sd-xl","title":"Setup for SD-XL","text":"<p>To facilitate easy use of SD-XL and swapping between refiners, backends, and pipelines, we recommend selecting the following items in your Settings Tab, on the User Interface page:</p> <p></p> <p>Once you select them, hit Apply settings, and then Restart server. When the server returns to being active and your browser page reloads, the Quicksettings at the top of your screen should look like this (assuming you were using SDXL):</p> <p></p>"},{"location":"SD-XL/#vram-optimization","title":"VRAM Optimization","text":"<p>There are now 3 methods of memory optimization with the Diffusers backend, and consequently SDXL: Model Shuffle, Medvram, and Lowvram. Choose one based on your GPU, VRAM, and how large you want your batches to be.</p> <p>Note: <code>VAE Tiling</code> can be enabled to save additional VRAM if necessary, but it is recommended to use <code>VAE Slicing</code> if you do not have abundant VRAM. <code>Enable attention slicing</code> should generally not be used, as the performance impact is significant.</p>"},{"location":"SD-XL/#option-1-model-shuffle","title":"Option 1: Model Shuffle","text":"<p>\"Model Shuffle\" is a memory optimization feature that dynamically moves different parts of the model between the GPU and CPU to efficiently utilize VRAM. This is enabled when the following 3 options are Enabled in the Diffusers settings page:</p> <ul> <li>Move the base model to CPU when using the refiner.</li> <li>Move the refiner model to CPU when not in use.</li> <li>Move the UNet to CPU during VAE decoding.</li> </ul> <p>To use <code>Model Shuffling</code> do not have <code>--medvram</code> or <code>--lowvram</code> active, then use the following settings:</p> <p></p> <p>The important parts are the 3 Move checkboxes.</p> <p>Note that if you activate either <code>CPU model offload</code> or <code>Sequential CPU offload</code>, they will deactivate and ignore Model Shuffling. VRAM Usage: \"Model Shuffle\" will work in 8 GB of VRAM.</p>"},{"location":"SD-XL/#option-2-medvram","title":"Option 2: MEDVRAM","text":"<p>If you have a GPU with 6GB VRAM or require larger batches of SD-XL images without VRAM constraints, you can use the <code>--medvram</code> command line argument. This option significantly reduces VRAM requirements at the expense of inference speed. Cannot be used with <code>--lowvram/Sequential CPU offloading</code> Note: Until some upstream fixes go in, this will not work with DML or MAC.</p> <p>Alternatively, you can enable the <code>Enable model CPU offload</code> checkbox in the <code>Settings</code> tab on the <code>Diffusers settings</code> page:</p> <ul> <li>Model CPU Offload (same as <code>--medvram</code>)</li> <li>VAE slicing (recommended)</li> <li>Attention slicing is NOT recommended.</li> </ul> <p></p> <p>VRAM Usage: \"Model CPU Offload\" can work in 6 GB of VRAM.</p> <p>Note: <code>--medvram</code> supersedes the <code>Model Shuffle</code> option (e.g., Move base model, refiner model, UNet), and is mutually exclusive and cannot be used together with <code>--lowvram/Sequential CPU offload</code></p>"},{"location":"SD-XL/#option-3-lowvram","title":"Option 3: LOWVRAM","text":"<p>If your GPU has as low as 2GB of VRAM, start your SD.Next session with <code>--lowvram</code> as a command line argument to vastly reduce VRAM requirements at the cost of even more inference speed. This is essentially the <code>Enable Sequential CPU offload</code> setting.</p> <p></p> <p>Note: VAE slicing, VAE tiling, and Attention slicing are all enabled by <code>--lowvram</code> regardless of the checkboxes.</p> <p>Using this setting with a GPU that has higher VRAM, your generations will take even longer, but you will be able to do ridiculously large batches of SD-XL images, up to and including 24 on a 12GB GPU.</p> <p>Note: Until some upstream fixes go in, this will not work with SDXL LoRA's and SD 1.5.</p> <p>We look forward to seeing how large your batches can get, do let us know on the Discord server, and we HIGHLY RECOMMEND that you continue down this guide and configure your SD.Next with the Fixed FP16 VAE!</p>"},{"location":"SD-XL/#fixed-fp16-vae","title":"Fixed FP16 VAE","text":"<p>It is currently recommended to use a Fixed FP16 VAE rather than the ones built into the SD-XL base and refiner for significant reductions in VRAM (from 6GB of VRAM to &lt;1GB VRAM) and a doubling of VAE processing speed.</p> <p>Below are the instructions for installation and use:</p> <ul> <li>Download Fixed FP16 VAE to your VAE folder.</li> <li>In your <code>Settings</code> tab, go to <code>Diffusers settings</code> and set <code>VAE Upcasting</code> to <code>False</code> and hit Apply.  </li> <li>Select the your VAE and simply <code>Reload Checkpoint</code> to reload the model or  hit <code>Restart server</code>.</li> </ul> <p>You should be good to go, Enjoy the huge performance boost!</p>"},{"location":"SD-XL/#using-sd-xl","title":"Using SD-XL","text":"<ul> <li>Then select Autodetect or Stable Diffusion XL from the Pipeline dropdown.</li> <li>Next select the sd_xl_base_1.0.safetensors file from the Checkpoint dropdown.</li> <li>(optional) Finally select the sd_xl_refiner_1.0.safetensors file from the Refiner dropdown.  </li> </ul>"},{"location":"SD-XL/#using-sd-xl-refiner","title":"Using SD-XL Refiner","text":"<p>To use refiner, it first needs to be loaded and then it can be enabled using <code>Second pass</code> option in the UI. Note that use of refiner is not necessary as base model can produce very good results on its own.</p> <p>Refiner can be used in two-modes: as in traditional workflow or with early handover from base to refiner. In either case, refiner will use calculated number of steps based on <code>Refiner steps</code>.</p> <p>If <code>denoise start</code> is set to <code>0</code> or <code>1</code>, then traditional workflow is used:</p> <ul> <li>Base model runs from <code>0</code> -&gt; <code>100%</code> using Sampling steps.</li> <li>Refiner model runs from <code>0</code> -&gt; <code>100%</code> using Refiner steps.</li> </ul> <p>However, in this mode, refiner may not produce much better result and will likely only smoothen the image as base model already reached 100% and there is insufficient remaining noise for refiner to do anything else.</p> <p>If <code>refiner start</code> is set to any other value, then handover mode is used:</p> <ul> <li>Base model runs from <code>0%</code> -&gt; <code>denoise_start%</code>   Exact number is calculated internally to be Sampling steps.</li> <li>Refiner model runs from <code>denoise_start%</code> -&gt; <code>100%</code>   Exact number is calculated internally to be Refiner steps.</li> </ul> <p>In this mode, using different ratio of steps for primary and refiner is allowed, but may result in unexpected results as base and refiner operations will not be perfectly aligned.</p> <p>Note on steps vs timesteps: steps do not refer directly do operations internally executed. Steps are used to calculate actual values at which operations will be executed. For example, steps=6 roughly means execute denoising at 0% -&gt; 20%  -&gt; 40%  -&gt; 60%  -&gt; 80%  -&gt; 100%. For that reason, specifying steps above 99 is meaningless.</p>"},{"location":"SD3/","title":"Stable Diffusion 3.x","text":"<p>StabilityAI's Stable Diffusion 3 family consists of:</p> <ul> <li>Stable Diffusion 3.0 Medium</li> <li>Stable Diffusion 3.5 Medium</li> <li>Stable Diffusion 3.5 Large</li> <li>Stable Diffusion 3.5 Large Turbo</li> </ul> <p></p> <p>Important</p> <p>Allow gated access This is a gated model, you need to accept the terms and conditions to use it For more information see Gated Access Wiki</p> <p>Important</p> <p>Set offloading Set appropriate offloading setting before loading the model to avoid out-of-memory errors For more information see Offloading Wiki</p> <p>Important</p> <p>Choose quantization Check compatibility of different quantizations with your platform and GPU! For more information see Quantization Wiki</p> <p>Warning</p> <p>Regardless of offloading settings, the entire model must be loaded into RAM before it can be used Look at the total model size in the table and make sure you have enough RAM to load the model</p> <p>Use reference models</p> <p>Use of reference models is recommended over manually downloaded models! Simply select it from Networks -&gt; Models -&gt; Reference</p> <p>and model will be auto-downloaded on first use  </p>"},{"location":"SD3/#components","title":"Components","text":"<p>SD3.x model consists of:</p> <ul> <li>Unet/Transformer: MMDiT  </li> <li>Text encoder 1: CLIP-ViT/L,</li> <li>Text encoder 2: OpenCLIP-ViT/G,</li> <li>Text encoder 3: T5-XXL Version 1.1 </li> <li>VAE</li> </ul> <p>When using reference models, all components will be loaded as needed. If using manually downloaded model, you need to ensure that all components are correctly configured and available. Note that majority of available downloads are not actually all-in-one models and are instead just a part of the full model with individual components.</p> <p>Important</p> <p>Do not attempt to assemble a full model by loading all individual components That may be how some other apps are designed to work, but its not how SD.Next works Always load full model and then replace individual components as needed</p> <p>Warning</p> <p>If you're getting error message during model load: <code>file=xxx is not a complete model</code> It means exactly that - you're trying to load a model component instead of full model</p> <p>Tip</p> <p>For convience, you can add setting that allow quick replacements of model components to your quicksettings by adding Settings -&gt; User Interface -&gt; Quicksettings  list -&gt; sd_unet, sd_vae, sd_text_encoder</p> <p></p>"},{"location":"SD3/#fine-tunes","title":"Fine-tunes","text":""},{"location":"SD3/#diffusers","title":"Diffusers","text":"<p>N/A: Currently there are no known diffusers fine-tunes of SD3.0 or SD3.5 models</p>"},{"location":"SD3/#loras","title":"LoRAs","text":"<p>SD.Next includes support for SD3 LoRAs  </p> <p>Since LoRA keys vary significantly between tools used to train LoRA as well as LoRA types, support for additional LoRAs will be added as needed - please report any non-functional LoRAs!</p> <p>Also note that compatibility of LoRA depends on the quantization type! If you have issues loading LoRA, try switching your SD3 base model to different quantization type  </p>"},{"location":"SD3/#all-in-one","title":"All-in-one","text":"<p>Since text encoders and VAE are same between all SD3 models, using all-in-one safetensors is not recommended due to large duplication of data  </p>"},{"location":"SD3/#unettransformer","title":"Unet/Transformer","text":"<p>Unet/Transformer component is a typical model fine-tune and is around 11GB in size  </p> <p>To load a Unet/Transformer safetensors file:  </p> <ol> <li>Download <code>safetensors</code> or <code>gguf</code> file from desired source and place it in <code>models/UNET</code> folder  </li> <li>Load model as usual and then  </li> <li>Replace transformer with one in desired safetensors file using: Settings -&gt; Execution &amp; Models -&gt; UNet </li> </ol>"},{"location":"SD3/#text-encoder","title":"Text Encoder","text":"<p>SD.Next allows changing optional text encoder on-the-fly  </p> <p>Go to Settings -&gt; Models -&gt; Text encoder and select the desired text encoder T5 enhances text rendering and some details, but its otherwise very lightly used and optional Loading lighter T5 will greatly decrease model resource usage, but may not be compatible with all offloading modes  </p>"},{"location":"SD3/#vae","title":"VAE","text":"<p>SD.Next allows changing VAE model used by SD3 on-the-fly There are no alternative VAE models released, so this setting is mostly for future use  </p> <p>Tip</p> <p>To enable image previews during generate, set Settings -&gt; Live Preview -&gt; Method to TAESD**</p> <p>To further speed up generation, you can disable \"full quality\" which triggers use of TAESD instead of full VAE to decode final image  </p>"},{"location":"SD3/#scheduler","title":"Scheduler","text":"<p>Due to specifics of flow-matching methods, number of steps also has strong influence on the image composition, not just on the way how its resolved</p>"},{"location":"SDNQ-Quantization/","title":"SDNQ Quantization","text":"<p>SD.Next Quantization provides full cross-platform quantization to reduce memory usage and increase performance for any device.  </p>"},{"location":"SDNQ-Quantization/#usage","title":"Usage","text":"<ol> <li>Go into <code>Settings -&gt; Quantization Settings</code> </li> <li>Enable the desired Quantization options under the <code>SDNQ</code> menu Model, TE and LLM are the main targets for most use cases  </li> <li>If model is already loaded, reload the model    Once quantization options are set, they will be applied to any model loaded after that  </li> </ol>"},{"location":"SDNQ-Quantization/#features","title":"Features","text":"<ul> <li>SDNQ is fully cross-platform, supports all GPUs and CPUs and includes many quantization methods:  </li> <li>8-bit, 7-bit, 6-bit, 5-bit, 4-bit, 3-bit, 2-bit and 1-bit int and uint </li> <li>8-bit e5, e4 and fnuz float </li> <li>note: <code>int8</code> is very close to the original 16 bit quality  </li> <li>Supports nearly all model types  </li> <li>Supports compute optimizations using Triton via <code>torch.compile</code> </li> <li>Supports Quantized MatMul with significant speedups on INT8 or FP8 supported GPUs  </li> <li>Supports on the fly quantization during model load with little to no overhead (called as <code>pre</code> mode)  </li> <li>Supports quantization for the convolutional layers with UNet models  </li> <li>Supports post load quantization for any model  </li> <li>Supports on the fly usage of LoRa models  </li> <li>Supports balanced offload  </li> </ul> <p>Benchmarks are available in the Quantization Wiki.</p>"},{"location":"SDNQ-Quantization/#recommended-options","title":"Recommended Options","text":"<ul> <li><code>Dequantize using torch.compile</code>   Highly recommended for much better performance if Triton is available  </li> <li><code>Use Quantized MatMul</code>   Recommended for much better performance if Triton is available on supported GPUs   Supported GPUs for quantized matmul are listed in the Use Quantized MatMul section.  </li> <li>Recommended quantization dtype is <code>INT8</code> for its fast speed and almost no loss in quality   You can use <code>INT6</code> with little quality loss to save more memory and <code>UINT4</code> to save even more memory <code>float8_e4m3fn</code> is another option for fast speed and high quality but <code>FP8</code> has slightly lower quality and performance than <code>INT8</code> </li> </ul>"},{"location":"SDNQ-Quantization/#triton","title":"Triton","text":"<p>Triton enables the use of optimized kernels for much better performance. Triton is not required for SDNQ but it is highly recommended for much better performance. SDNQ will use Triton by default via torch.compile if Triton is available. You can override this with dequantize using torch.compile option.  </p>"},{"location":"SDNQ-Quantization/#triton-with-nvidia","title":"Triton with Nvidia","text":"<ul> <li>Linux </li> <li>Triton comes built-in on Linux, you can use the Triton optimizations out of the box.  </li> <li>Windows </li> <li>Windows requires manual installation of Triton.     Installation steps are available in the Quantization Wiki </li> </ul>"},{"location":"SDNQ-Quantization/#triton-with-amd","title":"Triton with AMD","text":"<ul> <li>Linux </li> <li>Triton comes built-in on Linux, you can use the Triton optimizations out of the box.  </li> <li>Windows </li> <li>Windows requires manual installation of Triton and not guaranteed to work with Zluda.     Experimental installation steps are available in the ZLUDA Wiki </li> </ul>"},{"location":"SDNQ-Quantization/#triton-with-intel","title":"Triton with Intel","text":"<ul> <li>Triton comes built-in with Intel on both Windows and Linux, you can use the Triton optimizations out of the box.   Windows might require additional installation of MSVC if it is not already installed and activated.   Installation steps are available in the PyTorch Inductor Windows wiki </li> </ul>"},{"location":"SDNQ-Quantization/#options","title":"Options","text":""},{"location":"SDNQ-Quantization/#quantization-enabled","title":"Quantization enabled","text":"<p>Used to decide which parts of the model will get quantized. Recommended options are <code>Model</code> and <code>TE</code>. Default is none.  </p> <ul> <li><code>Model</code> is used quantize the Diffusion Models.  </li> <li><code>TE</code> is used to quantize the Text Encoders.  </li> <li><code>LLM</code> is used to quantize the LLMs with Prompt Enhance.  </li> <li><code>Control</code> is used to quantize ControlNets.  </li> <li><code>VAE</code> is used to quantize the VAE. Using the VAE option is not recommended.  </li> </ul> <p>Note</p> <p>VAE Upcast has to be set to false if you use the VAE option with FP16. If you get black images with SDXL models, use the FP16 Fixed VAE.</p>"},{"location":"SDNQ-Quantization/#quantization-mode","title":"Quantization mode","text":"<p>Used to decide when the quantization step will happen on model load. Default is <code>auto</code>.  </p> <ul> <li><code>Auto</code> mode will choose <code>pre</code> or <code>post</code> automatically depending on the model.  </li> <li><code>Pre</code> mode will quantize the model while the model is loading. Reduces system RAM usage.  </li> <li><code>Post</code> mode will quantize the model after the model is loaded into system RAM.  </li> </ul> <p><code>Pre</code> mode is compatible with DiT and Video models like Flux but older UNet models like SDXL are only compatible with <code>post</code> mode.  </p>"},{"location":"SDNQ-Quantization/#quantization-type","title":"Quantization type","text":"<p>Used to decide the data type used to store the model weights. Recommended types are <code>int8</code> for 8 bit, <code>int6</code> for 6 bit, <code>float8_e4m3fn</code> for fp8 and <code>uint4</code> for 4 bit. Default is <code>int8</code>.  </p> <p>INT8 quants have very similar quality to the full 16 bit precision while using 2 times less memory. INT6 quants are the middle ground. Similar quality to to the full 16 bit precision while using 2.7 times less memory. INT4 quants have lower quality and less performance but uses 3.6 times less memory. FP8 quants have similar quality to INT6 but with the same memory usage as INT8.  </p> <p>Unsigned quants have the extra <code>u</code> added to the start of their name while the symmetric quants don't have any prefix. Unsigned (asymmetric) types: <code>uint8</code>, <code>uint7</code>, <code>uint6</code>, <code>uint5</code>, <code>uint4</code>, <code>uint3</code>, <code>uint2</code> and <code>uint1</code> Symmetric types: <code>int8</code>, <code>int7</code>, <code>int6</code>, <code>int5</code>, <code>int4</code>, <code>int3</code>, <code>int2</code>, <code>float8_e4m3fn</code>, <code>float8_e5m2</code>, <code>float8_e4m3fnuz</code> and <code>float8_e5m2fnuz</code> </p> <p>Asymmetric quants uses unsigned integers, meaning they can't store negative values and will use another variable called zero point for this purpose. Symmetric quants can store negative and positive values meaning they don't have extra zero point value and they run faster than unsigned quants because of this.  </p> <p>Quality difference between asymmetric and symmetric quantization is very small for 8 to 6 bits but you should use asymmetric methods below 5 bits.  </p> <ul> <li><code>int8</code> uses int8 and has -128 to 127 range.  </li> <li><code>int7</code> uses eight int7 values packed into seven uint8 values and has -64 to 63 range.  </li> <li><code>int6</code> uses four int6 values packed into three uint8 values and has -32 to 31 range.  </li> <li><code>int5</code> uses eight int5 values packed into five uint8 values and has -16 to 15 range.  </li> <li><code>int4</code> uses two int4 values packed into a single uint8 value and has -8 to 7 range.  </li> <li><code>int3</code> uses eight int3 values packed into a three uint8 values and has -4 to 3 range.  </li> <li><code>int2</code> uses four int2 values packed into a single uint8 value and has -2 to 1 range.  </li> <li><code>uint8</code> uses uint8 and has 0 to 255 range.  </li> <li><code>uint7</code> uses eight uint7 values packed into seven uint8 values and has 0 to 127 range.  </li> <li><code>uint6</code> uses four uint6 values packed into three uint8 values and has 0 to 63 range.  </li> <li><code>uint5</code> uses eight uint5 values packed into five uint8 values and has 0 to 31 range.  </li> <li><code>uint4</code> uses two uint4 values packed into a single uint8 value and has 0 to 15 range.  </li> <li><code>uint3</code> uses eight uint3 values packed into a three uint8 value and has 0 to 7 range.  </li> <li><code>uint2</code> uses four uint2 values packed into a single uint8 value and has 0 to 3 range.  </li> <li><code>uint1</code> uses eight uint1 values packed into a single uint8 value and has 0 to 1 range.  </li> <li><code>float8_e4m3fn</code> uses float8_e4m3fn and has -448 to 448 range.  </li> <li><code>float8_e5m2</code> uses float8_e5m2 and has -57344 to 57344 range.  </li> <li><code>float8_e4m3fnuz</code> uses float8_e4m3fnuz and has -240 to 240 range.  </li> <li><code>float8_e5m2fnuz</code> uses float8_e5m2fnuz and has -57344 to 57344 range.  </li> </ul>"},{"location":"SDNQ-Quantization/#quantization-type-for-text-encoders","title":"Quantization type for Text Encoders","text":"<p>Same as <code>Quantization type</code> but for the Text Encoders. <code>default</code> option will use the same type as <code>Quantization type</code>.  </p>"},{"location":"SDNQ-Quantization/#modules-to-not-convert","title":"Modules to not convert","text":"<p>A comma separated list of module names to skip quantization. Modules listed in this option will not be quantized and will be kept in full precision. An example list: <code>transformer_blocks.0.img_mod.1.weight, transformer_blocks.0.*, img_in</code> Default is empty.  </p>"},{"location":"SDNQ-Quantization/#modules-dtype-dict","title":"Modules dtype dict","text":"<p>A JSON dictionary of quantization types and module names list used to quantize the model with mixed quantization types.  </p> <p>Quantization types can be any valid quantization type supported by SDNQ or it can also be <code>minimum_Xbit</code>. <code>minimum_Xbit</code> will quantize the specified modules into the specifed bit if the main quantization dtype has less precision. For example, <code>minimum_6bit</code> will quantize the specified modules to <code>int6</code> if you are using <code>int5</code> or below but won't do anything if you are using <code>int6</code> or above. Default is empty.  </p> <p>An example dict: <pre><code>{\n\"int8\": [\"transformer_blocks.0.img_mod.1.weight\", \"transformer_blocks.0.*\"],\n\"minimum_6bit\": [\"img_in\"]\n}\n</code></pre></p>"},{"location":"SDNQ-Quantization/#group-size","title":"Group size","text":"<p>Used to decide how many elements of a tensor will share the same quantization group. Higher values have better performance but less quality. Default is <code>0</code>, meaning it will decide the group size based on your quantization type setting. Linear layers will use this formula to find the group size: <code>2 ** (2 + number_of_bits)</code> Convolutions will use this formula to find the group size: <code>2 ** (1 + number_of_bits)</code> Setting the group size to <code>-1</code> will disable grouping.  </p> <p>Using Quantized MatMul with FP8 quant types will disable group sizes. Using Quantized Matmul with <code>int</code> or <code>uint</code> quant types will continue to use Group Sizes by default if the number of bits is less than 6.  </p>"},{"location":"SDNQ-Quantization/#quantize-convolutional-layers","title":"Quantize convolutional layers","text":"<p>Enabling this option will quantize convolutional layers in UNet models too. Has much better memory savings but lower quality. Convolutions will use <code>uint4</code> when using quants with less than 4 bits. Disabled by default.  </p>"},{"location":"SDNQ-Quantization/#dequantize-using-torchcompile","title":"Dequantize using torch.compile","text":"<p>Uses Triton via <code>torch.compile</code> on the dequantization step. Has significantly higher performance. This setting requires a full restart of the webui to apply. Enabled by default if Triton is available.  </p>"},{"location":"SDNQ-Quantization/#use-quantized-matmul","title":"Use Quantized MatMul","text":"<p>Enabling this option will use quantized INT8 or FP8 MatMul instead of BF16 / FP16. Has significantly higher performance on GPUs with INT8 or FP8 support.  Disabled by default.  </p> <p>Supported GPUs - Nvidia   Requires Turing (RTX 2000) or newer GPUs for INT8 matmul.   Requires Ada (RTX 4000) or newer GPUs for FP8 matmul. - AMD   Requires ROCm, not supported with Zluda.   Requires RDNA3 (RX 7000) or newer GPUs for INT8 matmul.   Requires MI300X for FP8 matmul.   - RDNA3 supports INT8 matmul but runs at the same speed as FP16.   - RDNA4 (RX 9000) supports fast INT8 and FP8 matmul but software support isn't ready yet.     FP8 matmul will work with RX 9000 when AMD adds FP8 matmul support to PyTorch. - Intel   Intel supports INT8 with Alchemist (Arc A770) or newer GPUs but software support isn't ready yet.  </p> <p>Quantized INT8 MatMul is compatible with any <code>int</code> or <code>uint</code> quant type. Quantized FP8 MatMul is only compatible with <code>float8_e4m3fn</code> quant type on most GPUs. CPUs and some GPUs can use the other FP8 types too.  </p> <p>Recommended quant type to use with this option is <code>int8</code> for quality and INT8 matmul tends to be faster than FP8. Recommended quant type for FP8 matmul is <code>float8_e4m3fn</code> for quality and better hardware support.  </p>"},{"location":"SDNQ-Quantization/#use-quantized-matmul-with-convolutional-layers","title":"Use Quantized MatMul with convolutional layers","text":"<p>Same as <code>Use Quantized MatMul</code> but for the convolutional layers with UNets like SDXL. Disabled by default.  </p>"},{"location":"SDNQ-Quantization/#quantize-using-gpu","title":"Quantize using GPU","text":"<p>Enabling this option will use the GPU for quantization calculations on model load. Can be faster with weak CPUs but can also be slower because of the GPU to CPU communication overhead. Enabled by default.  </p> <p>When <code>Model load device map</code> in the <code>Models &amp; Loading</code> settings is set to <code>default</code> or <code>cpu</code> this option will send a part of the model weights to the GPU and quantize it, then will send it back to the CPU right away. If device map is set to <code>gpu</code>, model weights will be loaded directly into GPU and the quantized model weights will be kept in the GPU until the quantization of the current model part is over.  </p> <p>If <code>Model offload mode</code> is set to <code>none</code>, quantized model weights will be sent to the GPU after quantization and will stay in the GPU. If <code>Model offload mode</code> is set to <code>model</code>, quantized model weights will be sent to the GPU after quantization and will be sent back to the CPU after the quantization of the current model part is over.  </p>"},{"location":"SDNQ-Quantization/#dequantize-using-full-precision","title":"Dequantize using full precision","text":"<p>Enabling this option will use <code>FP32</code> on the dequantization step. Has higher quality outputs but lower performance. Disabled by default.  </p>"},{"location":"Scripts/","title":"Scripts","text":""},{"location":"Scripts/#quick-links","title":"Quick links","text":"<ul> <li>X/Y/Z Grid</li> <li>Face script</li> <li>Kohya Hires Fix</li> <li>Layer diffuse</li> <li>Mixture tiling</li> <li>MuLan</li> <li>Prompt Matrix</li> <li>Prompt from file</li> <li>Regional prompting</li> <li>ResAdapter</li> <li>T-Gate</li> <li>Text-to-Video</li> <li>DemoFusion</li> </ul>"},{"location":"Scripts/#script-list","title":"Script List","text":""},{"location":"Scripts/#xyz-grid","title":"X/Y/Z Grid","text":"<p>The X/Y/Z Grid script is a way of generating multiple images with automatic changes in the image, and then displaying the result in labelled grids.</p> <p>To activate the X/Y/Z Grid, scroll down to the Script dropdown and select \"X/Y/Z Grid\" within.</p> <p>Several new UI elements will appear.</p> <p>X, Y, and Z types are where you can specify what to change in your image.</p> <p>X type will create columns, Y type will create rows, and Z type will create separate grid images, to emulate a \"3D grid\" The X, Y, Z values are where to specify what to change. For some types, there will be a dropdown box to select values, otherwise these values are comma-separated.</p> <p>Most of these are fairly self explanatory, such as Model, Seed, VAE, Clip skip, and so on.</p>"},{"location":"Scripts/#prompt-sr","title":"Prompt S/R","text":"<p>\"Prompt S/R\" is Prompt Search and Replace. After selecting this type, the first word in your value should be a word already in your prompt, followed by comma-separated words to change from this word to other words.</p> <p>For example, if you're generating an image with the prompt \"a lazy cat\" and you set Prompt S/R to <code>cat,dog,monkey</code>, the script will create 3 images of; <code>a lazy cat</code>, <code>a lazy dog</code>, and <code>a lazy monkey</code>.</p> <p>You're not restricted to a single word, you could have multiple words; <code>lazy cat,boisterous dog,mischeavous monkey</code>, or the entire prompt; <code>a lazy cat,three blind mice,an astronaut on the moon</code>.</p> <p>Embeddings and Loras are also valid Search and Replace terms; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:SecondLora:1&gt;,&lt;lora:ThirdLora:1&gt;</code>.</p> <p>You could also change the strength of a lora; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:FirstLora:0.75&gt;,&lt;lora:FirstLora:0.5&gt;,&lt;lora:FirstLora:0.25&gt;</code>. (Note: You could strip this down to <code>FirstLora:1,FirstLora:0.75,FirstLora:0.5,FirstLora:0.25</code>.)</p>"},{"location":"Scripts/#face-script","title":"Face script","text":"<p>SD.NEXT's face script is used for 4 different face scripts:</p> <ul> <li>FaceID</li> <li>FaceSwap</li> <li>InstantID</li> <li>PhotoMaker</li> </ul>"},{"location":"Scripts/#faceid","title":"FaceID","text":"<p>First select your desired FaceID model and then upload a good picture of the desired face.</p> <p>Strength: How much the script should be applied to the image. Structure: How much similarity there is between the uploaded image and the generated image.  </p>"},{"location":"Scripts/#faceswap","title":"FaceSwap","text":"<p>You only have to upload a good picture of the desired face.</p>"},{"location":"Scripts/#instantid","title":"InstantID","text":"<p>Add an input image with a good picture of the desired face. Strength: How much the script should be applied to the image. Control: How much similarity there is between the uploaded image and the generated image.  </p>"},{"location":"Scripts/#photomaker","title":"PhotoMaker","text":"<p>Add an input image with a good picture of the desired face. Strength: How much the script should be applied to the image. Start: When the script should be activated during the image generation process.  </p>"},{"location":"Scripts/#kohya-hires-fix","title":"Kohya HiRes fix","text":"<p>The Kohya HiRes fix in SD.NEXT is a great way to generate higher resolution images without getting deformities, it's quite easy to use and not that intensive on your system. It's pretty straight forward but it will take some experimenting to see what's the best settings for your image.</p> <p>You select the kohya hires fix in the scripts and then you can change the settings to your needs. (Note: it takes a lot of experimentation to see what works for you).</p> <ul> <li>Scale Factor: The value that determines the scaling factor applied to the input data during the processing. It controls the magnitude of the changes made to the data.</li> <li>Timestep: Timestep represents the time step used in the the processing. It determines the granularity of the processing and how the input data is transformed over time.</li> <li>Block: Block represents the number of blocks used in the processing. It determines the partitioning of the input data into smaller segments for processing.</li> </ul>"},{"location":"Scripts/#layerdiffuse","title":"LayerDiffuse","text":"<p>LayerDiffuse allows you to create transparent images with Diffusers.</p> <p>Simply select LayerDiffuse in the scripts and then click on apply to model after setting everything up. If you want to disable it, simply disable the script. (Note: You have to reload the model and applying it again after making changes like: adding Lora, Controlnet or IP Adapters).</p>"},{"location":"Scripts/#mixture-tiling","title":"Mixture Tiling","text":"<p>Mixture of Diffusers, an algorithm that builds over existing diffusion models to provide a more detailed control over composition. By harmonizing several diffusion processes acting on different regions of a canvas, it allows generating larger images, where the location of each object and style is controlled by a separate diffusion process.</p> <p>To use it you have to select in the scripts, then you have to write your prompts with newlines between them, so for example:  </p> <p>bird plane dog cat </p> <p>X and Y have to be so if you do X times Y the outcome would be the amount of lines/prompts you have. For the example above you would have to do X=2 and Y=2, X times Y = 4 and you have 4 lines of prompts in the example above.</p> <p>The X and Y overlap: if you set overlap regions to 0, you're basically getting a combined grid of images. adjust overlap so images can blend and the resulting image actually makes sense.</p> <p>Note: each region is a separate generate process and then they are combined.</p>"},{"location":"Scripts/#mulan","title":"MuLan","text":"<p>MuLan, a versatile framework to equip any diffusion model with multilingual generation abilities natively by up to 110+ languages around the world.</p> <p>Simply enable MuLan in the scripts and start prompting in your desired language, then click generate.</p>"},{"location":"Scripts/#prompt-matrix","title":"Prompt Matrix","text":"<p>Prompt Matrix is useful for testing and comparing the changes prompts are making to your generated images.</p> <p>First enable prompt matrix in your scripts.</p> <p>Then create your prompt like this: <code>Woman|Red hair|Blue eyes</code> </p> <p>You can make the prompt like this as big as you want. What it will do is it will generate a grid of images, one without the red hair and blue eyes, one with Woman + Red hair, one with Woman + Blue eyes and one with Woman + Red hair + Blue eyes.</p> <ul> <li>Set at prompt start: This will make it so the example of above will be used like this Red hair|Blue eyes|Woman, so in other words it will use the secondary prompts first before adding Woman to it, for example like this: Red hair, Woman.</li> <li>Random seeds: It will use a different seed for every image in the grid.</li> <li>Prompt type: To pick for what prompt you wanna use this script.</li> <li>Joining char: Comma: Woman, Red hair, space: Woman Red hair.</li> <li>Grid margins: The space between each image in the grid.</li> </ul>"},{"location":"Scripts/#prompt-from-file","title":"Prompt from file","text":"<p>Prompt from file allows you to use generation settings from a file including the prompt.</p> <p>First you need to create a .txt file and type something like this: <code>--prompt \"what ever you want\" --negative_prompt \"whatever you don't want\" --steps 30 --cfg_scale 10 --sampler_name \"DPM++ SDE Karras\" --seed -1 --width 512 --height 768</code>  Then upload the file to SD.NEXT at upload prompts. You can also type it in the prompts box for the same result although it won't be saved if you shutdown SD.NEXT.</p>"},{"location":"Scripts/#regional-prompting","title":"Regional prompting","text":"<p>This pipeline is a port of the Regional Prompter extension for Stable Diffusion web UI to diffusers. This code implements a pipeline for the Stable Diffusion model, enabling the division of the canvas into multiple regions, with different prompts applicable to each region. Users can specify regions in two ways: using Cols and Rows modes for grid-like divisions, or the Prompt mode for regions calculated based on prompts.</p>"},{"location":"Scripts/#cols-and-rows","title":"Cols and Rows","text":"<p>In the Cols, Rows mode, you can split the screen vertically and horizontally and assign prompts to each region. The split ratio can be specified by 'div', and you can set the division ratio like '3;3;2' or '0.1;0.5'. Furthermore, as will be described later, you can also subdivide the split Cols, Rows to specify more complex regions.  </p> <p>In this image, the image is divided into three parts, and a separate prompt is applied to each. The prompts are divided by 'BREAK', and each is applied to the respective region.  </p> <p>Example: Mode used: <code>rows</code> Prompt used: <code>green hair twintail BREAK</code> <code>red blouse BREAK</code> <code>blue skirt</code> Grid sections: <code>1,1,1</code> </p> <p>Here is a more advanced example: Mode used: <code>rows</code> Prompt used: <code>blue sky BREAK</code> <code>green hair BREAK</code> <code>book shelf BREAK</code> <code>terrarium on the desk BREAK</code> <code>orange dress and sofa</code> Grid sections: <code>1,2,1,1;2,4,6</code> </p>"},{"location":"Scripts/#prompt-and-prompt-ex","title":"Prompt and Prompt-EX","text":"<p>The difference is that in Prompt, duplicate regions are added, whereas in Prompt-EX, duplicate regions are overwritten sequentially. Since they are processed in order, setting a TARGET with a large regions first makes it easier for the effect of small regions to remain unmuffled.</p> <p>Prompt-EX example:</p> <p>Mode used: <code>Prompt-EX</code> Prompt used: <code>a girl in street with shirt, tie, skirt BREAK</code> <code>red, shirt BREAK</code> <code>green, tie BREAK</code> <code>blue , skirt</code> Prompt thresholds: <code>0.4,0.6,0.6</code> </p>"},{"location":"Scripts/#threshold","title":"Threshold","text":"<p>The threshold used to determine the mask created by the prompt. This can be set as many times as there are masks, as the range varies widely depending on the target prompt. If multiple regions are used, enter them separated by commas. For example, hair tends to be ambiguous and requires a small value, while face tends to be large and requires a small value. These should be ordered by BREAK.</p>"},{"location":"Scripts/#power","title":"Power","text":"<p>Idicates how much regional prompting is applied to the image generation.</p>"},{"location":"Scripts/#resadapter","title":"ResAdapter","text":"<p>ResAdapter, a plug-and-play resolution adapter for enabling any diffusion model generate resolution-free images: no additional training, no additional inference and no style transfer.</p> Models Parameters Resolution Range Ratio Range resadapter_v2_sd1.5 0.9M 128 &lt;= x &lt;= 1024 0.28 &lt;= r &lt;= 3.5 resadapter_v2_sdxl 0.5M 256 &lt;= x &lt;= 1536 0.28 &lt;= r &lt;= 3.5 resadapter_v1_sd1.5 0.9M 128 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2 resadapter_v1_sd1.5_extrapolation 0.9M 512 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2 resadapter_v1_sd1.5_interpolation 0.9M 128 &lt;= x &lt;= 512 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl 0.5M 256 &lt;= x &lt;= 1536 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl_extrapolation 0.5M 1024 &lt;= x &lt;= 1536 0.5 &lt;= r &lt;= 2 resadapter_v1_sdxl_interpolation 0.5M 256 &lt;= x &lt;= 1024 0.5 &lt;= r &lt;= 2"},{"location":"Scripts/#weight","title":"Weight","text":"<p>How much ResAdapter should be applied to the image generation.</p>"},{"location":"Scripts/#t-gate","title":"T-Gate","text":"<p>T-Gate efficiently generates images by caching and reusing attention outputs at scheduled time steps. Experiments show T-Gate\u2019s broad applicability to various existing text-conditional diffusion models which it speeds up by 10-50%.</p> <p>Simply enable T-Gate in the scripts, experiment with the steps a bit to see what works best for your needs.</p>"},{"location":"Scripts/#text-to-video","title":"Text-to-Video","text":"<p>Text-to-Video is a build in script that makes making animated art very easy, it has multiple models available all of them are personal preference and what works best for your configuration.</p> <p>First choose the script under the scripts, then choose the desired amount of frames, then like you would do normally fill in your positive prompt, negative prompts and etc., then choose the desired output format and click generate.</p>"},{"location":"Scripts/#demofusion","title":"DemoFusion","text":"<p>DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as \"previews\", facilitating rapid prompt iteration. You can find more information about DemoFusion here.</p> <ul> <li>Denoising batch size: The batch size for multiple denoising paths. Typically, a larger batch size can result in higher efficiency but comes with increased GPU memory requirements.</li> <li>Stride: The stride of moving local patches. A smaller stride is better for alleviating seam issues, but it also introduces additional computational overhead and inference time.</li> <li>Cosine_scale_1: Control the decreasing rate of skip-residual. A smaller value results in better consistency with low-resolution results, but it may lead to more pronounced upsampling noise. Please refer to Appendix C in the DemoFusion paper.</li> <li>Cosine_scale_2: Control the decreasing rate of dilated sampling. A smaller value can better address the repetition issue, but it may lead to grainy images. For specific impacts, please refer to Appendix C in the DemoFusion paper.</li> <li>Cosine_scale_3: Control the decrease rate of the Gaussian filter. A smaller value results in less grainy images, but it may lead to over-smoothing images. Please refer to Appendix C in the DemoFusion paper.</li> <li>Sigma: The standard value of the Gaussian filter. A larger sigma promotes the global guidance of dilated sampling, but it has the potential of over-smoothing.</li> <li>Multi_decoder: Determine whether to use a tiled decoder. Generally, a tiled decoder becomes necessary when the resolution exceeds 3072*3072 on an RTX 3090 GPU.</li> </ul>"},{"location":"Stable-Cascade/","title":"Stable-Casdade","text":"<p>Original repo: https://github.com/Stability-AI/StableCascade</p>"},{"location":"Stable-Cascade/#use","title":"Use","text":"<ol> <li>Set your compute precision in Settings -&gt; Compute -&gt; Precision    to either BF16 (if supported) or FP32 (if not supported) Note: FP16 is not supported for this model  </li> <li>Enable model offloading in Settings -&gt; Diffusers -&gt; Model CPU offload    without this, stable cascade will use &gt;16GB of VRAM  </li> <li>Recommended: Set sampler to Default </li> <li>Select model from Networks -&gt; Models -&gt; Reference    you can select either Full or Lite variation of the model    and it will automatically be downloaded on first use and loaded into SD.Next    attempting to load a manually downloaded safetensors files is not supported as model requires special handling    SD.Next automatically chooses BF16 variation when downloading from networs -&gt; reference    since its smaller and can be used with either BF16 or FP32 compute precision</li> </ol>"},{"location":"Stable-Cascade/#unet-models","title":"UNet models:","text":"<ol> <li>Put the UNet safetensors in <code>models/UNet</code> folder and put the text encoder (if you use one) in there too. Text encoder name should be the UNet Name + _text_encoder  </li> <li>Load the Stable Cascade base (or a custom decoder) from Huggingface as the main model first, then load the UNet (prior) model as the UNet model from settings.  </li> </ol> <p>Example UNet name: <code>sc_unet.safetensors</code> Example Text Encoder name: <code>sc_unet_text_encoder.safetensors</code> </p>"},{"location":"Stable-Cascade/#params","title":"Params","text":"<ul> <li>Prompt &amp; Negative prompt: as usual</li> <li>Width &amp; Height: as usual</li> <li>CFG scale: used to condition the prior model, reference value is ~4</li> <li>Secondary CFG scale: used to condition decoder model, reference value is ~1</li> <li>Steps: used to control number of steps of the prior model</li> <li>Refiner steps: used to control number of steps of the decoder model</li> <li>Sampler: recommended to set to Default before loading a model   Stable Cascade has its own sampler and results with standard samplers will look suboptimal   Built-in sampler is DDIM/DDPM based, so if you want to experiment at least use similar sampler  </li> </ul>"},{"location":"Stable-Cascade/#notes","title":"Notes","text":"<ul> <li>If model download fails, simply retry it, it will continue from where it left off</li> <li>Model consists out of 3 stages split into 2 pipelines which are exected as C -&gt; B -&gt; A:</li> <li>Full variation requires ~10GB VRAM and runs at ~3 it/s on RTX4090 at 1024px</li> <li>Lite variation requires ~4GB VRAM and runs at ~6 it/s on RTX4090 at 1024px</li> </ul> <p>Note: performance numbers are for combined pipeline, both decoder and prior</p>"},{"location":"Stable-Cascade/#variations","title":"Variations","text":""},{"location":"Stable-Cascade/#overview","title":"Overview","text":"<p>Stable cascade is a 3-stage model split into two pipelines (so-called prior and decoder) and comes into two main variations: Full and Lite You can select which one to use from Networks -&gt; Models -&gt; Reference  </p> <p>Additionally, each variation comes in 3 different precisions: FP32, BF16, and FP16 Note: FP16 is an unofficial version by @KohakuBlueleaf of the model fixed to work with FP16 and may result in slightly different output  </p> <p>Which precision is going to get loaded depends on:  </p> <ul> <li>your user preference in Settings -&gt; Compute -&gt; Precision  </li> <li>and GPU compatibility as not all GPUs support all precision types  </li> </ul>"},{"location":"Stable-Cascade/#sizes","title":"Sizes","text":"<p>Stage A and auxiliary models sizes are fixed and noted above Stage B and Stage C models are dependent on the variation and precision used  </p> Variation Precision Stage B Stage C Full FP32 6.2GB 14GB Full BF16 3.1GB 7GB Full FP16 N/A 7GB Lite FP32 2.8GB 4GB Lite BF16 1.4GB 2GB Lite FP16 N/A N/A"},{"location":"Styles/","title":"Styles","text":"<p>Styles are powerful feature in SD.Next that allow to apply various modifications to your generation parameters:</p> <ul> <li>Prompt: both postive and negative  </li> <li>Parameters: all generation parameters  </li> <li>Wildcards  </li> </ul> <p></p> <p>Note</p> <p>Styles wildcards are separate feature that standard wildcards which can also be used in parallel</p> <p>Styles can be selected via Networks interface -&gt; Styles or via shortcut combo-box control below generate buttons  </p> <p>There can be any number of styles selected and each style will be applied in order they are selected.</p> <p>Each style is a separate JSON file that can be edited manually or via UI. Location of styles is specified in Settings -&gt; System Paths -&gt; Styles folder, default is <code>models\\styles</code> </p> <p>Tip</p> <p>Button \u21b6 \"Apply selected style to prompt\" will apply currently selected styles to current prompt as-is and remove style from being applied during runtime</p> <p>Tip</p> <p>Button \u21b7 \"Save current prompt to style\" will simply save current prompt to named style. Such style can be later edited for more fine-tuning</p>"},{"location":"Styles/#migration","title":"Migration","text":"<p>Old A1111 style concept was a flat file in CSV format and SD.Next supports migration of such styles to new JSON format Simply put a full path to the CSV file in Settings -&gt; System Paths -&gt; Styles and restart server at which point server will migrate found styles into individual JSON files.</p>"},{"location":"Styles/#prompt","title":"Prompt","text":"<p>Prompt specified in style will be either used to replace a placeholder <code>{prompt}</code> in the current prompt or if there is no placeholder, it will be appeneded at the end of the current prompt  </p> <p>Example:</p> <p>\"national geographic style photo shot on sony a7 camera\"  </p>"},{"location":"Styles/#parameters","title":"Parameters","text":"<p>In addition to prompt, you can also specify any generation parameters in the style List of parameters is comma-separated and each parameter is a key-value pair indicated by colon <code>:</code> </p> <p>Note</p> <p>Recognized parameters are all parameters that can be typically found in image metadata, but they can also include and of the generation parameters and any of the server settings!</p> <p>Example:</p> <p>Model: PrometheusV1, Size: 1152x864, Sampler: DPM++ 2M SDE, CFG scale: 5, Steps: 30, Seed: 3849770518, Refine: True, Hires: True, Refiner steps: 20, hr_upscaler: ESRGAN 4x NMKD Siax, hr_scale: 1.5, hr_denoising_strength: 0.3, save_to_dirs: True</p>"},{"location":"Styles/#wildcards","title":"Wildcards","text":"<p>Both prompt and parameters can be modified using wildcards section inside the style List of wildcards is comma-separated and each wildcard is a key-value pair indicated by <code>=</code> Multiple wildcards can be listed and separated by semi-colon <code>;</code> </p> <p>Example#1:</p> <p>Prompt: \"a woman wearing a {color} dress\" Wildcard: \"{color}=red, green, blue\"  </p> <p>Example#2:</p> <p>Prompt: \"{style} a woman wearing a {color} dress\" Wildcard: \"{style}=photo, sketch, painting; {color}=red, green, blue\"  </p> <p>Example#3:</p> <p>Parameters: \"Size: {size}\" Wildcard: \"{size}=1024x1024, 1024x768, 768x1024, 1280x720, 720x1280, 1536x640\"  </p>"},{"location":"Styles/#json","title":"JSON","text":"<p>Structure of the style is a simple JSON object:</p> <pre><code>{\n  \"name\": \"Cute Robot\",\n  \"description\": \"This is a style of a random cute robot\",\n  \"prompt\": \"photo of a cute {color} robot, walking {where} with {background} visible in background\",\n  \"negative\": \"\",\n  \"extra\": \"Size: {size}\",\n  \"wildcards\": \"\n    {color}=blue, red, rusty, silver, cyan;\n    {where}=on alien planet, in rainforest, in the city street;\n    {background}=rocks and mountains, moon and planets, spaceship, battle;\n    {size}=1024x1024, 1280x720, 720x1280\"\n}\n</code></pre>"},{"location":"Styles/#validation","title":"Validation","text":"<p>Styles use will be logged in the standard log with debug level:</p> <p>DEBUG    Applying style: name=\"mine/Cute Robot\" extra=[] skipped=[] reference=False DEBUG    Wildcards applied: {'{color}': 'red', '{what}': 'water', '{background}': 'moon and planets'} path=\"/mnt/models/wildcards\" type=style time=0.00  </p>"},{"location":"Theme-User/","title":"User Themes","text":"<p>Creating custom themes can be done with minimal knowledge of <code>CSS</code> as each theme is a single CSS file</p> <p>Tip</p> <p>While you're modifying a theme, its changes will be visible immediately on page refresh, no need for server restart However, make sure that you clear browser cache between edits Easiest approach is to open browser inspector window (<code>F12</code> in Chrome) and select disable cache in network tab You can also experiment with live edits in browser inspector and only copy them to theme once you're satisfied with changes</p>"},{"location":"Theme-User/#standard-ui","title":"Standard UI","text":"<ul> <li>Theme is a CSS file in <code>/javascript</code> folder</li> <li>Copy existing theme file, for example <code>black-teal.css</code> or <code>light-teal.css</code> into new file, for example <code>my-theme.css</code></li> <li>Edit <code>my-theme.css</code> to change colors, fonts, sizes, borders, paddings, margings, etc.</li> <li>Theme will be selectable in UI after server restart</li> </ul>"},{"location":"Theme-User/#modern-ui","title":"Modern UI","text":"<ul> <li>Theme is a CSS file in <code>/extensions-builtin/sdnext-modernui/themes</code> folder</li> <li>Copy existing theme file, for example <code>default.css</code> into new file, for example <code>my-theme.css</code></li> <li>Edit <code>my-theme.css</code> to change colors, fonts, sizes, borders, paddings, margings, etc.</li> <li>Theme will be selectable in UI after server restart</li> </ul>"},{"location":"Theme-User/#contributing","title":"Contributing","text":"<p>Once you're happy with your theme, you can share it with the community by submitting a pull request that includes your CSS file!</p> <ul> <li>For Standard UI, create PR in SDNext repo</li> <li>For Modern UI, create PR in ModernUI repo</li> </ul> <p>But...If you're not comfortable with that, you can always share your theme in Discussions</p>"},{"location":"Themes/","title":"Themes","text":"<p>SD.Next supports two native theme engines plus option to disable it completely to use external themes:</p>"},{"location":"Themes/#set-themes","title":"Set Themes","text":"<p>Set theme via UI: settings -&gt; user interface</p> <ul> <li>theme type: modern, standard, none note: none disables native theme engine and is used   for gradio built-in themes, huggingface 3rd party themes and custom extension based themes  </li> <li>theme name  </li> <li>theme mode   to force light/dark or leave it as os-default (auto)  </li> </ul> <p>Set theme via CLI: <code>--theme theme-type/theme-name</code> </p> <ul> <li>theme <code>default</code> defaults to modern/default</li> <li>optional theme types: standard, modern, gradio, huggingface   if theme type is not specified, it will default to standard </li> <li>if theme name is not specified, it will default to:  </li> <li>default for modern</li> <li>black-teal for standard</li> <li>gradio/default for gradio</li> <li>huggingface/none for huggingface</li> <li>theme param can additionally be used to enable to specific theme extension:</li> <li>lobe</li> <li>cozy-next</li> </ul> <p>Selected theme type and name will be shown in the log on startup example:</p> <pre><code>11:41:37-649897 DEBUG    UI themes available: type=Standard themes=12\n11:41:37-650510 INFO     UI theme: type=Standard name=\"black-teal\" style=Auto\n11:41:37-651747 DEBUG    UI theme: css=\"/home/vlado/dev/sdnext/javascript/black-teal.css\" base=\"sdnext.css\" user=\"None\"\n</code></pre> <p>or:</p> <pre><code>11:42:42-946642 DEBUG    UI themes available: type=Modern themes=22\n11:42:42-947313 INFO     UI theme: type=Modern name=\"sdxl_alpha\" style=Auto\n11:42:42-948546 DEBUG    UI theme: css=\"extensions-builtin/sdnext-modernui/themes/sdxl_alpha.css\" base=\"base.css\" user=\"None\"\n</code></pre>"},{"location":"Themes/#switching-themes","title":"Switching themes","text":"<p>Once you set theme type, themes of type standard and modern can be switched on the fly without restarts  </p>"},{"location":"Themes/#creating-custom-themes","title":"Creating Custom Themes","text":"<p>See User Themes for details on creating custom themes  </p>"},{"location":"Themes/#available-themes","title":"Available Themes","text":""},{"location":"Themes/#standard-themes","title":"Standard Themes","text":"<p>SD.Next comes with number of built-in themes:</p> <ul> <li>Black teal, Light teal, Simple dark, Simple light, Black orange</li> </ul> <p>Following community created themes are included in SD.Next:</p> <ul> <li>Invoked, Amethisyt nightfall, Emerald paradise, Midnight barbie, Orchid dreams, Timeless beige</li> </ul> <p>Example screenshots can be found at: Standard Themes</p>"},{"location":"Themes/#modern-themes","title":"Modern Themes","text":"<p>Important</p> <p>Any issues related to modern ui should be reported at: https://github.com/BinaryQuantumSoul/sdnext-modernui/issues</p>"},{"location":"Themes/#gradio-themes","title":"Gradio Themes","text":"<p>List of built-in Gradio themes:</p> <ul> <li>Gradio default, Gradio base, Gradio soft, Gradio glass, Gradio monochrome</li> </ul> <p>Use of Gradio themes disables built-in theme engine and uses Gradio theme engine instead Gradio themes are not optimized for SDNext and will likely cause some UI components to look out of place  </p>"},{"location":"Themes/#huggingface-themes","title":"Huggingface Themes","text":"<p>When you refresh list of themes using System -&gt; Settings -&gt; User Interface -&gt; Themes -&gt; Refresh SD.Next will download list of 3rd party Gradio themes hosted on Huggingface  </p> <p>Note that formatting of UI components in that case depends on theme itself and is outside of SD.Next control  </p>"},{"location":"Themes/#extensions","title":"Extensions","text":"<p>SDNext also supports custom themes via extensions Currently listed are cozy-next and lobe themes, however those themes are not updated for recent SDNext releases - please contact extension authors for updates  </p>"},{"location":"Troubleshooting/","title":"Troubleshooting Common Issues","text":"<p>If you're having issues with SD.Next, please follow these steps designed to help weed out known issues first. All users should do Steps #1 and #2 regardless of having a problem or not.</p>"},{"location":"Troubleshooting/#1-cli-arguments","title":"1. CLI Arguments","text":"<p>You should familiarize yourself with all available CLI arguments by typing <code>webui.bat (or .sh) --help</code>, which will present a full list of them to you. There are likely options you have at your disposal that you are unaware of.</p>"},{"location":"Troubleshooting/#2-ui-config","title":"2. UI Config","text":"<p>If your <code>ui-config.json</code> file is larger than a few (1-20) kb, delete it. The way the UI config file works has changed, now it only saves the differences between SD.Next's defaults and what you have set rather than the older bloated file that contained everything. Issues arise from new settings and defaults being overridden by the existing old settings that are no longer valid, this can even lead to non-functional buttons.</p>"},{"location":"Troubleshooting/#3-config","title":"3. Config","text":"<p>Often many issues are cleared up by simply deleting the config.json file and letting SD.Next generate a new one. However this is destructive and annoying because you have to set all of your personal preferences again, including model/image paths. Instead we recommend simply renaming <code>config.json</code> to <code>config-backup.json</code>. This way the system will generate a new file when you restart SD.Next, while also preserving your paths and settings. You can always use <code>--config config-backup.json</code> to start SD.Next back up with your previous settings, or undo the rename entirely if it did not help.</p>"},{"location":"Troubleshooting/#4-use-debug-mode","title":"4. Use Debug Mode","text":"<p>If you're encountering errors of any kind, unexpected process terminations, or other issues, start up SD.Next with the <code>--debug</code> argument. This will allow you to see with greater detail what's going on, often exposing obvious fixes or indicating what the source of the errors are. In general we advise with running <code>--debug</code> all the time, but some users find it annoying to see it updating so often.</p>"},{"location":"Troubleshooting/#5-safe-mode","title":"5. Safe Mode","text":"<p>Unless the issues you are having are directly involving an extension, it can be helpful to take all non-essential extensions out of the equation (disabling them) for troubleshooting purposes. Therefore we advise starting up with the <code>--safe</code> argument to see if any non-essential extensions are causing the issue at hand.</p>"},{"location":"Update/","title":"Updating SD.Next","text":"<p>Recommended procedure to update SD.Next to latest release is to simply add command line arg: <code>--update</code> or <code>--upgrade</code> (they are aliases)  </p> <p>./webui.sh --update # linux/macos webui.bat --update # windows webui.ps1 --update # windows with powershell  </p> <p>Update under-the-hood uses standard git operations with some extras:  </p> <ul> <li>verifies status of core repo and performs auto-repair as needed  </li> <li>updates core SD.next repo  </li> <li>updates all built-in submodules  </li> <li>synchronizes levels of all submodules  </li> </ul>"},{"location":"Update/#manual-update","title":"Manual Update","text":"<p>Instead of using automatic update using <code>--update</code> flag, you can update SD.Next manually using GIT:</p> <p>git pull  </p> <p>Note</p> <p>In case you have any git conflicts, you must resolve then before running git pull or otherwise it will fail</p> <p>Note that in this case, you're updating only core repo Additionally, check status of submodules:  </p> <p>git submodule status git submodule update  </p>"},{"location":"Update/#switching-branch","title":"Switching Branch","text":"<p>SD.Next development is done in multiple branches on GitHub:  </p> <ul> <li>Stable branch: master   Typically updated every several weeks after latest features have been tested  </li> <li>Development branch: dev   Updated regularly (e.g. daily) with latest features, fixes and changes   This branch always contains latest patches, but it is not fully tested   Once we decide on release schedule and consider it as sufficiently stable and tested,   code from <code>dev</code> is merged into <code>master</code> </li> <li>Feature branches:   For larger features we create additional branches as needed   Once feature work has completed, feature branch is merged to <code>dev</code> and removed  </li> </ul> <p>You can switch which branch of the SD.Next you are using at any point in time You do not need a separate installation of SD.Next  </p> <p>git checkout master # to switch to master branch git checkout dev # to switch to dev branch  </p> <p>Tip</p> <p>Before switching branches, it is recommended to perform update</p> <p>Warning</p> <p>If differences between branches are significant, it may trigger installation of new requirements</p>"},{"location":"Using-LCM/","title":"LCM: Latent Consistency Model","text":"<p>LCM (Latent Consistency Model) is a new feature that provides support for SD 1.5 and SD-XL models.</p>"},{"location":"Using-LCM/#installation","title":"Installation","text":"<p>Download the LCM LoRA models and place them in your LoRA folder (models/lora or custom):</p> <ul> <li>For SD 1.5: lcm-lora-sdv1-5</li> <li>For SD-XL: lcm-lora-sdxl</li> </ul> <p>As they have the same name, we recommend doing them one at a time and then renaming it before downloading the next.  </p>"},{"location":"Using-LCM/#usage","title":"Usage","text":"<ol> <li>Load your preferred SD 1.5 or SD-XL model that you want to use LCM with</li> <li>Load the correct LCM lora (lcm-lora-sdv1-5 or lcm-lora-sdxl) into your prompt, ex: <code>&lt;lora:lcm-lora-sdv1-5:1&gt;</code></li> <li>Set your sampler to LCM </li> <li>Set number of steps to a low number, e.g. 4-6 steps for SD 1.5, 2-8 steps for SD-XL</li> <li>Set your CFG Scale to 1 or 2 (or somewhere between, play with it for best quality)</li> <li>Optionally, turning on Hypertile and/or FreeU will greatly increase speed and quality of output images</li> <li>Generate!</li> </ol>"},{"location":"Using-LCM/#notes","title":"Notes","text":"<ul> <li>This also works with latent upscaling, as a second pass/hires fix.</li> <li>LCM scheduler does not support steps higher than 50</li> <li>The <code>cli/lcm-convert.py</code> script can convert any SD 1.5 or SD-XL model to an LCM model by baking in the LoRA and uploading to Huggingface</li> </ul>"},{"location":"VAE/","title":"Variational Autoencoder (VAE)","text":"<p>VAE is a model that can be used to compress and decompress images. It is a type of autoencoder that learns a latent space representation of the input data. The model is trained to minimize the reconstruction error of the input data, while also learning a latent space that is continuous and smooth. This allows the model to generate new images by sampling from the latent space.</p> <p>Why? </p> <p>All popular image generation models work in compressed in compressed latent space to allow for faster inference and reduced memory footprint. Which means VAE is required to convert images to and from latent space.  </p> <p>When? </p> <ul> <li>VAE is used to decode latent to image as a final step of image generation and</li> <li>VAE is used to encode image to latent as a first step of image processing in case of inpaint or image-to-image workflows  </li> </ul> <p>Tip</p> <p>There is no image generation without VAE If you're not specifying a custom VAE model, it simply means you're using a default one specified in the model</p>"},{"location":"VAE/#vae-processing","title":"VAE Processing","text":"<p>SD.Next allows to use 3 types of VAE processing:</p> <ul> <li>Full: use full VAE model   See section on Choosing a VAE for more information  </li> <li>Tiny: use tiny VAE model   Tiny VAE model is a smaller version of the full VAE model and has a reduced memory footprint   It is useful for generating images quickly, but may not be as accurate as the full VAE model  </li> <li>Remote: use remote VAE model   Remote VAE model is a VAE model that is stored on a remote server and accessed over the network   It is useful for generating images on a machine with limited memory or processing power   See section on Remote VAE for more information</li> </ul>"},{"location":"VAE/#choosing-a-vae","title":"Choosing a VAE","text":"<p>When choosing a custom VAE, you must select a model that is compatible with the image generation model you are using. For example, SD-XL model can only use VAE models that are compatible with the SD-XL model.  </p> <p>Location for VAE models is specified in *Settings -&gt; System Paths -&gt; VAE, default is <code>models/VAE</code></p> <p>VAE can be set to:</p> <ul> <li>Default   Use the default VAE model specified in the image generation model</li> <li>Automatic   If there is a VAE model with the same name as image generation model, it will be used   Otherwise, it will use default VAE</li> <li>Custom   Load a custom VAE model from the list of available models  </li> </ul>"},{"location":"VAE/#remote-vae","title":"Remote VAE","text":"<p>Remote VAE is a free feature hosted by Huggingface. For more information, see https://huggingface.co/docs/diffusers/main/en/hybrid_inference/overview</p> <p>Notes:</p> <ul> <li>When using remote VAE, you must have an active internet connection  </li> <li>Remote VAE is only available for some models: for example, SD 1.5, SD-XL, FLUX.1 and HiDream </li> <li>Remote VAE is limited to 2048x2048 resolution  </li> <li>In case of remote VAE failure, SD.Next will auto-switch to local VAE  </li> </ul> <p>Note</p> <p>Privacy: when using remote VAE, your latents/images are sent to Huggingface servers for processing. However, they are NOT stored on the server. No other information is sent or stored to remote servers.</p>"},{"location":"VAE/#common-issues","title":"Common Issues","text":"<p>VAE decode is a critical step in image generation and as such, it can be a source of many issues. It is also the single most processing and memory intensive step in image generation.  </p> <ul> <li>Image generation results in a black image   This is usually caused due to numerical instability in the VAE model itself combined with choosen GPU settings. For example, many current VAE models are not stable at standard float16 precision and require either full float32 precision (which doubles the memory requirements) or bfloat16 which slightly reduces the precision, but allows the model to work without overflows.  </li> <li>If your model is generating black images, try switching to bfloat16 precision in the GPU settings.</li> <li> <p>If that doesn't help, try using a custom VAE model that is more stable at lower precision.     Such VAE may be commonly named as fp16-fixed</p> </li> <li> <p>Image generation hangs at 100%   Its not hanging, its just that means that all generate steps are done and now VAE is processing the latent space to generate the final image.   However, if you're running out of VRAM and system decided to swap it to RAM, it may easily take a 10x longer time to finish. It is recommended to disallow system RAM-&gt;VRAM swapping.  </p> </li> <li> <p>Image generation fails with out-of-memory error   VAE decode is the single most memory intensive step in image generation and can result in large memory usage. If you cannot process images due to memory constraints, try enabling VAE Tiling or reducing the resolution of the images. Note that running refine or hires steps will require more memory than the initial generate step - simply because they typically run at higher resolution.  </p> </li> </ul>"},{"location":"Video/","title":"Video","text":"<p>SD.Next supports video creation using top-level Video tab Supoport includes T2V: text-to-video and I2V: image-to-video </p> <p>Tip</p> <p>Latest video models use LLMs for prompting and due to that requires very long and descriptive prompt</p>"},{"location":"Video/#supported-models","title":"Supported models","text":"<p>SD.Next supports following models out-of-the-box:</p> <ul> <li>Hunyuan: HunyuanVideo, FastHunyuan, SkyReels | T2V, I2V </li> <li>WAN21: 1.3B, 14B | T2V, I2V, FLF2V </li> <li>LTXVideo: 0.9.0, 0.9.1, 0.9.5, 0.9.6 | T2V, I2V </li> <li>CogVideoX: 2B, 5B | T2V, I2V </li> <li>Allegro: T2V </li> <li>Mochi1: T2V </li> <li>Latte1: T2V </li> <li>FramePack: I2V, FLF2V</li> </ul> <p>Note</p> <p>All models are auto-downloaded upon first use Download location uses folder specificed by: Settings -&gt; System paths -&gt; Huggingface</p> <p>Note</p> <p>Optimized support for FramePack based on HunyuanVideo-I2V is implemented in a separate tab FramePack allows generation high-quality videos with pretty much unlimited duration and with limited VRAM!</p> <p>Note</p> <p>Support for LTXVideo is implemented in a separate tab LTXVideo allows flexible video guidance using text, image and video prompts with optional upsampling and refining!</p>"},{"location":"Video/#reference-list","title":"Reference list","text":"Engine Model Type Size Optimal Resolution Default Sampler Reference Values Special Notes License Hunyuan HunyuanVideo T2V 40.9GB 1280x720 Euler FlowMatch Frames:129 CFG:6.0 Steps:50 Proprietary Hunyuan HunyuanVideo I2V 59.2GB 1280x720 Euler FlowMatch Frames:129 CFG:1.0 Steps:50 Proprietary HunyuanVideo FramePack T2V/I2V/FLF2V 25.0GB+15GB 608x640 UniPC FlowMatch Frames:73 Steps:25 Hunyuan FastHunyuan T2V 25.0GB+15GB 1280x720 Euler FlowMatch Frames:125 CFG:6.0 True:1.0 Shift:17 Steps:6 Hunyuan SkyReels T2V 25.0GB+15GB 960x544 Euler FlowMatch Frames:97 CFG:1.0 True:6.0 Steps:50 Hunyuan SkyReels I2V 25.0GB+15GB 960x544 Euler FlowMatch Frames:97 CFG:1.0 True:6.0 Steps:50 WAN21 WAN 2.1 1.3B T2V 28.2GB 832x480 UniPC Frames:81 CFG:5.0 Steps:50 Apache 2.0 WAN21 WAN 2.1 14B T2V 78.1GB 1280x720 UniPC Frames:81 CFG:5.0 Steps:50 Apache 2.0 WAN21 WAN 2.1 14B 480p I2V 832x480 UniPC Frames:81 CFG:5.0 Steps:50 Apache 2.0 WAN21 WAN 2.1 14B 720p I2V 1280x720 UniPC Frames:81 CFG:5.0 Steps:50 Apache 2.0 WAN21 WAN 2.1 14B 720p FLF2V 1280x720 UniPC Frames:81 CFG:5.0 Steps:50 Apache 2.0 LTXVideo LTXVideo 0.9.0 T2V 704x480 Euler FlowMatch Frames:161 Steps:50 Proprietary LTXVideo LTXVideo 0.9.0 I2V 704x480 Euler FlowMatch Frames:161 Steps:50 Proprietary LTXVideo LTXVideo 0.9.1 T2V 24.1GB 704x512 Euler FlowMatch Frames:161 CFG:3 Steps:50 Proprietary LTXVideo LTXVideo 0.9.1 I2V 24.1GB 704x512 Euler FlowMatch Frames:161 CFG:3 Steps:50 Proprietary LTXVideo LTXVideo 0.9.5 T2V 24.8GB 768x512 Euler FlowMatch Frames:161 Steps:40 Proprietary LTXVideo LTXVideo 0.9.5 I2V 768x512 Euler FlowMatch Frames:161 Steps:40 Proprietary LTXVideo LTXVideo 0.9.6 2B T2V 768x512 Euler FlowMatch Frames:161 Steps:50 Proprietary LTXVideo LTXVideo 0.9.6 2B Distilled T2V 768x512 Euler FlowMatch Frames:161 Steps:8 Proprietary LTXVideo LTXVideo 0.9.7 13B T2V/I2V/V2V 46.3GB 768x512 Euler FlowMatch Frames:161 Steps:50 Proprietary CogVideoX CogVideoX 1.0 2B T2V 720x480 Cog DDIM Frames:49 CFG:6.0 Steps:50 Apache 2.0 CogVideoX CogVideoX 1.0 5B T2V 720x480 Cog DDIM Frames:49 CFG:6.0 Steps:50 Proprietary CogVideoX CogVideoX 1.0 5B I2V 720x480 Cog DDIM Frames:49 CFG:6.0 Steps:50 Proprietary CogVideoX CogVideoX 1.5 5B T2V 30.3GB 1360x768 Cog DDIM Frames:81 CFG:6.0 Steps:50 Issue: blank output Proprietary CogVideoX CogVideoX 1.5 5B I2V 1360x768 Cog DDIM Frames:81 CFG:6.0 Steps:50 Issue: blank output Proprietary Allegro Allegro T2V 24.7GB 1280x720 Euler a Frames:88 CFG:7.5 Steps=100 Issue: blank output Apache 2.0 Mochi Mochi1 T2V 23.4GB 512x512 Euler FlowMatch Frames:16 CFG:7.5 Steps:50 Apache 2.0 Latte Latte1 T2V 23.4GB 512x512 DDIM Frames:16 CFG:7.5 Steps:50 Apache 2.0 <p>Tip</p> <p>Each model may require specific resolution or parameters to produce quality results This also includes advanced paramters such as Sampler shift which would during normal text-to-image be considered not required to tweak See individual model's original notes for recommendations on parameters</p> <p>Note</p> <p>Its recommended to use Default as sampler for all models unless you need to change specific sampler setting For example, to change Sampler Shift, you need to select appropriate sampler for that model</p>"},{"location":"Video/#legacy-models","title":"Legacy models","text":"<p>Additional video models are available as individually selectable scripts in either text or image interfaces  </p> <ul> <li>Stable Video Diffusion, Base, XY 1.0 and XT 1.1</li> <li>VGen </li> <li>AnimateDiff </li> </ul>"},{"location":"Video/#lora","title":"LoRA","text":"<p>SD.Next includes LoRA support for Hunyuan, LTX, WAN, Mochi, Cog </p> <p>See LoRA for more details  </p>"},{"location":"Video/#optimizations","title":"Optimizations","text":"<p>Warning</p> <p>Any use on GPUs below 16GB and systems below 48GB RAM is experimental</p>"},{"location":"Video/#memory","title":"Memory","text":"<p>Offloading helps by moving data between system RAM and GPU VRAM memory as needed However, there is no way around requirement that entire model must be loaded into RAM before it can be used Look at the total model size in the table and make sure you have enough RAM to load the model  </p>"},{"location":"Video/#offloading","title":"Offloading","text":"<p>Enable offloading so model components can be moved in and out of VRAM as needed Most models support all offloading types:  </p> <ul> <li>Balanced: recommended, but may require extra tuning  </li> <li>Model: simplest  </li> <li>Sequential: highest memory savings, but slowest  </li> </ul> <p>See Offload for more details  </p>"},{"location":"Video/#quantization","title":"Quantization","text":"<p>Enable on-the-fly quantization during load in Settings -&gt; Quantization for additional memory savings  </p> <ul> <li>SDNQ </li> <li>BnB </li> <li>Optimum-Quanto </li> <li>TorchAO </li> </ul> <p>You can enable quantization for both or either Transformers and Text-Encoder separately  </p> <ul> <li>Most T2V and I2V models support on-the-fly quantization of transformers module  </li> <li>Most T2V support quantization of text-encoder while I2V model may not due to inability to quantize image vectors  </li> </ul> <p>See Quantization for more details</p>"},{"location":"Video/#decoding","title":"Decoding","text":"<p>Instead of using full VAE that is packaged with the model itself to decode final frames, SD.Next supports use of Tiny VAE as well as ability to use Remote VAE to decode video</p> <ul> <li>Tiny VAE: support for Hunyuan, WAN, Mochi </li> <li>Remote VAE: support for Hunyuan </li> </ul> <p>See VAE for more details  </p>"},{"location":"Video/#processing","title":"Processing","text":"<p>SD.Next supports two types of optional processing acceleration:  </p> <ul> <li>FasterCache   support for Hunyuan, Mochi, Latte, Allegro, Cog, WanDB, LTX </li> <li>PyramidAttentionBroadcast   support for Hunyuan, Mochi, Latte, Allegro, Cog, WanDB, LTX </li> </ul>"},{"location":"Video/#interpolation","title":"Interpolation","text":"<p>For all video modules, SD.Next supports adding interpolated frames to video for smoother output Interpolation (if enabled) is performed using RIFE Real-Time Intermediate Flow Estimation  </p>"},{"location":"Video/#issueslimitations","title":"Issues/Limitations","text":"<p>See TODO for known issues and limitations  </p>"},{"location":"WSL/","title":"SD.Next with WSL on Windows","text":""},{"location":"WSL/#wsl-windows-subsystem-for-linux","title":"WSL: Windows Subsystem for Linux","text":"<p>WSL is a way to run Linux inside Windows virtual machine thus having full Linux feature set without needing to have separate Linux installation</p> <p>Step-by-step guide to install WSL2 distro on Windows 10/11 and configure it for SD.Next development  </p> <p>Guide is targeted towards nVidia GPUs where WSL support is available out-of-the-box Additional GPU vendors may be supported, but are not covered by this guide</p> <p>Assumption is that WSL requirements from OS side are already installed and GPU has recent drivers installed  </p>"},{"location":"WSL/#wsl-installation","title":"WSL Installation","text":""},{"location":"WSL/#verify-wsl","title":"Verify WSL","text":"<p>Important</p> <p>WSL requires virtualization to be enabled in BIOS Note that this is not compatible with some overclocking tools such as Intel's XTU</p> <p>Make sure that wsl subsystem is installed: From command prompt:  </p> <p>wsl --status wsl --version</p> <pre><code>Default Version: 2\nWSL version: 2.2.1.0\nKernel version: 5.15.150.1-2\nWSLg version: 1.0.60\nMSRDC version: 1.2.5105\nDirect3D version: 1.611.1-81528511\nDXCore version: 10.0.25131.1002-220531-1700.rs-onecore-base2-hyp\nWindows version: 10.0.22635.3430\n</code></pre>"},{"location":"WSL/#install-wsl","title":"Install WSL","text":"<p>Pick Linux distro to use:</p> <p>wsl --list --online</p> <pre><code>NAME                            FRIENDLY NAME\nAlmaLinux-8                     AlmaLinux OS 8\nAlmaLinux-9                     AlmaLinux OS 9\nAlmaLinux-Kitten-10             AlmaLinux OS Kitten 10\nDebian                          Debian GNU/Linux\nFedoraLinux-42                  Fedora Linux 42\nSUSE-Linux-Enterprise-15-SP5    SUSE Linux Enterprise 15 SP5\nSUSE-Linux-Enterprise-15-SP6    SUSE Linux Enterprise 15 SP6\nUbuntu                          Ubuntu\nUbuntu-24.04                    Ubuntu 24.04 LTS\narchlinux                       Arch Linux\nkali-linux                      Kali Linux Rolling\nopenSUSE-Tumbleweed             openSUSE Tumbleweed\nopenSUSE-Leap-15.6              openSUSE Leap 15.6\nUbuntu-18.04                    Ubuntu 18.04 LTS\nUbuntu-20.04                    Ubuntu 20.04 LTS\nUbuntu-22.04                    Ubuntu 22.04 LTS\nOracleLinux_7_9                 Oracle Linux 7.9\nOracleLinux_8_7                 Oracle Linux 8.7\nOracleLinux_9_1                 Oracle Linux 9.1\n</code></pre> <p>Recommended is Ubuntu-22.04 LTS Install it:</p> <p>wsl --install -d Ubuntu-24.04</p> <pre><code>Installing: Ubuntu 24.04 LTS\n</code></pre> <p>When prompted to create user and password, provide them (in this example we'll use <code>myuser</code>) After installation completes you'll automatically be placed in the bash shell of the new distro  </p> <p>Note: WSL installation does not allow to pick distro friendly name or location, those can be changed later  </p>"},{"location":"WSL/#update-wsl","title":"Update WSL","text":"<p>From bash:  </p> <p>sudo apt update sudo apt dist-upgrade  </p> <p>ubuntu 24.04 already comes with python and git, so no need to install them but we do need to install venv tools:</p> <p>sudo apt install python3.11-venv python3-pip python3 --version git --version  </p> <pre><code>Python 3.11.12\ngit version 2.43.0\n</code></pre> <p>Also, required NV libs are already present and linked which makes using nVidia GPU with this distro very easy  </p>"},{"location":"WSL/#memory","title":"Memory","text":"<p>By default WSL allocates 50% of total memory on Windows If you want to give WSL option to use more memory, increase memory limits  </p> <p>For example, edit WSL configuration</p> <p><code>C:\\Users\\&lt;username&gt;\\.wslconfig</code></p> <pre><code>[wsl2]\nswap=0\nnestedVirtualization=false\ndebugConsole=false\nmemory=51539607552\n</code></pre> <p>This will allocate 48GB (48 * 1024^3) to WSL</p> <p>Warning</p> <p>Keep in mind that WSL allocated memory should be total available memory - reserved memory required for Windows, so don't go overboard</p>"},{"location":"WSL/#move-wsl","title":"Move WSL","text":"<p>This step is optional if you want to move WSL2 distro to another location Default installation path is <code>%USERPROFILE%\\AppData\\Local\\Packages\\&lt;PackageName_with_ID&gt;\\LocalState\\ext4.vhdx</code> For example: <code>C:\\Users\\mandiv\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu24.04LTS_79rhkp1fndgsc\\LocalState\\ext4.vhdx</code> </p> <p>In this example we'll move it to <code>D:\\WSL\\</code> and use friendly name <code>MyUbuntu</code> </p> <p>From command prompt: Shutdown WSL</p> <p>wsl --shutdown wsl --list --verbose  </p> <pre><code>Ubuntu-24.04    Stopped         2\n</code></pre> <p>Move file to new location:</p> <p>move ext4.vhdx D:\\WSL\\</p> <p>Unregister old installation, register new one and set it as default:</p> <p>wsl --unregister Ubuntu-24.04 wsl --import-in-place MyUbuntu D:\\WSL\\ext4.vhdx wsl --set-default MyUbuntu  </p>"},{"location":"WSL/#advanced","title":"Advanced","text":"<p>For advanced WSL settings, see original documentation: https://learn.microsoft.com/en-us/windows/wsl/wsl-config </p>"},{"location":"WSL/#sdnext-installation","title":"SD.Next Installation","text":""},{"location":"WSL/#install-sdnext","title":"Install SD.Next","text":"<p>Start from Windows using WSL shortcut or from command prompt:</p> <p>wsl --distribution MyUbuntu --user myuser</p> <p>And then from bash:</p> <p>cd git clone https://github.com/vladmandic/sdnext/ sdnext cd sdnext ./webui.sh --debug  </p> <pre><code>Create and activate python venv\nLaunching launch.py...\nStarting SD.Next\nLogger: file=\"/home/vlado/sdnext/sdnext.log\" level=DEBUG size=64 mode=create\nPython 3.10.12 on Linux\nVersion: app=sd.next updated=2024-04-06 hash=e783b098 branch=master url=https://github.com/vladmandic/sdnext//tree/master\nPlatform: arch=x86_64 cpu=x86_64 system=Linux release=5.15.150.1-microsoft-standard-WSL2 python=3.10.12\n...\nnVidia CUDA toolkit detected: nvidia-smi present\n...\nDevice: device=NVIDIA GeForce RTX 4090 n=1 arch=sm_90 cap=(8, 9) cuda=12.1 cudnn=8902 driver=551.86\n...\nLocal URL: http://127.0.0.1:7860/\n...\nStartup time: 10.98 torch=1.90 gradio=0.40 libraries=0.88 extensions=0.52 face-restore=6.00 ui-en=0.09 ui-control=0.06 ui-extras=0.13 ui-settings=0.13 ui-extensions=0.25 launch=0.21 api=0.05 app-started=0.12\n</code></pre> <p>Note: This will install sdnext into <code>/home/myuser/sdnext</code>, but feel free to modify path as desired  </p> <p>Now just use your browser to navigate to specified url and that's it</p>"},{"location":"WSL/#configure-sdnext","title":"Configure SD.Next","text":"<p>If you want to share entire configuration (config files, extensions, output folders, models, etc) between different SD.Next installations, start SD.Next with <code>--data-dir</code> cmd flag  </p> <p>For example, to access previous Windows data on <code>C:\\SDNext</code>, use <code>./webui.sh --data-dir /mnt/c/SDNext</code></p> <p>or if you want to share just models, use <code>--model-dir</code> cmd flag, for example <code>./webui.sh --model-dir /mnt/c/SDNext/models</code></p>"},{"location":"WSL/#additional-info","title":"Additional Info","text":""},{"location":"WSL/#additional-packages","title":"Additional Packages","text":"<p>If you're using some other distro than recommended one, you may need to install additional packages such as:</p> <ul> <li>upgrade python (if its below 3.9) or downgrade pthon (if its above 3.12)</li> </ul> <p>sudo apt install python3.11 python3.11-venv python3-pip export PYTHON=/usr/bin/python3.11</p> <p>and potentially manually install nvidia libraries</p> <p>sudo apt install nvidia-cudnn libgl1</p>"},{"location":"WSL/#memory-optimizations","title":"Memory Optimizations","text":"<p>See Malloc for details on how to optimize memory usage  </p>"},{"location":"WSL/#dev-vs-master","title":"Dev vs Master","text":"<p>to switch to to use development version of SD.Next:</p> <p>git pull git checkout dev</p> <p>to switch back to master:</p> <p>git checkout master</p>"},{"location":"WSL/#faster-storage-access","title":"Faster Storage Access","text":"<p>WSL access to mounted drives (<code>/mnt/c</code>) is slow Optionally install SMB client (<code>samba</code>) in Ubuntu, export models folder from Windows and mount it in WSL over loopback:  </p> <ol> <li> <p>Select a folder in Windows you'd like to access from WSL and create a share for that folder  </p> </li> <li> <p>Make sure you have the cifs tools installed for mounting the folder:  </p> </li> </ol> <p>sudo apt install cifs-utils</p> <ol> <li>Create credentials file <code>.cred</code> in your home folder (e.g. <code>/home/myuser/.cred</code>) with the following content:</li> </ol> <pre><code>touch .cred\nchmod 600 .cred\necho \"username=yourwindowsusername\" &gt;&gt; .cred\necho \"password=yourwindowspassword\" &gt;&gt; .cred\n</code></pre> <p>How to get internal-loopback IP of your Window host?</p> <p><code>ip route show | grep -i default | awk '{ print $3}'</code></p> <ol> <li>Then mount the folder in WSL:</li> </ol> <pre><code>sudo mount -t cifs -o async,noatime,rw,mfsymlinks,iocharset=utf8,uid=1000,vers=3.1.1,cache=loose,nostrictsync,resilienthandles,cred=/home/myuser/.cred //$HOST_IP/Models /mnt/models\n</code></pre>"},{"location":"Wildcards/","title":"Wildcards in Prompts","text":"<p>Wildcards are placeholders in the prompt text that are replaced with a random value from a list of choices which allows for more variety in the prompts generated.  </p> <ul> <li>SD.Next supports standard file-based wildcards in prompts.</li> <li>Wildcard support is enabled by default, can be disabled in settings -&gt; extra networks if you want to use 3rd party extension instead of SD.Next built-in support.  </li> <li>Wildcards folder is set in settings -&gt; system paths, default is <code>models\\wildcards</code></li> </ul>"},{"location":"Wildcards/#how-does-it-work","title":"How does it work?","text":"<p>TL;DR: string <code>\"__abc__\"</code> in prompt is matched to a file <code>abc.txt</code> inside wildcards folder</p> <p>The prompt syntax for wildcards is:</p> <p>a woman wearing a <code>__color__</code> dress</p> <p>In the wildcards folder, create file <code>color.txt</code> and add multiple choices with one choice per line:</p> <pre><code>red\ngreen\nblue\n</code></pre>"},{"location":"Wildcards/#tips","title":"Tips","text":"<ul> <li>Wildcards can be used in both positive and negative prompts  </li> <li>Prompt can have any number of wildcards <p>a woman wearing a <code>__color__</code> dress and a <code>__shape__</code> hat</p> </li> <li>Wildcards can be nested   Line inside wildcard file can also have a wildcard referrring to another wildcard, etc.  </li> <li>Supports filename-only and path-based wildcards with full subfolder support   If wildcard is refered as <code>__color__</code> then it will look for file <code>color.txt</code> in wildcards folder and any subfoldrer   If wildcard is refered as <code>__nsp/color__</code> then it will look for <code>color.txt</code> only in <code>nsp</code> folder inside wildcards folder  </li> <li>Wildcards files can be in one-choice per line or multiple choices per line separated by <code>|</code> format  </li> </ul>"},{"location":"Wildcards/#validation","title":"Validation","text":"<p>Wildcard matches and replacements will be logged in the standard log with debug level:</p> <p>DEBUG    Wildcards apply: wildcard=\"color\" choice=\"Yellow\" file=\"models/wildcards/my-variations/color.txt\" choices=930</p>"},{"location":"XYZ-Grid/","title":"How to use X/Y/Z Grid","text":""},{"location":"XYZ-Grid/#introduction","title":"Introduction","text":"<p>The X/Y/Z Grid script is a way of generating multiple images with automatic changes in the image, and then displaying the result in labelled grids.</p> <p>To activate the X/Y/Z Grid, scroll down to the Script dropdown and select \"X/Y/Z Grid\" within.</p> <p>Several new UI elements will appear.</p> <p>X, Y, and Z types are where you can specify what to change in your image.</p> <p>X type will create columns, Y type will create rows, and Z type will create separate grid images, to emulate a \"3D grid\" The X, Y, Z values are where to specify what to change. For some types, there will be a dropdown box to select values, otherwise these values are comma-separated.</p> <p>Most of these are fairly self explanatory, such as Model, Seed, VAE, Clip skip, and so on.</p>"},{"location":"XYZ-Grid/#prompt-sr","title":"Prompt S/R","text":"<p>\"Prompt S/R\" is Prompt Search and Replace. After selecting this type, the first word in your value should be a word already in your prompt, followed by comma-separated words to change from this word to other words.</p> <p>For example, if you're generating an image with the prompt \"a lazy cat\" and you set Prompt S/R to <code>cat,dog,monkey</code>, the script will create 3 images of; <code>a lazy cat</code>, <code>a lazy dog</code>, and <code>a lazy monkey</code>.</p> <p>You're not restricted to a single word, you could have multiple words; <code>lazy cat,boisterous dog,mischeavous monkey</code>, or the entire prompt; <code>a lazy cat,three blind mice,an astronaut on the moon</code>.</p> <p>Embeddings and Loras are also valid Search and Replace terms; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:SecondLora:1&gt;,&lt;lora:ThirdLora:1&gt;</code>.</p> <p>You could also change the strength of a lora; <code>&lt;lora:FirstLora:1&gt;,&lt;lora:FirstLora:0.75&gt;,&lt;lora:FirstLora:0.5&gt;,&lt;lora:FirstLora:0.25&gt;</code>. (Note: You could strip this down to <code>FirstLora:1,FirstLora:0.75,FirstLora:0.5,FirstLora:0.25</code>.)</p>"},{"location":"ZLUDA/","title":"ZLUDA","text":"<p>ZLUDA is a CUDA Wrapper that allows to run applications using normally unsupported GPUS such as AMD GPUs in Windows  </p>"},{"location":"ZLUDA/#warning","title":"Warning","text":"<p>ZLUDA support is unofficial and support is limited at this time  </p> <ul> <li>For unofficial instructions on how to manually build ROCm libraries, see ROCm Custom Build section  </li> <li>For unofficial instructions on how to install ROCm for older GPUs such as Polaris and Vega, see ROCm for Polaris and Vega post  </li> </ul>"},{"location":"ZLUDA/#installing-zluda-for-amd-gpus-in-windows","title":"Installing ZLUDA for AMD GPUs in Windows","text":"<p>Note</p> <p>This guide assumes you have Git and Python installed, and are comfortable using the command prompt, navigating Windows Explorer, renaming files and folders, and working with zip files.</p> <p>Important</p> <p>If you have an integrated AMD GPU (iGPU), you may need to disable it, or use the <code>HIP_VISIBLE_DEVICES</code> environment variable.</p>"},{"location":"ZLUDA/#install-visual-c-runtime","title":"Install Visual C++ Runtime","text":"<p>Note</p> <p>Most everyone would have this anyway, since it comes with a lot of games, but there's no harm in trying to install it.</p> <p>Grab the latest version of Visual C++ Runtime from https://aka.ms/vs/17/release/vc_redist.x64.exe (this is a direct download link) and then run it. If you get the options to Repair or Uninstall, then you already have it installed and can click Close. Otherwise, install it.  </p>"},{"location":"ZLUDA/#install-zluda","title":"Install ZLUDA","text":"<p>ZLUDA is now auto-installed, and automatically added to PATH, when starting webui.bat with <code>--use-zluda</code>.</p>"},{"location":"ZLUDA/#install-hip-sdk","title":"Install HIP SDK","text":"<p>Install HIP SDK 6.2 from https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html So long as your regular AMD GPU driver is up to date, you don't need to install the PRO driver HIP SDK suggests.</p>"},{"location":"ZLUDA/#replace-hip-sdk-library-files-for-unsupported-gpu-architectures","title":"Replace HIP SDK library files for unsupported GPU architectures","text":"<p>Go to https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/system-requirements.html and find your GPU model. If your GPU model has a \u2705 in both columns then skip to Install SD.Next. If your GPU model has an \u274c in the HIP SDK column, or if your GPU isn't listed, follow the instructions below;  </p> <ol> <li>Open Windows Explorer and copy and paste <code>C:\\Program Files\\AMD\\ROCm\\6.2\\bin\\rocblas</code> into the location bar. (Assuming you've installed the HIP SDK in the default location and Windows is located on C:)</li> <li>Make a copy of the <code>library</code> folder, for backup purposes.  </li> <li>Download one of the unofficial rocBLAS library, and unzip them in the original library folder, overwriting any files there. gfx1010: RX 5700, RX 5700 XT gfx1012: RX 5500, RX 5500 XT gfx1031: RX 6700, RX 6700 XT, RX 6750 XT gfx1032: RX 6600, RX 6600 XT, RX 6650 XT gfx1103: Radeon 780M gfx803: RX 570, RX 580 More...</li> <li>Open the zip file.</li> <li>Drag and drop the <code>library</code> folder from zip file into <code>%HIP_PATH%bin\\rocblas</code> (The folder you opened in step 1).</li> <li>Reboot PC</li> </ol> <p>If your GPU model not in the HIP SDK column or not available in the above list, follow the instructions in ROCm Support guide to build your own RocblasLibs.  </p> <p>Warning</p> <p>Building your own libraries is not for the faint of heart</p>"},{"location":"ZLUDA/#install-sdnext","title":"Install SD.Next","text":"<p>Using Windows Explorer, navigate to a place you'd like to install SD.Next. This should be a folder which your user account has read/write/execute access to. Installing SD.Next in a directory which requires admin permissions may cause it to not launch properly.  </p> <p>Note: Refrain from installing SD.Next into the Program Files, Users, or Windows folders, this includes the OneDrive folder or on the Desktop, or into a folder that begins with a period; (eg: <code>.sdnext</code>).  </p> <p>The best place would be on an SSD for model loading.  </p> <p>In the Location Bar, type <code>cmd</code>, then hit [Enter]. This will open a Command Prompt window at that location.  </p> <p></p> <p>Copy and paste the following commands into the Command Prompt window, one at a time;  </p> <p><code>git clone https://github.com/vladmandic/sdnext</code> <code>cd sdnext</code> <code>.\\webui.bat --use-zluda --debug --autolaunch</code></p>"},{"location":"ZLUDA/#compilation-and-first-generation","title":"Compilation and First Generation","text":"<p>Now, try to generate something. This should take a fair while to compile (10-15mins, or even longer; some reports state over an hour), but this compilation should only need to be done once. Note: The text <code>Compilation is in progress. Please wait...</code> will repeatedly appear, just be patient. Eventually your image will start generating. Subsequent generations will be significantly quicker.  </p>"},{"location":"ZLUDA/#upgrading-zluda","title":"Upgrading ZLUDA","text":"<p>If you have problem with ZLUDA after updating SD.Next, upgrading ZLUDA may help.</p> <ol> <li>Remove <code>.zluda</code> folder.</li> <li>Launch WebUI. The installer will download and install newer ZLUDA.</li> </ol> <p>\u203b You may have to wait for a while to compile as the first generation.</p>"},{"location":"ZLUDA/#experimental-features","title":"Experimental features","text":""},{"location":"ZLUDA/#cudnn","title":"cuDNN","text":"<p>Speed-up: \u2605\u2605\u2605\u2606\u2606 VRAM: \u2605\u2605\u2605\u2605\u2606 Stability: \u2605\u2605\u2605\u2606\u2606 Compatible with: Navi cards</p> <p>MIOpen, the equivalent of cuDNN for AMDGPUs, hasn't been released on Windows yet.</p> <p>However, you can enable it with a custom build of MIOpen.</p> <p>This section describes how to enable cuDNN.</p> <ol> <li>Install HIP SDK 6.2. If you already have older HIP SDK, uninstall it before installing 6.2.  </li> <li>Download and install HIP SDK extension from here. (unzip and paste folders upon <code>path/to/AMD/ROCm/6.2</code>)  </li> <li>Remove <code>.zluda</code> folder if exists.  </li> <li>Launch WebUI with command line arguments <code>--use-zluda --use-nightly</code>.  </li> </ol> <p>The first generation will take long time because MIOpen has to find the optimal solution and cache it.</p> <p>If you get driver crashes, restart webui and try again.</p>"},{"location":"ZLUDA/#cublaslt","title":"cuBLASLt","text":"<p>Speed-up: \u2605\u2606\u2606\u2606\u2606 VRAM: \u2605\u2606\u2606\u2606\u2606 Stability: \u2605\u2605\u2606\u2606\u2606 Compatible with: gfx1100, or CDNA accelerators</p> <p>hipBLASLt, the equivalent of cuBLASLt for AMDGPUs, hasn't been released on Windows yet.</p> <p>However, there're unofficial builds available.</p> <p>This section describes how to enable cuBLASLt.</p> <ol> <li>Install HIP SDK 6.2. If you already have older HIP SDK, uninstall it before installing 6.2.  </li> <li>Download and install HIP SDK extension from here. (unzip and paste folders upon <code>path/to/AMD/ROCm/6.2</code>)  </li> <li>Remove <code>.zluda</code> folder if exists.  </li> <li>Launch WebUI with command line arguments <code>--use-zluda --use-nightly</code>.  </li> </ol>"},{"location":"ZLUDA/#triton","title":"triton","text":"<p>Speed-up: \u2605\u2605\u2605\u2605\u2605 VRAM: \u2605\u2605\u2605\u2605\u2606 Stability: \u2605\u2605\u2605\u2605\u2606 Compatible with: Navi cards</p> <ol> <li>Prepare Python 3.11 (or 3.12) environment.  </li> <li>Download a triton wheel that matches your Python version from here.    (cp312 is Python 3.12, cp311 is Python 3.11 and cp310 is Python 3.10)  </li> <li>Open a PowerShell Windows in the SDNext folder and install via pip.  </li> </ol> <pre><code>venv\\scripts\\python -m pip install --upgrade setuptools\nvenv\\scripts\\python -m pip install --upgrade path/to/downloaded/triton.whl\n</code></pre> <p>Important</p> <p>Developer PowerShell for Visual Studio (or Prompt) will be needed to compile kernel using triton.</p>"},{"location":"ZLUDA/#flash-attention-2","title":"Flash Attention 2","text":"<p>Using triton, you can enable Flash Attention 2.</p> <ol> <li>Go to Settings.</li> <li>Set attention method to <code>Scaled Dot-product</code>.</li> <li>Enable <code>Triton Flash attention</code>.</li> <li>Restart WebUI.</li> </ol>"},{"location":"ZLUDA/#torchcompile","title":"torch.compile","text":"<p>Using triton, you can enable <code>torch.compile</code>.</p> <ol> <li>Go to Settings.</li> <li>Enable compilation.</li> <li>Set compilation method to <code>inductor</code> or <code>cuda-graph</code>.</li> </ol> <p>\u203b <code>torch.compile</code> is currently not compatible with flash attention 2 on ZLUDA.</p>"},{"location":"ZLUDA/#comparison-directml","title":"Comparison (DirectML)","text":"DirectML ZLUDA Speed Slower Faster VRAM Usage More Less VRAM GC \u274c \u2705 Traning * \u2705 Flash Attention \u274c \u2705 FFT \u2705 \u26a0\ufe0f DNN \u2753 \u2705 RTC \u2753 \u2705 Source Code Closed-source Open-source <p>\u2753: unknown \u26a0\ufe0f: partially supported *: known as possible, but uses too much VRAM to train stable diffusion models/LoRAs/etc.</p>"},{"location":"ZLUDA/#compatibility","title":"Compatibility","text":"DTYPE FP64 \u2705 FP32 \u2705 FP16 \u2705 BF16 \u2705 LONG \u2705 INT8 \u2705 UINT8 \u2705* INT4 \u2753 FP8 \u26a0\ufe0f BF8 \u26a0\ufe0f <p>*: Not tested.</p>"},{"location":"ZLUDA/#building-rocblas-for-unsupported-architectures","title":"Building rocBLAS for unsupported architectures","text":"<p>This is a guide to build rocBLAS based on the ROCm Official Documentations.</p> <p>You may have an AMD GPU without official support on ROCm HIP SDK OR if you are using integrated AMD GPU (iGPU), and want it to be supported by HIP SDK on Windows. You may follow the guide below to build your rocBLAS.</p> <p>If you do not need to build ROCmLibs or already have the library, please skip this.</p> <p>Make sure you have the following software available on your PC. Otherwise, you may fail to build the ROCmLibs: 1. Visual Studio 2022 2. Python 3. Strawberry Perl 4. CMake 5. Git 6. HIP SDK (Mentioned in the first step) 7. Download rocBLAS and Tensile (Download Tensile 4.38.0 for ROCm 5.7.0 (latest) on Windows)</p> <p>Edit line 41 in file rdeps.py for rocBLAS. The old repo has an outdated vckpg, which will lead to failed build. Update the vcpkg by entering the following line in the terminal:</p> <pre><code>git clone -b 2024.02.14 https://github.com/microsoft/vcpkg\n</code></pre> <p>Download <code>Tensile 4.38.0</code> from the release page.</p> <p>Download Tensile-fix-fallback-arch-build.patch, and place in the <code>Tensile</code> folder. In this example, the path is: <code>C:\\ROCm\\Tensile-rocm-5.7.0</code>.</p> <p>Enter the following line in the terminal opened in <code>Tensile-rocm-5.7.0</code>:</p> <pre><code>git apply Tensile-fix-fallback-arch-build.patch\n</code></pre> <p>if your vckpkg version is built later than April, 2023, please replace the <code>CMakeLists.txt</code> in <code>Tensile/tree/develop/Tensile/Source/lib/CMakeLists.txt</code> with this CMakeLists.txt, and put in same folder. (For more information, please access ROCm Official Guide)</p> <p>In <code>C:\\ROCm\\rocBLAS-rocm-5.7.0</code>, run:</p> <pre><code>python rdeps.py\n</code></pre> <p>If you encounter any mistake, try to Google and fix it or try it again. Use <code>install.sh -d</code> in Linux.</p> <p>Once done, run:</p> <pre><code>python rmake.py -a \"gfx906;gfx1012\" --lazy-library-loading --no-merge-architectures -t \"C:\\ROCm\\Tensile-rocm-5.7.0\"\n</code></pre> <p>Change <code>gfx906;gfx1012</code> to your GPU LLVM Target. If you want to build multiple ones at a time, make sure to separate with <code>;</code>.</p> <p>Upon successful compilation, rocblas.dll will be generated. In this example, the file path is <code>C:\\ROCm\\rocBLAS-rocm-5.7.0\\build\\release\\staging\\rocblas.dll</code>. In addition, some Tensile data files will also be produced in <code>C:\\ROCm\\rocBLAS-rocm-5.7.0\\build\\release\\Tensile\\library</code>.</p> <p>To compile HIP SDK programs that use hipBLAS/rocBLAS, you need to replace the rocblas.dll file in the SDK with the one that you have just made yourself. Then, place <code>rocblas.dll</code>into <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin</code> and the Tensile data files into <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\library</code>.</p> <p>Your programs should run smooth as silk on the designated graphics card now.</p>"},{"location":"ZLUDA/#rocm-custom-build","title":"ROCm Custom Build","text":"<p>This guide will walk you through building rocBLAS using the official ROCm documentation.</p> <p>This guide is for users with AMD GPUs lacking official ROCm/HIP SDK support, or those wanting to enable HIP SDK support for hip sdk 5.7 and 6.1.2  on Windows for integrated AMD GPUs(iGPUs).\"</p> <p>If you already have the libraries, you can skip this section! </p> <p>Prerequisites: Ensure the following software is installed on your PC. <code>python</code>, <code>git</code>, and the <code>HIP SDK</code>are essential.  The script <code>rdeps.py</code> will automatically download any missing dependencies when you run it.</p> <ul> <li>Visual Studio 2022: (Download from https://visualstudio.microsoft.com/)</li> <li>Python: (Download from https://www.python.org/)</li> <li>Strawberry Perl:  (Download from https://strawberryperl.com/)</li> <li>CMake: (Download from https://cmake.org/download/)</li> <li>Git: (Download from https://git-scm.com/)</li> <li>HIP SDK: (Download from https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html)</li> </ul>"},{"location":"ZLUDA/#downloading-the-source-code","title":"Downloading the Source Code:","text":"<ol> <li>rocBLAS: Download the latest version (https://github.com/ROCm/rocBLAS).</li> <li> <p>ROCm 5.7.0:  Download <code>rocBLAS 3.1.0</code> rocBLAS 3.1.0 for ROCm 5.7.0</p> <ul> <li>ROCm 6.1.2: Download <code>rocBLAS 4.1.2</code> rocBLAS 4.1.2 for ROCm 6.1.2</li> </ul> </li> <li> <p>Tensile: Download the appropriate version:(https://github.com/ROCm/Tensile)</p> </li> <li> <p>ROCm 5.7.0:  Download <code>Tensile 4.38.0</code> Tensile 4.38.0 for ROCm 5.7.0</p> </li> <li> <p>ROCm 6.1.2: Download <code>Tensile 4.40.0</code> Tensile 4.40.0 for ROCm 6.1.2</p> </li> </ol>"},{"location":"ZLUDA/#patching-tensile-for-rocm-for-advanced-users-not-a-must-do","title":"Patching Tensile for ROCm (For Advanced Users, Not-a-must-Do)","text":"<p>These steps are necessary for specific configurations of ROCm and may not be required in all cases. If you had a optimized logic for you gpu arche,you may skip this steps.Especily build libs for xnack- features.</p>"},{"location":"ZLUDA/#determine-your-rocm-version","title":"Determine Your ROCm Version:","text":"<ul> <li>ROCm 5.7.0: Follow the instructions for \"For hip 5.7\" below.</li> <li>ROCm 6.1.2: Follow the instructions for \"For hip 6.1.2\" below.</li> </ul>"},{"location":"ZLUDA/#patches-for-tensile","title":"Patches for Tensile:","text":""},{"location":"ZLUDA/#for-hip-570","title":"For hip 5.7.0:","text":"<ol> <li> <p>Download Tensile-fix-fallback-arch-build.patch.</p> </li> <li> <p>Place the patch file in your <code>Tensile</code> folder (e.g., <code>C:\\ROCM\\Tensile-rocm-5.7.0</code>).</p> </li> <li> <p>Open a terminal within the <code>Tensile</code> folder.</p> </li> <li> <p>Apply the patch:    <pre><code>git apply Tensile-fix-fallback-arch-build.patch\n</code></pre></p> </li> <li>If nothing appears after applying, it's patched successfully. Otherwise, you may need to manually add the patch content to <code>TensileCreateLibrary.py</code>, you may also skip this steps if you have optimized logic available.</li> </ol>"},{"location":"ZLUDA/#for-hip-612","title":"For hip 6.1.2:","text":"<ol> <li> <p>Download Tensile-fix-fallback-arch-build-hip-6.1.2.patch.</p> </li> <li> <p>Place the patch file in your <code>Tensile</code> folder (e.g., <code>C:\\ROCM\\Tensile-rocm-6.1.2</code>).</p> </li> <li> <p>Open a terminal within the <code>Tensile</code> folder.</p> </li> <li> <p>Apply the patch:    <pre><code>git apply Tensile-fix-fallback-arch-build-hip-6.1.2.patch\n</code></pre></p> </li> <li> <p>If nothing appears after applying, it's patched successfully. Otherwise, you may need to manually add the patch content to <code>TensileCreateLibrary.py</code>.</p> </li> </ol>"},{"location":"ZLUDA/#skip-this-step-for-rocm-612","title":"( Skip this step for ROCm 6.1.2 )","text":"<p>Note: edit the line 41 in file rdeps.py for rocBLAS  ,The old repo has an outdated vckpg, which will lead to fail build.update the vcpkg ,by replace with the following line  <pre><code>git clone -b 2024.02.14 https://github.com/microsoft/vcpkg\n</code></pre> to udpate the vckpg version.</p> <ul> <li>vcpkg Version: If your vcpkg version was built after April 2023, replace <code>CMakeLists.txt</code> in <code>Tensile/tree/develop/Tensile/Source/lib/CMakeLists.txt</code> with this version and place it in the same folder (e.g., <code>rocm</code>).</li> <li>For more information, see the official ROCm guide.</li> </ul>"},{"location":"ZLUDA/#build-with-rdeps-and-rmake","title":"Build with rdeps and rmake:","text":"<ol> <li>Navigate to the <code>rocm/rocBLAS</code> directory in your terminal.</li> <li> <p>Run <code>python rdeps.py</code>. This script will configure your environment and download necessary packages.  <code>python rdeps.py</code> ( using <code>install.sh -d</code> in linux , if you encounter any mistakes , try to google and fix with it or try it again  ) after done . try next step</p> </li> <li> <p>After <code>rdeps.py</code> completes, run  <pre><code>python rmake.py -a \"gfx1101;gfx1103\" --lazy-library-loading--no-merge-architectures -t \"C:\\rocm\\Tensile-rocm-5.7.0\"\n</code></pre> (adjust paths and architectures as needed).</p> </li> </ol> <p>Important:</p> <ul> <li>Replace <code>\"gfx1101;gfx1103\"</code> with the correct GPU or APU architecture names for your system.Make sure sepearte with \";\"if you have more than one arches build .</li> <li>Make sure read the  Editing Tensile/Common.py and blow before to build .</li> <li>For ROCm 6.1.2, change the path to <code>C:\\rocm\\Tensile-rocm-6.1.2</code>.</li> <li>The specific commands and patch files may vary depending on your setup and ROCm version.</li> </ul> <p>After successfully building rocBLAS from source, you need to replace the default <code>rocblas.dll</code> with your compiled version for your HIP programs to utilize it. Here's how:</p> <ol> <li>Locate your Compiled Files:</li> <li><code>rocblas.dll</code>: Located in <code>C:\\ROCM\\rocBLAS-rocm-5.7.0\\build\\release\\staging\\</code> (or a similar path based on your build location).</li> <li> <p>Tensile data files: Found within <code>C:\\ROCM\\rocBLAS-rocm-5.7.0\\build\\release\\Tensile\\library\\</code> (adjust the path if needed).</p> </li> <li> <p>Replace the Default rocBLAS:</p> </li> <li> <p>Copy <code>rocblas.dll</code>  to <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin</code>. This is where the HIP SDK looks for it by default.( make sure to bakc up the origianl rocblas.dll )</p> </li> <li> <p>Place Tensile Data Files:</p> </li> <li> <p>Navigate to <code>C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\</code></p> </li> <li> <p>Replace the <code>library</code> with new build ( back up the origianl library by rename to different name ,eg ,bklibrary).  This is where you should place all the Tensile data files from your build directory.</p> </li> <li> <p>Test Your HIP Program:</p> <ul> <li>Now, when you run your HIP program, it should use your newly compiled <code>rocblas.dll</code> and its associated Tensile data files.</li> </ul> </li> </ol> <p>Important Notes: * For ROCm 6.1.2, change the path to <code>C:\\Program Files\\AMD\\ROCm\\6.1\\bin\\</code>. * Always double-check the paths to ensure they match your installation configuration. * Make sure the ROCm version in the <code>bin</code> directory matches the version of rocBLAS you built.</p>"},{"location":"ZLUDA/#note-editing-tensilecommonpy","title":"Note: Editing Tensile/Common.py","text":"<p>This file contains general parameters used by the Tensile library. To ensure compatibility with your GPU, you need to update two specific settings.Update the value of <code>\" globalParameters[\"SupportedISA\"]\"</code>and <code>\"CACHED_ASM_CAPS\"</code> with your<code>gpu ISA and info</code> .and choose the simliar gpu achetecture. eg <code>RND2 for gfx1031 ,RND2 for gfx1032</code>, then copy and put below with your gpu number and others availble gpu data .For hip sdk 6.1.2 , <code>CACHED_ASM_CAPS</code> info move to tensile/AsmCaps.py, also edit architectureMap from line299 to 310 , add your arch infomation .map your arch information to correct logic file .however , some optimized logic don't exsit in the offoicial release. then we need to creat it.otherwilse ,it will creat a fallback no optimized rocblas and library.</p> <p>Here's a step-by-step guide:</p> <ol> <li>Choose Your Architecture:</li> <li>Select an existing architecture folder within <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic\\asm_full</code> (e.g., <code>navi21</code>). This will serve as a template for your new architecture.</li> <li> <p>Create a new folder with the name of your target architecture (e.g., <code>navi22</code>).</p> </li> <li> <p>Copy Files:</p> <ul> <li>Copy all the files from your chosen template folder into your new architecture folder.</li> </ul> </li> <li> <p>Modify Files:</p> </li> <li>Open the copied files in a code editor (like VS Code or Visual Studio).</li> <li>Search for instances of <code>navi21</code> and replace them with <code>navi22</code>.</li> <li>Update any <code>gfx1030</code> references to <code>gfx1031</code>  (or your target GPU's identifier).</li> <li>Find lines containing <code>ISA: [10, 3, 0]</code> and replace them with <code>ISA: [10, 3, 1]</code>. (Remember to adjust the ISA code according to your GPU)</li> <li>\"Rename all files within the new folder to reflect your architecture name (e.g., change 'navi21' to 'navi22'). You can use a file renaming tool like 'File Rename APP', a free application available in the Windows Store, for this task.\"</li> <li> <p>if build failed ,that's beacuse ROCm architectures have different capabilities. You need to ensure your <code>rocblas</code> is tailored to each architecture you're targeting:</p> <ul> <li> <p>gfx90c: Doesn't support <code>4x8II</code>.  Delete any logic or files related to <code>4x8II</code> within the <code>asm_full</code> folder under <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic</code>.</p> </li> <li> <p>gfx1010: Doesn't support <code>8II</code>. Do the same for files related to <code>8II</code> in the <code>asm_full</code> folder.</p> </li> <li>Checking Logic Files:  The \"new named logic file\" is likely a critical place where these operations are defined. Carefully review it and remove any unsupported calculations.</li> </ul> </li> <li> <p>Use Your New Architecture:</p> </li> <li>In <code>Tensile/Common.py</code>, update <code>\"CACHED_ASM_CAPS\"</code> or the relevant entries in  <code>architectureMap</code> to reference your new <code>navi22</code> folder.</li> </ol> <p>Important Notes:</p> <ul> <li>Carefully review the changes you make, as incorrect modifications can lead to errors.</li> </ul> <p>(Skip this for HIP 5.7, Necessary for HIP 6.1.2)</p> <p>Key Changes:</p> <ul> <li>Search for <code>gfx1030</code>: Begin by searching within both the Tensile and rocBLAS folders for instances of <code>gfx1030</code>. This identifier represents a gfx1030 GPU architecture.</li> <li>Replace with Your Target Architecture: Replace all occurrences of <code>gfx1030</code> with the corresponding code for your desired GPU architecture (e.g., <code>gfx1031</code>).</li> </ul> <p>Important Files to Modify:</p> <ul> <li> <p>Tensile: Within the Tensile folder, make changes to:</p> <ul> <li><code>CMakeLists.txt</code>: This file configures the build process and needs adjustments for new architectures.</li> <li><code>AMDGPU.hpp</code>: Defines the architecture-specific interface.</li> <li><code>PlaceholderLibrary.hpp</code>, <code>Predicaters.hpp</code>, <code>OclUtiles.cpp</code>: These files contain code related to specific functionalities, which might require modifications for your target GPU.</li> </ul> </li> <li> <p>rocBLAS: In the rocBLAS folder:</p> <ul> <li><code>CMakeLists.txt</code>: Similar to Tensile, update this file for your new architecture.</li> <li><code>handle.cpp</code>, <code>tensile_host.cpp</code>, <code>handle.hpp</code>: These files are likely involved in communication and interactions between rocBLAS and the GPU.</li> </ul> </li> </ul> <p>Caution:</p> <ul> <li>Modifying these core files can have unintended consequences.</li> </ul> <p>Advanced Usage:</p> <p>For maximum performance optimization, delve deeper into Tensile's logic files. Examples are provided in <code>rocBLAS\\library\\src\\blas3\\Tensile\\Logic\\asm_full</code>.</p> <p>For truly optimized libraries, you'll need to fine-tune these logic files specifically for your target hardware.The Tensile Tuning Guide provides practical guidance and techniques for start this process. Keep in mind that the tuning process requires patience, time, and a willingness to delve into Tensile's inner workings.</p> <p>More detail can be found in tuning , and tensile tuning .tex , A pdf version available in here</p> <p>Please feel welcome to edit this post and contribute optimized logic links. Remember to carefully consider the impact of any edits or additions.</p>"},{"location":"nVidia/","title":"Nvidia Graphics Card Note","text":"<p>You will likely want to follow Nvidia's instructions to disable System Memory Fallback for Stable Diffusion on the newest drivers, which you should be using.</p>"}]}